use super::super::super::prompts::prompts::{JobPromptGenerator, Prompt};
use crate::{llm_provider::{execution::prompts::subprompts::SubPromptType, job::JobStepResult}, tools::router::ShinkaiTool};
use shinkai_vector_resources::vector_resource::RetrievedNode;

impl JobPromptGenerator {
    /// A basic generic prompt generator
    /// summary_text is the content generated by an LLM on parsing (if exist)
    pub fn generic_inference_prompt(
        custom_system_prompt: Option<String>,
        custom_user_prompt: Option<String>,
        user_message: String,
        ret_nodes: Vec<RetrievedNode>,
        summary_text: Option<String>,
        job_step_history: Option<Vec<JobStepResult>>,
        tools: Vec<ShinkaiTool>,
    ) -> Prompt {
        let mut prompt = Prompt::new();

        // Add system prompt
        let system_prompt = custom_system_prompt.unwrap_or_else(|| "You are a very helpful assistant.".to_string());
        prompt.add_content(system_prompt, SubPromptType::System, 98);

        // Add previous messages
        if let Some(step_history) = job_step_history {
            prompt.add_step_history(step_history, 97);
        }

        // If there is a document summary from the vector search add it with higher priority that the chunks
        if let Some(summary) = summary_text {
            prompt.add_content(
                format!(
                    "Here is the current summary of content another assistant found to answer the question: `{}`",
                    summary
                ),
                SubPromptType::User,
                99,
            );
        }
        // Parses the retrieved nodes as individual sub-prompts, to support priority pruning
        // and also grouping i.e. instead of having 100 tiny messages, we have a message with the chunks grouped
        if !ret_nodes.is_empty() {
            prompt.add_content(
                "Here is some extra context to answer any future user questions: --- start --- \n".to_string(),
                SubPromptType::ExtraContext,
                97,
            );
            for node in ret_nodes {
                prompt.add_ret_node_content(node, SubPromptType::ExtraContext, 96);
            }
            prompt.add_content("--- end ---".to_string(), SubPromptType::ExtraContext, 97);
        }

        // Add the user question and the preference prompt for the answer
        let user_prompt = custom_user_prompt
            .unwrap_or_else(|| "Answer the question using markdown and the extra context provided.".to_string());
        prompt.add_content(format!("{}\n {}", user_message, user_prompt), SubPromptType::User, 100);

        prompt
    }
}
