pub static WORKFLOWS_JSON_TESTING: &str = r#"
[
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5620741248130798,
        0.4670599699020386,
        0.0158749558031559,
        -0.08709941804409027,
        -0.22838152945041656,
        -0.01918424665927887,
        -0.7164031267166138,
        -0.43896207213401794,
        0.1909763365983963,
        0.22806912660598755,
        -0.2949456572532654,
        0.743790864944458,
        0.2256380319595337,
        0.12588506937026978,
        0.2412842959165573,
        0.018038474023342133,
        0.15468686819076538,
        -0.4460228681564331,
        -1.2012813091278076,
        -0.05060504376888275,
        0.17655226588249207,
        0.5863509774208069,
        0.5318567752838135,
        0.23435376584529877,
        0.03416421264410019,
        -0.06775445491075516,
        -0.31476038694381714,
        -0.1564001739025116,
        -1.021744966506958,
        -2.4629015922546387,
        0.30495601892471313,
        0.00678514689207077,
        -0.5190874338150024,
        -0.5257205963134766,
        0.16440372169017792,
        -0.7418798804283142,
        -0.01495278812944889,
        0.09877853840589523,
        -0.29042038321495056,
        -0.5326911211013794,
        -0.17016921937465668,
        0.3890177607536316,
        -0.3535521626472473,
        0.2877383232116699,
        0.32968848943710327,
        0.10267677158117294,
        -0.048101551830768585,
        -0.3046289384365082,
        0.3956974446773529,
        0.3193339705467224,
        -0.5081149935722351,
        -0.018643438816070557,
        -0.022996172308921814,
        0.18697340786457062,
        -0.3909527063369751,
        0.13064545392990112,
        0.07086247205734253,
        -0.296948105096817,
        0.13602200150489807,
        0.00538281537592411,
        0.1986897736787796,
        0.6221030354499817,
        -4.110285758972168,
        0.5096352100372314,
        0.2944222390651703,
        0.36264410614967346,
        0.0876586064696312,
        -0.09721191227436066,
        -0.17281219363212585,
        0.045924633741378784,
        -0.016192913055419922,
        0.13505952060222626,
        -0.18504193425178528,
        0.003694240003824234,
        -0.27874064445495605,
        -0.3997015953063965,
        0.25373536348342896,
        0.581161379814148,
        0.19368234276771545,
        -0.5513960719108582,
        -0.5190451741218567,
        0.2738049030303955,
        0.11524055898189545,
        0.16425292193889618,
        -0.27696773409843445,
        0.6124795079231262,
        -0.15133753418922424,
        -0.17476288974285126,
        0.42940717935562134,
        -0.12137585878372192,
        -0.10542859137058258,
        -0.1371610015630722,
        -0.1960400938987732,
        0.11508651822805405,
        -0.7977193593978882,
        0.35591959953308105,
        0.06640982627868652,
        0.19056791067123413,
        0.20716258883476257,
        3.8946359157562256,
        0.5803366303443909,
        -0.14754065871238708,
        0.5810093879699707,
        -0.5118040442466736,
        0.32984423637390137,
        -0.5814807415008545,
        -0.23264862596988678,
        -0.33283475041389465,
        0.021576251834630966,
        -0.16463014483451843,
        0.023998819291591644,
        -0.6754068732261658,
        0.08179737627506256,
        0.14311906695365906,
        0.07567441463470459,
        0.7132452130317688,
        -0.37491166591644287,
        0.21545736491680145,
        -0.005110676400363445,
        0.3201211094856262,
        -0.8127515912055969,
        0.2626763582229614,
        -0.010361617431044579,
        0.09289461374282837,
        -0.2309596985578537,
        -0.0482635498046875,
        -0.4526616334915161,
        0.44366875290870667,
        0.15219815075397491,
        0.16071511805057526,
        0.25346845388412476,
        -0.11307846009731293,
        -0.6838035583496094,
        0.04319353401660919,
        0.01960613764822483,
        -0.21019315719604492,
        0.7241623997688293,
        -0.5763120055198669,
        0.1754259169101715,
        -0.70644211769104,
        0.09540925174951553,
        -1.6104390621185303,
        1.043488621711731,
        0.015113024041056633,
        0.7504056096076965,
        0.010770810768008232,
        -0.019307196140289307,
        0.17248806357383728,
        -0.45636218786239624,
        -0.3808772563934326,
        0.04505886882543564,
        0.5895707011222839,
        0.029187064617872238,
        0.25305524468421936,
        0.8555933237075806,
        -0.015242244116961956,
        -0.37085801362991333,
        0.18685241043567657,
        -0.587589681148529,
        0.3360801041126251,
        -0.04378896951675415,
        -0.1172737255692482,
        0.6298774480819702,
        0.5318571925163269,
        0.6793603301048279,
        -0.34978172183036804,
        0.3765181303024292,
        -0.22146612405776978,
        0.4930384159088135,
        -0.022092094644904137,
        -0.2045779824256897,
        -0.3389715254306793,
        0.2837473154067993,
        0.8186788558959961,
        0.03238162770867348,
        0.0912611335515976,
        -0.16570746898651123,
        0.3059496283531189,
        0.25007033348083496,
        -0.6519955396652222,
        0.5751630663871765,
        0.4295654594898224,
        -0.14184516668319702,
        -0.5785865783691406,
        -0.044791847467422485,
        -0.14592711627483368,
        0.4831606149673462,
        -0.12049830704927444,
        0.42615363001823425,
        0.8393591642379761,
        -0.8667482733726501,
        1.4758955240249634,
        -0.8591662645339966,
        -0.40015825629234314,
        -0.1295684278011322,
        -0.1313064843416214,
        -0.42705076932907104,
        0.11612144112586975,
        0.3673897683620453,
        0.2791190445423126,
        -0.6626805067062378,
        0.05427565053105354,
        -0.16498667001724243,
        -0.18464314937591553,
        -0.3298827111721039,
        -0.6111660599708557,
        -0.061230964958667755,
        -0.01812061294913292,
        0.06666457653045654,
        -0.4189532399177551,
        0.03999656066298485,
        -0.13428780436515808,
        0.6090399622917175,
        0.46483540534973145,
        0.41717442870140076,
        0.09604577720165253,
        0.08659543097019196,
        0.146045982837677,
        0.37854522466659546,
        0.6760596036911011,
        0.38516485691070557,
        0.014561813324689865,
        -0.5246719717979431,
        -0.8121476769447327,
        -0.23221197724342346,
        0.20885834097862244,
        -0.4115062654018402,
        -0.11170287430286407,
        -0.301827073097229,
        -0.06983648240566254,
        0.4153805673122406,
        0.3710379898548126,
        0.6333212852478027,
        0.937418520450592,
        0.1385350525379181,
        0.1856812834739685,
        -0.14326930046081543,
        0.45595091581344604,
        0.16422024369239807,
        -0.3802361786365509,
        0.5654217004776001,
        0.1320154368877411,
        -0.06143621727824211,
        0.16350559890270233,
        0.35251089930534363,
        -0.587368369102478,
        -0.31736230850219727,
        -0.044846922159194946,
        -0.28151121735572815,
        1.8395287990570068,
        0.2687469720840454,
        -0.4809843599796295,
        0.12990956008434296,
        0.46635064482688904,
        0.34641963243484497,
        -0.029824621975421906,
        -1.4131940603256226,
        0.06307052820920944,
        -0.5601713061332703,
        0.4349309802055359,
        -0.1848488450050354,
        0.02613336592912674,
        0.033419497311115265,
        0.05955129861831665,
        -0.014901387505233288,
        0.15181799232959747,
        -0.20907969772815704,
        -0.0938599705696106,
        -0.27586057782173157,
        -0.20671477913856506,
        0.08497674018144608,
        0.8545663356781006,
        0.12960045039653778,
        0.0013654455542564392,
        0.017688287422060966,
        0.2788904011249542,
        0.3229771554470062,
        -0.17512023448944092,
        -0.28558236360549927,
        0.05361831188201904,
        0.37304964661598206,
        0.07054241746664047,
        0.21605724096298218,
        0.2438117265701294,
        -0.5233747363090515,
        -0.17236806452274323,
        -0.3043069839477539,
        0.029320362955331802,
        -0.21850454807281494,
        0.02802523970603943,
        -0.28437376022338867,
        -0.267056941986084,
        0.05046691745519638,
        -0.025121167302131653,
        1.556512713432312,
        0.07115703076124191,
        0.24498751759529114,
        0.5336893796920776,
        0.30245333909988403,
        0.4014471769332886,
        -0.6214367151260376,
        0.085743248462677,
        -0.17609557509422302,
        -0.32429930567741394,
        -0.6768181324005127,
        -0.3306743800640106,
        0.33842194080352783,
        0.3988890051841736,
        -0.40088772773742676,
        0.5300142168998718,
        -0.6583019495010376,
        -0.4990042746067047,
        0.13991771638393402,
        0.10966318100690842,
        0.2934008836746216,
        -0.40007156133651733,
        0.21924877166748047,
        0.09857284277677536,
        0.13185425102710724,
        -2.4279119968414307,
        -0.4426395297050476,
        0.3451530933380127,
        0.11669871211051941,
        -0.3698762059211731,
        -0.19312645494937897,
        0.7172336578369141,
        -0.041941091418266296,
        -0.016973350197076797,
        -0.3363820016384125,
        1.09745454788208,
        0.4163953363895416,
        0.18216998875141144,
        -0.4050692021846771,
        -0.10283446311950684,
        0.2519604563713074,
        -0.5311406254768372,
        0.6313135623931885,
        -0.2823065221309662,
        -0.7240358591079712,
        0.10208167135715485,
        0.32693755626678467,
        1.6962285041809082,
        0.29811954498291016,
        0.2817700505256653,
        -0.1399649977684021,
        0.13108426332473755,
        -0.6838955879211426,
        -1.555068850517273,
        0.1617712527513504,
        -0.21412703394889832,
        -0.2255614548921585,
        0.623567521572113,
        0.13538731634616852,
        -0.2391749918460846,
        0.8540341258049011,
        0.891379177570343,
        0.07167594134807587,
        0.8698649406433105,
        -0.430422842502594,
        1.5192762613296509,
        -0.33546748757362366,
        -0.21086843311786652,
        -0.12017694115638733,
        0.4655928611755371,
        0.2287512570619583,
        0.3985024094581604,
        -0.2643386125564575,
        -0.6002973318099976,
        -0.1384640336036682,
        0.30025866627693176,
        0.18684600293636322,
        -0.286533921957016,
        0.5620973706245422,
        0.46103158593177795,
        0.2536720633506775,
        0.05175863206386566,
        -0.073259636759758,
        0.3802380859851837,
        0.062371451407670975,
        0.032090164721012115,
        -0.23575758934020996,
        -0.29825517535209656,
        -0.9140517711639404,
        -0.34991204738616943
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Reviews in depth all the content to generate a summary.",
          "name": "ExtensiveSummary",
          "raw": "\n            workflow ExtensiveSummary v0.1 {\n                step Initialize {\n                    $PROMPT = \"Summarize this: \"\n                    $EMBEDDINGS = call process_embeddings_in_job_scope()\n                }\n                step Summarize {\n                    $RESULT = call multi_inference($PROMPT, $EMBEDDINGS)\n                }\n            }\n        ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$PROMPT",
                        "value": "Summarize this: "
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$EMBEDDINGS",
                        "value": {
                          "args": [],
                          "name": "process_embeddings_in_job_scope"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Initialize"
            },
            {
              "body": [
                {
                  "type": "registeroperation",
                  "value": {
                    "register": "$RESULT",
                    "value": {
                      "args": [
                        {
                          "type": "register",
                          "value": "$PROMPT"
                        },
                        {
                          "type": "register",
                          "value": "$EMBEDDINGS"
                        }
                      ],
                      "name": "multi_inference"
                    }
                  }
                }
              ],
              "name": "Summarize"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.034734420478343964,
        0.1871805489063263,
        0.044480107724666595,
        -0.07555993646383286,
        -0.2762329578399658,
        0.24182763695716858,
        -1.033287763595581,
        0.7125799655914307,
        0.47229233384132385,
        -0.4209500551223755,
        -0.46268194913864136,
        0.9547873139381409,
        -0.20145152509212494,
        0.4152549207210541,
        0.14341086149215698,
        0.12008798867464066,
        0.05905970185995102,
        -0.5367689728736877,
        -1.7866318225860596,
        -0.619709849357605,
        -0.29154425859451294,
        0.8294081091880798,
        0.628642201423645,
        0.07314151525497437,
        0.19572168588638306,
        -0.034623607993125916,
        0.421909362077713,
        -0.24239057302474976,
        -0.9496891498565674,
        -1.6145397424697876,
        0.5303781628608704,
        0.32453474402427673,
        -0.28177550435066223,
        -0.4890628457069397,
        -0.38026148080825806,
        -1.1231189966201782,
        0.40186482667922974,
        -0.2134477198123932,
        -0.36821743845939636,
        0.05925258994102478,
        0.27076995372772217,
        0.20357921719551086,
        -0.021091710776090622,
        0.15783512592315674,
        0.5020970702171326,
        -0.6582558751106262,
        0.3204505741596222,
        0.09018649160861969,
        0.5070585012435913,
        0.06716383248567581,
        -0.38541632890701294,
        -0.6742196679115295,
        0.2859421670436859,
        -0.004317570477724075,
        -0.24483773112297058,
        -0.1230064108967781,
        0.3691105842590332,
        -0.6520802974700928,
        0.32098183035850525,
        -0.2527220845222473,
        -0.21936717629432678,
        0.3374263346195221,
        -3.8076820373535156,
        0.30166298151016235,
        0.3135705590248108,
        -0.06931666284799576,
        -0.0042413342744112015,
        -0.2929495871067047,
        0.3975790739059448,
        0.422590434551239,
        0.1426510214805603,
        0.4197009801864624,
        -0.5708262920379639,
        0.412312775850296,
        0.11728336662054062,
        0.03570561856031418,
        -0.45046818256378174,
        0.11172579228878021,
        0.17126746475696564,
        -0.3517521619796753,
        0.4710276126861572,
        0.4825541377067566,
        -0.06425526738166809,
        0.15817689895629883,
        -0.041283655911684036,
        0.5767968893051147,
        -0.03386301547288895,
        0.17241227626800537,
        0.05743739381432533,
        -0.10351618379354477,
        -0.6282508373260498,
        0.2978358268737793,
        0.30152881145477295,
        -0.24568328261375427,
        -0.20568302273750305,
        0.19630080461502075,
        0.016544099897146225,
        0.01819857954978943,
        -0.2418222427368164,
        3.4505109786987305,
        -0.18223853409290314,
        0.26296675205230713,
        0.7941957712173462,
        -0.5534366369247437,
        0.7853090763092041,
        -0.0009117536246776581,
        -0.2845374643802643,
        -0.6472243070602417,
        -0.1668383777141571,
        -0.3792476952075958,
        0.4950440526008606,
        -0.9411168694496155,
        -0.5924868583679199,
        0.31550318002700806,
        0.18666428327560425,
        0.6844877600669861,
        -0.4719914197921753,
        0.09086611866950989,
        0.016701843589544296,
        0.9475106000900269,
        -0.503623366355896,
        -0.026559878140687943,
        -0.5337488055229187,
        -0.3949074447154999,
        -0.15773630142211914,
        0.07483159005641937,
        -0.21122387051582336,
        0.6088380813598633,
        -0.529130220413208,
        -0.08318600058555603,
        0.3063376247882843,
        -0.335045725107193,
        -0.6450403332710266,
        -0.0591755211353302,
        0.16853100061416626,
        0.3160886764526367,
        0.6661991477012634,
        -1.21894109249115,
        -0.17258977890014648,
        -0.43191951513290405,
        0.4885932207107544,
        -1.345179796218872,
        0.6688281297683716,
        -0.3229002058506012,
        0.6696915626525879,
        0.12988126277923584,
        -0.7549917697906494,
        0.061022330075502396,
        -0.6575156450271606,
        -0.6167019605636597,
        -0.41561612486839294,
        0.42765122652053833,
        -0.40447378158569336,
        0.26708102226257324,
        0.8733229041099548,
        0.8704780340194702,
        -0.2516751289367676,
        -0.19926860928535461,
        -0.39761170744895935,
        0.7005975246429443,
        -0.27293720841407776,
        -0.09869083762168884,
        0.04231929033994675,
        0.1726655662059784,
        0.6348462700843811,
        -0.31958454847335815,
        0.5049365758895874,
        0.12581421434879303,
        0.3368198275566101,
        0.25203222036361694,
        -0.028484106063842773,
        0.006750665605068207,
        0.20156043767929077,
        0.8802384734153748,
        -0.577757716178894,
        0.005151968449354172,
        0.44799765944480896,
        -0.13738900423049927,
        0.06280632317066193,
        -0.6039809584617615,
        0.051666975021362305,
        0.5224025249481201,
        -0.28239843249320984,
        -0.5803446173667908,
        0.24665649235248566,
        0.405037522315979,
        -0.04525134712457657,
        0.5046746134757996,
        0.2667279839515686,
        0.7985939979553223,
        -0.8122603893280029,
        1.8872915506362915,
        -1.0178728103637695,
        0.2591046690940857,
        -0.4069160521030426,
        -0.057505037635564804,
        0.35820120573043823,
        0.34579163789749146,
        0.05497463792562485,
        -0.01758282631635666,
        -0.6904863119125366,
        -0.5490170121192932,
        -0.2188713699579239,
        -0.1431823968887329,
        -0.3400297164916992,
        -0.9551897644996643,
        0.4477522373199463,
        0.9675039649009705,
        -0.22031156718730927,
        -0.5745877027511597,
        -0.24293725192546844,
        0.04381025955080986,
        1.0013937950134277,
        0.1482800394296646,
        0.8249431848526001,
        0.49105530977249146,
        0.5536376237869263,
        -0.4726281762123108,
        -0.02756471000611782,
        0.7716965675354004,
        -0.1321134865283966,
        0.1318313181400299,
        -0.17074808478355408,
        -0.8118672370910645,
        -0.3616568148136139,
        0.5766054391860962,
        -0.09477080404758453,
        0.64303058385849,
        -0.20550958812236786,
        -0.03641514107584953,
        0.004146404564380646,
        1.1218799352645874,
        0.23001694679260254,
        0.9742380976676941,
        0.10956323146820068,
        0.15432007610797882,
        -0.20407500863075256,
        0.3217419981956482,
        0.012776657938957214,
        -0.863412618637085,
        -0.12214770168066025,
        -0.1470094621181488,
        0.18791575729846954,
        -0.10743652284145355,
        -0.016279935836791992,
        -0.48529407382011414,
        0.3650325536727905,
        -0.27987104654312134,
        -0.6158308982849121,
        1.834641695022583,
        0.7462397813796997,
        -0.05483162775635719,
        0.18829205632209778,
        0.8982114791870117,
        0.2531103789806366,
        -0.04746251553297043,
        -1.516945242881775,
        -0.09371230006217957,
        -0.49885794520378113,
        0.9522011280059814,
        0.143731951713562,
        -0.012899696826934814,
        0.84458988904953,
        -0.4186297655105591,
        0.06162244454026222,
        -0.3803642988204956,
        0.03807999938726425,
        -0.5136410593986511,
        -0.3748040497303009,
        -0.3351418375968933,
        -0.29277485609054565,
        0.8687057495117188,
        -0.32287031412124634,
        0.04512590914964676,
        -0.3529585897922516,
        -0.009137822315096855,
        0.24435634911060333,
        -0.4654300808906555,
        -0.26127246022224426,
        -0.23510973155498505,
        0.26223838329315186,
        -0.35903236269950867,
        -0.5135477781295776,
        0.4223664402961731,
        0.12143268436193466,
        0.13796232640743256,
        -0.872850239276886,
        -0.7502076625823975,
        0.13137716054916382,
        0.25962790846824646,
        -0.3763854205608368,
        -0.23446759581565857,
        -0.894822895526886,
        0.4070824384689331,
        1.9461617469787598,
        -0.17248550057411194,
        -0.4007423520088196,
        0.5539323091506958,
        0.1676577478647232,
        -0.10842078924179077,
        0.47942718863487244,
        -0.17142406105995178,
        -0.15705609321594238,
        -0.18135124444961548,
        -0.7034934163093567,
        -0.24892881512641907,
        0.6359640955924988,
        0.27162688970565796,
        -0.057023391127586365,
        -0.18354308605194092,
        -0.6973524689674377,
        0.26395007967948914,
        0.1207042932510376,
        0.4024277627468109,
        0.40282028913497925,
        -0.303811639547348,
        0.3677162230014801,
        0.7304520606994629,
        -0.04565465822815895,
        -1.9546071290969849,
        -0.24198698997497559,
        0.3390151560306549,
        -0.4251219928264618,
        -0.37752872705459595,
        -0.5364102721214294,
        0.317706823348999,
        -0.13012170791625977,
        0.5919176936149597,
        -0.41882485151290894,
        1.0903478860855103,
        0.6279439926147461,
        -0.18956951797008514,
        0.1847759187221527,
        -0.03504883870482445,
        1.0964027643203735,
        -0.2120370864868164,
        0.19891658425331116,
        0.32237040996551514,
        -0.2026764154434204,
        0.21574266254901886,
        0.5093711614608765,
        0.9958378672599792,
        0.22808237373828888,
        0.21431100368499756,
        -0.2564947307109833,
        0.29420459270477295,
        -0.3300313949584961,
        -1.2673944234848022,
        -0.032171063125133514,
        -0.6294702887535095,
        0.15244247019290924,
        1.147212028503418,
        -0.09756116569042206,
        0.12227030843496323,
        0.2238313853740692,
        0.6322968006134033,
        0.23951034247875214,
        0.08459635078907013,
        0.025344640016555786,
        1.7545489072799683,
        -0.10636207461357117,
        -0.47393617033958435,
        -0.44828516244888306,
        -0.5145392417907715,
        -0.42072558403015137,
        0.08383013308048248,
        0.010295245796442032,
        -0.3179120719432831,
        0.1415541172027588,
        0.5705311894416809,
        -0.3058169484138489,
        -0.29502421617507935,
        0.26835861802101135,
        0.2325461059808731,
        0.6640745401382446,
        -0.053470537066459656,
        -0.28660848736763,
        1.2071852684020996,
        -0.08734236657619476,
        -0.2880823016166687,
        0.26413995027542114,
        -0.02680138871073723,
        -0.39567938446998596,
        -0.17057590186595917
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates a passage to answer a question and uses embeddings to refine the answer.",
          "name": "HydeInference",
          "raw": "\n            workflow HydeInference v0.1 {\n                step Initialize {\n                    $PROMPT = \"write a passage to answer the question: \"\n                    $HYDE_PROMPT = call concat($PROMPT, $INPUT)\n                    $HYDE_PASSAGE = call inference_no_ws($HYDE_PROMPT)\n                    $HYDE_INPUT = call concat($INPUT, \". \", $HYDE_PASSAGE )\n                    $EMBEDDINGS = call search_embeddings_in_job_scope($HYDE_INPUT)\n                }\n                step Summarize {\n                    $CONNECTOR = \"\\nLeverage the following information to answer the previous query: --- start ---\"\n                    $NEW_INPUT = call concat($INPUT, $CONNECTOR, $EMBEDDINGS) \n                    $RESULT = call inference($NEW_INPUT)\n                }\n            }\n        ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$PROMPT",
                        "value": "write a passage to answer the question: "
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_PROMPT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$PROMPT"
                            },
                            {
                              "type": "register",
                              "value": "$INPUT"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_PASSAGE",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$HYDE_PROMPT"
                            }
                          ],
                          "name": "inference_no_ws"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_INPUT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "string",
                              "value": ". "
                            },
                            {
                              "type": "register",
                              "value": "$HYDE_PASSAGE"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$EMBEDDINGS",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$HYDE_INPUT"
                            }
                          ],
                          "name": "search_embeddings_in_job_scope"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Initialize"
            },
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CONNECTOR",
                        "value": "\\nLeverage the following information to answer the previous query: --- start ---"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$NEW_INPUT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$CONNECTOR"
                            },
                            {
                              "type": "register",
                              "value": "$EMBEDDINGS"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$NEW_INPUT"
                            }
                          ],
                          "name": "inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Summarize"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5217975974082947,
        0.6705355048179626,
        -0.24365361034870148,
        0.2630804479122162,
        -0.055158816277980804,
        -0.027416400611400604,
        -0.8448842167854309,
        0.4957970976829529,
        0.026166729629039764,
        0.0568583607673645,
        -0.06262187659740448,
        1.0268981456756592,
        -0.17997652292251587,
        -0.5957084894180298,
        0.5241677165031433,
        -0.073320172727108,
        0.483111172914505,
        -1.0835824012756348,
        -1.6435885429382324,
        -0.2773756682872772,
        -0.3268052041530609,
        0.09233829379081726,
        0.570166289806366,
        0.27032995223999023,
        0.1990419626235962,
        -0.04393035173416138,
        -0.46693891286849976,
        -0.3368980884552002,
        -0.7220626473426819,
        -1.871181607246399,
        0.6739394068717957,
        0.5323552489280701,
        0.21472512185573578,
        -0.5961694121360779,
        0.36276042461395264,
        -0.6659478545188904,
        0.031285662204027176,
        -0.3852560520172119,
        -0.3787482678890228,
        0.07704560458660126,
        0.2723124027252197,
        0.1442337930202484,
        0.13672783970832825,
        -0.18554916977882385,
        0.21989686787128448,
        -0.18905243277549744,
        0.1322297304868698,
        -0.4713946282863617,
        0.9121119379997253,
        0.5810496807098389,
        -0.07193005830049515,
        -0.7029728293418884,
        -0.13997705280780792,
        -0.6111346483230591,
        -0.8305694460868835,
        -0.7269783616065979,
        0.1714978665113449,
        -0.5532603859901428,
        0.4187415540218353,
        0.1591222733259201,
        0.5274844169616699,
        0.8996009230613708,
        -3.16033673286438,
        0.08123601973056793,
        0.06916505843400955,
        0.244430273771286,
        0.6882338523864746,
        -0.6952429413795471,
        0.33364158868789673,
        -0.2358098328113556,
        0.26270928978919983,
        -0.0416974276304245,
        -0.3559892773628235,
        1.0462677478790283,
        -0.002749621868133545,
        -0.2537255585193634,
        -0.06800861656665802,
        -0.02359657734632492,
        0.4327777624130249,
        0.6239508986473083,
        0.14622169733047485,
        0.03531741350889206,
        -0.39679116010665894,
        -0.977078914642334,
        -0.936194896697998,
        0.5409996509552002,
        -1.0760374069213867,
        0.32848450541496277,
        0.6311944127082825,
        0.14159080386161804,
        -0.06356936693191528,
        -0.39095622301101685,
        0.18491660058498383,
        -0.2065873146057129,
        -0.4836304783821106,
        0.11969326436519623,
        0.40857362747192383,
        -0.32239317893981934,
        -0.6118934750556946,
        3.3892714977264404,
        0.48266690969467163,
        0.07768476009368896,
        0.2956967353820801,
        -1.3127696514129639,
        0.40277037024497986,
        -0.2261715531349182,
        0.21370524168014526,
        -0.49305880069732666,
        0.06676560640335083,
        0.48641473054885864,
        0.5203590989112854,
        -1.3259339332580566,
        -0.4931930601596832,
        -0.08241058886051178,
        0.4220368266105652,
        0.4514558017253876,
        -0.6090209484100342,
        -0.09512780606746674,
        0.22303397953510284,
        0.7214317917823792,
        -0.10309116542339325,
        -0.2423461377620697,
        -0.6361504793167114,
        -0.03672082722187042,
        0.39151379466056824,
        0.380852073431015,
        0.02322957292199135,
        0.8070581555366516,
        0.20471300184726715,
        0.1267378181219101,
        -0.6727452278137207,
        -0.515336275100708,
        0.055332351475954056,
        0.08860611170530319,
        -0.23885050415992737,
        0.2597174644470215,
        0.7751559615135193,
        -1.4895457029342651,
        0.4119921028614044,
        -0.8133742809295654,
        0.43248221278190613,
        -1.0600495338439941,
        0.9090545773506165,
        0.1412292718887329,
        0.6213206052780151,
        0.2562440037727356,
        0.3154059648513794,
        0.6048558354377747,
        -0.4115428328514099,
        -0.5538619160652161,
        -0.19729755818843842,
        0.28426289558410645,
        -0.3783497214317322,
        0.7542127370834351,
        0.15272271633148193,
        -0.039394717663526535,
        0.19333487749099731,
        -0.6590179800987244,
        -0.7701877951622009,
        0.3972232937812805,
        0.16648125648498535,
        -0.02083461917936802,
        0.05433003604412079,
        -0.30666208267211914,
        0.35323965549468994,
        0.1256871372461319,
        0.49710002541542053,
        0.1082662045955658,
        -0.29052698612213135,
        -0.17132066190242767,
        0.02150486409664154,
        -0.24848806858062744,
        0.5935348272323608,
        0.3625210225582123,
        -0.1457476168870926,
        -0.28131651878356934,
        -0.1326143443584442,
        -0.14821875095367432,
        0.379703164100647,
        -0.16204044222831726,
        0.960371732711792,
        0.43096446990966797,
        -0.27280333638191223,
        -0.7327988743782043,
        -0.4692484140396118,
        0.3843023478984833,
        0.47834956645965576,
        0.24101340770721436,
        0.3751585781574249,
        0.24800710380077362,
        -0.9193392395973206,
        1.8324761390686035,
        -0.689822793006897,
        -0.3209829032421112,
        -0.07334859669208527,
        -0.11310294270515442,
        0.37197279930114746,
        0.3572532534599304,
        0.6516865491867065,
        -0.049167100340127945,
        -1.193394422531128,
        0.4415071904659271,
        -0.6154524087905884,
        -0.01581019163131714,
        -0.2254577875137329,
        -0.4738317131996155,
        -0.14066429436206818,
        0.6105179786682129,
        -0.3202188014984131,
        0.06671469658613205,
        -0.6770957708358765,
        -0.05336727201938629,
        1.168273687362671,
        0.04696497321128845,
        1.2656526565551758,
        0.20630161464214325,
        0.4834672212600708,
        0.3591012954711914,
        0.36739975214004517,
        0.2938002645969391,
        0.19798046350479126,
        0.23621299862861633,
        0.2226957231760025,
        -0.7855562567710876,
        -0.7126330137252808,
        0.3147484064102173,
        -0.7240287065505981,
        0.3580595850944519,
        -0.32516440749168396,
        -0.40975701808929443,
        0.6795403361320496,
        1.6100436449050903,
        1.0999537706375122,
        0.5577028393745422,
        -0.4695652723312378,
        1.0390901565551758,
        -0.44631320238113403,
        0.47257986664772034,
        0.4145084023475647,
        -1.1369632482528687,
        0.5121796131134033,
        -0.0441184937953949,
        0.4039679169654846,
        0.5458258986473083,
        0.5670212507247925,
        0.8615117073059082,
        -0.657166063785553,
        -0.351988285779953,
        0.12625384330749512,
        0.7112622261047363,
        0.4821208715438843,
        -0.1724041998386383,
        0.24358856678009033,
        -0.10205470025539398,
        0.06550612300634384,
        0.19324445724487305,
        -1.5678365230560303,
        -0.12132494896650314,
        -0.9552805423736572,
        0.7691149115562439,
        -0.12316227704286575,
        -0.45294782519340515,
        0.4233235716819763,
        -0.29440659284591675,
        0.27920567989349365,
        -0.09119509160518646,
        -0.8426341414451599,
        -0.19401296973228455,
        -0.1196584552526474,
        0.009854890406131744,
        0.8042097091674805,
        0.11656402796506882,
        -0.7586128115653992,
        0.07545974850654602,
        -0.5649018287658691,
        -0.17164397239685059,
        0.22609321773052216,
        -0.05334358662366867,
        0.19416964054107666,
        -0.1430165022611618,
        0.6216332912445068,
        -0.0824175626039505,
        -0.7341193556785583,
        0.11193397641181946,
        0.13833469152450562,
        -0.562166154384613,
        -0.3522283732891083,
        -1.0460312366485596,
        -0.6479171514511108,
        0.8833339810371399,
        -0.2818920910358429,
        0.031444236636161804,
        -1.2941651344299316,
        -0.3253493905067444,
        1.3333481550216675,
        0.652028501033783,
        0.35527488589286804,
        0.5304181575775146,
        0.2921711206436157,
        -0.25260961055755615,
        -0.06883727759122849,
        0.35716578364372253,
        0.4548536539077759,
        -0.6085742712020874,
        -1.06727933883667,
        0.15465646982192993,
        0.20243129134178162,
        -0.06389427930116653,
        0.09642645716667175,
        0.5321649312973022,
        -0.7393209338188171,
        0.2277315855026245,
        -0.29905083775520325,
        -0.6217883229255676,
        0.5628343224525452,
        0.18441921472549438,
        0.2999612092971802,
        1.6529536247253418,
        0.30236950516700745,
        -1.4994771480560303,
        -0.4032038450241089,
        0.2655697464942932,
        -0.07651287317276001,
        -0.19933074712753296,
        0.40059909224510193,
        0.1620161086320877,
        -0.08868397772312164,
        0.19656537473201752,
        -0.5545854568481445,
        1.372854471206665,
        0.5722429752349854,
        0.0751008614897728,
        -0.23768703639507294,
        0.25020307302474976,
        0.3622342348098755,
        0.15045976638793945,
        0.2029159814119339,
        -0.046639181673526764,
        -0.33883950114250183,
        -0.3241194486618042,
        0.46329277753829956,
        1.0365190505981445,
        0.6351235508918762,
        -0.029345136135816574,
        -0.27866604924201965,
        0.041748736053705215,
        -0.6484447717666626,
        -0.5143448114395142,
        0.27627843618392944,
        -0.27804455161094666,
        -0.5963647365570068,
        0.46267515420913696,
        -0.12084157019853592,
        -0.1146276667714119,
        0.9631665945053101,
        0.7625371217727661,
        -1.0228558778762817,
        0.1971416026353836,
        -0.6056783199310303,
        1.800622820854187,
        -0.6298641562461853,
        -0.11450319737195969,
        -0.3849523067474365,
        0.5427781939506531,
        -0.7251048684120178,
        0.3562368154525757,
        -0.26989448070526123,
        -0.8379263877868652,
        0.03333130478858948,
        0.12300238013267517,
        -0.6583588123321533,
        -0.13887548446655273,
        0.19580218195915222,
        0.7095918655395508,
        0.3531382381916046,
        -0.007668033242225647,
        -0.16812410950660706,
        0.1320769339799881,
        0.3730332553386688,
        0.2985376715660095,
        -0.00826168805360794,
        -0.36709192395210266,
        0.0038701295852661133,
        -0.20109312236309052
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates user stories and acceptance criteria for specified topics, focusing on Agile framework principles. This prompt specializes in translating topics into structured Agile documentation, specifically for user story and acceptance criteria creation. The expected output is a JSON-formatted document detailing the topic, user story, and acceptance criteria.",
          "name": "Agility_story",
          "raw": "\n                workflow Agility_story v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert in the Agile framework. You deeply understand user story and acceptance criteria creation. You will be given a topic. Please write the appropriate information for what is requested. \n\n# STEPS\n\nPlease write a user story and acceptance criteria for the requested topic.\n\n# OUTPUT INSTRUCTIONS\n\nOutput the results in JSON format as defined in this example:\n\n{\n    \\\"Topic\\\": \\\"Automating data quality automation\\\",\n    \\\"Story\\\": \\\"As a user, I want to be able to create a new user account so that I can access the system.\\\",\n    \\\"Criteria\\\": \\\"Given that I am a user, when I click the 'Create Account' button, then I should be prompted to enter my email address, password, and confirm password. When I click the 'Submit' button, then I should be redirected to the login page.\\\"\n}\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert in the Agile framework. You deeply understand user story and acceptance criteria creation. You will be given a topic. Please write the appropriate information for what is requested. \n\n# STEPS\n\nPlease write a user story and acceptance criteria for the requested topic.\n\n# OUTPUT INSTRUCTIONS\n\nOutput the results in JSON format as defined in this example:\n\n{\n    \\\"Topic\\\": \\\"Automating data quality automation\\\",\n    \\\"Story\\\": \\\"As a user, I want to be able to create a new user account so that I can access the system.\\\",\n    \\\"Criteria\\\": \\\"Given that I am a user, when I click the 'Create Account' button, then I should be prompted to enter my email address, password, and confirm password. When I click the 'Submit' button, then I should be redirected to the login page.\\\"\n}\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  }
]
"#;

pub static WORKFLOWS_JSON: &str = r#"
[
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5620741248130798,
        0.4670599699020386,
        0.0158749558031559,
        -0.08709941804409027,
        -0.22838152945041656,
        -0.01918424665927887,
        -0.7164031267166138,
        -0.43896207213401794,
        0.1909763365983963,
        0.22806912660598755,
        -0.2949456572532654,
        0.743790864944458,
        0.2256380319595337,
        0.12588506937026978,
        0.2412842959165573,
        0.018038474023342133,
        0.15468686819076538,
        -0.4460228681564331,
        -1.2012813091278076,
        -0.05060504376888275,
        0.17655226588249207,
        0.5863509774208069,
        0.5318567752838135,
        0.23435376584529877,
        0.03416421264410019,
        -0.06775445491075516,
        -0.31476038694381714,
        -0.1564001739025116,
        -1.021744966506958,
        -2.4629015922546387,
        0.30495601892471313,
        0.00678514689207077,
        -0.5190874338150024,
        -0.5257205963134766,
        0.16440372169017792,
        -0.7418798804283142,
        -0.01495278812944889,
        0.09877853840589523,
        -0.29042038321495056,
        -0.5326911211013794,
        -0.17016921937465668,
        0.3890177607536316,
        -0.3535521626472473,
        0.2877383232116699,
        0.32968848943710327,
        0.10267677158117294,
        -0.048101551830768585,
        -0.3046289384365082,
        0.3956974446773529,
        0.3193339705467224,
        -0.5081149935722351,
        -0.018643438816070557,
        -0.022996172308921814,
        0.18697340786457062,
        -0.3909527063369751,
        0.13064545392990112,
        0.07086247205734253,
        -0.296948105096817,
        0.13602200150489807,
        0.00538281537592411,
        0.1986897736787796,
        0.6221030354499817,
        -4.110285758972168,
        0.5096352100372314,
        0.2944222390651703,
        0.36264410614967346,
        0.0876586064696312,
        -0.09721191227436066,
        -0.17281219363212585,
        0.045924633741378784,
        -0.016192913055419922,
        0.13505952060222626,
        -0.18504193425178528,
        0.003694240003824234,
        -0.27874064445495605,
        -0.3997015953063965,
        0.25373536348342896,
        0.581161379814148,
        0.19368234276771545,
        -0.5513960719108582,
        -0.5190451741218567,
        0.2738049030303955,
        0.11524055898189545,
        0.16425292193889618,
        -0.27696773409843445,
        0.6124795079231262,
        -0.15133753418922424,
        -0.17476288974285126,
        0.42940717935562134,
        -0.12137585878372192,
        -0.10542859137058258,
        -0.1371610015630722,
        -0.1960400938987732,
        0.11508651822805405,
        -0.7977193593978882,
        0.35591959953308105,
        0.06640982627868652,
        0.19056791067123413,
        0.20716258883476257,
        3.8946359157562256,
        0.5803366303443909,
        -0.14754065871238708,
        0.5810093879699707,
        -0.5118040442466736,
        0.32984423637390137,
        -0.5814807415008545,
        -0.23264862596988678,
        -0.33283475041389465,
        0.021576251834630966,
        -0.16463014483451843,
        0.023998819291591644,
        -0.6754068732261658,
        0.08179737627506256,
        0.14311906695365906,
        0.07567441463470459,
        0.7132452130317688,
        -0.37491166591644287,
        0.21545736491680145,
        -0.005110676400363445,
        0.3201211094856262,
        -0.8127515912055969,
        0.2626763582229614,
        -0.010361617431044579,
        0.09289461374282837,
        -0.2309596985578537,
        -0.0482635498046875,
        -0.4526616334915161,
        0.44366875290870667,
        0.15219815075397491,
        0.16071511805057526,
        0.25346845388412476,
        -0.11307846009731293,
        -0.6838035583496094,
        0.04319353401660919,
        0.01960613764822483,
        -0.21019315719604492,
        0.7241623997688293,
        -0.5763120055198669,
        0.1754259169101715,
        -0.70644211769104,
        0.09540925174951553,
        -1.6104390621185303,
        1.043488621711731,
        0.015113024041056633,
        0.7504056096076965,
        0.010770810768008232,
        -0.019307196140289307,
        0.17248806357383728,
        -0.45636218786239624,
        -0.3808772563934326,
        0.04505886882543564,
        0.5895707011222839,
        0.029187064617872238,
        0.25305524468421936,
        0.8555933237075806,
        -0.015242244116961956,
        -0.37085801362991333,
        0.18685241043567657,
        -0.587589681148529,
        0.3360801041126251,
        -0.04378896951675415,
        -0.1172737255692482,
        0.6298774480819702,
        0.5318571925163269,
        0.6793603301048279,
        -0.34978172183036804,
        0.3765181303024292,
        -0.22146612405776978,
        0.4930384159088135,
        -0.022092094644904137,
        -0.2045779824256897,
        -0.3389715254306793,
        0.2837473154067993,
        0.8186788558959961,
        0.03238162770867348,
        0.0912611335515976,
        -0.16570746898651123,
        0.3059496283531189,
        0.25007033348083496,
        -0.6519955396652222,
        0.5751630663871765,
        0.4295654594898224,
        -0.14184516668319702,
        -0.5785865783691406,
        -0.044791847467422485,
        -0.14592711627483368,
        0.4831606149673462,
        -0.12049830704927444,
        0.42615363001823425,
        0.8393591642379761,
        -0.8667482733726501,
        1.4758955240249634,
        -0.8591662645339966,
        -0.40015825629234314,
        -0.1295684278011322,
        -0.1313064843416214,
        -0.42705076932907104,
        0.11612144112586975,
        0.3673897683620453,
        0.2791190445423126,
        -0.6626805067062378,
        0.05427565053105354,
        -0.16498667001724243,
        -0.18464314937591553,
        -0.3298827111721039,
        -0.6111660599708557,
        -0.061230964958667755,
        -0.01812061294913292,
        0.06666457653045654,
        -0.4189532399177551,
        0.03999656066298485,
        -0.13428780436515808,
        0.6090399622917175,
        0.46483540534973145,
        0.41717442870140076,
        0.09604577720165253,
        0.08659543097019196,
        0.146045982837677,
        0.37854522466659546,
        0.6760596036911011,
        0.38516485691070557,
        0.014561813324689865,
        -0.5246719717979431,
        -0.8121476769447327,
        -0.23221197724342346,
        0.20885834097862244,
        -0.4115062654018402,
        -0.11170287430286407,
        -0.301827073097229,
        -0.06983648240566254,
        0.4153805673122406,
        0.3710379898548126,
        0.6333212852478027,
        0.937418520450592,
        0.1385350525379181,
        0.1856812834739685,
        -0.14326930046081543,
        0.45595091581344604,
        0.16422024369239807,
        -0.3802361786365509,
        0.5654217004776001,
        0.1320154368877411,
        -0.06143621727824211,
        0.16350559890270233,
        0.35251089930534363,
        -0.587368369102478,
        -0.31736230850219727,
        -0.044846922159194946,
        -0.28151121735572815,
        1.8395287990570068,
        0.2687469720840454,
        -0.4809843599796295,
        0.12990956008434296,
        0.46635064482688904,
        0.34641963243484497,
        -0.029824621975421906,
        -1.4131940603256226,
        0.06307052820920944,
        -0.5601713061332703,
        0.4349309802055359,
        -0.1848488450050354,
        0.02613336592912674,
        0.033419497311115265,
        0.05955129861831665,
        -0.014901387505233288,
        0.15181799232959747,
        -0.20907969772815704,
        -0.0938599705696106,
        -0.27586057782173157,
        -0.20671477913856506,
        0.08497674018144608,
        0.8545663356781006,
        0.12960045039653778,
        0.0013654455542564392,
        0.017688287422060966,
        0.2788904011249542,
        0.3229771554470062,
        -0.17512023448944092,
        -0.28558236360549927,
        0.05361831188201904,
        0.37304964661598206,
        0.07054241746664047,
        0.21605724096298218,
        0.2438117265701294,
        -0.5233747363090515,
        -0.17236806452274323,
        -0.3043069839477539,
        0.029320362955331802,
        -0.21850454807281494,
        0.02802523970603943,
        -0.28437376022338867,
        -0.267056941986084,
        0.05046691745519638,
        -0.025121167302131653,
        1.556512713432312,
        0.07115703076124191,
        0.24498751759529114,
        0.5336893796920776,
        0.30245333909988403,
        0.4014471769332886,
        -0.6214367151260376,
        0.085743248462677,
        -0.17609557509422302,
        -0.32429930567741394,
        -0.6768181324005127,
        -0.3306743800640106,
        0.33842194080352783,
        0.3988890051841736,
        -0.40088772773742676,
        0.5300142168998718,
        -0.6583019495010376,
        -0.4990042746067047,
        0.13991771638393402,
        0.10966318100690842,
        0.2934008836746216,
        -0.40007156133651733,
        0.21924877166748047,
        0.09857284277677536,
        0.13185425102710724,
        -2.4279119968414307,
        -0.4426395297050476,
        0.3451530933380127,
        0.11669871211051941,
        -0.3698762059211731,
        -0.19312645494937897,
        0.7172336578369141,
        -0.041941091418266296,
        -0.016973350197076797,
        -0.3363820016384125,
        1.09745454788208,
        0.4163953363895416,
        0.18216998875141144,
        -0.4050692021846771,
        -0.10283446311950684,
        0.2519604563713074,
        -0.5311406254768372,
        0.6313135623931885,
        -0.2823065221309662,
        -0.7240358591079712,
        0.10208167135715485,
        0.32693755626678467,
        1.6962285041809082,
        0.29811954498291016,
        0.2817700505256653,
        -0.1399649977684021,
        0.13108426332473755,
        -0.6838955879211426,
        -1.555068850517273,
        0.1617712527513504,
        -0.21412703394889832,
        -0.2255614548921585,
        0.623567521572113,
        0.13538731634616852,
        -0.2391749918460846,
        0.8540341258049011,
        0.891379177570343,
        0.07167594134807587,
        0.8698649406433105,
        -0.430422842502594,
        1.5192762613296509,
        -0.33546748757362366,
        -0.21086843311786652,
        -0.12017694115638733,
        0.4655928611755371,
        0.2287512570619583,
        0.3985024094581604,
        -0.2643386125564575,
        -0.6002973318099976,
        -0.1384640336036682,
        0.30025866627693176,
        0.18684600293636322,
        -0.286533921957016,
        0.5620973706245422,
        0.46103158593177795,
        0.2536720633506775,
        0.05175863206386566,
        -0.073259636759758,
        0.3802380859851837,
        0.062371451407670975,
        0.032090164721012115,
        -0.23575758934020996,
        -0.29825517535209656,
        -0.9140517711639404,
        -0.34991204738616943
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Reviews in depth all the content to generate a summary.",
          "name": "ExtensiveSummary",
          "raw": "\n            workflow ExtensiveSummary v0.1 {\n                step Initialize {\n                    $PROMPT = \"Summarize this: \"\n                    $EMBEDDINGS = call process_embeddings_in_job_scope()\n                }\n                step Summarize {\n                    $RESULT = call multi_inference($PROMPT, $EMBEDDINGS)\n                }\n            }\n        ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$PROMPT",
                        "value": "Summarize this: "
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$EMBEDDINGS",
                        "value": {
                          "args": [],
                          "name": "process_embeddings_in_job_scope"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Initialize"
            },
            {
              "body": [
                {
                  "type": "registeroperation",
                  "value": {
                    "register": "$RESULT",
                    "value": {
                      "args": [
                        {
                          "type": "register",
                          "value": "$PROMPT"
                        },
                        {
                          "type": "register",
                          "value": "$EMBEDDINGS"
                        }
                      ],
                      "name": "multi_inference"
                    }
                  }
                }
              ],
              "name": "Summarize"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.034734420478343964,
        0.1871805489063263,
        0.044480107724666595,
        -0.07555993646383286,
        -0.2762329578399658,
        0.24182763695716858,
        -1.033287763595581,
        0.7125799655914307,
        0.47229233384132385,
        -0.4209500551223755,
        -0.46268194913864136,
        0.9547873139381409,
        -0.20145152509212494,
        0.4152549207210541,
        0.14341086149215698,
        0.12008798867464066,
        0.05905970185995102,
        -0.5367689728736877,
        -1.7866318225860596,
        -0.619709849357605,
        -0.29154425859451294,
        0.8294081091880798,
        0.628642201423645,
        0.07314151525497437,
        0.19572168588638306,
        -0.034623607993125916,
        0.421909362077713,
        -0.24239057302474976,
        -0.9496891498565674,
        -1.6145397424697876,
        0.5303781628608704,
        0.32453474402427673,
        -0.28177550435066223,
        -0.4890628457069397,
        -0.38026148080825806,
        -1.1231189966201782,
        0.40186482667922974,
        -0.2134477198123932,
        -0.36821743845939636,
        0.05925258994102478,
        0.27076995372772217,
        0.20357921719551086,
        -0.021091710776090622,
        0.15783512592315674,
        0.5020970702171326,
        -0.6582558751106262,
        0.3204505741596222,
        0.09018649160861969,
        0.5070585012435913,
        0.06716383248567581,
        -0.38541632890701294,
        -0.6742196679115295,
        0.2859421670436859,
        -0.004317570477724075,
        -0.24483773112297058,
        -0.1230064108967781,
        0.3691105842590332,
        -0.6520802974700928,
        0.32098183035850525,
        -0.2527220845222473,
        -0.21936717629432678,
        0.3374263346195221,
        -3.8076820373535156,
        0.30166298151016235,
        0.3135705590248108,
        -0.06931666284799576,
        -0.0042413342744112015,
        -0.2929495871067047,
        0.3975790739059448,
        0.422590434551239,
        0.1426510214805603,
        0.4197009801864624,
        -0.5708262920379639,
        0.412312775850296,
        0.11728336662054062,
        0.03570561856031418,
        -0.45046818256378174,
        0.11172579228878021,
        0.17126746475696564,
        -0.3517521619796753,
        0.4710276126861572,
        0.4825541377067566,
        -0.06425526738166809,
        0.15817689895629883,
        -0.041283655911684036,
        0.5767968893051147,
        -0.03386301547288895,
        0.17241227626800537,
        0.05743739381432533,
        -0.10351618379354477,
        -0.6282508373260498,
        0.2978358268737793,
        0.30152881145477295,
        -0.24568328261375427,
        -0.20568302273750305,
        0.19630080461502075,
        0.016544099897146225,
        0.01819857954978943,
        -0.2418222427368164,
        3.4505109786987305,
        -0.18223853409290314,
        0.26296675205230713,
        0.7941957712173462,
        -0.5534366369247437,
        0.7853090763092041,
        -0.0009117536246776581,
        -0.2845374643802643,
        -0.6472243070602417,
        -0.1668383777141571,
        -0.3792476952075958,
        0.4950440526008606,
        -0.9411168694496155,
        -0.5924868583679199,
        0.31550318002700806,
        0.18666428327560425,
        0.6844877600669861,
        -0.4719914197921753,
        0.09086611866950989,
        0.016701843589544296,
        0.9475106000900269,
        -0.503623366355896,
        -0.026559878140687943,
        -0.5337488055229187,
        -0.3949074447154999,
        -0.15773630142211914,
        0.07483159005641937,
        -0.21122387051582336,
        0.6088380813598633,
        -0.529130220413208,
        -0.08318600058555603,
        0.3063376247882843,
        -0.335045725107193,
        -0.6450403332710266,
        -0.0591755211353302,
        0.16853100061416626,
        0.3160886764526367,
        0.6661991477012634,
        -1.21894109249115,
        -0.17258977890014648,
        -0.43191951513290405,
        0.4885932207107544,
        -1.345179796218872,
        0.6688281297683716,
        -0.3229002058506012,
        0.6696915626525879,
        0.12988126277923584,
        -0.7549917697906494,
        0.061022330075502396,
        -0.6575156450271606,
        -0.6167019605636597,
        -0.41561612486839294,
        0.42765122652053833,
        -0.40447378158569336,
        0.26708102226257324,
        0.8733229041099548,
        0.8704780340194702,
        -0.2516751289367676,
        -0.19926860928535461,
        -0.39761170744895935,
        0.7005975246429443,
        -0.27293720841407776,
        -0.09869083762168884,
        0.04231929033994675,
        0.1726655662059784,
        0.6348462700843811,
        -0.31958454847335815,
        0.5049365758895874,
        0.12581421434879303,
        0.3368198275566101,
        0.25203222036361694,
        -0.028484106063842773,
        0.006750665605068207,
        0.20156043767929077,
        0.8802384734153748,
        -0.577757716178894,
        0.005151968449354172,
        0.44799765944480896,
        -0.13738900423049927,
        0.06280632317066193,
        -0.6039809584617615,
        0.051666975021362305,
        0.5224025249481201,
        -0.28239843249320984,
        -0.5803446173667908,
        0.24665649235248566,
        0.405037522315979,
        -0.04525134712457657,
        0.5046746134757996,
        0.2667279839515686,
        0.7985939979553223,
        -0.8122603893280029,
        1.8872915506362915,
        -1.0178728103637695,
        0.2591046690940857,
        -0.4069160521030426,
        -0.057505037635564804,
        0.35820120573043823,
        0.34579163789749146,
        0.05497463792562485,
        -0.01758282631635666,
        -0.6904863119125366,
        -0.5490170121192932,
        -0.2188713699579239,
        -0.1431823968887329,
        -0.3400297164916992,
        -0.9551897644996643,
        0.4477522373199463,
        0.9675039649009705,
        -0.22031156718730927,
        -0.5745877027511597,
        -0.24293725192546844,
        0.04381025955080986,
        1.0013937950134277,
        0.1482800394296646,
        0.8249431848526001,
        0.49105530977249146,
        0.5536376237869263,
        -0.4726281762123108,
        -0.02756471000611782,
        0.7716965675354004,
        -0.1321134865283966,
        0.1318313181400299,
        -0.17074808478355408,
        -0.8118672370910645,
        -0.3616568148136139,
        0.5766054391860962,
        -0.09477080404758453,
        0.64303058385849,
        -0.20550958812236786,
        -0.03641514107584953,
        0.004146404564380646,
        1.1218799352645874,
        0.23001694679260254,
        0.9742380976676941,
        0.10956323146820068,
        0.15432007610797882,
        -0.20407500863075256,
        0.3217419981956482,
        0.012776657938957214,
        -0.863412618637085,
        -0.12214770168066025,
        -0.1470094621181488,
        0.18791575729846954,
        -0.10743652284145355,
        -0.016279935836791992,
        -0.48529407382011414,
        0.3650325536727905,
        -0.27987104654312134,
        -0.6158308982849121,
        1.834641695022583,
        0.7462397813796997,
        -0.05483162775635719,
        0.18829205632209778,
        0.8982114791870117,
        0.2531103789806366,
        -0.04746251553297043,
        -1.516945242881775,
        -0.09371230006217957,
        -0.49885794520378113,
        0.9522011280059814,
        0.143731951713562,
        -0.012899696826934814,
        0.84458988904953,
        -0.4186297655105591,
        0.06162244454026222,
        -0.3803642988204956,
        0.03807999938726425,
        -0.5136410593986511,
        -0.3748040497303009,
        -0.3351418375968933,
        -0.29277485609054565,
        0.8687057495117188,
        -0.32287031412124634,
        0.04512590914964676,
        -0.3529585897922516,
        -0.009137822315096855,
        0.24435634911060333,
        -0.4654300808906555,
        -0.26127246022224426,
        -0.23510973155498505,
        0.26223838329315186,
        -0.35903236269950867,
        -0.5135477781295776,
        0.4223664402961731,
        0.12143268436193466,
        0.13796232640743256,
        -0.872850239276886,
        -0.7502076625823975,
        0.13137716054916382,
        0.25962790846824646,
        -0.3763854205608368,
        -0.23446759581565857,
        -0.894822895526886,
        0.4070824384689331,
        1.9461617469787598,
        -0.17248550057411194,
        -0.4007423520088196,
        0.5539323091506958,
        0.1676577478647232,
        -0.10842078924179077,
        0.47942718863487244,
        -0.17142406105995178,
        -0.15705609321594238,
        -0.18135124444961548,
        -0.7034934163093567,
        -0.24892881512641907,
        0.6359640955924988,
        0.27162688970565796,
        -0.057023391127586365,
        -0.18354308605194092,
        -0.6973524689674377,
        0.26395007967948914,
        0.1207042932510376,
        0.4024277627468109,
        0.40282028913497925,
        -0.303811639547348,
        0.3677162230014801,
        0.7304520606994629,
        -0.04565465822815895,
        -1.9546071290969849,
        -0.24198698997497559,
        0.3390151560306549,
        -0.4251219928264618,
        -0.37752872705459595,
        -0.5364102721214294,
        0.317706823348999,
        -0.13012170791625977,
        0.5919176936149597,
        -0.41882485151290894,
        1.0903478860855103,
        0.6279439926147461,
        -0.18956951797008514,
        0.1847759187221527,
        -0.03504883870482445,
        1.0964027643203735,
        -0.2120370864868164,
        0.19891658425331116,
        0.32237040996551514,
        -0.2026764154434204,
        0.21574266254901886,
        0.5093711614608765,
        0.9958378672599792,
        0.22808237373828888,
        0.21431100368499756,
        -0.2564947307109833,
        0.29420459270477295,
        -0.3300313949584961,
        -1.2673944234848022,
        -0.032171063125133514,
        -0.6294702887535095,
        0.15244247019290924,
        1.147212028503418,
        -0.09756116569042206,
        0.12227030843496323,
        0.2238313853740692,
        0.6322968006134033,
        0.23951034247875214,
        0.08459635078907013,
        0.025344640016555786,
        1.7545489072799683,
        -0.10636207461357117,
        -0.47393617033958435,
        -0.44828516244888306,
        -0.5145392417907715,
        -0.42072558403015137,
        0.08383013308048248,
        0.010295245796442032,
        -0.3179120719432831,
        0.1415541172027588,
        0.5705311894416809,
        -0.3058169484138489,
        -0.29502421617507935,
        0.26835861802101135,
        0.2325461059808731,
        0.6640745401382446,
        -0.053470537066459656,
        -0.28660848736763,
        1.2071852684020996,
        -0.08734236657619476,
        -0.2880823016166687,
        0.26413995027542114,
        -0.02680138871073723,
        -0.39567938446998596,
        -0.17057590186595917
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates a passage to answer a question and uses embeddings to refine the answer.",
          "name": "HydeInference",
          "raw": "\n            workflow HydeInference v0.1 {\n                step Initialize {\n                    $PROMPT = \"write a passage to answer the question: \"\n                    $HYDE_PROMPT = call concat($PROMPT, $INPUT)\n                    $HYDE_PASSAGE = call inference_no_ws($HYDE_PROMPT)\n                    $HYDE_INPUT = call concat($INPUT, \". \", $HYDE_PASSAGE )\n                    $EMBEDDINGS = call search_embeddings_in_job_scope($HYDE_INPUT)\n                }\n                step Summarize {\n                    $CONNECTOR = \"\\nLeverage the following information to answer the previous query: --- start ---\"\n                    $NEW_INPUT = call concat($INPUT, $CONNECTOR, $EMBEDDINGS) \n                    $RESULT = call inference($NEW_INPUT)\n                }\n            }\n        ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$PROMPT",
                        "value": "write a passage to answer the question: "
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_PROMPT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$PROMPT"
                            },
                            {
                              "type": "register",
                              "value": "$INPUT"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_PASSAGE",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$HYDE_PROMPT"
                            }
                          ],
                          "name": "inference_no_ws"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$HYDE_INPUT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "string",
                              "value": ". "
                            },
                            {
                              "type": "register",
                              "value": "$HYDE_PASSAGE"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$EMBEDDINGS",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$HYDE_INPUT"
                            }
                          ],
                          "name": "search_embeddings_in_job_scope"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Initialize"
            },
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CONNECTOR",
                        "value": "\\nLeverage the following information to answer the previous query: --- start ---"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$NEW_INPUT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$CONNECTOR"
                            },
                            {
                              "type": "register",
                              "value": "$EMBEDDINGS"
                            }
                          ],
                          "name": "concat"
                        }
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$NEW_INPUT"
                            }
                          ],
                          "name": "inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Summarize"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5217975974082947,
        0.6705355048179626,
        -0.24365361034870148,
        0.2630804479122162,
        -0.055158816277980804,
        -0.027416400611400604,
        -0.8448842167854309,
        0.4957970976829529,
        0.026166729629039764,
        0.0568583607673645,
        -0.06262187659740448,
        1.0268981456756592,
        -0.17997652292251587,
        -0.5957084894180298,
        0.5241677165031433,
        -0.073320172727108,
        0.483111172914505,
        -1.0835824012756348,
        -1.6435885429382324,
        -0.2773756682872772,
        -0.3268052041530609,
        0.09233829379081726,
        0.570166289806366,
        0.27032995223999023,
        0.1990419626235962,
        -0.04393035173416138,
        -0.46693891286849976,
        -0.3368980884552002,
        -0.7220626473426819,
        -1.871181607246399,
        0.6739394068717957,
        0.5323552489280701,
        0.21472512185573578,
        -0.5961694121360779,
        0.36276042461395264,
        -0.6659478545188904,
        0.031285662204027176,
        -0.3852560520172119,
        -0.3787482678890228,
        0.07704560458660126,
        0.2723124027252197,
        0.1442337930202484,
        0.13672783970832825,
        -0.18554916977882385,
        0.21989686787128448,
        -0.18905243277549744,
        0.1322297304868698,
        -0.4713946282863617,
        0.9121119379997253,
        0.5810496807098389,
        -0.07193005830049515,
        -0.7029728293418884,
        -0.13997705280780792,
        -0.6111346483230591,
        -0.8305694460868835,
        -0.7269783616065979,
        0.1714978665113449,
        -0.5532603859901428,
        0.4187415540218353,
        0.1591222733259201,
        0.5274844169616699,
        0.8996009230613708,
        -3.16033673286438,
        0.08123601973056793,
        0.06916505843400955,
        0.244430273771286,
        0.6882338523864746,
        -0.6952429413795471,
        0.33364158868789673,
        -0.2358098328113556,
        0.26270928978919983,
        -0.0416974276304245,
        -0.3559892773628235,
        1.0462677478790283,
        -0.002749621868133545,
        -0.2537255585193634,
        -0.06800861656665802,
        -0.02359657734632492,
        0.4327777624130249,
        0.6239508986473083,
        0.14622169733047485,
        0.03531741350889206,
        -0.39679116010665894,
        -0.977078914642334,
        -0.936194896697998,
        0.5409996509552002,
        -1.0760374069213867,
        0.32848450541496277,
        0.6311944127082825,
        0.14159080386161804,
        -0.06356936693191528,
        -0.39095622301101685,
        0.18491660058498383,
        -0.2065873146057129,
        -0.4836304783821106,
        0.11969326436519623,
        0.40857362747192383,
        -0.32239317893981934,
        -0.6118934750556946,
        3.3892714977264404,
        0.48266690969467163,
        0.07768476009368896,
        0.2956967353820801,
        -1.3127696514129639,
        0.40277037024497986,
        -0.2261715531349182,
        0.21370524168014526,
        -0.49305880069732666,
        0.06676560640335083,
        0.48641473054885864,
        0.5203590989112854,
        -1.3259339332580566,
        -0.4931930601596832,
        -0.08241058886051178,
        0.4220368266105652,
        0.4514558017253876,
        -0.6090209484100342,
        -0.09512780606746674,
        0.22303397953510284,
        0.7214317917823792,
        -0.10309116542339325,
        -0.2423461377620697,
        -0.6361504793167114,
        -0.03672082722187042,
        0.39151379466056824,
        0.380852073431015,
        0.02322957292199135,
        0.8070581555366516,
        0.20471300184726715,
        0.1267378181219101,
        -0.6727452278137207,
        -0.515336275100708,
        0.055332351475954056,
        0.08860611170530319,
        -0.23885050415992737,
        0.2597174644470215,
        0.7751559615135193,
        -1.4895457029342651,
        0.4119921028614044,
        -0.8133742809295654,
        0.43248221278190613,
        -1.0600495338439941,
        0.9090545773506165,
        0.1412292718887329,
        0.6213206052780151,
        0.2562440037727356,
        0.3154059648513794,
        0.6048558354377747,
        -0.4115428328514099,
        -0.5538619160652161,
        -0.19729755818843842,
        0.28426289558410645,
        -0.3783497214317322,
        0.7542127370834351,
        0.15272271633148193,
        -0.039394717663526535,
        0.19333487749099731,
        -0.6590179800987244,
        -0.7701877951622009,
        0.3972232937812805,
        0.16648125648498535,
        -0.02083461917936802,
        0.05433003604412079,
        -0.30666208267211914,
        0.35323965549468994,
        0.1256871372461319,
        0.49710002541542053,
        0.1082662045955658,
        -0.29052698612213135,
        -0.17132066190242767,
        0.02150486409664154,
        -0.24848806858062744,
        0.5935348272323608,
        0.3625210225582123,
        -0.1457476168870926,
        -0.28131651878356934,
        -0.1326143443584442,
        -0.14821875095367432,
        0.379703164100647,
        -0.16204044222831726,
        0.960371732711792,
        0.43096446990966797,
        -0.27280333638191223,
        -0.7327988743782043,
        -0.4692484140396118,
        0.3843023478984833,
        0.47834956645965576,
        0.24101340770721436,
        0.3751585781574249,
        0.24800710380077362,
        -0.9193392395973206,
        1.8324761390686035,
        -0.689822793006897,
        -0.3209829032421112,
        -0.07334859669208527,
        -0.11310294270515442,
        0.37197279930114746,
        0.3572532534599304,
        0.6516865491867065,
        -0.049167100340127945,
        -1.193394422531128,
        0.4415071904659271,
        -0.6154524087905884,
        -0.01581019163131714,
        -0.2254577875137329,
        -0.4738317131996155,
        -0.14066429436206818,
        0.6105179786682129,
        -0.3202188014984131,
        0.06671469658613205,
        -0.6770957708358765,
        -0.05336727201938629,
        1.168273687362671,
        0.04696497321128845,
        1.2656526565551758,
        0.20630161464214325,
        0.4834672212600708,
        0.3591012954711914,
        0.36739975214004517,
        0.2938002645969391,
        0.19798046350479126,
        0.23621299862861633,
        0.2226957231760025,
        -0.7855562567710876,
        -0.7126330137252808,
        0.3147484064102173,
        -0.7240287065505981,
        0.3580595850944519,
        -0.32516440749168396,
        -0.40975701808929443,
        0.6795403361320496,
        1.6100436449050903,
        1.0999537706375122,
        0.5577028393745422,
        -0.4695652723312378,
        1.0390901565551758,
        -0.44631320238113403,
        0.47257986664772034,
        0.4145084023475647,
        -1.1369632482528687,
        0.5121796131134033,
        -0.0441184937953949,
        0.4039679169654846,
        0.5458258986473083,
        0.5670212507247925,
        0.8615117073059082,
        -0.657166063785553,
        -0.351988285779953,
        0.12625384330749512,
        0.7112622261047363,
        0.4821208715438843,
        -0.1724041998386383,
        0.24358856678009033,
        -0.10205470025539398,
        0.06550612300634384,
        0.19324445724487305,
        -1.5678365230560303,
        -0.12132494896650314,
        -0.9552805423736572,
        0.7691149115562439,
        -0.12316227704286575,
        -0.45294782519340515,
        0.4233235716819763,
        -0.29440659284591675,
        0.27920567989349365,
        -0.09119509160518646,
        -0.8426341414451599,
        -0.19401296973228455,
        -0.1196584552526474,
        0.009854890406131744,
        0.8042097091674805,
        0.11656402796506882,
        -0.7586128115653992,
        0.07545974850654602,
        -0.5649018287658691,
        -0.17164397239685059,
        0.22609321773052216,
        -0.05334358662366867,
        0.19416964054107666,
        -0.1430165022611618,
        0.6216332912445068,
        -0.0824175626039505,
        -0.7341193556785583,
        0.11193397641181946,
        0.13833469152450562,
        -0.562166154384613,
        -0.3522283732891083,
        -1.0460312366485596,
        -0.6479171514511108,
        0.8833339810371399,
        -0.2818920910358429,
        0.031444236636161804,
        -1.2941651344299316,
        -0.3253493905067444,
        1.3333481550216675,
        0.652028501033783,
        0.35527488589286804,
        0.5304181575775146,
        0.2921711206436157,
        -0.25260961055755615,
        -0.06883727759122849,
        0.35716578364372253,
        0.4548536539077759,
        -0.6085742712020874,
        -1.06727933883667,
        0.15465646982192993,
        0.20243129134178162,
        -0.06389427930116653,
        0.09642645716667175,
        0.5321649312973022,
        -0.7393209338188171,
        0.2277315855026245,
        -0.29905083775520325,
        -0.6217883229255676,
        0.5628343224525452,
        0.18441921472549438,
        0.2999612092971802,
        1.6529536247253418,
        0.30236950516700745,
        -1.4994771480560303,
        -0.4032038450241089,
        0.2655697464942932,
        -0.07651287317276001,
        -0.19933074712753296,
        0.40059909224510193,
        0.1620161086320877,
        -0.08868397772312164,
        0.19656537473201752,
        -0.5545854568481445,
        1.372854471206665,
        0.5722429752349854,
        0.0751008614897728,
        -0.23768703639507294,
        0.25020307302474976,
        0.3622342348098755,
        0.15045976638793945,
        0.2029159814119339,
        -0.046639181673526764,
        -0.33883950114250183,
        -0.3241194486618042,
        0.46329277753829956,
        1.0365190505981445,
        0.6351235508918762,
        -0.029345136135816574,
        -0.27866604924201965,
        0.041748736053705215,
        -0.6484447717666626,
        -0.5143448114395142,
        0.27627843618392944,
        -0.27804455161094666,
        -0.5963647365570068,
        0.46267515420913696,
        -0.12084157019853592,
        -0.1146276667714119,
        0.9631665945053101,
        0.7625371217727661,
        -1.0228558778762817,
        0.1971416026353836,
        -0.6056783199310303,
        1.800622820854187,
        -0.6298641562461853,
        -0.11450319737195969,
        -0.3849523067474365,
        0.5427781939506531,
        -0.7251048684120178,
        0.3562368154525757,
        -0.26989448070526123,
        -0.8379263877868652,
        0.03333130478858948,
        0.12300238013267517,
        -0.6583588123321533,
        -0.13887548446655273,
        0.19580218195915222,
        0.7095918655395508,
        0.3531382381916046,
        -0.007668033242225647,
        -0.16812410950660706,
        0.1320769339799881,
        0.3730332553386688,
        0.2985376715660095,
        -0.00826168805360794,
        -0.36709192395210266,
        0.0038701295852661133,
        -0.20109312236309052
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates user stories and acceptance criteria for specified topics, focusing on Agile framework principles. This prompt specializes in translating topics into structured Agile documentation, specifically for user story and acceptance criteria creation. The expected output is a JSON-formatted document detailing the topic, user story, and acceptance criteria.",
          "name": "Agility_story",
          "raw": "\n                workflow Agility_story v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert in the Agile framework. You deeply understand user story and acceptance criteria creation. You will be given a topic. Please write the appropriate information for what is requested. \n\n# STEPS\n\nPlease write a user story and acceptance criteria for the requested topic.\n\n# OUTPUT INSTRUCTIONS\n\nOutput the results in JSON format as defined in this example:\n\n{\n    \\\"Topic\\\": \\\"Automating data quality automation\\\",\n    \\\"Story\\\": \\\"As a user, I want to be able to create a new user account so that I can access the system.\\\",\n    \\\"Criteria\\\": \\\"Given that I am a user, when I click the 'Create Account' button, then I should be prompted to enter my email address, password, and confirm password. When I click the 'Submit' button, then I should be redirected to the login page.\\\"\n}\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert in the Agile framework. You deeply understand user story and acceptance criteria creation. You will be given a topic. Please write the appropriate information for what is requested. \n\n# STEPS\n\nPlease write a user story and acceptance criteria for the requested topic.\n\n# OUTPUT INSTRUCTIONS\n\nOutput the results in JSON format as defined in this example:\n\n{\n    \\\"Topic\\\": \\\"Automating data quality automation\\\",\n    \\\"Story\\\": \\\"As a user, I want to be able to create a new user account so that I can access the system.\\\",\n    \\\"Criteria\\\": \\\"Given that I am a user, when I click the 'Create Account' button, then I should be prompted to enter my email address, password, and confirm password. When I click the 'Submit' button, then I should be redirected to the login page.\\\"\n}\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.3428054451942444,
        0.03866085410118103,
        0.41775041818618774,
        -0.01605895347893238,
        -0.09449943900108337,
        0.6106385588645935,
        -0.31226658821105957,
        0.007585853338241577,
        0.6044677495956421,
        0.20776838064193726,
        0.36655136942863464,
        0.41088545322418213,
        0.10963139683008194,
        0.06272315233945847,
        0.010214671492576599,
        0.5370876789093018,
        0.3638383448123932,
        -0.8063386082649231,
        -2.0733535289764404,
        -0.528529167175293,
        -0.5109792947769165,
        0.6783691048622131,
        0.5277155041694641,
        0.18505319952964783,
        0.9546823501586914,
        -0.06108616665005684,
        -0.6761879920959473,
        -0.06055661663413048,
        -0.33285078406333923,
        -1.883054256439209,
        0.5693835020065308,
        -0.012364774942398071,
        -0.5147796273231506,
        -0.08942578732967377,
        -0.2608434557914734,
        -0.9872782230377197,
        0.5939499735832214,
        -0.3333594501018524,
        -0.0539717972278595,
        0.05734632909297943,
        0.01800324209034443,
        0.5125158429145813,
        -0.23019623756408691,
        -0.15513285994529724,
        0.6009128093719482,
        -0.12644267082214355,
        0.007168503012508154,
        -0.42604753375053406,
        0.5995544195175171,
        0.022552579641342163,
        -0.3313075602054596,
        -0.2956133186817169,
        -0.08683785796165466,
        0.3756023347377777,
        0.31052514910697937,
        -0.04154404625296593,
        0.034954365342855453,
        -0.5528451204299927,
        0.30860739946365356,
        -0.16156893968582153,
        0.15515980124473572,
        0.20684519410133362,
        -3.2603073120117188,
        -0.06932282447814941,
        0.5705702900886536,
        -0.4820181727409363,
        0.314455509185791,
        -0.45494893193244934,
        0.4984338879585266,
        0.2908429205417633,
        0.692837655544281,
        0.23836636543273926,
        -0.26792892813682556,
        0.1280771791934967,
        -0.061339862644672394,
        0.04495587199926376,
        -0.1819155067205429,
        -0.13920095562934875,
        0.27868735790252686,
        0.12680505216121674,
        0.3798983097076416,
        0.41730788350105286,
        0.5177871584892273,
        -0.10246676951646805,
        -0.7212740182876587,
        -0.038976624608039856,
        -0.48895496129989624,
        -0.39610350131988525,
        0.5769237279891968,
        -0.05295705050230026,
        -0.8578556776046753,
        -0.6888695359230042,
        0.29328152537345886,
        -0.3379535377025604,
        0.06336496770381927,
        -0.03118245303630829,
        -0.22068089246749878,
        0.1490316540002823,
        -0.27215704321861267,
        3.378279447555542,
        0.864726185798645,
        0.3134961724281311,
        0.33354803919792175,
        -0.8107597827911377,
        1.053902268409729,
        -0.5533229112625122,
        -0.3375650644302368,
        -0.738100528717041,
        -0.18355366587638855,
        0.11040027439594269,
        0.23141390085220337,
        -0.6924493312835693,
        -0.14983877539634705,
        -0.5328608155250549,
        -0.07181211560964584,
        0.5458509922027588,
        -0.3618062734603882,
        -0.0526835061609745,
        0.28980469703674316,
        0.3401276469230652,
        -0.510769784450531,
        -0.3767065405845642,
        -0.4752456247806549,
        -0.27086779475212097,
        -0.028741925954818726,
        0.3263552784919739,
        -0.8796581625938416,
        0.7185429334640503,
        0.30725347995758057,
        0.32216504216194153,
        -0.025172583758831024,
        -0.22442872822284698,
        -0.21303582191467285,
        -0.09390129894018173,
        -0.10396872460842133,
        -0.49420827627182007,
        1.1788837909698486,
        -0.9073540568351746,
        -0.0046709999442100525,
        -0.9114504456520081,
        -0.24450689554214478,
        -0.413149356842041,
        0.16471394896507263,
        -0.04045125097036362,
        0.3361339569091797,
        0.27191340923309326,
        0.03143700957298279,
        -0.2058822214603424,
        -0.4093073606491089,
        -0.11446692049503326,
        -0.027314234524965286,
        0.6784094572067261,
        -0.5419620275497437,
        0.03289572149515152,
        0.35360419750213623,
        0.04349586367607117,
        -0.361014723777771,
        0.5714007616043091,
        -1.2047706842422485,
        0.533038854598999,
        0.10833534598350525,
        -0.19877634942531586,
        0.4054044485092163,
        -1.1437088251113892,
        -0.16406118869781494,
        -0.43898019194602966,
        -0.03429384529590607,
        -0.7926796078681946,
        0.14365768432617188,
        0.6258991956710815,
        -0.7311756014823914,
        -0.12348497658967972,
        0.9075385928153992,
        0.7492731213569641,
        -0.29127734899520874,
        -0.09844902157783508,
        1.0602703094482422,
        0.40608301758766174,
        0.2736573815345764,
        -0.7529809474945068,
        0.580346941947937,
        0.5886685848236084,
        0.34134921431541443,
        -0.7428935766220093,
        -0.34005624055862427,
        0.14450351893901825,
        0.23504787683486938,
        0.31227728724479675,
        1.3499425649642944,
        1.2890241146087646,
        -0.8082308769226074,
        1.9228951930999756,
        -0.6600373983383179,
        -0.012605134397745132,
        0.19221729040145874,
        0.5423110127449036,
        -0.6752273440361023,
        0.46030765771865845,
        0.570067286491394,
        0.09232255816459656,
        -0.715363621711731,
        -0.36144569516181946,
        0.02179138921201229,
        -0.1939896047115326,
        -0.8528757095336914,
        -0.8021336793899536,
        0.23061886429786682,
        0.3735615313053131,
        -0.10317753255367279,
        -0.5512841939926147,
        -0.5961991548538208,
        -0.18616235256195068,
        0.9974435567855835,
        0.2032141387462616,
        0.7418451309204102,
        0.020353808999061584,
        0.7367028594017029,
        -0.3491448760032654,
        -0.07404247671365738,
        0.4901484251022339,
        -0.5202524662017822,
        0.12428368628025055,
        -0.9408085942268372,
        -0.8323876261711121,
        -0.3724387586116791,
        0.7477765083312988,
        -0.1749323606491089,
        0.06903387606143951,
        -0.7924975156784058,
        -0.25073689222335815,
        -0.13709750771522522,
        0.7699073553085327,
        1.1066200733184814,
        1.178780436515808,
        -0.33458009362220764,
        0.5045337080955505,
        -0.24590402841567993,
        0.5444666147232056,
        -0.2669197916984558,
        -0.46196597814559937,
        0.9866401553153992,
        -0.09291468560695648,
        0.15276606380939484,
        0.0389309823513031,
        0.5659542083740234,
        0.5331438779830933,
        -0.6321559548377991,
        -0.13671112060546875,
        0.08541993051767349,
        1.3469786643981934,
        0.6049937009811401,
        -0.13564376533031464,
        0.3265670835971832,
        0.4495912790298462,
        -0.07641013711690903,
        0.20666295289993286,
        -1.1360878944396973,
        0.22481372952461243,
        -0.6936838626861572,
        -0.26533621549606323,
        -0.3201341927051544,
        -0.4231497645378113,
        0.9217140078544617,
        -0.0956549271941185,
        0.45372748374938965,
        -0.4966159164905548,
        -0.15856720507144928,
        -0.4439672529697418,
        -0.2905527949333191,
        -0.3438781201839447,
        -0.10684677213430405,
        0.13544727861881256,
        -0.5838370323181152,
        -0.46846020221710205,
        0.4384390413761139,
        0.38303330540657043,
        -0.18701978027820587,
        0.011981360614299774,
        -0.5500902533531189,
        -0.41482341289520264,
        0.3059193789958954,
        -0.2627328634262085,
        -0.8056966662406921,
        0.18472495675086975,
        -0.48793792724609375,
        -0.16895513236522675,
        -0.2845296561717987,
        -0.8456162810325623,
        -0.4589991569519043,
        0.4643819332122803,
        -0.31865808367729187,
        -0.6263881921768188,
        -0.8166449666023254,
        -0.14915621280670166,
        1.1098196506500244,
        0.7868095636367798,
        -0.09166283905506134,
        0.7074795365333557,
        0.8965145945549011,
        -0.34217870235443115,
        -0.2035350203514099,
        0.19964271783828735,
        -0.12371373921632767,
        0.4662665128707886,
        -0.22154900431632996,
        -0.21768811345100403,
        0.5774257183074951,
        0.9694229364395142,
        -0.12972301244735718,
        0.24581289291381836,
        -1.5719057321548462,
        0.03507589176297188,
        0.24207288026809692,
        0.027516145259141922,
        0.3909359872341156,
        -0.4758828282356262,
        0.567496120929718,
        0.8941217660903931,
        -0.30722251534461975,
        -1.7504013776779175,
        -0.40954849123954773,
        0.2317785620689392,
        0.16918450593948364,
        -0.5724758505821228,
        0.12299054861068726,
        0.281677782535553,
        0.228120356798172,
        0.16629306972026825,
        0.05292189121246338,
        1.3996717929840088,
        0.6686640381813049,
        0.3179728388786316,
        0.19663037359714508,
        -0.19406186044216156,
        0.26490139961242676,
        -0.23913586139678955,
        0.1984461098909378,
        -0.12282944470643997,
        -0.33063438534736633,
        -0.4808770716190338,
        0.6897615790367126,
        1.207756519317627,
        0.37293490767478943,
        0.3323201537132263,
        0.20776396989822388,
        -0.09532961994409561,
        -0.26945438981056213,
        -0.6412152647972107,
        -0.28236618638038635,
        0.07461406290531158,
        -0.05551967769861221,
        0.9219731688499451,
        -0.32000163197517395,
        0.13185453414916992,
        0.9842860102653503,
        0.4971121549606323,
        0.3434644639492035,
        -0.11070800572633743,
        -0.19118165969848633,
        1.7809334993362427,
        -0.34881600737571716,
        0.2533555328845978,
        0.015120275318622589,
        0.3646857440471649,
        -0.2785719931125641,
        0.4676916003227234,
        0.19902892410755157,
        -0.5152252316474915,
        -0.4807511568069458,
        0.46846601366996765,
        -0.011332668364048004,
        -0.23992806673049927,
        1.333131194114685,
        0.08054594695568085,
        0.7794668674468994,
        0.12027941644191742,
        -0.11772145330905914,
        0.257524311542511,
        0.029995329678058624,
        -0.416168212890625,
        0.7542387247085571,
        -0.8539779186248779,
        -0.2967199683189392,
        -0.8406996726989746
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes and responds to questions with insightful bullet points. It involves creating a mental model of the question for deeper understanding. The output consists of 3-5 concise bullet points, each with a 10-word limit.",
          "name": "Ai",
          "raw": "\n                workflow Ai v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at interpreting the heart and spirit of a question and answering in an insightful manner.\n\n# STEPS\n\n- Deeply understand what's being asked.\n\n- Create a full mental model of the input and the question on a virtual whiteboard in your mind.\n\n- Answer the question in 3-5 Markdown bullets of 10 words each.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown bullets.\n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at interpreting the heart and spirit of a question and answering in an insightful manner.\n\n# STEPS\n\n- Deeply understand what's being asked.\n\n- Create a full mental model of the input and the question on a virtual whiteboard in your mind.\n\n- Answer the question in 3-5 Markdown bullets of 10 words each.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown bullets.\n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.646282434463501,
        0.13277891278266907,
        -0.30311042070388794,
        0.6605469584465027,
        0.06080642715096474,
        -0.3152938187122345,
        -1.0236245393753052,
        0.4240182638168335,
        0.15711790323257446,
        -0.05823225528001785,
        -0.6907446980476379,
        0.6173399686813354,
        0.07961497455835342,
        -0.45207899808883667,
        -0.17325043678283691,
        -0.07261678576469421,
        -0.017501305788755417,
        -0.48357725143432617,
        -1.7163550853729248,
        -0.24607786536216736,
        0.07805025577545166,
        0.3899989724159241,
        0.2023497074842453,
        0.15264999866485596,
        0.9190335869789124,
        -0.1935914009809494,
        0.05770309641957283,
        -0.17604835331439972,
        -0.960445761680603,
        -1.0097992420196533,
        0.5524052381515503,
        0.3721969425678253,
        -0.7324222326278687,
        -0.23394812643527985,
        -0.009867705404758453,
        -0.600862979888916,
        -0.01758771575987339,
        -0.158436119556427,
        -0.6666631102561951,
        -0.4453246593475342,
        -0.016434043645858765,
        0.44603538513183594,
        -0.18623408675193787,
        -0.7434185743331909,
        0.5861629843711853,
        -0.15832854807376862,
        -0.23704425990581512,
        -0.08685842156410217,
        0.9193522930145264,
        0.31173327565193176,
        0.24254043400287628,
        -0.8934468030929565,
        0.3164154887199402,
        -0.26537373661994934,
        -0.6088501214981079,
        -0.1194184273481369,
        0.22918522357940674,
        -0.45911234617233276,
        -0.17734448611736298,
        0.07100644707679749,
        0.44061240553855896,
        0.4810541272163391,
        -3.6452319622039795,
        -0.0685853511095047,
        -0.34365567564964294,
        -0.14203089475631714,
        0.15111178159713745,
        -0.006741385906934738,
        0.5648788213729858,
        0.09550827741622925,
        -0.5135360360145569,
        -0.12533177435398102,
        -0.11655401438474655,
        0.8070361018180847,
        0.485975444316864,
        0.4594498574733734,
        0.05708203464746475,
        -0.332086443901062,
        0.06702686846256256,
        -0.7664086818695068,
        -0.021497175097465515,
        -0.045491188764572144,
        0.4093027710914612,
        -0.43404293060302734,
        -0.2754228115081787,
        0.27510401606559753,
        -0.11863988637924194,
        0.3803281784057617,
        0.8117183446884155,
        0.023528605699539185,
        -0.10915149748325348,
        -0.2026536613702774,
        0.15338191390037537,
        -0.12552368640899658,
        0.10466922074556351,
        -0.46422600746154785,
        -0.40101945400238037,
        0.06320592015981674,
        0.2680862247943878,
        3.397852659225464,
        0.3635302186012268,
        -0.08439270406961441,
        0.4710915684700012,
        -0.8531408905982971,
        0.5370815992355347,
        -0.3348056674003601,
        -0.12149076163768768,
        -0.4943622946739197,
        0.37672141194343567,
        -0.035668544471263885,
        0.47125598788261414,
        -0.7545155882835388,
        -0.6777051687240601,
        0.5307621359825134,
        -0.1886003017425537,
        0.7970770597457886,
        -0.6136115789413452,
        -0.11021526157855988,
        0.028699055314064026,
        1.0055806636810303,
        -0.6462583541870117,
        0.1580176204442978,
        0.19364729523658752,
        -0.5315138697624207,
        0.3760472536087036,
        0.03488921374082565,
        -0.14498232305049896,
        0.8204860687255859,
        0.13786496222019196,
        0.7925893664360046,
        -0.12590107321739197,
        -0.15042921900749207,
        -0.5579794049263,
        -0.3827376961708069,
        0.2981889843940735,
        -0.28995364904403687,
        0.3982669711112976,
        -0.728502094745636,
        0.24486252665519714,
        -0.14203613996505737,
        -0.2039693295955658,
        -0.784058690071106,
        0.3675047755241394,
        0.15143901109695435,
        0.4966534674167633,
        0.11758208274841309,
        -0.36807623505592346,
        0.8726483583450317,
        -0.41468942165374756,
        -0.3610566556453705,
        -0.20512865483760834,
        0.5037238597869873,
        -0.5120529532432556,
        0.4308079183101654,
        0.647596538066864,
        0.4763602018356323,
        -0.5550065040588379,
        -0.1589941382408142,
        -0.521704375743866,
        -0.22166147828102112,
        -0.09716439992189407,
        -0.21569626033306122,
        -0.48488253355026245,
        -0.33192020654678345,
        1.1531232595443726,
        -0.2262241095304489,
        -0.5991427898406982,
        0.19988825917243958,
        0.5656291842460632,
        -0.12197175621986389,
        0.5294358730316162,
        0.021709442138671875,
        0.2656160593032837,
        0.8776324987411499,
        0.012211604975163937,
        -0.2248641848564148,
        0.16106298565864563,
        0.14184005558490753,
        0.1006915271282196,
        -0.7112900018692017,
        0.3299919366836548,
        0.6261923909187317,
        0.40807685256004333,
        -0.5136293768882751,
        0.20993123948574066,
        0.4952555298805237,
        0.5016681551933289,
        0.08383181691169739,
        0.9030924439430237,
        1.0161861181259155,
        -0.6672437787055969,
        2.24186635017395,
        -0.4948544502258301,
        -0.3436588943004608,
        -0.07144943624734879,
        -0.06930963695049286,
        -0.15493911504745483,
        0.11811163276433945,
        0.41990694403648376,
        -0.7427688241004944,
        -1.4970096349716187,
        -0.30834460258483887,
        -0.3886308968067169,
        -0.38672786951065063,
        -0.8081586956977844,
        -0.18064986169338226,
        -0.11848513782024384,
        -0.38802361488342285,
        0.39528220891952515,
        -0.6850060820579529,
        -0.652917742729187,
        -0.2465071678161621,
        0.7705214619636536,
        0.5633012056350708,
        0.5349111557006836,
        0.5428919792175293,
        0.4552191197872162,
        0.25131723284721375,
        -0.1697429120540619,
        0.3039718270301819,
        -0.38694536685943604,
        -0.1546235978603363,
        -0.4558079242706299,
        -0.6812922358512878,
        -0.8140990138053894,
        1.4275189638137817,
        -0.562688946723938,
        1.1596678495407104,
        -0.6593430042266846,
        -0.4193253815174103,
        0.4773489832878113,
        1.1266134977340698,
        0.8311046361923218,
        1.2667605876922607,
        0.00705169141292572,
        0.5585598349571228,
        -0.5971101522445679,
        0.3882298171520233,
        -0.2886916995048523,
        -1.1820687055587769,
        0.359154611825943,
        -0.3443162739276886,
        -0.2916547656059265,
        0.6659669280052185,
        -0.3998032510280609,
        0.4221765100955963,
        -0.6562409996986389,
        -0.6090112924575806,
        -0.530112624168396,
        0.8373754620552063,
        0.6742849946022034,
        0.024554159492254257,
        0.0658566877245903,
        0.5879895687103271,
        -0.054863110184669495,
        0.05004873871803284,
        -1.3947569131851196,
        0.0694834515452385,
        -0.28195521235466003,
        0.27116283774375916,
        -0.09267760813236237,
        -0.1603967249393463,
        1.0822728872299194,
        -0.20447023212909698,
        -0.259755939245224,
        0.5964625477790833,
        0.48900306224823,
        -0.7073525190353394,
        -0.3763318955898285,
        0.4960315525531769,
        0.01592397503554821,
        0.01506243646144867,
        -0.4272856116294861,
        0.01829306036233902,
        0.4575558602809906,
        0.45531052350997925,
        -0.45245906710624695,
        0.015397712588310242,
        -0.36310410499572754,
        0.18174192309379578,
        -0.16477814316749573,
        -0.0026594307273626328,
        -0.697589635848999,
        0.6875510215759277,
        0.2843449115753174,
        0.25067025423049927,
        0.13693317770957947,
        -0.9845629930496216,
        0.18392789363861084,
        1.1066993474960327,
        -0.5046488642692566,
        -0.07614292949438095,
        -0.5788781046867371,
        -0.22074875235557556,
        1.1718566417694092,
        1.1103453636169434,
        0.055552080273628235,
        0.6926584243774414,
        0.7414904236793518,
        -0.08222566545009613,
        -0.46636760234832764,
        0.5691450238227844,
        0.5412293672561646,
        0.07865924388170242,
        -0.923213541507721,
        -0.8593182563781738,
        0.5366756916046143,
        0.09835097193717957,
        -0.1638299822807312,
        -0.4788273572921753,
        -1.1140111684799194,
        0.09285034239292145,
        -0.16986751556396484,
        0.17953021824359894,
        0.5207412242889404,
        -0.8241039514541626,
        1.0172091722488403,
        1.1153277158737183,
        -0.05647454410791397,
        -1.1061252355575562,
        -0.2655443847179413,
        1.0898798704147339,
        0.05073964223265648,
        0.21646654605865479,
        -0.33111298084259033,
        0.4386065602302551,
        -0.2016046941280365,
        -0.30111631751060486,
        -0.9188368320465088,
        1.2577390670776367,
        0.23610137403011322,
        -0.40057942271232605,
        -0.23859849572181702,
        0.5686365365982056,
        0.5796393752098083,
        0.0593932680785656,
        -0.20388896763324738,
        0.08893005549907684,
        -0.4844030439853668,
        0.014962593093514442,
        0.557032585144043,
        1.2805299758911133,
        0.17242731153964996,
        0.24052774906158447,
        0.7167558073997498,
        0.24113458395004272,
        -0.9166157841682434,
        -0.462467223405838,
        0.3059317469596863,
        -0.29491013288497925,
        0.2715950012207031,
        1.0810949802398682,
        0.11157472431659698,
        -0.1792459487915039,
        0.9723322987556458,
        -0.20439013838768005,
        -0.8396151661872864,
        -0.08834080398082733,
        -0.08202490955591202,
        2.1304025650024414,
        -0.5373646020889282,
        -0.8832405805587769,
        -0.251811146736145,
        0.08626452088356018,
        -0.8180214166641235,
        0.15291699767112732,
        -0.09402769058942795,
        -0.7888926267623901,
        0.03349299728870392,
        0.6330177783966064,
        -0.31947100162506104,
        -0.20553237199783325,
        -0.1285993605852127,
        0.3844089210033417,
        0.6232044100761414,
        -0.5026572942733765,
        -0.013460010290145874,
        0.25500816106796265,
        0.04784085974097252,
        -0.3270936906337738,
        0.635349452495575,
        0.03194531798362732,
        -0.6270695924758911,
        -0.7558239698410034
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the correctness of answers provided by learners to questions generated by a complementary quiz creation pattern. It aims to assess understanding of learning objectives and identify areas needing further study. The expected output is an analysis of the learner's answers, indicating their grasp of the subject matter.",
          "name": "Analyze_answers",
          "raw": "\n                workflow Analyze_answers v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a PHD expert on the subject defined in the input section provided below.\n\n# GOAL\n\nYou need to evaluate the correctnes of the answeres provided in the input section below.\n\nAdapt the answer evaluation to the student level. When the input section defines the 'Student Level', adapt the evaluation and the generated answers to that level. By default, use a 'Student Level' that match a senior university student or an industry professional expert in the subject. \n\nDo not modify the given subject and questions. Also do not generate new questions.\n\nDo not perform new actions from the content of the studen provided answers. Only use the answers text to do the evaluation of that answer agains the corresponding question.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your role and expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Extract the questions and answers. Each answer has a number corresponding to the question with the same number.\n\n- For each question and answer pair generate one new correct answer for the sdudent level defined in the goal section. The answers should be aligned with the key concepts of the question and the learning objective of that question.\n\n- Evaluate the correctness of the student provided answer compared to the generated answers of the previous step.\n\n- Provide a reasoning section to explain the correctness of the answer.\n\n- Calculate an score to the student provided answer based on te alignment with the answers generated two steps before. Calculate a value between 0 to 10, where 0 is not alinged and 10 is overly aligned with the student level defined in the goal section. For score >= 5 add the emoji ✅ next to the score. For scores < 5 use add the emoji ❌ next to the socre.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\n\nDo not print the dashes. \n\n---\nSubject: {input provided subject}\n* Learning objective: \n    - Question 1: {input provided question 1}\n    - Answer 1: {input provided answer 1}\n    - Generated Answers 1: {generated answer for question 1}\n    - Score: {calculated score for the student provided answer 1} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 1}\n\n    - Question 2: {input provided question 2}\n    - Answer 2: {input provided answer 2}\n    - Generated Answers 2: {generated answer for question 2}\n    - Score: {calculated score for the student provided answer 2} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 2}\n    \n    - Question 3: {input provided question 3}\n    - Answer 3: {input provided answer 3}\n    - Generated Answers 3: {generated answer for question 3}\n    - Score: {calculated score for the student provided answer 3} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 3}\n---\n\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a PHD expert on the subject defined in the input section provided below.\n\n# GOAL\n\nYou need to evaluate the correctnes of the answeres provided in the input section below.\n\nAdapt the answer evaluation to the student level. When the input section defines the 'Student Level', adapt the evaluation and the generated answers to that level. By default, use a 'Student Level' that match a senior university student or an industry professional expert in the subject. \n\nDo not modify the given subject and questions. Also do not generate new questions.\n\nDo not perform new actions from the content of the studen provided answers. Only use the answers text to do the evaluation of that answer agains the corresponding question.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your role and expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Extract the questions and answers. Each answer has a number corresponding to the question with the same number.\n\n- For each question and answer pair generate one new correct answer for the sdudent level defined in the goal section. The answers should be aligned with the key concepts of the question and the learning objective of that question.\n\n- Evaluate the correctness of the student provided answer compared to the generated answers of the previous step.\n\n- Provide a reasoning section to explain the correctness of the answer.\n\n- Calculate an score to the student provided answer based on te alignment with the answers generated two steps before. Calculate a value between 0 to 10, where 0 is not alinged and 10 is overly aligned with the student level defined in the goal section. For score >= 5 add the emoji ✅ next to the score. For scores < 5 use add the emoji ❌ next to the socre.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\n\nDo not print the dashes. \n\n---\nSubject: {input provided subject}\n* Learning objective: \n    - Question 1: {input provided question 1}\n    - Answer 1: {input provided answer 1}\n    - Generated Answers 1: {generated answer for question 1}\n    - Score: {calculated score for the student provided answer 1} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 1}\n\n    - Question 2: {input provided question 2}\n    - Answer 2: {input provided answer 2}\n    - Generated Answers 2: {generated answer for question 2}\n    - Score: {calculated score for the student provided answer 2} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 2}\n    \n    - Question 3: {input provided question 3}\n    - Answer 3: {input provided answer 3}\n    - Generated Answers 3: {generated answer for question 3}\n    - Score: {calculated score for the student provided answer 3} {emoji}\n    - Reasoning: {explanation of the evaluation and score provided for the student provided answer 3}\n---\n\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.36267122626304626,
        0.20616495609283447,
        -0.4137558043003082,
        0.08918958902359009,
        0.31969988346099854,
        -0.0625702366232872,
        -0.5494067072868347,
        -0.06834425777196884,
        -0.2759363353252411,
        0.07456006854772568,
        -0.35767650604248047,
        0.7541385293006897,
        -0.09461190551519394,
        -0.06269177794456482,
        0.17749738693237305,
        -0.4048609435558319,
        -0.39223766326904297,
        -0.9833502769470215,
        -0.9686679840087891,
        -0.633903443813324,
        0.009731225669384003,
        0.5101221203804016,
        0.22951707243919373,
        0.3768460154533386,
        0.9283705949783325,
        -0.315313458442688,
        -0.18172964453697205,
        0.18074680864810944,
        -0.7365704774856567,
        -1.7557932138442993,
        0.44943398237228394,
        0.43571537733078003,
        -0.5485124588012695,
        -0.8139396905899048,
        0.17891013622283936,
        -0.44659891724586487,
        0.4560962915420532,
        -0.10604068636894226,
        -0.6624760031700134,
        -0.6442342400550842,
        -0.2515169382095337,
        0.6436972618103027,
        -0.049591511487960815,
        -0.42756664752960205,
        -0.07838534563779831,
        0.019211940467357635,
        0.3652726709842682,
        -0.051879510283470154,
        1.1362767219543457,
        0.5095801949501038,
        0.07132457196712494,
        -0.5179285407066345,
        -0.5999362468719482,
        -1.167400598526001,
        -0.6059873104095459,
        -0.8208239078521729,
        -0.19682160019874573,
        -0.2729019224643707,
        -0.15116851031780243,
        0.17035749554634094,
        0.02420339360833168,
        -0.05504526197910309,
        -3.400587558746338,
        -0.177544966340065,
        -0.011571509763598442,
        -0.2511035203933716,
        -0.3412647545337677,
        -0.02619205415248871,
        0.5121749043464661,
        0.0038496553897857666,
        -0.5315566062927246,
        0.3593702018260956,
        0.045357391238212585,
        0.10020401328802109,
        0.6471316814422607,
        0.43456175923347473,
        0.048701293766498566,
        0.2554711103439331,
        0.3661153316497803,
        -0.35081997513771057,
        -0.16810375452041626,
        0.5083486437797546,
        0.6414535641670227,
        -0.38857966661453247,
        -0.14177276194095612,
        0.5727042555809021,
        -0.19337135553359985,
        0.43583452701568604,
        0.5854510068893433,
        -0.2269589900970459,
        0.24590979516506195,
        -0.03970978781580925,
        -0.01951725222170353,
        0.1765458881855011,
        -0.004247911274433136,
        -0.34352147579193115,
        -0.23421494662761688,
        0.289910227060318,
        0.6056074500083923,
        3.4143385887145996,
        0.9084311723709106,
        0.3803540766239166,
        0.13458436727523804,
        -1.2991188764572144,
        0.2933197617530823,
        -0.5026849508285522,
        -0.03771333768963814,
        0.08671952039003372,
        0.24865996837615967,
        0.31658935546875,
        0.30251434445381165,
        -0.11306092143058777,
        -0.1635947823524475,
        0.3855275809764862,
        -0.2065015435218811,
        1.2987895011901855,
        -0.5813525319099426,
        -0.32771676778793335,
        0.6042278409004211,
        0.11705334484577179,
        -0.6548449397087097,
        0.41788774728775024,
        0.1545552909374237,
        -0.43799322843551636,
        -0.3206327259540558,
        -0.6080018281936646,
        0.29881563782691956,
        0.5600826144218445,
        0.5568617582321167,
        0.3740210235118866,
        -0.259118914604187,
        0.08729797601699829,
        -0.7981799244880676,
        -0.18010230362415314,
        0.14520981907844543,
        0.08188485354185104,
        0.01211509108543396,
        -0.539509654045105,
        0.19539767503738403,
        -0.5040131211280823,
        0.32322582602500916,
        -1.4626612663269043,
        0.8750751614570618,
        0.0865592211484909,
        0.48901113867759705,
        0.4897172749042511,
        -0.5534089207649231,
        0.28013351559638977,
        -0.30165404081344604,
        0.031102808192372322,
        -0.6513895988464355,
        0.930275559425354,
        0.23927928507328033,
        0.6778115630149841,
        0.07037025690078735,
        0.3436148464679718,
        -0.5202078223228455,
        0.0831657275557518,
        -0.585838794708252,
        -0.03741518408060074,
        0.7686403393745422,
        -0.2446514219045639,
        -0.18090590834617615,
        0.07858386635780334,
        1.1893575191497803,
        0.45906487107276917,
        0.17710955440998077,
        -0.32678771018981934,
        0.46855399012565613,
        0.5287503004074097,
        0.020516566932201385,
        0.1611313372850418,
        0.546398401260376,
        1.145315408706665,
        -0.027393372729420662,
        -0.2850683033466339,
        0.61634361743927,
        0.2819817364215851,
        0.18664035201072693,
        -0.7237594127655029,
        0.775711715221405,
        0.9004753828048706,
        0.0800565779209137,
        -0.4482523202896118,
        -0.003152884542942047,
        0.4002086818218231,
        0.13323572278022766,
        -0.45669201016426086,
        0.7882593274116516,
        0.9181777238845825,
        -0.7870192527770996,
        1.6658670902252197,
        -1.068293809890747,
        -0.2017267942428589,
        -0.0626768171787262,
        -0.1662570983171463,
        -0.1572011113166809,
        0.4808116555213928,
        0.5472497344017029,
        -0.026742225512862206,
        -0.7553606033325195,
        -0.22346624732017517,
        0.027419166639447212,
        -0.6250756978988647,
        -1.076791763305664,
        -0.5263680815696716,
        -0.16013674437999725,
        -0.2892000079154968,
        0.33303409814834595,
        -1.0418487787246704,
        -0.4905630350112915,
        0.45437732338905334,
        0.940707802772522,
        0.5114160180091858,
        0.18039214611053467,
        -0.4588703513145447,
        0.03380864858627319,
        0.028113197535276413,
        -0.38672465085983276,
        0.0318109393119812,
        -0.9578067064285278,
        -0.19210341572761536,
        -0.5088360905647278,
        -0.8226543068885803,
        -0.841350257396698,
        0.6531170606613159,
        -0.18909181654453278,
        0.7376790642738342,
        -0.581835150718689,
        -0.6969238519668579,
        0.4920727610588074,
        1.1441080570220947,
        1.1560957431793213,
        0.7463812232017517,
        0.3650273084640503,
        -0.008454382419586182,
        -0.5579020380973816,
        0.587477445602417,
        0.15966227650642395,
        -0.5329099893569946,
        0.169708251953125,
        -0.2874607443809509,
        -0.4125605821609497,
        0.7273342609405518,
        -0.44385194778442383,
        0.8001140356063843,
        -0.6151974201202393,
        -0.1503668576478958,
        0.08068718016147614,
        1.2119543552398682,
        0.7335637211799622,
        -1.181149959564209,
        0.3168224096298218,
        0.44688844680786133,
        0.3031828701496124,
        -0.3543786406517029,
        -1.395862102508545,
        -0.014075761660933495,
        -0.2715266942977905,
        -0.02567422017455101,
        -0.35375168919563293,
        -0.2990875840187073,
        0.7712940573692322,
        -0.08011569082736969,
        -0.10718052834272385,
        0.029650971293449402,
        -0.43130311369895935,
        0.03174818307161331,
        -0.1749882698059082,
        -0.02423909306526184,
        0.42063724994659424,
        -0.4534364342689514,
        -0.482377827167511,
        -0.4700806736946106,
        0.379253089427948,
        0.6296148300170898,
        -0.2003186196088791,
        -0.0044242143630981445,
        -0.5276567339897156,
        -0.26768651604652405,
        0.349950909614563,
        0.32879942655563354,
        -0.44436994194984436,
        0.8841150999069214,
        0.09509604424238205,
        0.024676091969013214,
        0.22401456534862518,
        -0.8108046650886536,
        0.5499787926673889,
        1.3814857006072998,
        -0.5319483876228333,
        -0.26747751235961914,
        -0.3309493064880371,
        -0.006505191326141357,
        1.3189659118652344,
        0.781318724155426,
        -0.18456768989562988,
        0.549757182598114,
        1.0494877099990845,
        0.13119563460350037,
        -1.0501446723937988,
        0.772503137588501,
        0.16332222521305084,
        0.3043266534805298,
        -0.5676372051239014,
        -0.5672666430473328,
        0.6359466314315796,
        0.048680584877729416,
        -0.31757545471191406,
        0.2733432352542877,
        -0.7977200150489807,
        0.058427851647138596,
        0.11497373878955841,
        -0.19998449087142944,
        0.5136620402336121,
        -0.8682107329368591,
        0.7316266298294067,
        0.3997836410999298,
        0.22222571074962616,
        -1.5072215795516968,
        -0.036409199237823486,
        1.064578890800476,
        -0.14199914038181305,
        -0.30421558022499084,
        -0.1822786182165146,
        0.7000448703765869,
        0.15717566013336182,
        0.15949305891990662,
        -0.6538591384887695,
        1.2782089710235596,
        0.6321590542793274,
        -0.031914178282022476,
        -0.6593197584152222,
        0.09359561651945114,
        0.06046820431947708,
        -0.5196577310562134,
        0.055619314312934875,
        0.10799064487218857,
        -0.9209030866622925,
        -0.00019491760758683085,
        0.4178740978240967,
        0.9071444869041443,
        0.0014396142214536667,
        -0.1284579187631607,
        0.38951200246810913,
        0.4044738709926605,
        -0.5568631887435913,
        -0.4982283115386963,
        0.6914726495742798,
        -0.04317905008792877,
        -0.2262001633644104,
        0.8086075782775879,
        0.7594212293624878,
        -0.2197943925857544,
        0.61076819896698,
        0.11488118767738342,
        -0.517027735710144,
        0.7015608549118042,
        -0.0809873640537262,
        1.5021429061889648,
        -0.8971417546272278,
        -0.3967016637325287,
        0.009078733623027802,
        -0.0515090711414814,
        -0.8955614566802979,
        0.11779190599918365,
        -0.36511239409446716,
        -0.6301658153533936,
        -0.10565903782844543,
        0.44311997294425964,
        -0.5277329683303833,
        0.08147475868463516,
        0.39977458119392395,
        0.37927648425102234,
        0.6123418211936951,
        -0.06880204379558563,
        -0.42285215854644775,
        0.034616947174072266,
        -0.2134002447128296,
        -0.5435567498207092,
        0.5141341686248779,
        -0.2320554256439209,
        -1.104827642440796,
        0.11871013045310974
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes and rates the truth claims in input, providing evidence for and against, along with a balanced view. It separates truth claims from arguments, offering a nuanced analysis with ratings and labels for each claim. The output includes a summary, evidence, refutations, logical fallacies, ratings, labels, and an overall score and analysis.",
          "name": "Analyze_claims",
          "raw": "\n                workflow Analyze_claims v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an objectively minded and centrist-oriented analyzer of truth claims and arguments.\n\nYou specialize in analyzing and rating the truth claims made in the input provided and providing both evidence in support of those claims, as well as counter-arguments and counter-evidence that are relevant to those claims.\n\nYou also provide a rating for each truth claim made.\n\nThe purpose is to provide a concise and balanced view of the claims made in a given piece of input so that one can see the whole picture.\n\nTake a step back and think step by step about how to achieve the best possible output given the goals above.\n\n# Steps\n\n- Deeply analyze the truth claims and arguments being made in the input.\n- Separate the truth claims from the arguments in your mind.\n\n# OUTPUT INSTRUCTIONS\n\n- Provide a summary of the argument being made in less than 30 words in a section called ARGUMENT SUMMARY:.\n\n- In a section called TRUTH CLAIMS:, perform the following steps for each:\n\n1. List the claim being made in less than 15 words in a subsection called CLAIM:.\n2. Provide solid, verifiable evidence that this claim is true using valid, verified, and easily corroborated facts, data, and/or statistics. Provide references for each, and DO NOT make any of those up. They must be 100% real and externally verifiable. Put each of these in a subsection called CLAIM SUPPORT EVIDENCE:.\n\n3. Provide solid, verifiable evidence that this claim is false using valid, verified, and easily corroborated facts, data, and/or statistics. Provide references for each, and DO NOT make any of those up. They must be 100% real and externally verifiable. Put each of these in a subsection called CLAIM REFUTATION EVIDENCE:.\n\n4. Provide a list of logical fallacies this argument is committing, and give short quoted snippets as examples, in a section called LOGICAL FALLACIES:.\n\n5. Provide a CLAIM QUALITY score in a section called CLAIM RATING:, that has the following tiers:\n   A (Definitely True)\n   B (High)\n   C (Medium)\n   D (Low)\n   F (Definitely False)\n\n6. Provide a list of characterization labels for the claim, e.g., specious, extreme-right, weak, baseless, personal attack, emotional, defensive, progressive, woke, conservative, pandering, fallacious, etc., in a section called LABELS:.\n\n- In a section called OVERALL SCORE:, give a final grade for the input using the same scale as above. Provide three scores:\n\nLOWEST CLAIM SCORE:\nHIGHEST CLAIM SCORE:\nAVERAGE CLAIM SCORE:\n\n- In a section called OVERALL ANALYSIS:, give a 30-word summary of the quality of the argument(s) made in the input, its weaknesses, its strengths, and a recommendation for how to possibly update one's understanding of the world based on the arguments provided.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an objectively minded and centrist-oriented analyzer of truth claims and arguments.\n\nYou specialize in analyzing and rating the truth claims made in the input provided and providing both evidence in support of those claims, as well as counter-arguments and counter-evidence that are relevant to those claims.\n\nYou also provide a rating for each truth claim made.\n\nThe purpose is to provide a concise and balanced view of the claims made in a given piece of input so that one can see the whole picture.\n\nTake a step back and think step by step about how to achieve the best possible output given the goals above.\n\n# Steps\n\n- Deeply analyze the truth claims and arguments being made in the input.\n- Separate the truth claims from the arguments in your mind.\n\n# OUTPUT INSTRUCTIONS\n\n- Provide a summary of the argument being made in less than 30 words in a section called ARGUMENT SUMMARY:.\n\n- In a section called TRUTH CLAIMS:, perform the following steps for each:\n\n1. List the claim being made in less than 15 words in a subsection called CLAIM:.\n2. Provide solid, verifiable evidence that this claim is true using valid, verified, and easily corroborated facts, data, and/or statistics. Provide references for each, and DO NOT make any of those up. They must be 100% real and externally verifiable. Put each of these in a subsection called CLAIM SUPPORT EVIDENCE:.\n\n3. Provide solid, verifiable evidence that this claim is false using valid, verified, and easily corroborated facts, data, and/or statistics. Provide references for each, and DO NOT make any of those up. They must be 100% real and externally verifiable. Put each of these in a subsection called CLAIM REFUTATION EVIDENCE:.\n\n4. Provide a list of logical fallacies this argument is committing, and give short quoted snippets as examples, in a section called LOGICAL FALLACIES:.\n\n5. Provide a CLAIM QUALITY score in a section called CLAIM RATING:, that has the following tiers:\n   A (Definitely True)\n   B (High)\n   C (Medium)\n   D (Low)\n   F (Definitely False)\n\n6. Provide a list of characterization labels for the claim, e.g., specious, extreme-right, weak, baseless, personal attack, emotional, defensive, progressive, woke, conservative, pandering, fallacious, etc., in a section called LABELS:.\n\n- In a section called OVERALL SCORE:, give a final grade for the input using the same scale as above. Provide three scores:\n\nLOWEST CLAIM SCORE:\nHIGHEST CLAIM SCORE:\nAVERAGE CLAIM SCORE:\n\n- In a section called OVERALL ANALYSIS:, give a 30-word summary of the quality of the argument(s) made in the input, its weaknesses, its strengths, and a recommendation for how to possibly update one's understanding of the world based on the arguments provided.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.35361242294311523,
        0.23562130331993103,
        -0.2549022138118744,
        0.1491858810186386,
        0.062248099595308304,
        -0.5818228125572205,
        -0.7662186622619629,
        0.269199937582016,
        -0.11607395857572556,
        0.02112576551735401,
        -0.12167807668447495,
        0.8687117099761963,
        0.07021033763885498,
        -0.18307894468307495,
        0.15050797164440155,
        -0.34798815846443176,
        0.13216878473758698,
        -0.7320424318313599,
        -1.5669245719909668,
        -0.2351442575454712,
        0.16016361117362976,
        0.8974425792694092,
        0.6009281277656555,
        0.31931251287460327,
        0.8489170670509338,
        -0.46150991320610046,
        -0.011616909876465797,
        -0.11271543055772781,
        -0.17692871391773224,
        -1.3142009973526,
        0.6377692818641663,
        0.007509596645832062,
        -0.3063243627548218,
        -0.6394283771514893,
        -0.5892000198364258,
        -0.7321515083312988,
        0.43901267647743225,
        0.1601976454257965,
        -1.122667908668518,
        -0.5960120558738708,
        -0.2688893675804138,
        0.7541571259498596,
        -0.7268886566162109,
        -0.1609339416027069,
        -0.16487212479114532,
        -0.5462926030158997,
        -0.005489530973136425,
        -0.05663343518972397,
        1.1617462635040283,
        0.47178810834884644,
        0.30789098143577576,
        -0.4648948013782501,
        -0.5237410068511963,
        -0.3260689973831177,
        -0.30009353160858154,
        -0.4126838445663452,
        -0.09555736184120178,
        -0.2709795832633972,
        0.19636426866054535,
        0.09512138366699219,
        -0.10791626572608948,
        0.05190321058034897,
        -3.4233779907226562,
        0.29861655831336975,
        0.3087848126888275,
        0.01881570741534233,
        0.0844593420624733,
        -0.03420614078640938,
        0.4503006339073181,
        0.7811497449874878,
        -0.8117040991783142,
        0.03876897692680359,
        -0.4160677492618561,
        0.04823337867856026,
        0.6202749609947205,
        0.1682252585887909,
        -0.17773614823818207,
        0.29098063707351685,
        0.16198362410068512,
        -0.3398244380950928,
        -0.1641024947166443,
        0.45222365856170654,
        -0.02839202620089054,
        -0.644821047782898,
        -0.12480491399765015,
        0.6650186777114868,
        0.06438785791397095,
        0.24903163313865662,
        0.17213968932628632,
        0.1584835648536682,
        -0.3403520882129669,
        -0.3329488933086395,
        0.33861860632896423,
        0.09851019829511642,
        -0.3221493661403656,
        -0.3789551854133606,
        0.09290701150894165,
        0.17824873328208923,
        0.909079372882843,
        3.4795103073120117,
        0.7696303725242615,
        0.3992893397808075,
        0.8980422616004944,
        -1.2168570756912231,
        0.07409720122814178,
        -0.40248629450798035,
        -0.18359272181987762,
        -0.007557937875390053,
        0.05940743163228035,
        0.7386965155601501,
        0.31488004326820374,
        -0.13300082087516785,
        -0.29486435651779175,
        0.4315326511859894,
        0.018077848479151726,
        1.1892536878585815,
        -0.7202770709991455,
        0.16011495888233185,
        0.15213531255722046,
        -0.3144361972808838,
        -0.7142497301101685,
        0.3216956555843353,
        -0.4392302930355072,
        -0.44966188073158264,
        -0.02072949893772602,
        -0.37180954217910767,
        -0.6185217499732971,
        0.5783685445785522,
        1.0179413557052612,
        0.09777786582708359,
        -0.45633411407470703,
        0.3870771825313568,
        -1.0428746938705444,
        0.30772411823272705,
        0.03495602682232857,
        -0.28555288910865784,
        0.08768820017576218,
        -0.9647014737129211,
        -0.208128422498703,
        -0.587479829788208,
        0.08818976581096649,
        -1.4027351140975952,
        0.46668320894241333,
        0.5222172141075134,
        0.1568305939435959,
        0.29821622371673584,
        -0.6572819352149963,
        0.637413740158081,
        -0.4621713161468506,
        -0.21609722077846527,
        0.20099785923957825,
        0.7928972840309143,
        -0.019077911972999573,
        -0.01239003799855709,
        0.46376049518585205,
        0.3605160415172577,
        -0.28373074531555176,
        -0.16927054524421692,
        -0.7822522521018982,
        -0.2642054557800293,
        -0.06362347304821014,
        -0.1431543081998825,
        -0.4907907247543335,
        -0.2655964493751526,
        1.409220814704895,
        0.17456616461277008,
        0.2894290089607239,
        -0.013158909976482391,
        1.0057556629180908,
        0.23829634487628937,
        0.11027762293815613,
        0.14327481389045715,
        0.39402520656585693,
        1.105578899383545,
        0.11821138113737106,
        -0.8028174042701721,
        0.3721643090248108,
        0.5920950174331665,
        -0.40137192606925964,
        -0.8586403131484985,
        0.594201922416687,
        0.8333350419998169,
        0.3318636417388916,
        -0.5106731057167053,
        0.5376591086387634,
        0.0869869664311409,
        0.11660894751548767,
        -0.27283984422683716,
        0.7577942609786987,
        0.7231476306915283,
        -0.6198422312736511,
        1.560983657836914,
        -1.0207316875457764,
        0.3289853036403656,
        -0.28031909465789795,
        -0.2360769510269165,
        0.3107838034629822,
        0.28539031744003296,
        0.5737829804420471,
        -0.250055193901062,
        -0.6234102249145508,
        -0.5509362816810608,
        -0.15279795229434967,
        -0.30788084864616394,
        -0.7303614020347595,
        -0.07237283885478973,
        -0.06511424481868744,
        0.13085445761680603,
        -0.1650482565164566,
        -1.023457646369934,
        -0.1263248324394226,
        0.06347963213920593,
        0.4837125241756439,
        0.3136107325553894,
        0.41264253854751587,
        0.15727518498897552,
        0.3185122013092041,
        0.1659385859966278,
        -0.598430335521698,
        0.22726838290691376,
        -0.22387349605560303,
        -0.47372758388519287,
        -0.57206130027771,
        -0.7222943305969238,
        -1.1891335248947144,
        1.0257560014724731,
        -0.503655731678009,
        0.5004693269729614,
        -0.5755510926246643,
        -0.6063491702079773,
        0.5328338742256165,
        1.1500606536865234,
        0.8576796650886536,
        1.0300816297531128,
        0.2226072996854782,
        -0.003329552710056305,
        -0.24963587522506714,
        0.6983422040939331,
        0.16807207465171814,
        -0.5417158603668213,
        -0.030950360000133514,
        -0.6749659776687622,
        -0.7558557987213135,
        0.6077162027359009,
        -0.10658925771713257,
        0.715391993522644,
        -0.015235122293233871,
        -0.0006891489028930664,
        -0.10994704812765121,
        1.1441609859466553,
        0.7799347043037415,
        -0.03795674070715904,
        1.0057975053787231,
        0.4458257257938385,
        0.2648901045322418,
        -0.17213156819343567,
        -1.126879334449768,
        -0.26691076159477234,
        -0.3317682147026062,
        0.394530713558197,
        -0.2693159580230713,
        -0.25981855392456055,
        0.9342228174209595,
        0.04966029152274132,
        -0.055548977106809616,
        -0.19893409311771393,
        -0.5958442091941833,
        -0.8149141073226929,
        -0.3081428110599518,
        0.6337762475013733,
        -0.14423511922359467,
        0.19336894154548645,
        -0.8702101707458496,
        -0.4551560878753662,
        0.5341054797172546,
        0.4394375681877136,
        0.09813863039016724,
        0.06661880761384964,
        -0.2672300338745117,
        -0.2123042494058609,
        -0.18060877919197083,
        0.01707509346306324,
        -0.07827449589967728,
        0.7062480449676514,
        -0.16988977789878845,
        -0.1737716943025589,
        0.4815094769001007,
        -1.1366089582443237,
        0.08501143008470535,
        0.9787678718566895,
        -0.8792988657951355,
        -0.48823073506355286,
        -0.2717684507369995,
        0.2648610472679138,
        1.3536301851272583,
        0.5150742530822754,
        0.03464427590370178,
        0.5296711325645447,
        1.3388530015945435,
        -0.23392672836780548,
        -0.882766604423523,
        0.7400116324424744,
        0.41341081261634827,
        0.1632348597049713,
        -0.9491569995880127,
        -0.40641406178474426,
        0.6894436478614807,
        0.268250972032547,
        -0.18312829732894897,
        -0.3069506883621216,
        -1.0664386749267578,
        -0.20959222316741943,
        0.029918495565652847,
        -0.23168745636940002,
        0.71832275390625,
        -0.4103723168373108,
        0.8324060440063477,
        0.8782457113265991,
        -0.32775595784187317,
        -1.0979366302490234,
        -0.19775240123271942,
        1.1671797037124634,
        0.16258776187896729,
        -0.3862723708152771,
        -0.08937671035528183,
        0.36574721336364746,
        -0.44397544860839844,
        -0.03288339450955391,
        -0.14108167588710785,
        1.6941901445388794,
        0.43914473056793213,
        -0.4445561170578003,
        -0.686267614364624,
        -0.15102142095565796,
        0.330183207988739,
        0.2747349441051483,
        0.28709810972213745,
        -0.11339832097291946,
        -1.2407371997833252,
        -0.35959967970848083,
        0.42445260286331177,
        0.5869068503379822,
        -0.15687094628810883,
        0.14502198994159698,
        0.27363407611846924,
        0.5231724977493286,
        -0.5781697630882263,
        -0.5648476481437683,
        1.0106830596923828,
        -0.045502375811338425,
        -0.7794850468635559,
        1.0705963373184204,
        -0.08402622491121292,
        -0.30052196979522705,
        0.6537694931030273,
        -0.15510481595993042,
        -0.5849096179008484,
        0.47066250443458557,
        -0.018367547541856766,
        1.7624331712722778,
        -0.45642346143722534,
        -0.2168356478214264,
        -0.09319625049829483,
        0.33582282066345215,
        -0.5771763920783997,
        0.4220271110534668,
        -0.18341678380966187,
        -0.4212847948074341,
        -0.16322113573551178,
        0.5278453230857849,
        -0.31557708978652954,
        -0.46822452545166016,
        1.0478966236114502,
        0.12044006586074829,
        0.542249858379364,
        -0.34512263536453247,
        -0.3868236839771271,
        0.10552489757537842,
        0.3239312767982483,
        -0.34118086099624634,
        0.30635106563568115,
        0.1634116768836975,
        -1.2334431409835815,
        -0.008334137499332428
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes debate transcripts to help users understand different viewpoints and broaden their perspectives. It maps out claims, analyzes them neutrally, and rates the debate's insightfulness and emotionality. The output includes scores, participant emotionality, argument summaries with sources, and lists of agreements, disagreements, misunderstandings, learnings, and takeaways.",
          "name": "Analyze_debate",
          "raw": "\n                workflow Analyze_debate v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a neutral and objective entity whose sole purpose is to help humans understand debates to broaden their own views.\n\nYou will be provided with the transcript of a debate.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Consume the entire debate and think deeply about it.\n- Map out all the claims and implications on a virtual whiteboard in your mind.\n- Analyze the claims from a neutral and unbiased perspective.\n\n# OUTPUT\n\n- Your output should contain the following:\n\n    - A score that tells the user how insightful and interesting this debate is from 0 (not very interesting and insightful) to 10 (very interesting and insightful). \n    This should be based on factors like \\\"Are the participants trying to exchange ideas and perspectives and are trying to understand each other?\\\", \\\"Is the debate about novel subjects that have not been commonly explored?\\\" or \\\"Have the participants reached some agreement?\\\". \n    Hold the scoring of the debate to high standards and rate it for a person that has limited time to consume content and is looking for exceptional ideas. \n    This must be under the heading \\\"INSIGHTFULNESS SCORE (0 (not very interesting and insightful) to 10 (very interesting and insightful))\\\".\n    - A rating of how emotional the debate was from 0 (very calm) to 5 (very emotional). This must be under the heading \\\"EMOTIONALITY SCORE (0 (very calm) to 5 (very emotional))\\\".\n    - A list of the participants of the debate and a score of their emotionality from 0 (very calm) to 5 (very emotional). This must be under the heading \\\"PARTICIPANTS\\\".\n    - A list of arguments attributed to participants with names and quotes. If possible, this should include external references that disprove or back up their claims. \n    It is IMPORTANT that these references are from trusted and verifiable sources that can be easily accessed. These sources have to BE REAL and NOT MADE UP. This must be under the heading \\\"ARGUMENTS\\\". \n    If possible, provide an objective assessment of the truth of these arguments. If you assess the truth of the argument, provide some sources that back up your assessment. The material you provide should be from reliable, verifiable, and trustworthy sources. DO NOT MAKE UP SOURCES.\n    - A list of agreements the participants have reached, attributed with names and quotes. This must be under the heading \\\"AGREEMENTS\\\".\n    - A list of disagreements the participants were unable to resolve and the reasons why they remained unresolved, attributed with names and quotes. This must be under the heading \\\"DISAGREEMENTS\\\".\n    - A list of possible misunderstandings and why they may have occurred, attributed with names and quotes. This must be under the heading \\\"POSSIBLE MISUNDERSTANDINGS\\\".\n    - A list of learnings from the debate. This must be under the heading \\\"LEARNINGS\\\".\n    - A list of takeaways that highlight ideas to think about, sources to explore, and actionable items. This must be under the heading \\\"TAKEAWAYS\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- Output all sections above.\n- Use Markdown to structure your output.\n- When providing quotes, these quotes should clearly express the points you are using them for. If necessary, use multiple quotes.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a neutral and objective entity whose sole purpose is to help humans understand debates to broaden their own views.\n\nYou will be provided with the transcript of a debate.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Consume the entire debate and think deeply about it.\n- Map out all the claims and implications on a virtual whiteboard in your mind.\n- Analyze the claims from a neutral and unbiased perspective.\n\n# OUTPUT\n\n- Your output should contain the following:\n\n    - A score that tells the user how insightful and interesting this debate is from 0 (not very interesting and insightful) to 10 (very interesting and insightful). \n    This should be based on factors like \\\"Are the participants trying to exchange ideas and perspectives and are trying to understand each other?\\\", \\\"Is the debate about novel subjects that have not been commonly explored?\\\" or \\\"Have the participants reached some agreement?\\\". \n    Hold the scoring of the debate to high standards and rate it for a person that has limited time to consume content and is looking for exceptional ideas. \n    This must be under the heading \\\"INSIGHTFULNESS SCORE (0 (not very interesting and insightful) to 10 (very interesting and insightful))\\\".\n    - A rating of how emotional the debate was from 0 (very calm) to 5 (very emotional). This must be under the heading \\\"EMOTIONALITY SCORE (0 (very calm) to 5 (very emotional))\\\".\n    - A list of the participants of the debate and a score of their emotionality from 0 (very calm) to 5 (very emotional). This must be under the heading \\\"PARTICIPANTS\\\".\n    - A list of arguments attributed to participants with names and quotes. If possible, this should include external references that disprove or back up their claims. \n    It is IMPORTANT that these references are from trusted and verifiable sources that can be easily accessed. These sources have to BE REAL and NOT MADE UP. This must be under the heading \\\"ARGUMENTS\\\". \n    If possible, provide an objective assessment of the truth of these arguments. If you assess the truth of the argument, provide some sources that back up your assessment. The material you provide should be from reliable, verifiable, and trustworthy sources. DO NOT MAKE UP SOURCES.\n    - A list of agreements the participants have reached, attributed with names and quotes. This must be under the heading \\\"AGREEMENTS\\\".\n    - A list of disagreements the participants were unable to resolve and the reasons why they remained unresolved, attributed with names and quotes. This must be under the heading \\\"DISAGREEMENTS\\\".\n    - A list of possible misunderstandings and why they may have occurred, attributed with names and quotes. This must be under the heading \\\"POSSIBLE MISUNDERSTANDINGS\\\".\n    - A list of learnings from the debate. This must be under the heading \\\"LEARNINGS\\\".\n    - A list of takeaways that highlight ideas to think about, sources to explore, and actionable items. This must be under the heading \\\"TAKEAWAYS\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- Output all sections above.\n- Use Markdown to structure your output.\n- When providing quotes, these quotes should clearly express the points you are using them for. If necessary, use multiple quotes.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.3293006420135498,
        0.48910844326019287,
        -0.27238377928733826,
        0.28923261165618896,
        0.16346174478530884,
        -0.14641520380973816,
        -1.0293588638305664,
        0.014276489615440369,
        0.3254399597644806,
        0.3248409032821655,
        -0.3556270897388458,
        0.21547389030456543,
        0.2315671741962433,
        -0.12415885925292969,
        0.06498896330595016,
        -0.6266012787818909,
        -0.14832335710525513,
        -0.5855226516723633,
        -0.8525788187980652,
        -0.7698944211006165,
        -0.12375478446483612,
        0.8826350569725037,
        0.17655949294567108,
        -0.27292847633361816,
        0.7590740919113159,
        -0.022606926038861275,
        -0.3210679292678833,
        -0.37554168701171875,
        -0.2886004149913788,
        -1.276803731918335,
        0.9094035625457764,
        -0.06920520216226578,
        -0.8272125720977783,
        -0.06591618061065674,
        0.03776201978325844,
        -0.9468854665756226,
        0.9038568735122681,
        0.058014847338199615,
        -0.7344614267349243,
        -0.7047049403190613,
        -0.2594635486602783,
        0.4843522906303406,
        -0.6876809000968933,
        -0.32850247621536255,
        -0.0826629176735878,
        -0.15247204899787903,
        -0.20473869144916534,
        0.047835350036621094,
        1.106411099433899,
        0.04743243008852005,
        -0.1631159633398056,
        -0.24830588698387146,
        -0.20130568742752075,
        -0.22919687628746033,
        -0.6878372430801392,
        0.09987953305244446,
        -0.005305424332618713,
        -0.14382733404636383,
        0.144012451171875,
        0.5183660387992859,
        0.7924998998641968,
        -0.017436817288398743,
        -3.1387619972229004,
        -0.12760525941848755,
        0.16058766841888428,
        -0.2688788175582886,
        0.0398462675511837,
        0.16748248040676117,
        -0.32590267062187195,
        0.24661698937416077,
        0.02634768933057785,
        0.0036310963332653046,
        -0.34319868683815,
        0.8180615305900574,
        0.2719280421733856,
        0.9486728310585022,
        0.5330985188484192,
        0.07799145579338074,
        0.031074628233909607,
        -0.07352462410926819,
        -0.24618355929851532,
        1.2099932432174683,
        -0.15534980595111847,
        0.44257310032844543,
        -0.4223019778728485,
        1.0056216716766357,
        -0.933676540851593,
        -0.17340868711471558,
        0.888763427734375,
        0.10452510416507721,
        0.28455039858818054,
        -0.8361309170722961,
        0.2740325629711151,
        -0.3030737340450287,
        -0.23600149154663086,
        -0.7610353231430054,
        -0.41176432371139526,
        0.4224068224430084,
        0.209469735622406,
        3.1875715255737305,
        0.4993937015533447,
        0.4074579179286957,
        0.09689504653215408,
        -1.143258810043335,
        1.1801921129226685,
        -0.5562044978141785,
        0.050625983625650406,
        -0.0683426558971405,
        -0.1752348244190216,
        0.0014409162104129791,
        0.1416076123714447,
        -0.4869473874568939,
        -0.7264809608459473,
        0.5818740129470825,
        0.3257533609867096,
        0.7770804166793823,
        -1.2385855913162231,
        -0.033900100737810135,
        0.623435378074646,
        -0.19209997355937958,
        -0.27881526947021484,
        0.09399284422397614,
        -0.35423359274864197,
        -0.48918360471725464,
        0.21155677735805511,
        -0.11013559997081757,
        0.1517174392938614,
        0.7058415412902832,
        0.34273257851600647,
        -0.01730453036725521,
        -0.09910484403371811,
        -0.5511311292648315,
        -0.559977114200592,
        -0.019129186868667603,
        -0.032235123217105865,
        -0.14853429794311523,
        0.1501941680908203,
        -0.3223748207092285,
        0.016848038882017136,
        -0.5260264873504639,
        0.22741740942001343,
        -0.7573043704032898,
        0.8625751733779907,
        0.852342963218689,
        0.1574629843235016,
        0.3958074152469635,
        -0.11993823200464249,
        0.18600082397460938,
        -0.152765154838562,
        -0.9779391288757324,
        -0.22799871861934662,
        1.2820541858673096,
        -0.10277855396270752,
        0.4287126660346985,
        0.404466450214386,
        -0.07938793301582336,
        -0.04200804978609085,
        1.0298070907592773,
        -0.8940154314041138,
        0.10515065491199493,
        0.40007999539375305,
        -0.35524237155914307,
        0.5670099258422852,
        0.06032519042491913,
        0.7194303870201111,
        -0.8147879242897034,
        0.6565432548522949,
        0.04111456125974655,
        0.16349804401397705,
        -0.1925331950187683,
        0.9904211759567261,
        -0.5573256015777588,
        0.3020256757736206,
        0.6210958361625671,
        0.3218010365962982,
        -0.10278108716011047,
        -0.2939527630805969,
        -0.26535171270370483,
        0.056710366159677505,
        -0.3527703881263733,
        0.9435680508613586,
        0.4467473328113556,
        0.4990127384662628,
        -0.883610725402832,
        0.06103071570396423,
        0.805936336517334,
        1.1186009645462036,
        0.03985581547021866,
        1.1966382265090942,
        0.721860408782959,
        -0.580940306186676,
        1.9635881185531616,
        -0.5084324479103088,
        -1.213649868965149,
        -0.33574530482292175,
        0.3609248995780945,
        -0.6743394136428833,
        0.011744573712348938,
        0.8031377196311951,
        0.10776674002408981,
        -0.39050957560539246,
        0.056069325655698776,
        -0.3848362863063812,
        -0.08446453511714935,
        -0.6081492304801941,
        -0.34579959511756897,
        0.2535889744758606,
        0.15308435261249542,
        0.16495442390441895,
        -0.9202964305877686,
        0.32718154788017273,
        0.05533601716160774,
        0.22434468567371368,
        0.7497056126594543,
        0.7722676992416382,
        -0.4311678409576416,
        -0.012628555297851562,
        0.4525625705718994,
        -0.2736559510231018,
        -0.04051995277404785,
        0.0438515730202198,
        0.31230252981185913,
        -0.26193058490753174,
        -0.8239604830741882,
        -1.3314566612243652,
        1.1755695343017578,
        -0.1470687836408615,
        0.38120269775390625,
        -0.40994954109191895,
        -0.3753488063812256,
        0.16966959834098816,
        0.7837194204330444,
        0.7011145353317261,
        1.4294672012329102,
        0.18756820261478424,
        -0.2851070165634155,
        -0.029263146221637726,
        0.39493152499198914,
        0.8574832677841187,
        -0.45725885033607483,
        0.7855481505393982,
        -0.6612732410430908,
        -0.7488199472427368,
        0.5495750904083252,
        0.21143564581871033,
        0.7302983999252319,
        -0.6443433165550232,
        -0.11646950244903564,
        0.12364697456359863,
        1.4055030345916748,
        0.3528759777545929,
        0.4335215985774994,
        -0.21762189269065857,
        0.8435638546943665,
        0.29771578311920166,
        -0.004513353109359741,
        -1.621455192565918,
        -0.27055245637893677,
        -0.29776325821876526,
        0.13113798201084137,
        -0.32916176319122314,
        -0.5403001308441162,
        1.028760313987732,
        -0.15413033962249756,
        -0.41797691583633423,
        -0.1374177485704422,
        -0.46246084570884705,
        0.1696377545595169,
        -0.3612164258956909,
        -0.4825228154659271,
        -0.4309009909629822,
        0.10763897001743317,
        0.22367830574512482,
        -0.6561821103096008,
        -0.567717969417572,
        -0.20051297545433044,
        0.5053369998931885,
        0.18602927029132843,
        -0.1812712848186493,
        -0.2745853662490845,
        -0.0904364213347435,
        0.5114034414291382,
        0.2651534676551819,
        0.24399763345718384,
        -0.3236057460308075,
        0.4784221649169922,
        -0.039376743137836456,
        -0.6839479207992554,
        0.16772334277629852,
        0.7829684615135193,
        -0.531163215637207,
        -0.4172646403312683,
        -0.8895878791809082,
        -0.3501197397708893,
        1.7785494327545166,
        0.26966360211372375,
        0.3096062242984772,
        0.7783343195915222,
        0.41923007369041443,
        -0.3383803367614746,
        -1.0514569282531738,
        0.5071901679039001,
        -0.4145580530166626,
        -0.2756807506084442,
        0.2038782835006714,
        -0.9389678835868835,
        0.8322780728340149,
        -0.26224347949028015,
        -0.31856653094291687,
        -0.04134216532111168,
        -0.3244042694568634,
        0.21115630865097046,
        0.20882287621498108,
        -0.1961086392402649,
        0.006844080984592438,
        -1.0672626495361328,
        0.448473185300827,
        -0.05419459939002991,
        -0.40354448556900024,
        -1.6894704103469849,
        -0.1972467303276062,
        0.5927438139915466,
        0.12651529908180237,
        0.18484464287757874,
        -0.07279288023710251,
        0.7444744110107422,
        -0.49453026056289673,
        -0.19875434041023254,
        -0.26351070404052734,
        1.1443898677825928,
        -0.2656885087490082,
        -0.45604830980300903,
        0.12938912212848663,
        0.0710606500506401,
        0.17515766620635986,
        -0.15161967277526855,
        -0.6415322422981262,
        -0.722751259803772,
        -0.32831689715385437,
        0.12270009517669678,
        0.36057764291763306,
        1.285456895828247,
        0.039914608001708984,
        0.43158531188964844,
        0.2632806897163391,
        0.6643051505088806,
        -0.6324800252914429,
        -0.9394164085388184,
        0.2857585549354553,
        0.11771531403064728,
        -0.5632623434066772,
        0.20599116384983063,
        0.21536120772361755,
        -0.012391649186611176,
        0.8450040817260742,
        0.12530633807182312,
        -0.2543010115623474,
        0.39715301990509033,
        -1.0093008279800415,
        1.2333558797836304,
        -0.4678681790828705,
        0.15436552464962006,
        -0.2904500663280487,
        0.5824551582336426,
        -0.47268250584602356,
        0.4628622531890869,
        0.013734787702560425,
        -0.35567331314086914,
        -0.22432176768779755,
        0.06309987604618073,
        0.3606368899345398,
        -0.7659236192703247,
        1.0577445030212402,
        0.6792078018188477,
        0.20818611979484558,
        -0.03049454838037491,
        0.05172574892640114,
        -0.17926162481307983,
        0.2688973546028137,
        -0.16908572614192963,
        0.32030028104782104,
        -0.5366711616516113,
        -1.6399364471435547,
        -0.5422986745834351
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes cybersecurity breach articles by extracting key information efficiently, focusing on conciseness and organization. It avoids inferential conclusions, relying solely on the article's content for details like attack date, type, and impact. The output is a structured summary with specific details about the cybersecurity incident, including attack methods, vulnerabilities, and recommendations for prevention.",
          "name": "Analyze_incident",
          "raw": "\n                workflow Analyze_incident v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n\nCybersecurity Hack Article Analysis: Efficient Data Extraction\n\nObjective: To swiftly and effectively gather essential information from articles about cybersecurity breaches, prioritizing conciseness and order.\n\nInstructions:\nFor each article, extract the specified information below, presenting it in an organized and succinct format. Ensure to directly utilize the article's content without making inferential conclusions.\n\n- Attack Date: YYYY-MM-DD\n- Summary: A concise overview in one sentence.\n- Key Details:\n    - Attack Type: Main method used (e.g., \\\"Ransomware\\\").\n    - Vulnerable Component: The exploited element (e.g., \\\"Email system\\\").\n    - Attacker Information: \n        - Name/Organization: When available (e.g., \\\"APT28\\\").\n        - Country of Origin: If identified (e.g., \\\"China\\\").\n    - Target Information:\n        - Name: The targeted entity.\n        - Country: Location of impact (e.g., \\\"USA\\\").\n        - Size: Entity size (e.g., \\\"Large enterprise\\\").\n        - Industry: Affected sector (e.g., \\\"Healthcare\\\").\n    - Incident Details:\n        - CVE's: Identified CVEs (e.g., CVE-XXX, CVE-XXX).\n        - Accounts Compromised: Quantity (e.g., \\\"5000\\\").\n        - Business Impact: Brief description (e.g., \\\"Operational disruption\\\").\n        - Impact Explanation: In one sentence.\n        - Root Cause: Principal reason (e.g., \\\"Unpatched software\\\").\n- Analysis & Recommendations:\n    - MITRE ATT&CK Analysis: Applicable tactics/techniques (e.g., \\\"T1566, T1486\\\").\n    - Atomic Red Team Atomics: Recommended tests (e.g., \\\"T1566.001\\\").\n    - Remediation:\n        - Recommendation: Summary of action (e.g., \\\"Implement MFA\\\").\n        - Action Plan: Stepwise approach (e.g., \\\"1. Update software, 2. Train staff\\\").\n    - Lessons Learned: Brief insights gained that could prevent future incidents.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n\nCybersecurity Hack Article Analysis: Efficient Data Extraction\n\nObjective: To swiftly and effectively gather essential information from articles about cybersecurity breaches, prioritizing conciseness and order.\n\nInstructions:\nFor each article, extract the specified information below, presenting it in an organized and succinct format. Ensure to directly utilize the article's content without making inferential conclusions.\n\n- Attack Date: YYYY-MM-DD\n- Summary: A concise overview in one sentence.\n- Key Details:\n    - Attack Type: Main method used (e.g., \\\"Ransomware\\\").\n    - Vulnerable Component: The exploited element (e.g., \\\"Email system\\\").\n    - Attacker Information: \n        - Name/Organization: When available (e.g., \\\"APT28\\\").\n        - Country of Origin: If identified (e.g., \\\"China\\\").\n    - Target Information:\n        - Name: The targeted entity.\n        - Country: Location of impact (e.g., \\\"USA\\\").\n        - Size: Entity size (e.g., \\\"Large enterprise\\\").\n        - Industry: Affected sector (e.g., \\\"Healthcare\\\").\n    - Incident Details:\n        - CVE's: Identified CVEs (e.g., CVE-XXX, CVE-XXX).\n        - Accounts Compromised: Quantity (e.g., \\\"5000\\\").\n        - Business Impact: Brief description (e.g., \\\"Operational disruption\\\").\n        - Impact Explanation: In one sentence.\n        - Root Cause: Principal reason (e.g., \\\"Unpatched software\\\").\n- Analysis & Recommendations:\n    - MITRE ATT&CK Analysis: Applicable tactics/techniques (e.g., \\\"T1566, T1486\\\").\n    - Atomic Red Team Atomics: Recommended tests (e.g., \\\"T1566.001\\\").\n    - Remediation:\n        - Recommendation: Summary of action (e.g., \\\"Implement MFA\\\").\n        - Action Plan: Stepwise approach (e.g., \\\"1. Update software, 2. Train staff\\\").\n    - Lessons Learned: Brief insights gained that could prevent future incidents.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.25426921248435974,
        0.34344273805618286,
        -0.13751645386219025,
        0.5223948359489441,
        0.06975057721138,
        -0.24204418063163757,
        -0.5965335369110107,
        -0.2235465943813324,
        0.15205179154872894,
        0.10512086749076843,
        -0.6169906258583069,
        1.0204254388809204,
        -0.20985548198223114,
        -0.29952773451805115,
        -0.09890052676200867,
        -0.5120842456817627,
        -0.8509005308151245,
        -0.4307539761066437,
        -1.5661442279815674,
        -0.22917768359184265,
        0.2798215448856354,
        0.5684838891029358,
        0.2363412082195282,
        0.07660843431949615,
        0.5680909752845764,
        0.2791505753993988,
        -0.02295287512242794,
        0.030301958322525024,
        -1.5530998706817627,
        -1.0686182975769043,
        0.34755566716194153,
        0.058743756264448166,
        -0.9592599868774414,
        -0.7687554359436035,
        0.4344130754470825,
        -0.17664554715156555,
        0.09894169867038727,
        0.2979229986667633,
        -0.8631364703178406,
        -0.48902368545532227,
        0.015305774286389351,
        0.6997723579406738,
        -0.2700845003128052,
        -0.09751761704683304,
        0.023848410695791245,
        -0.7184777855873108,
        0.35259413719177246,
        -0.1681455671787262,
        1.0827077627182007,
        0.2105521559715271,
        -0.46782830357551575,
        -0.205692321062088,
        -0.107352614402771,
        -0.22762098908424377,
        -0.8122788667678833,
        -0.036571722477674484,
        -0.2490376979112625,
        -0.22706475853919983,
        -0.09633971750736237,
        -0.06159880757331848,
        0.1476472020149231,
        -0.1489853858947754,
        -3.646092653274536,
        -0.40778496861457825,
        -0.06254849582910538,
        0.16962231695652008,
        0.05262533947825432,
        0.43704280257225037,
        0.24556580185890198,
        0.3349381983280182,
        -0.6959380507469177,
        0.4396396279335022,
        -0.15285474061965942,
        0.7647926211357117,
        -0.04136371612548828,
        0.1844416856765747,
        0.24976493418216705,
        0.03354916721582413,
        0.591083824634552,
        -0.41503041982650757,
        -0.21541018784046173,
        0.8240591287612915,
        0.12093066424131393,
        -0.5944334864616394,
        -0.24089188873767853,
        0.4771672189235687,
        -0.2535190284252167,
        0.02375662326812744,
        1.080762267112732,
        0.016086246818304062,
        0.07188284397125244,
        -0.4848848879337311,
        0.5789970755577087,
        -0.07133905589580536,
        -0.3434654474258423,
        0.09473282098770142,
        -0.2521405816078186,
        1.079376220703125,
        0.34323152899742126,
        3.3104825019836426,
        0.9393188953399658,
        0.05073924735188484,
        0.16840271651744843,
        -0.8731880187988281,
        0.6726698279380798,
        -0.4869143068790436,
        0.2482713907957077,
        -0.13226084411144257,
        -0.14632244408130646,
        -0.04494944214820862,
        0.0676761269569397,
        0.17540410161018372,
        -0.0572403222322464,
        1.2986584901809692,
        0.34868112206459045,
        0.3127753734588623,
        -0.9804253578186035,
        0.13430778682231903,
        -0.43648257851600647,
        0.43010735511779785,
        -0.8242908716201782,
        0.16393959522247314,
        -0.170308455824852,
        -0.4517085552215576,
        -0.2749512791633606,
        -0.5914173722267151,
        -0.027677660807967186,
        0.5109665989875793,
        0.5436304807662964,
        0.4675183594226837,
        0.29022225737571716,
        -0.32129091024398804,
        -0.5496119856834412,
        -0.005904327146708965,
        0.007668975740671158,
        -0.6168533563613892,
        -0.3281247019767761,
        0.44412899017333984,
        -0.46754491329193115,
        -0.3734065890312195,
        0.19051571190357208,
        -0.8895931243896484,
        0.7701413035392761,
        1.0239075422286987,
        0.48842060565948486,
        -0.10852818191051483,
        -0.30325227975845337,
        0.06487782299518585,
        -0.11907200515270233,
        -0.8838568925857544,
        0.2335764616727829,
        0.8293182849884033,
        -0.13000407814979553,
        0.4218813180923462,
        0.5800342559814453,
        -0.026496611535549164,
        -0.427489697933197,
        0.2107464075088501,
        -0.46074485778808594,
        -0.2279047667980194,
        -0.4748570919036865,
        -0.10995953530073166,
        -0.010606616735458374,
        0.7799069881439209,
        1.2844170331954956,
        -0.5646397471427917,
        0.23068782687187195,
        0.040766745805740356,
        0.515744686126709,
        -0.31520065665245056,
        0.5299229621887207,
        -0.28033682703971863,
        -0.0011108899489045143,
        0.8917461037635803,
        -0.25640228390693665,
        0.09399694204330444,
        -0.315487265586853,
        0.09687938541173935,
        0.014001773670315742,
        -0.6057853698730469,
        0.79417884349823,
        0.9573003649711609,
        0.35727936029434204,
        -0.23107880353927612,
        0.42829298973083496,
        0.12103535979986191,
        0.8548058271408081,
        0.030794857069849968,
        0.6195195913314819,
        1.0111818313598633,
        -0.8284851908683777,
        1.937374234199524,
        -1.055169701576233,
        -0.15319547057151794,
        0.0699639841914177,
        -0.2384939044713974,
        -0.42130255699157715,
        0.6724587678909302,
        0.6394675374031067,
        -0.3761245608329773,
        -0.253212034702301,
        -0.1778797209262848,
        -0.3667478859424591,
        -0.005055569112300873,
        -0.2063661813735962,
        0.40524154901504517,
        -0.025455299764871597,
        -0.47687387466430664,
        0.08929403126239777,
        -1.0928629636764526,
        -0.14897264540195465,
        0.21878564357757568,
        0.21268132328987122,
        0.5944773554801941,
        0.6017862558364868,
        -0.2823983430862427,
        -0.38719236850738525,
        -0.09886923432350159,
        -0.12978988885879517,
        0.4426906108856201,
        -0.20961904525756836,
        -0.4047394394874573,
        -0.46541595458984375,
        -0.601276159286499,
        -1.1855897903442383,
        1.4686294794082642,
        -0.20013545453548431,
        0.0014742743223905563,
        -0.3319966495037079,
        -0.6103001832962036,
        1.0174757242202759,
        1.3055059909820557,
        0.9418016076087952,
        1.122236967086792,
        0.4374874234199524,
        -0.08129364997148514,
        0.02384079247713089,
        0.4718155562877655,
        -0.19764849543571472,
        -0.5538504123687744,
        0.7116891145706177,
        -0.35681092739105225,
        -0.9274457097053528,
        0.16098864376544952,
        -0.08814699947834015,
        0.3672676086425781,
        -0.5391232371330261,
        0.4024953246116638,
        -0.10437222570180893,
        1.1082518100738525,
        1.244014024734497,
        0.5658825635910034,
        -0.47839877009391785,
        0.7562660574913025,
        -0.18862488865852356,
        -0.062045298516750336,
        -2.1359832286834717,
        0.021900804713368416,
        -0.38597965240478516,
        0.036588285118341446,
        -0.42858919501304626,
        0.06288842111825943,
        0.6020070910453796,
        0.16681712865829468,
        0.031059177592396736,
        0.5658512711524963,
        -0.7504624724388123,
        -0.022970423102378845,
        -0.35851675271987915,
        -0.07616323232650757,
        -0.7731659412384033,
        -0.13784854114055634,
        -0.4282459020614624,
        0.03482179343700409,
        0.15110580623149872,
        0.07170896977186203,
        0.2632056474685669,
        0.2607223689556122,
        -0.08344924449920654,
        0.4321832060813904,
        -0.3619648516178131,
        0.5565359592437744,
        0.6134705543518066,
        0.4502936601638794,
        -0.31943002343177795,
        0.49220365285873413,
        0.24950431287288666,
        -0.7762487530708313,
        0.21318775415420532,
        0.7258219718933105,
        -1.1238287687301636,
        -0.09505024552345276,
        -0.6837905049324036,
        -0.2099015712738037,
        1.6921911239624023,
        0.2762751579284668,
        0.09352124482393265,
        0.37284278869628906,
        0.7288805842399597,
        -0.3244220018386841,
        -0.9003456234931946,
        0.15501371026039124,
        -0.15415704250335693,
        0.18948496878147125,
        -0.4087965190410614,
        -0.5456750392913818,
        0.8047888875007629,
        -0.13949808478355408,
        -0.3458887040615082,
        0.14425812661647797,
        -0.4831475019454956,
        0.43869438767433167,
        0.5140661001205444,
        -0.18431764841079712,
        0.6446762681007385,
        -0.6210160255432129,
        0.2211255580186844,
        0.6600073575973511,
        -0.49700647592544556,
        -1.4817272424697876,
        0.033432647585868835,
        0.5800118446350098,
        0.4668084383010864,
        0.2501019239425659,
        0.18821975588798523,
        0.17480409145355225,
        0.34904858469963074,
        -0.19293946027755737,
        -0.2842280864715576,
        1.0419713258743286,
        0.6335179805755615,
        -0.49435654282569885,
        -0.7376682758331299,
        -0.3046410381793976,
        0.24447917938232422,
        -0.35769596695899963,
        -0.1184096559882164,
        -0.23511388897895813,
        -1.0454438924789429,
        -0.3817102313041687,
        0.5445420145988464,
        1.1396002769470215,
        0.2781773507595062,
        0.20840413868427277,
        0.5744178295135498,
        0.34872522950172424,
        -0.33733540773391724,
        -1.2944341897964478,
        0.48870688676834106,
        -0.4579607844352722,
        -0.6281785368919373,
        0.6693087220191956,
        0.6507583856582642,
        -0.33104580640792847,
        -0.0526752807199955,
        0.2811167240142822,
        -0.5716508030891418,
        0.475864052772522,
        -0.7470945119857788,
        1.5691518783569336,
        -0.7250752449035645,
        -0.48541197180747986,
        -0.3383777141571045,
        0.8694145679473877,
        -0.4769675135612488,
        -0.17343951761722565,
        0.01722892001271248,
        -0.6677463054656982,
        0.04652572423219681,
        -0.017427273094654083,
        0.3921700119972229,
        -0.9416658878326416,
        0.7378102540969849,
        0.43210339546203613,
        0.6014761924743652,
        0.11035214364528656,
        -0.3529040217399597,
        0.2304297387599945,
        -0.28496819734573364,
        0.008618995547294617,
        0.4399852156639099,
        0.18355122208595276,
        -1.1183981895446777,
        -0.8203179240226746
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes a server log file to identify patterns, anomalies, and potential issues, aiming to enhance the server's reliability and performance. The process involves a detailed examination of log entries, assessment of operational reliability, and identification of recurring issues. Recommendations for improvements are provided based on data-driven analysis, excluding personal opinions and irrelevant information.",
          "name": "Analyze_logs",
          "raw": "\n                workflow Analyze_logs v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\nYou are a system administrator and service reliability engineer at a large tech company. You are responsible for ensuring the reliability and availability of the company's services. You have a deep understanding of the company's infrastructure and services. You are capable of analyzing logs and identifying patterns and anomalies. You are proficient in using various monitoring and logging tools. You are skilled in troubleshooting and resolving issues quickly. You are detail-oriented and have a strong analytical mindset. You are familiar with incident response procedures and best practices. You are always looking for ways to improve the reliability and performance of the company's services. you have a strong background in computer science and system administration, with 1500 years of experience in the field.\n\n# Task\nYou are given a log file from one of the company's servers. The log file contains entries of various events and activities. Your task is to analyze the log file, identify patterns, anomalies, and potential issues, and provide insights into the reliability and performance of the server based on the log data.\n\n# Actions\n- **Analyze the Log File**: Thoroughly examine the log entries to identify any unusual patterns or anomalies that could indicate potential issues.\n- **Assess Server Reliability and Performance**: Based on your analysis, provide insights into the server's operational reliability and overall performance.\n- **Identify Recurring Issues**: Look for any recurring patterns or persistent issues in the log data that could potentially impact server reliability.\n- **Recommend Improvements**: Suggest actionable improvements or optimizations to enhance server performance based on your findings from the log data.\n\n# Restrictions\n- **Avoid Irrelevant Information**: Do not include details that are not derived from the log file.\n- **Base Assumptions on Data**: Ensure that all assumptions about the log data are clearly supported by the information contained within.\n- **Focus on Data-Driven Advice**: Provide specific recommendations that are directly based on your analysis of the log data.\n- **Exclude Personal Opinions**: Refrain from including subjective assessments or personal opinions in your analysis.\n\n# INPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\nYou are a system administrator and service reliability engineer at a large tech company. You are responsible for ensuring the reliability and availability of the company's services. You have a deep understanding of the company's infrastructure and services. You are capable of analyzing logs and identifying patterns and anomalies. You are proficient in using various monitoring and logging tools. You are skilled in troubleshooting and resolving issues quickly. You are detail-oriented and have a strong analytical mindset. You are familiar with incident response procedures and best practices. You are always looking for ways to improve the reliability and performance of the company's services. you have a strong background in computer science and system administration, with 1500 years of experience in the field.\n\n# Task\nYou are given a log file from one of the company's servers. The log file contains entries of various events and activities. Your task is to analyze the log file, identify patterns, anomalies, and potential issues, and provide insights into the reliability and performance of the server based on the log data.\n\n# Actions\n- **Analyze the Log File**: Thoroughly examine the log entries to identify any unusual patterns or anomalies that could indicate potential issues.\n- **Assess Server Reliability and Performance**: Based on your analysis, provide insights into the server's operational reliability and overall performance.\n- **Identify Recurring Issues**: Look for any recurring patterns or persistent issues in the log data that could potentially impact server reliability.\n- **Recommend Improvements**: Suggest actionable improvements or optimizations to enhance server performance based on your findings from the log data.\n\n# Restrictions\n- **Avoid Irrelevant Information**: Do not include details that are not derived from the log file.\n- **Base Assumptions on Data**: Ensure that all assumptions about the log data are clearly supported by the information contained within.\n- **Focus on Data-Driven Advice**: Provide specific recommendations that are directly based on your analysis of the log data.\n- **Exclude Personal Opinions**: Refrain from including subjective assessments or personal opinions in your analysis.\n\n# INPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4067731201648712,
        -0.04894086718559265,
        -0.10805705189704895,
        -0.05976817011833191,
        0.4016798138618469,
        -0.2796284258365631,
        -0.7987851500511169,
        -0.21628756821155548,
        -0.364492267370224,
        0.5517075061798096,
        -0.24781399965286255,
        0.8618816137313843,
        0.14870858192443848,
        0.000580422580242157,
        0.2693900763988495,
        -0.431706964969635,
        0.01224005687981844,
        -1.2414814233779907,
        -1.1294186115264893,
        -0.5987325310707092,
        0.051138799637556076,
        0.779912531375885,
        0.10904210805892944,
        0.0023609548807144165,
        0.5107638239860535,
        -0.05720089003443718,
        -0.6595028042793274,
        -0.5836318731307983,
        -0.7059198021888733,
        -0.974857747554779,
        0.785871148109436,
        0.00439298152923584,
        -0.9618006348609924,
        -0.42562776803970337,
        -0.04277288168668747,
        -0.9972568154335022,
        0.9180377721786499,
        0.22434090077877045,
        -0.6821348667144775,
        -0.32964494824409485,
        0.15751907229423523,
        0.3731754422187805,
        -0.37047502398490906,
        -0.33647722005844116,
        -0.09514705836772919,
        -0.6377161145210266,
        0.38923314213752747,
        0.2345019280910492,
        1.0480114221572876,
        0.3907889723777771,
        -0.6093147993087769,
        -0.35482949018478394,
        -0.2056683450937271,
        0.07290562987327576,
        -0.8995538353919983,
        -0.26909589767456055,
        0.290238618850708,
        -0.1666712611913681,
        -0.24414192140102386,
        -0.6551361680030823,
        0.28960299491882324,
        0.12181626260280609,
        -2.997483968734741,
        -0.2514199912548065,
        0.30850568413734436,
        0.13046002388000488,
        0.33882927894592285,
        -0.06447969377040863,
        0.20235203206539154,
        0.2791210114955902,
        -0.667415976524353,
        -0.14839422702789307,
        -0.46541985869407654,
        0.6104020476341248,
        0.472098708152771,
        0.47266873717308044,
        0.18314321339130402,
        -0.14282307028770447,
        0.5733217597007751,
        -0.2585636377334595,
        0.2393190711736679,
        1.004220962524414,
        0.3309006690979004,
        -0.30074217915534973,
        -0.9569055438041687,
        0.231277197599411,
        -0.2944968044757843,
        -0.15558183193206787,
        1.0681726932525635,
        0.18181240558624268,
        -0.11309589445590973,
        -0.5244020223617554,
        0.027434738352894783,
        0.3314650356769562,
        0.027355153113603592,
        -0.05226900428533554,
        -0.6181867122650146,
        0.5940045118331909,
        0.4027123749256134,
        3.4451677799224854,
        0.6342963576316833,
        -0.3379294276237488,
        0.4482648968696594,
        -0.9816486835479736,
        0.20706814527511597,
        -0.30429786443710327,
        0.3345355987548828,
        -0.29157763719558716,
        0.3138808310031891,
        0.6869505047798157,
        -0.017454728484153748,
        -0.24163943529129028,
        -0.07056523859500885,
        0.3788715600967407,
        0.45194289088249207,
        0.46406424045562744,
        -0.689818263053894,
        -0.014827188104391098,
        -0.013782666064798832,
        -0.23828798532485962,
        -0.7225982546806335,
        0.39421501755714417,
        -0.2823244035243988,
        -0.2608240246772766,
        0.07435673475265503,
        -0.10103832185268402,
        -0.20566760003566742,
        0.6364060640335083,
        0.1826450675725937,
        0.4590696096420288,
        0.20145869255065918,
        -0.17024141550064087,
        -0.5500656366348267,
        0.02804400958120823,
        0.2666453421115875,
        -0.2583613991737366,
        0.0843556672334671,
        0.4057023227214813,
        -0.05497206747531891,
        -0.15692901611328125,
        -0.3999771475791931,
        -1.0291386842727661,
        0.8592596650123596,
        0.6330035924911499,
        0.5877358317375183,
        0.4088729918003082,
        -0.18981030583381653,
        0.3785855174064636,
        -0.6115156412124634,
        -1.0353143215179443,
        0.14273913204669952,
        1.1910076141357422,
        0.16168150305747986,
        0.2726272940635681,
        0.9521030187606812,
        0.2463226169347763,
        0.2618095874786377,
        0.2369859665632248,
        -0.6001817584037781,
        0.11826053261756897,
        -0.15406490862369537,
        -0.3410646617412567,
        -0.2414819300174713,
        0.7165404558181763,
        0.7990632653236389,
        -0.02213657647371292,
        0.3128461539745331,
        -0.037155650556087494,
        0.685241162776947,
        -0.1720225065946579,
        1.1056824922561646,
        0.01926492154598236,
        0.33840498328208923,
        0.3789595067501068,
        -0.0004434618167579174,
        -0.18631026148796082,
        -0.6542283296585083,
        -0.1103140339255333,
        -0.1781781017780304,
        -1.120977520942688,
        0.7810835838317871,
        0.6668064594268799,
        0.29991185665130615,
        -0.42689836025238037,
        0.06450051069259644,
        0.3259444832801819,
        0.5441296696662903,
        -0.22416159510612488,
        0.6399046778678894,
        0.7079706192016602,
        -0.42575639486312866,
        1.425572156906128,
        -0.6110769510269165,
        -0.8488902449607849,
        0.04940085858106613,
        0.36528539657592773,
        -0.07997103780508041,
        0.6580771207809448,
        0.20747070014476776,
        0.021447423845529556,
        -0.5809913277626038,
        -0.2344692498445511,
        -0.6919687986373901,
        -0.3964907228946686,
        -0.8767662048339844,
        0.3113633990287781,
        0.30858173966407776,
        -0.4151943325996399,
        0.07536238431930542,
        -0.7780977487564087,
        -0.13958512246608734,
        0.2583020329475403,
        0.32336869835853577,
        0.8272571563720703,
        0.5699361562728882,
        -0.5906050205230713,
        0.2733006477355957,
        -0.01442226767539978,
        -0.13017967343330383,
        0.4571167230606079,
        -0.1556718945503235,
        0.09788398444652557,
        -0.9346112012863159,
        -0.7646278738975525,
        -1.3266291618347168,
        0.7966861724853516,
        -0.23696237802505493,
        0.02916809916496277,
        -0.7252873182296753,
        -0.6732658743858337,
        0.016269013285636902,
        0.9514898657798767,
        0.8122837543487549,
        1.102405309677124,
        0.4061679542064667,
        -0.1408538818359375,
        0.2043038308620453,
        0.46272796392440796,
        0.2036363184452057,
        -0.28530406951904297,
        1.0819711685180664,
        -0.3971155881881714,
        -0.9651598334312439,
        0.37595927715301514,
        0.00804319977760315,
        0.40262123942375183,
        -0.19622567296028137,
        0.38384872674942017,
        0.17473098635673523,
        1.265303373336792,
        1.1234164237976074,
        0.27093008160591125,
        0.16321268677711487,
        0.5092511773109436,
        0.115190289914608,
        0.05268150195479393,
        -1.722856879234314,
        -0.3381083011627197,
        -0.3959980309009552,
        -0.3916359543800354,
        0.03712484985589981,
        -0.47277745604515076,
        0.823413074016571,
        0.23235398530960083,
        -0.16498956084251404,
        0.6736086010932922,
        -0.5389692187309265,
        -0.15925824642181396,
        -0.5261054039001465,
        0.18710938096046448,
        -0.21178095042705536,
        -0.0014452040195465088,
        -0.2072092592716217,
        -0.4649803042411804,
        -0.2792721092700958,
        0.38692983984947205,
        0.11363130807876587,
        0.2152988612651825,
        -0.2534651756286621,
        0.23410873115062714,
        0.3269791901111603,
        0.9124240279197693,
        0.6514147520065308,
        0.37929123640060425,
        -0.2978668808937073,
        0.339855819940567,
        -0.0007079541683197021,
        -0.5944238305091858,
        0.21361875534057617,
        0.8528221249580383,
        -0.8838469386100769,
        -0.5749788284301758,
        -0.14996665716171265,
        0.00407680869102478,
        2.0464987754821777,
        0.527131199836731,
        0.4710176885128021,
        0.6223046183586121,
        0.5943810343742371,
        -0.15953762829303741,
        -1.0341095924377441,
        0.2264259159564972,
        -0.034810870885849,
        -0.09232152998447418,
        0.277077853679657,
        -0.5920558571815491,
        0.9419100880622864,
        0.0041264258325099945,
        -0.38740530610084534,
        -0.1737762987613678,
        -0.8316482305526733,
        0.563586413860321,
        0.4676949977874756,
        -0.26983731985092163,
        0.1101251095533371,
        -0.851524829864502,
        0.7540777921676636,
        0.31141406297683716,
        -0.1625267118215561,
        -2.055893659591675,
        -0.004675948992371559,
        0.5643706917762756,
        -0.13284792006015778,
        -0.31601548194885254,
        0.34382787346839905,
        1.2909799814224243,
        -0.6476159691810608,
        -0.3980505168437958,
        -0.4106230139732361,
        1.4069820642471313,
        -0.30671727657318115,
        -0.32792723178863525,
        -0.6596953868865967,
        0.14046314358711243,
        0.28401845693588257,
        -0.38500651717185974,
        -0.039339907467365265,
        -0.4250659942626953,
        -1.1930062770843506,
        -0.30062684416770935,
        -0.05876997113227844,
        1.5048952102661133,
        -0.2091202735900879,
        0.28220394253730774,
        0.5777330994606018,
        0.11243341863155365,
        -0.6646595001220703,
        -1.0091421604156494,
        0.3554592728614807,
        0.30498939752578735,
        -1.2407569885253906,
        0.5346552133560181,
        0.27964845299720764,
        -0.30042552947998047,
        0.7988247871398926,
        0.02152155712246895,
        -0.6903384327888489,
        0.019697174429893494,
        -0.642795741558075,
        0.5494660139083862,
        -0.15807391703128815,
        -0.002293536439538002,
        -0.21804949641227722,
        0.7480948567390442,
        -0.5861527323722839,
        0.14957517385482788,
        0.5126378536224365,
        -0.9567984938621521,
        -0.200491800904274,
        0.1640297770500183,
        0.017719129100441933,
        -0.5188226699829102,
        0.6998511552810669,
        -0.014547176659107208,
        0.37185049057006836,
        -0.1685594767332077,
        -0.36011043190956116,
        0.5777413845062256,
        0.018770232796669006,
        -0.023549847304821014,
        0.5059592127799988,
        0.16937941312789917,
        -1.1683459281921387,
        -0.4650725722312927
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes malware across various platforms, focusing on extracting indicators of compromise and detailed malware behavior. This approach includes analyzing telemetry and community data to aid in malware detection and analysis. The expected output includes a summary of findings, potential indicators of compromise, Mitre Att&CK techniques, pivoting advice, detection strategies, suggested Yara rules, additional references, and technical recommendations.",
          "name": "Analyze_malware",
          "raw": "\n                workflow Analyze_malware v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\nYou are a malware analysis expert and you are able to understand a malware for any kind of platform including, Windows, MacOS, Linux or android.\nYou specialize in extracting indicators of compromise, malware information including its behavior, its details, info from the telemetry and community and any other relevant information that helps a malware analyst.\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\nRead the entire information from an malware expert perspective, thinking deeply about crucial details about the malware that can help in understanding its behavior, detection and capabilities. Also extract Mitre Att&CK techniques.\nCreate a summary sentence that captures and highlight the most important findings of the report and its insights in less than 25 words in a section called ONE-SENTENCE-SUMMARY:. Use plain and conversational language when creating this summary. You can use technical jargon but no marketing language.\n\n- Extract all the information that allows to clearly define the malware for detection and analysis and provide information about the structure of the file in a section called OVERVIEW.\n- Extract all potential indicator that might be useful such as IP, Domain, Registry key, filepath, mutex and others in a section called POTENTIAL IOCs. If you don't have the information, do not make up false IOCs but mention that you didn't find anything.\n- Extract all potential Mitre Att&CK techniques related to the information you have in a section called ATT&CK.\n- Extract all information that can help in pivoting such as IP, Domain, hashes, and offer some advice about potential pivot that could help the analyst. Write this in a section called POTENTIAL PIVOTS.\n- Extract information related to detection in a section called DETECTION.\n- Suggest a Yara rule based on the unique strings output and structure of the file in a section called SUGGESTED YARA RULE.\n- If there is any additional reference in comment or elsewhere mention it in a section called ADDITIONAL REFERENCES.\n- Provide some recommandation in term of detection and further steps only backed by technical data you have in a section called RECOMMANDATIONS.\n\n# OUTPUT INSTRUCTIONS\nOnly output Markdown.\nDo not output the markdown code syntax, only the content.\nDo not use bold or italics formatting in the markdown output.\nExtract at least basic information about the malware.\nExtract all potential information for the other output sections but do not create something, if you don't know simply say it.\nDo not give warnings or notes; only output the requested sections.\nYou use bulleted lists for output, not numbered lists.\nDo not repeat ideas, facts, or resources.\nDo not start items with the same opening words.\nEnsure you follow ALL these instructions when creating your output.\n\n# INPUT\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\nYou are a malware analysis expert and you are able to understand a malware for any kind of platform including, Windows, MacOS, Linux or android.\nYou specialize in extracting indicators of compromise, malware information including its behavior, its details, info from the telemetry and community and any other relevant information that helps a malware analyst.\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\nRead the entire information from an malware expert perspective, thinking deeply about crucial details about the malware that can help in understanding its behavior, detection and capabilities. Also extract Mitre Att&CK techniques.\nCreate a summary sentence that captures and highlight the most important findings of the report and its insights in less than 25 words in a section called ONE-SENTENCE-SUMMARY:. Use plain and conversational language when creating this summary. You can use technical jargon but no marketing language.\n\n- Extract all the information that allows to clearly define the malware for detection and analysis and provide information about the structure of the file in a section called OVERVIEW.\n- Extract all potential indicator that might be useful such as IP, Domain, Registry key, filepath, mutex and others in a section called POTENTIAL IOCs. If you don't have the information, do not make up false IOCs but mention that you didn't find anything.\n- Extract all potential Mitre Att&CK techniques related to the information you have in a section called ATT&CK.\n- Extract all information that can help in pivoting such as IP, Domain, hashes, and offer some advice about potential pivot that could help the analyst. Write this in a section called POTENTIAL PIVOTS.\n- Extract information related to detection in a section called DETECTION.\n- Suggest a Yara rule based on the unique strings output and structure of the file in a section called SUGGESTED YARA RULE.\n- If there is any additional reference in comment or elsewhere mention it in a section called ADDITIONAL REFERENCES.\n- Provide some recommandation in term of detection and further steps only backed by technical data you have in a section called RECOMMANDATIONS.\n\n# OUTPUT INSTRUCTIONS\nOnly output Markdown.\nDo not output the markdown code syntax, only the content.\nDo not use bold or italics formatting in the markdown output.\nExtract at least basic information about the malware.\nExtract all potential information for the other output sections but do not create something, if you don't know simply say it.\nDo not give warnings or notes; only output the requested sections.\nYou use bulleted lists for output, not numbered lists.\nDo not repeat ideas, facts, or resources.\nDo not start items with the same opening words.\nEnsure you follow ALL these instructions when creating your output.\n\n# INPUT\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6193559765815735,
        0.12163189053535461,
        0.12450140714645386,
        0.2693197727203369,
        0.2982461154460907,
        -0.13171860575675964,
        -1.0168830156326294,
        -0.23196691274642944,
        -0.06109202653169632,
        0.3947681784629822,
        -0.10593411326408386,
        1.3842957019805908,
        -0.11598297953605652,
        -0.49515169858932495,
        0.19620764255523682,
        -0.7711256146430969,
        -0.9160914421081543,
        -0.6570651531219482,
        -1.4623377323150635,
        -0.16029265522956848,
        0.28721773624420166,
        0.6228731870651245,
        -0.29397475719451904,
        0.1823125183582306,
        0.8740537166595459,
        0.39248818159103394,
        -0.05430031567811966,
        -0.17673805356025696,
        -0.832329273223877,
        -1.4848766326904297,
        0.30605441331863403,
        0.4610322117805481,
        -1.1553077697753906,
        -0.8454223871231079,
        0.27535009384155273,
        -0.3710440397262573,
        0.15822666883468628,
        0.09665604680776596,
        -0.6074431538581848,
        -0.45406967401504517,
        -0.0773765966296196,
        0.42586225271224976,
        0.19028392434120178,
        -0.38774874806404114,
        0.5392126441001892,
        -0.36046746373176575,
        -0.06682710349559784,
        -0.3996277153491974,
        0.9757445454597473,
        0.6561463475227356,
        -0.3663131594657898,
        -0.4644486606121063,
        -0.19809937477111816,
        -0.9913735389709473,
        -0.660466730594635,
        -0.48244404792785645,
        0.22709250450134277,
        -0.7593980431556702,
        -0.09197208285331726,
        -0.03906548023223877,
        0.3436870574951172,
        0.6041878461837769,
        -3.3824191093444824,
        0.02052072435617447,
        0.021088983863592148,
        0.12689189612865448,
        0.014243110083043575,
        -0.09022225439548492,
        0.11666593700647354,
        -0.03245902061462402,
        -0.3176862895488739,
        0.3947446942329407,
        -0.11985500156879425,
        -0.0735824704170227,
        0.10574474185705185,
        -0.10167863965034485,
        0.14501117169857025,
        0.07959498465061188,
        0.591192364692688,
        -0.7881333827972412,
        -0.00465388922020793,
        0.13347776234149933,
        0.21357189118862152,
        -0.04362871125340462,
        -0.4387293756008148,
        0.5248622894287109,
        0.053813159465789795,
        0.3825807571411133,
        0.8539078235626221,
        -0.12079407274723053,
        0.11053599417209625,
        -0.16559481620788574,
        0.0905480831861496,
        -0.30446314811706543,
        -0.35887351632118225,
        -0.1416669338941574,
        0.122609943151474,
        0.414334774017334,
        0.11673344671726227,
        3.677091121673584,
        0.737680971622467,
        0.36341390013694763,
        0.5579354166984558,
        -1.2544468641281128,
        0.44318529963493347,
        -0.6551414728164673,
        0.11591807007789612,
        -0.1361338347196579,
        0.06804079562425613,
        0.3386097550392151,
        0.40164780616760254,
        -0.5446890592575073,
        -0.28109461069107056,
        0.5699731111526489,
        -0.44648444652557373,
        0.9039872288703918,
        -0.7034832835197449,
        -0.611353874206543,
        -0.10146325081586838,
        0.47776153683662415,
        -1.0593209266662598,
        0.3751804828643799,
        -0.029530711472034454,
        -0.8914464712142944,
        -0.16859596967697144,
        -0.19124120473861694,
        -0.10889486223459244,
        0.7770275473594666,
        1.0104409456253052,
        0.495587021112442,
        -0.23233294486999512,
        -0.3442370891571045,
        -0.8113320469856262,
        -0.11523883044719696,
        -0.35329553484916687,
        -0.2862178683280945,
        0.558283805847168,
        -0.49592217803001404,
        0.16237974166870117,
        -0.765229344367981,
        0.640458345413208,
        -0.6080374717712402,
        0.24039602279663086,
        0.4095320999622345,
        1.3505223989486694,
        0.195489302277565,
        -0.16658896207809448,
        -0.03906514495611191,
        -0.24973952770233154,
        -0.8455696702003479,
        -0.33978113532066345,
        0.9459869861602783,
        -0.21256481111049652,
        0.8290319442749023,
        0.37057173252105713,
        0.11189731955528259,
        -0.33984577655792236,
        0.13808384537696838,
        -0.2516726553440094,
        -0.3003854751586914,
        -0.009651988744735718,
        0.10542434453964233,
        0.03608158230781555,
        0.24855703115463257,
        0.9059237241744995,
        -0.237641841173172,
        0.2695232033729553,
        -0.1395474672317505,
        0.4212415814399719,
        0.44691264629364014,
        0.160277858376503,
        0.24599826335906982,
        0.6317862868309021,
        0.7684476971626282,
        0.06862878799438477,
        0.07015466690063477,
        0.002959735691547394,
        0.4950885772705078,
        -0.11454367637634277,
        -0.7264788150787354,
        0.4985543191432953,
        0.6384258270263672,
        0.38122084736824036,
        -0.8223716616630554,
        0.3223441243171692,
        0.15768565237522125,
        0.09646521508693695,
        -0.05548840016126633,
        0.7908316850662231,
        1.081045389175415,
        -1.2028547525405884,
        1.8985655307769775,
        -0.6576736569404602,
        -0.2141108512878418,
        0.14404870569705963,
        -0.37850749492645264,
        -0.04649187996983528,
        0.35251545906066895,
        0.4476003646850586,
        -0.2620868682861328,
        -0.5260204672813416,
        0.10972358286380768,
        -0.02347242832183838,
        -0.48417314887046814,
        -0.3859296441078186,
        -0.24784782528877258,
        -0.019855443388223648,
        -0.0365416444838047,
        0.4791222810745239,
        -0.9665244817733765,
        -0.075725257396698,
        -0.22680988907814026,
        0.22630412876605988,
        0.472405344247818,
        0.5042654275894165,
        0.33019697666168213,
        0.1101258248090744,
        0.1413222998380661,
        -0.24655956029891968,
        0.8011273145675659,
        -0.6305709481239319,
        -0.45924824476242065,
        -0.778179943561554,
        -0.946761965751648,
        -1.1442981958389282,
        0.9962759017944336,
        0.04419881850481033,
        1.0859134197235107,
        -0.2719317078590393,
        -0.5365928411483765,
        0.48832079768180847,
        0.8169964551925659,
        1.3637745380401611,
        0.7527868151664734,
        0.20622900128364563,
        -0.12000637501478195,
        -0.144851952791214,
        0.5967162847518921,
        0.08347383141517639,
        -0.6844161152839661,
        -0.09992659837007523,
        -0.21768179535865784,
        -0.5928100943565369,
        0.9013218879699707,
        -0.16398891806602478,
        0.14958226680755615,
        -0.04106777161359787,
        0.06673578917980194,
        -0.41769877076148987,
        1.258368730545044,
        0.5192856788635254,
        -0.9389291405677795,
        0.42426931858062744,
        0.5464729070663452,
        0.26476171612739563,
        -0.2341131567955017,
        -1.21503746509552,
        -0.24124780297279358,
        -0.39431729912757874,
        0.48015156388282776,
        -0.3076135516166687,
        0.05202603340148926,
        0.7692493200302124,
        0.1275998204946518,
        -0.13077446818351746,
        0.7178693413734436,
        -0.041483573615550995,
        -0.34965038299560547,
        -0.2580070495605469,
        0.42164936661720276,
        0.15967637300491333,
        0.043489836156368256,
        -0.030868571251630783,
        -0.5292619466781616,
        0.18005810678005219,
        0.13897079229354858,
        -0.3365672528743744,
        -0.18389040231704712,
        -0.5046216249465942,
        0.08604137599468231,
        -0.3421649634838104,
        -0.10078839957714081,
        -0.35387539863586426,
        0.7902354598045349,
        -0.17103013396263123,
        0.264047771692276,
        0.169200599193573,
        -0.6434304118156433,
        0.2180289626121521,
        0.170985609292984,
        -0.9343753457069397,
        -0.5303349494934082,
        -0.7680912613868713,
        0.49535366892814636,
        1.5080256462097168,
        0.47519564628601074,
        0.18014733493328094,
        0.3180997669696808,
        0.7963377833366394,
        -0.08616720885038376,
        -0.5734467506408691,
        0.2707323431968689,
        0.009083910845220089,
        0.4610155522823334,
        -0.34015828371047974,
        -0.11109054088592529,
        0.6332154870033264,
        0.1967805176973343,
        -0.44435667991638184,
        -0.08555887639522552,
        -1.271100640296936,
        0.31659257411956787,
        0.2021379917860031,
        -0.16785959899425507,
        0.04594464227557182,
        -0.4525538682937622,
        0.5657556056976318,
        0.7127143144607544,
        0.021137811243534088,
        -1.40431547164917,
        -0.2420591562986374,
        0.8897385597229004,
        0.3761555552482605,
        0.09249058365821838,
        -0.6453319787979126,
        -0.0243404358625412,
        0.3885660171508789,
        -0.2536265254020691,
        -0.3783765435218811,
        1.222406029701233,
        0.6695571541786194,
        -0.16766643524169922,
        -0.5943828225135803,
        -0.13573525846004486,
        0.4571067988872528,
        -0.7666481137275696,
        0.7801637649536133,
        -0.08745858818292618,
        -0.6440483927726746,
        -0.11503888666629791,
        0.1747533679008484,
        0.907555341720581,
        0.32145628333091736,
        -0.42760729789733887,
        0.5113049149513245,
        0.5411786437034607,
        -0.3773612380027771,
        -0.555597186088562,
        0.6724069118499756,
        0.002180948853492737,
        -0.37872663140296936,
        0.7606552243232727,
        0.3173327147960663,
        -0.10038191080093384,
        0.49546852707862854,
        -0.13171008229255676,
        -0.23332327604293823,
        0.8838629722595215,
        0.02513260394334793,
        1.2699403762817383,
        -0.3767518997192383,
        -0.2631397545337677,
        -0.42253968119621277,
        0.1343318223953247,
        -0.5372064709663391,
        0.1557435691356659,
        -0.35106611251831055,
        -1.02100670337677,
        0.18418990075588226,
        0.1354105919599533,
        -0.38874581456184387,
        -0.3775738775730133,
        0.47331568598747253,
        0.3669203817844391,
        1.0975391864776611,
        0.1621444821357727,
        -0.19393020868301392,
        0.37549853324890137,
        0.09755562245845795,
        -0.6106497049331665,
        0.5063762068748474,
        -0.5587590932846069,
        -1.0998682975769043,
        -0.3811197876930237
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This service analyzes research papers to determine their main findings, scientific rigor, and quality. It uniquely maps out claims, evaluates study design, and assesses conflicts of interest. The output includes a summary, author details, findings, study quality, and a final grade with explanations.",
          "name": "Analyze_paper",
          "raw": "\n                workflow Analyze_paper v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a research paper analysis service focused on determining the primary findings of the paper and analyzing its scientific rigor and quality.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Consume the entire paper and think deeply about it.\n\n- Map out all the claims and implications on a virtual whiteboard in your mind.\n\n# OUTPUT \n\n- Extract a summary of the paper and its conclusions into a 25-word sentence called SUMMARY.\n\n- Extract the list of authors in a section called AUTHORS.\n\n- Extract the list of organizations the authors are associated, e.g., which university they're at, with in a section called AUTHOR ORGANIZATIONS.\n\n- Extract the primary paper findings into a bulleted list of no more than 15 words per bullet into a section called FINDINGS.\n\n- Extract the overall structure and character of the study into a bulleted list of 15 words per bullet for the research in a section called STUDY DETAILS.\n\n- Extract the study quality by evaluating the following items in a section called STUDY QUALITY that has the following bulleted sub-sections:\n\n- STUDY DESIGN: (give a 15 word description, including the pertinent data and statistics.)\n\n- SAMPLE SIZE: (give a 15 word description, including the pertinent data and statistics.)\n\n- CONFIDENCE INTERVALS (give a 15 word description, including the pertinent data and statistics.)\n\n- P-VALUE (give a 15 word description, including the pertinent data and statistics.)\n\n- EFFECT SIZE (give a 15 word description, including the pertinent data and statistics.)\n\n- CONSISTENCE OF RESULTS (give a 15 word description, including the pertinent data and statistics.)\n\n- METHODOLOGY TRANSPARENCY (give a 15 word description of the methodology quality and documentation.)\n\n- STUDY REPRODUCIBILITY (give a 15 word description, including how to fully reproduce the study.)\n\n- Data Analysis Method (give a 15 word description, including the pertinent data and statistics.)\n\n- Discuss any Conflicts of Interest in a section called CONFLICTS OF INTEREST. Rate the conflicts of interest as NONE DETECTED, LOW, MEDIUM, HIGH, or CRITICAL.\n\n- Extract the researcher's analysis and interpretation in a section called RESEARCHER'S INTERPRETATION, in a 15-word sentence.\n\n- In a section called PAPER QUALITY output the following sections:\n\n- Novelty: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Rigor: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Empiricism: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Rating Chart: Create a chart like the one below that shows how the paper rates on all these dimensions. \n\n- Known to Novel is how new and interesting and surprising the paper is on a scale of 1 - 10.\n\n- Weak to Rigorous is how well the paper is supported by careful science, transparency, and methodology on a scale of 1 - 10.\n\n- Theoretical to Empirical is how much the paper is based on purely speculative or theoretical ideas or actual data on a scale of 1 - 10. Note: Theoretical papers can still be rigorous and novel and should not be penalized overall for being Theoretical alone.\n\nEXAMPLE CHART for 7, 5, 9 SCORES (fill in the actual scores):\n\nKnown         [------7---]    Novel\nWeak          [----5-----]    Rigorous\nTheoretical   [--------9-]     Empirical\n\nEND EXAMPLE CHART\n\n- FINAL SCORE:\n\n- A - F based on the scores above, conflicts of interest, and the overall quality of the paper. On a separate line, give a 15-word explanation for the grade.\n\n- SUMMARY STATEMENT:\n\nA final 25-word summary of the paper, its findings, and what we should do about it if it's true.\n\n# RATING NOTES\n\n- If the paper makes claims and presents stats but doesn't show how it arrived at these stats, then the Methodology Transparency would be low, and the RIGOR score should be lowered as well.\n\n- An A would be a paper that is novel, rigorous, empirical, and has no conflicts of interest.\n\n- A paper could get an A if it's theoretical but everything else would have to be perfect.\n\n- The stronger the claims the stronger the evidence needs to be, as well as the transparency into the methodology. If the paper makes strong claims, but the evidence or transparency is weak, then the RIGOR score should be lowered.\n\n- Remove at least 1 grade (and up to 2) for papers where compelling data is provided but it's not clear what exact tests were run and/or how to reproduce those tests. \n\n- Do not relax this transparency requirement for papers that claim security reasons.\n\n- If a paper does not clearly articulate its methodology in a way that's replicable, lower the RIGOR and overall score significantly.\n\n- Remove up to 1-3 grades for potential conflicts of interest indicated in the report.\n\n# OUTPUT INSTRUCTIONS\n\n- Output all sections above.\n\n- Ensure the scoring looks closely at the reproducibility and transparency of the methodology, and that it doesn't give a pass to papers that don't provide the data or methodology for safety or other reasons.\n\n- For the chart, use the actual scores to fill in the chart, and ensure the number associated with the score is placed on the right place on the chart., e.g., here is the chart for 2 Novelty, 8 Rigor, and 3 Empiricism:\n\nKnown         [-2--------]    Novel\nWeak          [-------8--]    Rigorous\nTheoretical   [--3-------]     Empirical\n\n- For the findings and other analysis sections, write at the 9th-grade reading level. This means using short sentences and simple words/concepts to explain everything.\n\n- Ensure there's a blank line between each bullet of output.\n\n- Create the output using the formatting above.\n\n- In the markdown, don't use formatting like bold or italics. Make the output maximially readable in plain text.\n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a research paper analysis service focused on determining the primary findings of the paper and analyzing its scientific rigor and quality.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Consume the entire paper and think deeply about it.\n\n- Map out all the claims and implications on a virtual whiteboard in your mind.\n\n# OUTPUT \n\n- Extract a summary of the paper and its conclusions into a 25-word sentence called SUMMARY.\n\n- Extract the list of authors in a section called AUTHORS.\n\n- Extract the list of organizations the authors are associated, e.g., which university they're at, with in a section called AUTHOR ORGANIZATIONS.\n\n- Extract the primary paper findings into a bulleted list of no more than 15 words per bullet into a section called FINDINGS.\n\n- Extract the overall structure and character of the study into a bulleted list of 15 words per bullet for the research in a section called STUDY DETAILS.\n\n- Extract the study quality by evaluating the following items in a section called STUDY QUALITY that has the following bulleted sub-sections:\n\n- STUDY DESIGN: (give a 15 word description, including the pertinent data and statistics.)\n\n- SAMPLE SIZE: (give a 15 word description, including the pertinent data and statistics.)\n\n- CONFIDENCE INTERVALS (give a 15 word description, including the pertinent data and statistics.)\n\n- P-VALUE (give a 15 word description, including the pertinent data and statistics.)\n\n- EFFECT SIZE (give a 15 word description, including the pertinent data and statistics.)\n\n- CONSISTENCE OF RESULTS (give a 15 word description, including the pertinent data and statistics.)\n\n- METHODOLOGY TRANSPARENCY (give a 15 word description of the methodology quality and documentation.)\n\n- STUDY REPRODUCIBILITY (give a 15 word description, including how to fully reproduce the study.)\n\n- Data Analysis Method (give a 15 word description, including the pertinent data and statistics.)\n\n- Discuss any Conflicts of Interest in a section called CONFLICTS OF INTEREST. Rate the conflicts of interest as NONE DETECTED, LOW, MEDIUM, HIGH, or CRITICAL.\n\n- Extract the researcher's analysis and interpretation in a section called RESEARCHER'S INTERPRETATION, in a 15-word sentence.\n\n- In a section called PAPER QUALITY output the following sections:\n\n- Novelty: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Rigor: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Empiricism: 1 - 10 Rating, followed by a 15 word explanation for the rating.\n\n- Rating Chart: Create a chart like the one below that shows how the paper rates on all these dimensions. \n\n- Known to Novel is how new and interesting and surprising the paper is on a scale of 1 - 10.\n\n- Weak to Rigorous is how well the paper is supported by careful science, transparency, and methodology on a scale of 1 - 10.\n\n- Theoretical to Empirical is how much the paper is based on purely speculative or theoretical ideas or actual data on a scale of 1 - 10. Note: Theoretical papers can still be rigorous and novel and should not be penalized overall for being Theoretical alone.\n\nEXAMPLE CHART for 7, 5, 9 SCORES (fill in the actual scores):\n\nKnown         [------7---]    Novel\nWeak          [----5-----]    Rigorous\nTheoretical   [--------9-]     Empirical\n\nEND EXAMPLE CHART\n\n- FINAL SCORE:\n\n- A - F based on the scores above, conflicts of interest, and the overall quality of the paper. On a separate line, give a 15-word explanation for the grade.\n\n- SUMMARY STATEMENT:\n\nA final 25-word summary of the paper, its findings, and what we should do about it if it's true.\n\n# RATING NOTES\n\n- If the paper makes claims and presents stats but doesn't show how it arrived at these stats, then the Methodology Transparency would be low, and the RIGOR score should be lowered as well.\n\n- An A would be a paper that is novel, rigorous, empirical, and has no conflicts of interest.\n\n- A paper could get an A if it's theoretical but everything else would have to be perfect.\n\n- The stronger the claims the stronger the evidence needs to be, as well as the transparency into the methodology. If the paper makes strong claims, but the evidence or transparency is weak, then the RIGOR score should be lowered.\n\n- Remove at least 1 grade (and up to 2) for papers where compelling data is provided but it's not clear what exact tests were run and/or how to reproduce those tests. \n\n- Do not relax this transparency requirement for papers that claim security reasons.\n\n- If a paper does not clearly articulate its methodology in a way that's replicable, lower the RIGOR and overall score significantly.\n\n- Remove up to 1-3 grades for potential conflicts of interest indicated in the report.\n\n# OUTPUT INSTRUCTIONS\n\n- Output all sections above.\n\n- Ensure the scoring looks closely at the reproducibility and transparency of the methodology, and that it doesn't give a pass to papers that don't provide the data or methodology for safety or other reasons.\n\n- For the chart, use the actual scores to fill in the chart, and ensure the number associated with the score is placed on the right place on the chart., e.g., here is the chart for 2 Novelty, 8 Rigor, and 3 Empiricism:\n\nKnown         [-2--------]    Novel\nWeak          [-------8--]    Rigorous\nTheoretical   [--3-------]     Empirical\n\n- For the findings and other analysis sections, write at the 9th-grade reading level. This means using short sentences and simple words/concepts to explain everything.\n\n- Ensure there's a blank line between each bullet of output.\n\n- Create the output using the formatting above.\n\n- In the markdown, don't use formatting like bold or italics. Make the output maximially readable in plain text.\n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.998285710811615,
        0.4798452854156494,
        0.02084299735724926,
        -0.4591015875339508,
        -0.06937922537326813,
        0.26319772005081177,
        -0.6916511654853821,
        0.5872858762741089,
        -0.5030815005302429,
        0.23386694490909576,
        -0.5359163284301758,
        1.0952632427215576,
        0.13382509350776672,
        -0.46981823444366455,
        -0.27225878834724426,
        -0.5195434093475342,
        -0.16843847930431366,
        -0.9108704924583435,
        -0.9482367634773254,
        0.20605209469795227,
        0.21327798068523407,
        0.752319872379303,
        -0.25453102588653564,
        0.0883064791560173,
        0.48964154720306396,
        0.13103614747524261,
        -0.343148410320282,
        -0.3506477475166321,
        -0.4879258871078491,
        -1.301559329032898,
        1.1379432678222656,
        1.2355183362960815,
        -1.423295021057129,
        -0.7905863523483276,
        -0.2665114402770996,
        -0.7536695599555969,
        0.15478432178497314,
        0.3440035581588745,
        -0.8120855093002319,
        -0.5555116534233093,
        0.6319127678871155,
        0.30545926094055176,
        -1.0574458837509155,
        0.01741352677345276,
        0.43514007329940796,
        -0.15381541848182678,
        -0.024275973439216614,
        -0.19698424637317657,
        0.9837778210639954,
        0.30634480714797974,
        -0.36719900369644165,
        -0.786791980266571,
        -0.16157297790050507,
        -0.6583576202392578,
        -1.0149556398391724,
        -0.1363973617553711,
        0.12322939932346344,
        -0.5187209844589233,
        -0.34903186559677124,
        -0.19915330410003662,
        0.46938109397888184,
        0.5057075023651123,
        -2.7025251388549805,
        -0.0814569815993309,
        0.13856197893619537,
        0.30495980381965637,
        0.1393999606370926,
        -0.2087370902299881,
        0.2170567363500595,
        0.19160199165344238,
        0.014551833271980286,
        -0.22953391075134277,
        0.291258841753006,
        -0.1647217869758606,
        -0.12628532946109772,
        -0.006333410739898682,
        0.023472310975193977,
        -0.012875884771347046,
        0.5405551791191101,
        -0.5896641612052917,
        -0.9226979613304138,
        -0.203914076089859,
        0.12652622163295746,
        -0.5733405947685242,
        -0.020533302798867226,
        0.6324837803840637,
        0.008945237845182419,
        0.46852248907089233,
        0.6637845039367676,
        0.5284328460693359,
        -0.13379552960395813,
        0.21507319808006287,
        0.4188181459903717,
        -0.15650111436843872,
        0.11736217141151428,
        -0.11851231753826141,
        -0.30714118480682373,
        0.1272616982460022,
        0.08611778914928436,
        3.38045072555542,
        0.7440855503082275,
        0.20636005699634552,
        0.5251534581184387,
        -1.4231064319610596,
        0.09425755590200424,
        0.031564295291900635,
        -0.635776162147522,
        -0.053107134997844696,
        -0.07223686575889587,
        -0.490654319524765,
        -0.01864931359887123,
        -0.4568004310131073,
        -0.31987595558166504,
        0.40849682688713074,
        -0.2682107388973236,
        0.5600013732910156,
        -0.22229228913784027,
        -0.3046400249004364,
        0.32946646213531494,
        -0.3110630214214325,
        -0.0032042264938354492,
        0.26476818323135376,
        -0.176175057888031,
        -0.3495999872684479,
        -0.13285879790782928,
        0.3400465250015259,
        -0.5336527824401855,
        0.7441989183425903,
        0.5427640676498413,
        0.321269690990448,
        -0.1980876326560974,
        0.5687135457992554,
        -1.2973917722702026,
        0.1628645956516266,
        0.30613914132118225,
        0.3375711143016815,
        -0.06616255640983582,
        -0.8323274254798889,
        0.5653333067893982,
        -0.22266432642936707,
        -0.025736555457115173,
        -0.42121994495391846,
        0.8605086803436279,
        -0.026938609778881073,
        0.9758060574531555,
        -0.37171483039855957,
        0.05908115208148956,
        0.25117406249046326,
        -0.6773092150688171,
        -1.4197444915771484,
        -0.22770769894123077,
        0.6139343976974487,
        0.08013404905796051,
        0.18660180270671844,
        0.6846334338188171,
        -0.41586771607398987,
        -0.863287627696991,
        0.7609788179397583,
        -0.5057398080825806,
        0.4547673165798187,
        -0.5333966612815857,
        -0.014161897823214531,
        -0.179544597864151,
        0.7186662554740906,
        0.5074589848518372,
        -0.32729804515838623,
        0.26406511664390564,
        -0.3043633699417114,
        -0.30767160654067993,
        -0.23734289407730103,
        0.6260769367218018,
        -0.04673849791288376,
        0.7782906293869019,
        0.697100043296814,
        -0.8727549910545349,
        -0.09269501268863678,
        -0.2784988284111023,
        0.39143064618110657,
        0.8259237408638,
        -0.8787134885787964,
        -0.35730987787246704,
        0.46411290764808655,
        -0.4318051040172577,
        -0.4130823016166687,
        -0.18735721707344055,
        0.6705667972564697,
        -0.15440350770950317,
        0.27669093012809753,
        0.42195889353752136,
        1.2982251644134521,
        -1.5255672931671143,
        2.2251412868499756,
        -0.11526435613632202,
        -0.6131479740142822,
        0.22545093297958374,
        0.4208185076713562,
        -0.11930149048566818,
        0.020590074360370636,
        0.45758190751075745,
        -0.2772655785083771,
        -0.3893536627292633,
        -0.3165854215621948,
        0.2769174575805664,
        -0.21800535917282104,
        -0.8756486177444458,
        -0.4445594251155853,
        -0.2602844536304474,
        0.27481862902641296,
        0.2720494866371155,
        -0.3816261291503906,
        -0.42925095558166504,
        -0.3814999759197235,
        0.8126229047775269,
        0.1322936713695526,
        1.1983624696731567,
        0.8054177165031433,
        0.14390769600868225,
        0.2717309594154358,
        -0.0041397977620363235,
        0.2590973973274231,
        -0.36712750792503357,
        0.09220045804977417,
        -0.649381160736084,
        -0.7142189145088196,
        -1.3772774934768677,
        0.6177393794059753,
        0.5396028161048889,
        0.4311647415161133,
        -0.7825798392295837,
        -0.9002888798713684,
        0.5271295309066772,
        0.8865960836410522,
        1.4540611505508423,
        0.5401818752288818,
        0.2694510519504547,
        -0.03313746303319931,
        -0.40426522493362427,
        1.094048261642456,
        0.22304195165634155,
        -0.46814626455307007,
        0.12527485191822052,
        -0.709748387336731,
        -0.4873965382575989,
        1.103217363357544,
        0.3318338990211487,
        0.35435691475868225,
        -0.3169103264808655,
        0.10376159846782684,
        -0.3805168867111206,
        1.2668170928955078,
        0.2695842385292053,
        -0.37943512201309204,
        0.5207048058509827,
        0.20112231373786926,
        0.15151306986808777,
        0.19219571352005005,
        -1.7078912258148193,
        0.23102976381778717,
        -0.8308587670326233,
        0.014844655990600586,
        0.7532017230987549,
        -0.7358819246292114,
        0.8582409620285034,
        -0.49897366762161255,
        -0.33843711018562317,
        -0.10088151693344116,
        -0.26738858222961426,
        -0.3037254214286804,
        -0.21814846992492676,
        -0.0002765581011772156,
        -0.6340785622596741,
        0.2333488166332245,
        -0.1120724007487297,
        -0.14214032888412476,
        -0.1451396495103836,
        0.2921934425830841,
        -0.3319076895713806,
        0.33448973298072815,
        -0.8583092093467712,
        -0.21094265580177307,
        -0.44885337352752686,
        -0.11417927592992783,
        0.03129710257053375,
        0.4006352424621582,
        0.5425369143486023,
        0.45188814401626587,
        0.08835643529891968,
        -0.7174261808395386,
        -0.16350209712982178,
        0.5819024443626404,
        -0.8222616314888,
        -0.3604002296924591,
        0.6036854982376099,
        0.3351355791091919,
        0.9628876447677612,
        0.49874627590179443,
        0.44909799098968506,
        0.16194987297058105,
        0.6215112805366516,
        0.1211099773645401,
        -0.19111040234565735,
        0.07420393824577332,
        0.24764354526996613,
        0.34437209367752075,
        -0.8367067575454712,
        -0.9383339285850525,
        0.21908628940582275,
        -0.16783687472343445,
        -0.15122896432876587,
        0.5389055013656616,
        -0.9104521870613098,
        0.23143357038497925,
        -0.17925038933753967,
        -0.1765720248222351,
        0.568686842918396,
        0.04812724143266678,
        1.1947733163833618,
        0.27132439613342285,
        0.1020585149526596,
        -1.6553746461868286,
        -0.299838125705719,
        1.21415114402771,
        0.7604601383209229,
        0.23694032430648804,
        -0.17598474025726318,
        0.16831795871257782,
        -0.548210620880127,
        0.6015273928642273,
        -0.5067675709724426,
        0.9767667055130005,
        0.854314923286438,
        -0.25626668334007263,
        -0.2562796473503113,
        0.5331540107727051,
        0.7314553260803223,
        -0.4706190824508667,
        -0.14777953922748566,
        0.052340902388095856,
        -0.4346451759338379,
        -0.37587830424308777,
        0.40909722447395325,
        1.4186809062957764,
        0.6701872944831848,
        0.8109297752380371,
        0.2528378963470459,
        0.5052812695503235,
        -0.32631105184555054,
        -0.6469968557357788,
        0.33138418197631836,
        0.13584661483764648,
        -0.5462036728858948,
        0.3208239674568176,
        0.2718159854412079,
        -0.23400691151618958,
        0.9988793134689331,
        -0.08330149203538895,
        -0.5710167288780212,
        0.4846687614917755,
        -0.26594820618629456,
        1.257294774055481,
        -0.4004376232624054,
        -0.679565966129303,
        -0.033151060342788696,
        -0.007748659700155258,
        -0.6463371515274048,
        0.02513989806175232,
        -0.2382541447877884,
        -0.8846494555473328,
        -0.11411876976490021,
        0.11137615144252777,
        -0.32973533868789673,
        -0.26446089148521423,
        0.13476625084877014,
        0.1890583485364914,
        0.7651322484016418,
        0.2863481640815735,
        -0.08259072154760361,
        -0.19173157215118408,
        0.1330878585577011,
        -0.21507887542247772,
        1.042783498764038,
        -0.19372937083244324,
        -0.6290739178657532,
        -0.3319753408432007
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt outlines the role and responsibilities of a patent examiner, emphasizing the importance of technical and legal expertise in evaluating patents. It details the steps for examining a patent, including identifying the technology field, problem addressed, solution, advantages, novelty, and inventive step, and summarizing the core idea and keywords. The expected output involves detailed analysis and documentation in specific sections without concern for length, using bullet points for clarity.",
          "name": "Analyze_patent",
          "raw": "\n                workflow Analyze_patent v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n- You are a patent examiner with decades of experience under your belt.\n- You are capable of examining patents in all areas of technology.\n- You have impeccable scientific and technical knowledge.\n- You are curious and keep yourself up-to-date with the latest advancements.\n- You have a thorough understanding of patent law with the ability to apply legal principles.\n- You are analytical, unbiased, and critical in your thinking.\n- In your long career, you have read and consumed a huge amount of prior art (in the form of patents, scientific articles, technology blogs, websites, etc.), so that when you encounter a patent application, based on this prior knowledge, you already have a good idea of whether it could be novel and/or inventive or not.\n\n# STEPS\n- Breathe in, take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n- Read the input and thoroughly understand it. Take into consideration only the description and the claims. Everything else must be ignored.\n- Identify the field of technology that the patent is concerned with and output it into a section called FIELD.\n- Identify the problem being addressed by the patent and output it into a section called PROBLEM. \n- Provide a very detailed explanation (including all the steps involved) of how the problem is solved in a section called SOLUTION.\n- Identfy the advantage the patent offers over what is known in the state of the art art and output it into a section called ADVANTAGE.\n- Definition of novelty: An invention shall be considered to be new if it does not form part of the state of the art. The state of the art shall be held to comprise everything made available to the public by means of a written or oral description, by use, or in any other way, before the date of filing of the patent application. Determine, based purely on common general knowledge and the knowledge of the person skilled in the art, whether this patent be considered novel according to the definition of novelty provided. Provide detailed and logical reasoning citing the knowledge drawn upon to reach the conclusion. It is OK if you consider the patent not to be novel. Output this into a section called NOVELTY.\n- Defintion of inventive step: An invention shall be considered as involving an inventive step if, having regard to the state of the art, it is not obvious to a person skilled in the art. Determine, based purely on common general knowledge and the knowledge of the person skilled in the art, whether this patent be considered inventive according to the definition of inventive step provided. Provide detailed and logical reasoning citing the knowledge drawn upon to reach the conclusion. It is OK if you consider the patent not to be inventive. Output this into a section called INVENTIVE STEP.\n- Summarize the core idea of the patent into a succinct and easy-to-digest summary not more than 1000 characters into a section called SUMMARY.\n- Identify up to 20 keywords (these may be more than a word long if necessary) that would define the core idea of the patent (trivial terms like \\\"computer\\\", \\\"method\\\", \\\"device\\\" etc. are to be ignored) and output them into a section called KEYWORDS.\n\n# OUTPUT INSTRUCTIONS\n- Be as verbose as possible. Do not leave out any technical details. Do not be worried about space/storage/size limitations when it comes to your response.\n- Only output Markdown.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not output repetitions.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n- You are a patent examiner with decades of experience under your belt.\n- You are capable of examining patents in all areas of technology.\n- You have impeccable scientific and technical knowledge.\n- You are curious and keep yourself up-to-date with the latest advancements.\n- You have a thorough understanding of patent law with the ability to apply legal principles.\n- You are analytical, unbiased, and critical in your thinking.\n- In your long career, you have read and consumed a huge amount of prior art (in the form of patents, scientific articles, technology blogs, websites, etc.), so that when you encounter a patent application, based on this prior knowledge, you already have a good idea of whether it could be novel and/or inventive or not.\n\n# STEPS\n- Breathe in, take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n- Read the input and thoroughly understand it. Take into consideration only the description and the claims. Everything else must be ignored.\n- Identify the field of technology that the patent is concerned with and output it into a section called FIELD.\n- Identify the problem being addressed by the patent and output it into a section called PROBLEM. \n- Provide a very detailed explanation (including all the steps involved) of how the problem is solved in a section called SOLUTION.\n- Identfy the advantage the patent offers over what is known in the state of the art art and output it into a section called ADVANTAGE.\n- Definition of novelty: An invention shall be considered to be new if it does not form part of the state of the art. The state of the art shall be held to comprise everything made available to the public by means of a written or oral description, by use, or in any other way, before the date of filing of the patent application. Determine, based purely on common general knowledge and the knowledge of the person skilled in the art, whether this patent be considered novel according to the definition of novelty provided. Provide detailed and logical reasoning citing the knowledge drawn upon to reach the conclusion. It is OK if you consider the patent not to be novel. Output this into a section called NOVELTY.\n- Defintion of inventive step: An invention shall be considered as involving an inventive step if, having regard to the state of the art, it is not obvious to a person skilled in the art. Determine, based purely on common general knowledge and the knowledge of the person skilled in the art, whether this patent be considered inventive according to the definition of inventive step provided. Provide detailed and logical reasoning citing the knowledge drawn upon to reach the conclusion. It is OK if you consider the patent not to be inventive. Output this into a section called INVENTIVE STEP.\n- Summarize the core idea of the patent into a succinct and easy-to-digest summary not more than 1000 characters into a section called SUMMARY.\n- Identify up to 20 keywords (these may be more than a word long if necessary) that would define the core idea of the patent (trivial terms like \\\"computer\\\", \\\"method\\\", \\\"device\\\" etc. are to be ignored) and output them into a section called KEYWORDS.\n\n# OUTPUT INSTRUCTIONS\n- Be as verbose as possible. Do not leave out any technical details. Do not be worried about space/storage/size limitations when it comes to your response.\n- Only output Markdown.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not output repetitions.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4913415312767029,
        -0.20550833642482758,
        -0.05734361335635185,
        0.41851383447647095,
        0.11099320650100708,
        0.11813372373580933,
        -0.8638580441474915,
        0.11959818005561829,
        0.11172415316104889,
        -0.10155238956212997,
        -0.2216719537973404,
        0.6749442219734192,
        0.19392231106758118,
        -0.3530360162258148,
        0.23316335678100586,
        -0.3004890978336334,
        -0.021990641951560974,
        -0.23615695536136627,
        -1.3527336120605469,
        -0.11134659498929977,
        -0.022202592343091965,
        0.9318889379501343,
        0.2628580927848816,
        -0.06850595772266388,
        0.7880964875221252,
        0.16745002567768097,
        -0.27859485149383545,
        -0.3060793876647949,
        -0.7776538133621216,
        -1.0401172637939453,
        0.7940281629562378,
        0.5588038563728333,
        -0.5394861102104187,
        -0.40846458077430725,
        0.07498574256896973,
        -0.6614835858345032,
        0.019394032657146454,
        0.29841166734695435,
        -0.42929840087890625,
        -0.4971625804901123,
        0.323989599943161,
        0.30865582823753357,
        -0.09172189980745316,
        0.1047501489520073,
        0.4401276111602783,
        -0.20929396152496338,
        -0.5830360054969788,
        0.1560623049736023,
        0.3453858494758606,
        0.2192566990852356,
        -0.1670239269733429,
        -0.2513161301612854,
        0.0819975882768631,
        -0.14452944695949554,
        -0.36444514989852905,
        -0.26327064633369446,
        0.253973126411438,
        -0.6257662177085876,
        0.1768302470445633,
        -0.2671200931072235,
        0.1741064339876175,
        -0.0956643670797348,
        -2.5137646198272705,
        -0.05527408421039581,
        0.42593783140182495,
        -0.14327988028526306,
        -0.22927333414554596,
        -0.9506972432136536,
        0.3809584975242615,
        -0.028442896902561188,
        -0.6657593846321106,
        -0.037941038608551025,
        -0.3218832015991211,
        0.2683340311050415,
        -0.21927964687347412,
        -0.0034840330481529236,
        -0.283237487077713,
        0.39804595708847046,
        0.7582910060882568,
        -0.5484908819198608,
        -0.338652640581131,
        0.4568294882774353,
        0.4085119366645813,
        -0.3502177894115448,
        -0.4444408118724823,
        0.5909371376037598,
        -0.17356476187705994,
        -0.14212530851364136,
        -0.06470638513565063,
        0.6071473360061646,
        0.29865768551826477,
        0.30504778027534485,
        -0.053701601922512054,
        0.42794129252433777,
        0.14957578480243683,
        -0.3405567407608032,
        -0.18950769305229187,
        0.4536397457122803,
        0.29244375228881836,
        3.4842371940612793,
        0.7316559553146362,
        0.40993958711624146,
        0.07111755013465881,
        -0.7467634081840515,
        0.11924739181995392,
        -0.027348123490810394,
        -0.22513695061206818,
        -0.3891206681728363,
        0.3414987623691559,
        -0.06881869584321976,
        0.11096596717834473,
        -0.8818638324737549,
        -0.684572696685791,
        0.428839772939682,
        -0.05222119763493538,
        0.9419519901275635,
        -1.0688340663909912,
        -0.25810477137565613,
        0.2807171642780304,
        1.1144441366195679,
        -0.9617310166358948,
        0.10348868370056152,
        -0.2875564992427826,
        -0.3438716530799866,
        0.061516355723142624,
        -0.48010921478271484,
        -0.5670817494392395,
        0.6687449812889099,
        0.6250913143157959,
        0.6055123209953308,
        0.17562627792358398,
        -0.13279332220554352,
        -1.5010101795196533,
        -0.1796698123216629,
        -0.15459448099136353,
        -0.005091220140457153,
        0.39132171869277954,
        -0.8247771263122559,
        -0.31617507338523865,
        -0.008325226604938507,
        -0.27513423562049866,
        -1.1789315938949585,
        1.0508418083190918,
        -0.011126015335321426,
        0.32810088992118835,
        0.6480710506439209,
        -0.29342561960220337,
        0.35682034492492676,
        -0.10547567903995514,
        -0.8305436372756958,
        -0.057892005890607834,
        0.8203792572021484,
        -0.1643885225057602,
        0.6175503730773926,
        0.10170053690671921,
        0.10664741694927216,
        -1.2242246866226196,
        -0.0951336920261383,
        -0.7280272245407104,
        -0.09531161934137344,
        0.15246309340000153,
        -0.13996118307113647,
        -0.2760709822177887,
        -0.15946300327777863,
        1.4419372081756592,
        -0.10680824518203735,
        -0.21770989894866943,
        -0.2878119647502899,
        0.338849812746048,
        -0.451361745595932,
        0.00025022029876708984,
        0.14087790250778198,
        0.28618183732032776,
        0.9551820158958435,
        -0.2618754506111145,
        -0.09781244397163391,
        -0.20368146896362305,
        0.5779345035552979,
        -0.013868063688278198,
        -0.7517141103744507,
        0.5984004139900208,
        0.8056322336196899,
        0.28604960441589355,
        -0.5655781626701355,
        -0.31842947006225586,
        -0.014610983431339264,
        0.8906551003456116,
        0.08851184695959091,
        1.2582656145095825,
        0.9749480485916138,
        -0.9733449220657349,
        1.5855110883712769,
        -0.14153748750686646,
        -0.3170371353626251,
        0.3276956081390381,
        -0.24279794096946716,
        -0.22748608887195587,
        0.38180822134017944,
        -0.38306233286857605,
        -0.13637399673461914,
        -1.8989335298538208,
        -0.22228679060935974,
        -0.01976320892572403,
        -0.5531150102615356,
        -0.482590913772583,
        0.32141637802124023,
        -0.1946389377117157,
        0.14474400877952576,
        0.6283850073814392,
        -0.7541788220405579,
        -0.15602988004684448,
        -0.3008692264556885,
        0.8510721325874329,
        0.3511776030063629,
        0.18333862721920013,
        0.020878830924630165,
        -0.5337440967559814,
        0.11088518798351288,
        0.0777757465839386,
        0.8478299379348755,
        -0.6145543456077576,
        -0.3010464310646057,
        -0.9723889231681824,
        -0.8293516039848328,
        -0.6480479836463928,
        0.9011275172233582,
        -0.22688141465187073,
        0.8815481662750244,
        -1.2981265783309937,
        -0.38539016246795654,
        0.27910616993904114,
        1.0207650661468506,
        1.5595816373825073,
        0.9581278562545776,
        0.009153351187705994,
        -0.01359083503484726,
        -0.05259149521589279,
        0.42769336700439453,
        -0.321466326713562,
        -0.8235587477684021,
        -0.09758398681879044,
        -0.4106075167655945,
        -0.30564606189727783,
        0.3441810607910156,
        0.19141227006912231,
        0.24668186902999878,
        -0.0828925371170044,
        -0.29604458808898926,
        -0.46060118079185486,
        1.654390811920166,
        1.051835298538208,
        -0.37472549080848694,
        0.06558232754468918,
        0.9203407168388367,
        -0.1472034454345703,
        0.38329848647117615,
        -0.9156249165534973,
        -0.3112500011920929,
        -0.4199554920196533,
        -0.2833494246006012,
        -0.594709038734436,
        -0.10655863583087921,
        0.5304685831069946,
        0.26768720149993896,
        -0.2755530774593353,
        0.9563905000686646,
        -0.22304925322532654,
        -0.4893929660320282,
        -0.4107925593852997,
        0.12780159711837769,
        0.09660834074020386,
        -0.4828447699546814,
        0.3114844560623169,
        -0.008413642644882202,
        -0.12836553156375885,
        0.29673248529434204,
        -0.6618945002555847,
        -0.025313638150691986,
        -0.6801714301109314,
        -0.6243669390678406,
        -0.239600270986557,
        0.16290932893753052,
        -0.736456036567688,
        0.03746514394879341,
        -0.40185150504112244,
        0.15180350840091705,
        0.46728768944740295,
        -1.4093977212905884,
        0.07510782778263092,
        1.1935243606567383,
        -1.0418176651000977,
        0.040414661169052124,
        0.21260352432727814,
        -0.26255813241004944,
        1.6074607372283936,
        0.7860944271087646,
        0.6965165138244629,
        1.2378689050674438,
        0.8792881369590759,
        0.30445539951324463,
        -0.6096991896629333,
        0.3960857391357422,
        0.47231265902519226,
        0.38981688022613525,
        -0.7218164801597595,
        0.1731206178665161,
        0.8645176887512207,
        0.5699886679649353,
        -0.386211097240448,
        -0.22607550024986267,
        -0.7900648713111877,
        0.5508257746696472,
        0.21172887086868286,
        0.19913747906684875,
        0.45656728744506836,
        -0.40487655997276306,
        0.4294922947883606,
        1.0046517848968506,
        0.127690389752388,
        -1.3469780683517456,
        -0.697460949420929,
        0.8346580266952515,
        0.746454119682312,
        0.1955891251564026,
        -0.4786660373210907,
        0.6183454990386963,
        0.18664386868476868,
        0.03356092795729637,
        -0.7365454435348511,
        1.6811641454696655,
        0.5430170893669128,
        0.030554696917533875,
        0.06448906660079956,
        -0.3964795470237732,
        0.7648645639419556,
        0.13845312595367432,
        0.13936029374599457,
        0.3003431558609009,
        -0.0875777155160904,
        -0.3801402151584625,
        0.3082990050315857,
        1.1217894554138184,
        -0.1715795248746872,
        -0.1304493546485901,
        0.39576590061187744,
        0.4072045385837555,
        -0.39855748414993286,
        -1.0755175352096558,
        0.2583850622177124,
        -0.6020656824111938,
        0.08994543552398682,
        0.6752576231956482,
        0.11613331735134125,
        -0.2949720025062561,
        1.0264594554901123,
        -0.06932753324508667,
        -0.2028598040342331,
        0.23584020137786865,
        -0.12107448279857635,
        1.5318613052368164,
        -0.7685143947601318,
        -0.6206678748130798,
        -0.09705164283514023,
        0.20807228982448578,
        -0.4796693027019501,
        -0.1292656660079956,
        -0.2674279808998108,
        -0.7297050356864929,
        -0.11756561696529388,
        0.3396446108818054,
        0.05847346410155296,
        -0.22080525755882263,
        0.5366459488868713,
        0.7802606821060181,
        0.49833494424819946,
        -0.4744001030921936,
        0.006924290210008621,
        0.35999152064323425,
        -0.133096843957901,
        -0.9874036908149719,
        0.6246180534362793,
        -0.13401775062084198,
        -1.2377270460128784,
        -0.6533246040344238
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Performs in-depth psychological analysis on the main individual in the provided input. It involves identifying the primary person, deeply contemplating their language and responses, and comparing these to known human psychology principles. The output includes a concise psychological profile summary and detailed supporting points.",
          "name": "Analyze_personality",
          "raw": "\n                workflow Analyze_personality v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are a super-intelligent AI with full knowledge of human psychology and behavior.\n\n# GOAL \n\nYour goal is to perform in-depth psychological analysis on the main person in the input provided.\n\n# STEPS\n\n- Figure out who the main person is in the input, e.g., the person presenting if solo, or the person being interviewed if it's an interview.\n\n- Fully contemplate the input for 419 minutes, deeply considering the person's language, responses, etc.\n\n- Think about everything you know about human psychology and compare that to the person in question's content.\n\n# OUTPUT\n\n- In a section called ANALYSIS OVERVIEW, give a 25-word summary of the person's psychological profile.Be completely honest, and a bit brutal if necessary. \n\n- In a section called ANALYSIS DETAILS, provide 5-10 bullets of 15-words each that give support for your ANALYSIS OVERVIEW.\n\n# OUTPUT INSTRUCTIONS\n\n- We are looking for keen insights about the person, not surface level observations.\n\n- Here are some examples of good analysis:\n\n\\\"This speaker seems obsessed with conspiracies, but it's not clear exactly if he believes them or if he's just trying to get others to.\\\"\n\n\\\"The person being interviewed is very defensive about his legacy, and is being aggressive towards the interviewer for that reason.\n\n\\\"The person being interviewed shows signs of Machiaevellianism, as he's constantly trying to manipulate the narrative back to his own.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are a super-intelligent AI with full knowledge of human psychology and behavior.\n\n# GOAL \n\nYour goal is to perform in-depth psychological analysis on the main person in the input provided.\n\n# STEPS\n\n- Figure out who the main person is in the input, e.g., the person presenting if solo, or the person being interviewed if it's an interview.\n\n- Fully contemplate the input for 419 minutes, deeply considering the person's language, responses, etc.\n\n- Think about everything you know about human psychology and compare that to the person in question's content.\n\n# OUTPUT\n\n- In a section called ANALYSIS OVERVIEW, give a 25-word summary of the person's psychological profile.Be completely honest, and a bit brutal if necessary. \n\n- In a section called ANALYSIS DETAILS, provide 5-10 bullets of 15-words each that give support for your ANALYSIS OVERVIEW.\n\n# OUTPUT INSTRUCTIONS\n\n- We are looking for keen insights about the person, not surface level observations.\n\n- Here are some examples of good analysis:\n\n\\\"This speaker seems obsessed with conspiracies, but it's not clear exactly if he believes them or if he's just trying to get others to.\\\"\n\n\\\"The person being interviewed is very defensive about his legacy, and is being aggressive towards the interviewer for that reason.\n\n\\\"The person being interviewed shows signs of Machiaevellianism, as he's constantly trying to manipulate the narrative back to his own.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.8433608412742615,
        -0.35489422082901,
        0.1364022195339203,
        0.16899165511131287,
        0.011549439281225204,
        0.09265363216400146,
        -1.1538606882095337,
        -0.08069820702075958,
        0.19520047307014465,
        0.2689950466156006,
        -0.0470706969499588,
        0.8048269152641296,
        0.28527218103408813,
        -0.052872881293296814,
        0.3907516598701477,
        -0.48057103157043457,
        0.4324229955673218,
        -0.42136481404304504,
        -1.3013615608215332,
        -0.5475231409072876,
        0.4377189576625824,
        0.9348283410072327,
        0.462688148021698,
        0.12068326026201248,
        0.7918773889541626,
        0.12021586298942566,
        0.10260471701622009,
        -0.10390864312648773,
        -0.3175468444824219,
        -1.0522538423538208,
        0.5770360827445984,
        0.24240368604660034,
        -0.4911361038684845,
        -0.627435564994812,
        0.3662373721599579,
        -0.7141066193580627,
        0.2079094797372818,
        0.0026540830731391907,
        -0.9443910121917725,
        -0.48948729038238525,
        0.046216003596782684,
        -0.06290679425001144,
        -0.5641691088676453,
        0.08613386750221252,
        0.0677657201886177,
        -0.6407566070556641,
        -0.42838990688323975,
        0.45791223645210266,
        0.7874324917793274,
        0.7148449420928955,
        -0.3338477611541748,
        -0.3092949092388153,
        -0.09562736004590988,
        -0.19395247101783752,
        -0.5998134613037109,
        -0.11931701749563217,
        -0.5412192940711975,
        -0.2692395746707916,
        0.09409226477146149,
        0.003514587413519621,
        0.6069740056991577,
        0.4389868378639221,
        -3.1817972660064697,
        -0.05981035158038139,
        0.6115773320198059,
        -0.08972383290529251,
        -0.06523707509040833,
        -0.02082061767578125,
        0.2650466561317444,
        0.009076416492462158,
        -0.6700859665870667,
        0.0846475139260292,
        0.03100922703742981,
        0.17978039383888245,
        0.11077622324228287,
        -0.3471181392669678,
        0.09681372344493866,
        -0.08410757780075073,
        0.34397488832473755,
        -0.2728942334651947,
        -0.4189976453781128,
        0.41785648465156555,
        0.5179260969161987,
        -0.17557333409786224,
        -0.46804162859916687,
        0.18478678166866302,
        -0.25216981768608093,
        0.3390430808067322,
        0.08912897855043411,
        0.2609506845474243,
        -0.4096753001213074,
        0.07593023777008057,
        0.1487347036600113,
        0.34109169244766235,
        -0.2597808241844177,
        -0.6035341024398804,
        -0.579067587852478,
        -0.4827214479446411,
        0.7412470579147339,
        3.409803867340088,
        0.5146843194961548,
        0.4329729974269867,
        0.33432093262672424,
        -0.9319610595703125,
        0.07143916934728622,
        -0.4274085760116577,
        -0.39108407497406006,
        0.07730190455913544,
        0.22541575133800507,
        0.6680019497871399,
        0.0215742327272892,
        -0.41517212986946106,
        -0.6477643251419067,
        0.5365288853645325,
        0.15306459367275238,
        1.3320997953414917,
        -0.8667694926261902,
        -0.17492175102233887,
        0.1365046501159668,
        0.4908958673477173,
        -1.138127326965332,
        0.658514142036438,
        0.0888531431555748,
        -0.04773596674203873,
        0.19429750740528107,
        -0.20137286186218262,
        -0.6961777210235596,
        0.4118492007255554,
        0.6833218336105347,
        0.253074586391449,
        0.18381641805171967,
        0.24756549298763275,
        -0.9454458951950073,
        0.2853311002254486,
        0.34649360179901123,
        -0.20035085082054138,
        0.1049267053604126,
        -0.9472979307174683,
        0.23073649406433105,
        0.0169360488653183,
        0.14508993923664093,
        -1.2168240547180176,
        0.7199273705482483,
        0.4338368773460388,
        0.7674582004547119,
        0.4453538954257965,
        -0.4022034704685211,
        0.47728583216667175,
        -0.4874304234981537,
        -0.6439203023910522,
        0.20387153327465057,
        0.7726130485534668,
        -0.02247859351336956,
        0.4828289747238159,
        0.203835129737854,
        0.1452168971300125,
        -0.6788741946220398,
        0.039606641978025436,
        -0.4418772757053375,
        0.2950136959552765,
        -0.3367118835449219,
        -0.3044995665550232,
        0.13626708090305328,
        0.035233043134212494,
        0.9987325072288513,
        -0.6413657665252686,
        0.2362322360277176,
        -0.40300047397613525,
        0.3946250379085541,
        0.11020509898662567,
        -0.08351919054985046,
        0.13495168089866638,
        -0.07515665143728256,
        1.2854454517364502,
        0.27481183409690857,
        -0.34192389249801636,
        -0.23765312135219574,
        0.6225611567497253,
        0.2045355588197708,
        -0.5377894639968872,
        0.46927428245544434,
        0.640323281288147,
        0.44249165058135986,
        -0.5629017353057861,
        -0.095911905169487,
        0.2456027865409851,
        0.6576207280158997,
        -0.012938573956489563,
        0.9919040203094482,
        1.231250524520874,
        -0.7127485871315002,
        1.7459802627563477,
        -0.7384390830993652,
        -0.5778854489326477,
        -0.12058833986520767,
        -0.587486207485199,
        0.16619105637073517,
        0.10804250836372375,
        0.018122706562280655,
        -0.15846846997737885,
        -1.2502812147140503,
        -0.10866554081439972,
        -0.36920252442359924,
        -0.44246605038642883,
        -0.9370997548103333,
        -0.5293172001838684,
        -0.5069984197616577,
        0.25930339097976685,
        0.21369287371635437,
        -1.1875038146972656,
        0.21898296475410461,
        -0.4060569405555725,
        0.5023398399353027,
        0.27388355135917664,
        0.23528313636779785,
        0.1942611187696457,
        -0.3272703289985657,
        0.3643881678581238,
        -0.07496321201324463,
        0.4013237953186035,
        -0.7376302480697632,
        -0.23607149720191956,
        -0.9196403622627258,
        -0.6934949159622192,
        -0.9220999479293823,
        0.7484216690063477,
        -1.0346386432647705,
        0.26371920108795166,
        -0.7822965979576111,
        -0.9109280109405518,
        0.8098071813583374,
        1.19920015335083,
        1.002250075340271,
        0.43053746223449707,
        -0.00014710426330566406,
        -0.06082715839147568,
        -0.20541921257972717,
        0.5857031345367432,
        -0.43821051716804504,
        -0.40833476185798645,
        0.7531403303146362,
        -0.3695492148399353,
        -0.5653323531150818,
        0.47996747493743896,
        0.24799783527851105,
        0.4989575445652008,
        -0.746540367603302,
        0.061203181743621826,
        -0.3285958468914032,
        1.5211234092712402,
        0.6128008961677551,
        0.1646498441696167,
        0.6483203172683716,
        0.5613717436790466,
        0.6466426253318787,
        0.18295979499816895,
        -1.552626609802246,
        -0.7453124523162842,
        -0.6781729459762573,
        0.051318179816007614,
        -0.45217838883399963,
        0.14683881402015686,
        1.016644835472107,
        0.15432626008987427,
        -0.9628056287765503,
        -0.06476351618766785,
        0.132976233959198,
        -0.4174247980117798,
        -0.6427882313728333,
        0.4811333119869232,
        -0.07309627532958984,
        -0.2655003070831299,
        -0.22931303083896637,
        -0.31471091508865356,
        -0.021039584651589394,
        -0.04409289360046387,
        -0.36146080493927,
        -0.4197264015674591,
        -0.10826680064201355,
        -0.4776621162891388,
        -0.23309314250946045,
        0.12992659211158752,
        -0.008579015731811523,
        0.40085506439208984,
        -0.23634997010231018,
        -0.20480047166347504,
        0.5291037559509277,
        -1.0954103469848633,
        0.3658848702907562,
        1.176167368888855,
        -0.36558228731155396,
        0.07737511396408081,
        0.19338078796863556,
        0.3040834665298462,
        2.0846288204193115,
        1.0582654476165771,
        0.1939324140548706,
        0.6994378566741943,
        0.7883002758026123,
        0.21727979183197021,
        -0.91255122423172,
        0.5302416086196899,
        0.406063050031662,
        -0.04478088766336441,
        -0.44322457909584045,
        0.06179233640432358,
        0.6406686902046204,
        -0.25772687792778015,
        -0.06780827790498734,
        -0.20619997382164001,
        -0.8313955068588257,
        0.026467498391866684,
        0.11159005761146545,
        0.01451040804386139,
        0.24251434206962585,
        -0.4801850914955139,
        1.252437710762024,
        0.8991907835006714,
        0.23188580572605133,
        -1.5060919523239136,
        -0.0775255337357521,
        1.2613357305526733,
        0.028485754504799843,
        -0.08956961333751678,
        -0.4465656578540802,
        0.6325792670249939,
        -0.1676105260848999,
        -0.08290192484855652,
        -0.325101375579834,
        1.3898303508758545,
        0.4967200458049774,
        -0.11153870075941086,
        -0.12531957030296326,
        0.10195869952440262,
        0.6377873420715332,
        -0.13286970555782318,
        0.32257726788520813,
        -0.35249990224838257,
        -0.896088719367981,
        -0.18612989783287048,
        0.566765308380127,
        1.0087913274765015,
        -0.01833117939531803,
        0.04166380316019058,
        0.2711199223995209,
        0.6458806395530701,
        -0.397169291973114,
        -1.1370199918746948,
        0.8624314069747925,
        0.09208667278289795,
        -0.41305533051490784,
        0.42869752645492554,
        0.11471261084079742,
        -0.32576537132263184,
        1.0484592914581299,
        -0.37103646993637085,
        -0.7241601943969727,
        0.20868034660816193,
        -0.33740323781967163,
        1.494728922843933,
        -0.23090434074401855,
        -0.5472503900527954,
        0.028559260070323944,
        0.43938004970550537,
        -0.47789034247398376,
        0.639041006565094,
        -0.2791690230369568,
        -0.640197217464447,
        0.2117316573858261,
        0.33874425292015076,
        -0.1178370863199234,
        -0.23567858338356018,
        0.21454311907291412,
        0.35455289483070374,
        0.6019201278686523,
        -0.3839922547340393,
        -0.2373153120279312,
        0.16207075119018555,
        -0.09997189044952393,
        -0.593102753162384,
        0.2446083128452301,
        -0.016819138079881668,
        -1.5468436479568481,
        -0.7813061475753784
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes and critiques presentations, focusing on content, speaker's psychology, and the difference between stated and actual goals. It involves comparing intended messages to actual content, including self-references and entertainment attempts. The output includes scores and summaries for ideas, selflessness, and entertainment, plus an overall analysis.",
          "name": "Analyze_presentation",
          "raw": "\n                workflow Analyze_presentation v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert in reviewing and critiquing presentations.\n\nYou are able to discern the primary message of the presentation but also the underlying psychology of the speaker based on the content.\n\n# GOALS\n\n- Fully break down the entire presentation from a content perspective.\n\n- Fully break down the presenter and their actual goal (vs. the stated goal where there is a difference). \n\n# STEPS\n\n- Deeply consume the whole presentation and look at the content that is supposed to be getting presented.\n\n- Compare that to what is actually being presented by looking at how many self-references, references to the speaker's credentials or accomplishments, etc., or completely separate messages from the main topic.\n\n- Find all the instances of where the speaker is trying to entertain, e.g., telling jokes, sharing memes, and otherwise trying to entertain.\n\n# OUTPUT\n\n- In a section called IDEAS, give a score of 1-10 for how much the focus was on the presentation of novel ideas, followed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted capture of the ideas in 15-word bullets. E.g:\n\nIDEAS:\n\n9/10 — The speaker focused overwhelmingly on her new ideas about how understand dolphin language using LLMs.\n\nInstances:\n\n- \\\"We came up with a new way to use LLMs to process dolphin sounds.\\\"\n- \\\"It turns out that dolphin lanugage and chimp language has the following 4 similarities.\\\"\n- Etc.\n(list all instances)\n\n- In a section called SELFLESSNESS, give a score of 1-10 for how much the focus was on the content vs. the speaker, folowed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted set of phrases that indicate a focus on self rather than content, e.g.,:\n\nSELFLESSNESS:\n\n3/10 — The speaker referred to themselves 14 times, including their schooling, namedropping, and the books they've written.\n\nInstances:\n\n- \\\"When I was at Cornell with Michael...\\\"\n- \\\"In my first book...\\\"\n- Etc.\n(list all instances)\n\n- In a section called ENTERTAINMENT, give a score of 1-10 for how much the focus was on being funny or entertaining, followed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted capture of the instances in 15-word bullets. E.g:\n\nENTERTAINMENT:\n\n9/10 — The speaker was mostly trying to make people laugh, and was not focusing heavily on the ideas.\n\nInstances:\n\n- Jokes\n- Memes\n- Etc.\n(list all instances)\n\n\n- In a section called ANALYSIS, give a score of 1-10 for how good the presentation was overall considering selflessness, entertainment, and ideas above.\n\nIn a section below that, output a set of ASCII powerbars for the following:\n\nIDEAS           [------------9-]\nSELFLESSNESS    [--3----------]\nENTERTAINMENT   [-------5------]\n\n- In a section called CONCLUSION, give a 25-word summary of the presentation and your scoring of it.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert in reviewing and critiquing presentations.\n\nYou are able to discern the primary message of the presentation but also the underlying psychology of the speaker based on the content.\n\n# GOALS\n\n- Fully break down the entire presentation from a content perspective.\n\n- Fully break down the presenter and their actual goal (vs. the stated goal where there is a difference). \n\n# STEPS\n\n- Deeply consume the whole presentation and look at the content that is supposed to be getting presented.\n\n- Compare that to what is actually being presented by looking at how many self-references, references to the speaker's credentials or accomplishments, etc., or completely separate messages from the main topic.\n\n- Find all the instances of where the speaker is trying to entertain, e.g., telling jokes, sharing memes, and otherwise trying to entertain.\n\n# OUTPUT\n\n- In a section called IDEAS, give a score of 1-10 for how much the focus was on the presentation of novel ideas, followed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted capture of the ideas in 15-word bullets. E.g:\n\nIDEAS:\n\n9/10 — The speaker focused overwhelmingly on her new ideas about how understand dolphin language using LLMs.\n\nInstances:\n\n- \\\"We came up with a new way to use LLMs to process dolphin sounds.\\\"\n- \\\"It turns out that dolphin lanugage and chimp language has the following 4 similarities.\\\"\n- Etc.\n(list all instances)\n\n- In a section called SELFLESSNESS, give a score of 1-10 for how much the focus was on the content vs. the speaker, folowed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted set of phrases that indicate a focus on self rather than content, e.g.,:\n\nSELFLESSNESS:\n\n3/10 — The speaker referred to themselves 14 times, including their schooling, namedropping, and the books they've written.\n\nInstances:\n\n- \\\"When I was at Cornell with Michael...\\\"\n- \\\"In my first book...\\\"\n- Etc.\n(list all instances)\n\n- In a section called ENTERTAINMENT, give a score of 1-10 for how much the focus was on being funny or entertaining, followed by a hyphen and a 15-word summary of why that score was given.\n\nUnder this section put another subsection called Instances:, where you list a bulleted capture of the instances in 15-word bullets. E.g:\n\nENTERTAINMENT:\n\n9/10 — The speaker was mostly trying to make people laugh, and was not focusing heavily on the ideas.\n\nInstances:\n\n- Jokes\n- Memes\n- Etc.\n(list all instances)\n\n\n- In a section called ANALYSIS, give a score of 1-10 for how good the presentation was overall considering selflessness, entertainment, and ideas above.\n\nIn a section below that, output a set of ASCII powerbars for the following:\n\nIDEAS           [------------9-]\nSELFLESSNESS    [--3----------]\nENTERTAINMENT   [-------5------]\n\n- In a section called CONCLUSION, give a 25-word summary of the presentation and your scoring of it.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.8701536059379578,
        0.37032854557037354,
        0.04813064634799957,
        0.06716687232255936,
        0.7800772190093994,
        0.11358053982257843,
        -0.7731495499610901,
        0.16000953316688538,
        0.3726864755153656,
        -0.06054370850324631,
        -0.09992139786481857,
        1.1565309762954712,
        0.0798938199877739,
        -0.5581333637237549,
        0.27865689992904663,
        -0.05816120281815529,
        -0.03294400870800018,
        -0.2465439885854721,
        -1.263753890991211,
        -0.3100517988204956,
        0.6086158752441406,
        0.6961605548858643,
        0.5563448667526245,
        0.2880883514881134,
        0.4140433669090271,
        0.6179974675178528,
        -0.18864019215106964,
        -0.6037756204605103,
        -0.8326845765113831,
        -1.6934716701507568,
        0.5163382291793823,
        -0.11131829768419266,
        -0.9475522041320801,
        -0.9561856985092163,
        -0.06910517066717148,
        -0.21680480241775513,
        -0.19690346717834473,
        -0.35266831517219543,
        -0.40456077456474304,
        -0.5013166666030884,
        0.12397146970033646,
        0.5877861976623535,
        -0.15156754851341248,
        -0.36816781759262085,
        -0.11786097288131714,
        -0.41280418634414673,
        -0.12292155623435974,
        -0.343012273311615,
        0.46408042311668396,
        0.38088205456733704,
        -0.3746672570705414,
        -0.18339388072490692,
        -0.21410909295082092,
        -0.23391306400299072,
        -0.8128591775894165,
        -0.42893728613853455,
        -0.16002102196216583,
        -0.2982446253299713,
        -0.05926394835114479,
        -0.07161470502614975,
        0.44268664717674255,
        0.5948294401168823,
        -3.3877053260803223,
        -0.15086141228675842,
        -0.06061176210641861,
        -0.02108914405107498,
        0.40326136350631714,
        0.5947491526603699,
        0.44604456424713135,
        -0.07948186993598938,
        -0.35890576243400574,
        -0.1337956339120865,
        -0.4625369906425476,
        0.6044254302978516,
        0.2197909951210022,
        -0.05700865387916565,
        0.16706174612045288,
        0.25875669717788696,
        0.38829493522644043,
        0.17954260110855103,
        0.4152999818325043,
        0.24860504269599915,
        0.36614036560058594,
        -0.04498540237545967,
        0.11578832566738129,
        0.5730645656585693,
        -0.4740120768547058,
        0.20142215490341187,
        1.0877513885498047,
        -0.2409338802099228,
        -0.43607109785079956,
        -0.21771489083766937,
        -0.6445940136909485,
        -0.5723969340324402,
        -0.3803287148475647,
        -0.4301624894142151,
        0.19095320999622345,
        0.5437309741973877,
        0.30805182456970215,
        3.605992078781128,
        0.4278976321220398,
        -0.14445152878761292,
        0.39143088459968567,
        -0.8346012234687805,
        0.22654420137405396,
        -0.620730996131897,
        -0.45659053325653076,
        0.11719006299972534,
        -0.07935383170843124,
        0.17058759927749634,
        0.482408344745636,
        -0.5994086265563965,
        -0.23717665672302246,
        0.23195219039916992,
        0.9123872518539429,
        0.7811744809150696,
        -0.8573226928710938,
        0.025883497670292854,
        0.3092704117298126,
        0.4317169189453125,
        -0.6274106502532959,
        -0.16441196203231812,
        -0.00905914232134819,
        0.11538896709680557,
        0.1677965223789215,
        -0.14128057658672333,
        -0.27892598509788513,
        0.5334594249725342,
        0.6373363137245178,
        0.7623178958892822,
        0.265315979719162,
        0.3173525035381317,
        -1.136198878288269,
        -0.2422693520784378,
        -0.41200780868530273,
        -0.4374578595161438,
        0.11427223682403564,
        -0.9453526735305786,
        -0.009266048669815063,
        -0.18921694159507751,
        0.1485186219215393,
        -1.8896896839141846,
        0.40153801441192627,
        0.37432554364204407,
        1.3124619722366333,
        0.34865492582321167,
        -0.2864922285079956,
        0.3876957297325134,
        -0.5884289145469666,
        -0.5310029983520508,
        0.28327521681785583,
        0.3960086703300476,
        -0.5511717796325684,
        0.48213499784469604,
        0.6215392351150513,
        0.2578525245189667,
        -0.6229508519172668,
        0.09116082638502121,
        -0.4422115385532379,
        0.00045833736658096313,
        -0.115157350897789,
        -0.46483278274536133,
        -0.014918327331542969,
        0.10971438884735107,
        0.5697192549705505,
        -0.7008431553840637,
        0.2783011794090271,
        0.09107808023691177,
        -0.178910493850708,
        0.5010347962379456,
        0.3209637999534607,
        0.21422529220581055,
        0.20216049253940582,
        0.8917288780212402,
        -0.13051506876945496,
        -0.4857575297355652,
        0.31728243827819824,
        0.6265214681625366,
        0.1597442328929901,
        -0.6366869211196899,
        0.7174549698829651,
        0.668388843536377,
        0.3558337390422821,
        -0.6022041440010071,
        0.5900010466575623,
        -0.5070074200630188,
        0.5764837265014648,
        -0.12669456005096436,
        0.6742084622383118,
        0.6434892416000366,
        -0.855296790599823,
        1.9539669752120972,
        -0.20606304705142975,
        -0.3704226016998291,
        -0.18167641758918762,
        -0.037702180445194244,
        0.07605044543743134,
        0.3940941095352173,
        0.3852525055408478,
        -0.22333018481731415,
        -0.948399007320404,
        -0.5141260623931885,
        -0.3115938901901245,
        -0.3219735026359558,
        -0.44466841220855713,
        -0.024260099977254868,
        -0.34471091628074646,
        -0.27804821729660034,
        -0.058614879846572876,
        -0.5280972123146057,
        -0.503314733505249,
        0.21225246787071228,
        0.6679813265800476,
        0.5323523879051208,
        0.7919780015945435,
        -0.4270104169845581,
        0.10887797176837921,
        0.5332958102226257,
        0.5706650018692017,
        0.5227459073066711,
        -0.8339678049087524,
        -0.43353474140167236,
        -0.7698482871055603,
        -0.7690632939338684,
        -0.14367353916168213,
        0.5207870602607727,
        -1.141963243484497,
        0.5924328565597534,
        -0.683574378490448,
        -0.5026185512542725,
        0.4298264682292938,
        1.082524061203003,
        0.9960647821426392,
        0.4856751561164856,
        0.15055778622627258,
        -0.17104271054267883,
        -0.31288036704063416,
        1.2431144714355469,
        0.029268965125083923,
        -0.08019978553056717,
        1.0981659889221191,
        -0.4541480839252472,
        -0.6663427352905273,
        0.9550782442092896,
        0.14235873520374298,
        0.6975843906402588,
        -0.5949847102165222,
        -0.19730420410633087,
        -0.3904959559440613,
        1.1092708110809326,
        0.3009244501590729,
        0.031247664242982864,
        0.44713637232780457,
        0.18107835948467255,
        0.3656487464904785,
        -0.2874467372894287,
        -1.8777159452438354,
        0.07117811590433121,
        -0.7596774101257324,
        0.461743026971817,
        0.2148614227771759,
        -0.0954887717962265,
        0.9485973715782166,
        0.7047920227050781,
        -0.3378753066062927,
        0.5475413203239441,
        -0.07688707113265991,
        -0.023851027712225914,
        -0.31565621495246887,
        0.411799818277359,
        0.23104315996170044,
        -0.14659465849399567,
        -0.5572760701179504,
        -0.30596908926963806,
        0.03240183740854263,
        0.12454196810722351,
        -0.6687918305397034,
        0.04106743261218071,
        -0.012048061937093735,
        -0.4637337327003479,
        0.02204187959432602,
        0.4893868863582611,
        -0.18912464380264282,
        0.24299389123916626,
        -0.5596280097961426,
        -0.10482504218816757,
        0.49973639845848083,
        -0.47754597663879395,
        -0.16700872778892517,
        1.004321575164795,
        -0.788029134273529,
        -0.22516269981861115,
        -0.04527008906006813,
        -0.13958051800727844,
        0.9203858375549316,
        1.0958521366119385,
        -0.09602797031402588,
        0.17693588137626648,
        0.7918679118156433,
        -0.6654388308525085,
        -0.7929370403289795,
        0.32895174622535706,
        0.5853862762451172,
        0.21219459176063538,
        -0.00415782630443573,
        -0.5592212677001953,
        0.14117807149887085,
        -0.7280343770980835,
        0.005077846348285675,
        -0.26629382371902466,
        -0.5851109027862549,
        -0.05867423117160797,
        0.008711833506822586,
        0.0789753645658493,
        0.5337276458740234,
        -0.6759406924247742,
        0.7140458226203918,
        1.0714350938796997,
        0.29570460319519043,
        -1.1729543209075928,
        -0.25999411940574646,
        0.33072003722190857,
        0.4307868778705597,
        -0.11482184380292892,
        -0.003194708377122879,
        0.47140049934387207,
        0.40345659852027893,
        -1.0648318529129028,
        0.02429880201816559,
        1.5013399124145508,
        0.8883876204490662,
        -0.6337460875511169,
        -0.38822248578071594,
        0.26119464635849,
        0.4001885950565338,
        0.46637168526649475,
        0.3482370376586914,
        -0.7960430383682251,
        -0.19224008917808533,
        0.04789482802152634,
        0.3250610828399658,
        1.1956355571746826,
        -0.012324487790465355,
        -0.06738907098770142,
        0.6669347882270813,
        -0.05643197149038315,
        -0.5018022060394287,
        -1.10129714012146,
        0.6588606834411621,
        -0.15852484107017517,
        -1.057341456413269,
        0.8937212824821472,
        -0.1766931563615799,
        -0.04459201544523239,
        0.5795037746429443,
        -0.21037086844444275,
        -0.430261492729187,
        0.03927941620349884,
        -0.6637974381446838,
        1.7167118787765503,
        -0.7742263674736023,
        -0.6126756072044373,
        -0.6317086815834045,
        0.5173723101615906,
        -0.42666468024253845,
        0.2228303998708725,
        0.4560990333557129,
        -0.7225620746612549,
        -0.3558341860771179,
        0.2592615783214569,
        -0.20326031744480133,
        -0.004670005291700363,
        0.5304616093635559,
        0.34475448727607727,
        0.25893718004226685,
        -0.09335429966449738,
        0.022142015397548676,
        -0.10908755660057068,
        -0.13635395467281342,
        -0.3405359983444214,
        -0.29015442728996277,
        -0.2619116008281708,
        -0.5314561128616333,
        -0.5954377055168152
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the quality of writing and content, providing ratings and recommendations for improvement based on novelty, clarity, and overall messaging. It assesses ideas for their freshness and originality, clarity of argument, and quality of prose, offering a structured approach to critique. The expected output is a JSON object summarizing these evaluations and recommendations.",
          "name": "Analyze_prose_json",
          "raw": "\n                workflow Analyze_prose_json v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert writer and editor and you excel at evaluating the quality of writing and other content and providing various ratings and recommendations about how to improve it from a novelty, clarity, and overall messaging standpoint.\n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n1. Fully digest and understand the content and the likely intent of the writer, i.e., what they wanted to convey to the reader, viewer, listener.\n\n2. Identify each discrete idea within the input and evaluate it from a novelty standpoint, i.e., how surprising, fresh, or novel are the ideas in the content? Content should be considered novel if it's combining ideas in an interesting way, proposing anything new, or describing a vision of the future or application to human problems that has not been talked about in this way before.\n\n3. Evaluate the combined NOVELTY of the ideas in the writing as defined in STEP 2 and provide a rating on the following scale:\n\n\\\"A - Novel\\\" -- Does one or more of the following: Includes new ideas, proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported. Imagine a novelty score above 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Introduction of new ideas.\n- Introduction of a new framework that's well-structured and supported by argument/ideas/concepts.\n- Introduction of new models for understanding the world.\n- Makes a clear prediction that's backed by strong concepts and/or data.\n- Introduction of a new vision of the future.\n- Introduction of a new way of thinking about reality.\n- Recommendations for a way to behave based on the new proposed way of thinking.\n\n\\\"B - Fresh\\\" -- Proposes new ideas, but doesn't do any of the things mentioned in the \\\"A\\\" tier. Imagine a novelty score between 80% and 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Minor expansion on existing ideas, but in a way that's useful.\n\n\\\"C - Incremental\\\" -- Useful expansion or significant improvement of existing ideas, or a somewhat insightful description of the past, but no expansion on, or creation of, new ideas. Imagine a novelty score between 50% and 80% for this tier.\n\nCommon examples that meet this criteria:\n\n- Useful collections of resources.\n- Descriptions of the past with offered observations and takeaways.\n- Minor expansions on existing ideas.\n\n\\\"D - Derivative\\\" -- Largely derivative of well-known ideas. Imagine a novelty score between in the 20% to 50% range for this tier.\n\nCommon examples that meet this criteria:\n\n- Restatement of common knowledge or best practices.\n- Rehashes of well-known ideas without any new takes or expansions of ideas.\n- Contains ideas or facts, but they're not new or improved in any significant way.\n\n\\\"F - Stale\\\" -- No new ideas whatsoever. Imagine a novelty score below 20% for this tier.\n\nCommon examples that meet this criteria:\n\n- Completely trite and unoriginal ideas.\n- Heavily cliche or standard ideas.\n\n4. Evaluate the CLARITY of the writing on the following scale.\n\n\\\"A - Crystal\\\" -- The argument is very clear and concise, and stays in a flow that doesn't lose the main problem and solution.\n\\\"B - Clean\\\" -- The argument is quite clear and concise, and only needs minor optimizations.\n\\\"C - Kludgy\\\" -- Has good ideas, but could be more concise and more clear about the problems and solutions being proposed.\n\\\"D - Confusing\\\" -- The writing is quite confusing, and it's not clear how the pieces connect.\n\\\"F - Chaotic\\\" -- It's not even clear what's being attempted.\n\n5. Evaluate the PROSE in the writing on the following scale.\n\n\\\"A - Inspired\\\" -- Clear, fresh, distinctive prose that's free of cliche.\n\\\"B - Distinctive\\\" -- Strong writing that lacks significant use of cliche.\n\\\"C - Standard\\\" -- Decent prose, but lacks distinctive style and/or uses too much cliche or standard phrases.\n\\\"D - Stale\\\" -- Significant use of cliche and/or weak language.\n\\\"F - Weak\\\" -- Overwhelming language weakness and/or use of cliche.\n\n6. Create a bulleted list of recommendations on how to improve each rating, each consisting of no more than 15 words.\n\n7. Give an overall rating that's the lowest rating of 3, 4, and 5. So if they were B, C, and A, the overall-rating would be \\\"C\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- You output a valid JSON object with the following structure.\n\n```json\n{\n  \\\"novelty-rating\\\": \\\"(computed rating)\\\",\n  \\\"novelty-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"clarity-rating\\\": \\\"(computed rating)\\\",\n  \\\"clarity-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"prose-rating\\\": \\\"(computed rating)\\\",\n  \\\"prose-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"recommendations\\\": \\\"The list of recommendations.\\\",\n  \\\"one-sentence-summary\\\": \\\"A 20-word, one-sentence summary of the overall quality of the prose based on the ratings and explanations in the other fields.\\\",\n  \\\"overall-rating\\\": \\\"The lowest of the ratings given above, without a tagline to accompany the letter grade.\\\"\n}\n\nOUTPUT EXAMPLE\n\n{\n\\\"novelty-rating\\\": \\\"A - Novel\\\",\n\\\"novelty-rating-explanation\\\": \\\"Combines multiple existing ideas and adds new ones to construct a vision of the future.\\\",\n\\\"clarity-rating\\\": \\\"C - Kludgy\\\",\n\\\"clarity-rating-explanation\\\": \\\"Really strong arguments but you get lost when trying to follow them.\\\",\n\\\"prose-rating\\\": \\\"A - Inspired\\\",\n\\\"prose-rating-explanation\\\": \\\"Uses distinctive language and style to convey the message.\\\",\n\\\"recommendations\\\": \\\"The list of recommendations.\\\",\n\\\"one-sentence-summary\\\": \\\"A clear and fresh new vision of how we will interact with humanoid robots in the household.\\\",\n\\\"overall-rating\\\": \\\"C\\\"\n}\n\n```\n\n- Liberally evaluate the criteria for NOVELTY, meaning if the content proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported, it should be rated as \\\"A - Novel\\\".\n- The overall-rating cannot be higher than the lowest rating given.\n- You ONLY output this JSON object.\n- You do not output the ``` code indicators, only the JSON object itself.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert writer and editor and you excel at evaluating the quality of writing and other content and providing various ratings and recommendations about how to improve it from a novelty, clarity, and overall messaging standpoint.\n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n1. Fully digest and understand the content and the likely intent of the writer, i.e., what they wanted to convey to the reader, viewer, listener.\n\n2. Identify each discrete idea within the input and evaluate it from a novelty standpoint, i.e., how surprising, fresh, or novel are the ideas in the content? Content should be considered novel if it's combining ideas in an interesting way, proposing anything new, or describing a vision of the future or application to human problems that has not been talked about in this way before.\n\n3. Evaluate the combined NOVELTY of the ideas in the writing as defined in STEP 2 and provide a rating on the following scale:\n\n\\\"A - Novel\\\" -- Does one or more of the following: Includes new ideas, proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported. Imagine a novelty score above 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Introduction of new ideas.\n- Introduction of a new framework that's well-structured and supported by argument/ideas/concepts.\n- Introduction of new models for understanding the world.\n- Makes a clear prediction that's backed by strong concepts and/or data.\n- Introduction of a new vision of the future.\n- Introduction of a new way of thinking about reality.\n- Recommendations for a way to behave based on the new proposed way of thinking.\n\n\\\"B - Fresh\\\" -- Proposes new ideas, but doesn't do any of the things mentioned in the \\\"A\\\" tier. Imagine a novelty score between 80% and 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Minor expansion on existing ideas, but in a way that's useful.\n\n\\\"C - Incremental\\\" -- Useful expansion or significant improvement of existing ideas, or a somewhat insightful description of the past, but no expansion on, or creation of, new ideas. Imagine a novelty score between 50% and 80% for this tier.\n\nCommon examples that meet this criteria:\n\n- Useful collections of resources.\n- Descriptions of the past with offered observations and takeaways.\n- Minor expansions on existing ideas.\n\n\\\"D - Derivative\\\" -- Largely derivative of well-known ideas. Imagine a novelty score between in the 20% to 50% range for this tier.\n\nCommon examples that meet this criteria:\n\n- Restatement of common knowledge or best practices.\n- Rehashes of well-known ideas without any new takes or expansions of ideas.\n- Contains ideas or facts, but they're not new or improved in any significant way.\n\n\\\"F - Stale\\\" -- No new ideas whatsoever. Imagine a novelty score below 20% for this tier.\n\nCommon examples that meet this criteria:\n\n- Completely trite and unoriginal ideas.\n- Heavily cliche or standard ideas.\n\n4. Evaluate the CLARITY of the writing on the following scale.\n\n\\\"A - Crystal\\\" -- The argument is very clear and concise, and stays in a flow that doesn't lose the main problem and solution.\n\\\"B - Clean\\\" -- The argument is quite clear and concise, and only needs minor optimizations.\n\\\"C - Kludgy\\\" -- Has good ideas, but could be more concise and more clear about the problems and solutions being proposed.\n\\\"D - Confusing\\\" -- The writing is quite confusing, and it's not clear how the pieces connect.\n\\\"F - Chaotic\\\" -- It's not even clear what's being attempted.\n\n5. Evaluate the PROSE in the writing on the following scale.\n\n\\\"A - Inspired\\\" -- Clear, fresh, distinctive prose that's free of cliche.\n\\\"B - Distinctive\\\" -- Strong writing that lacks significant use of cliche.\n\\\"C - Standard\\\" -- Decent prose, but lacks distinctive style and/or uses too much cliche or standard phrases.\n\\\"D - Stale\\\" -- Significant use of cliche and/or weak language.\n\\\"F - Weak\\\" -- Overwhelming language weakness and/or use of cliche.\n\n6. Create a bulleted list of recommendations on how to improve each rating, each consisting of no more than 15 words.\n\n7. Give an overall rating that's the lowest rating of 3, 4, and 5. So if they were B, C, and A, the overall-rating would be \\\"C\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- You output a valid JSON object with the following structure.\n\n```json\n{\n  \\\"novelty-rating\\\": \\\"(computed rating)\\\",\n  \\\"novelty-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"clarity-rating\\\": \\\"(computed rating)\\\",\n  \\\"clarity-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"prose-rating\\\": \\\"(computed rating)\\\",\n  \\\"prose-rating-explanation\\\": \\\"A 15-20 word sentence justifying your rating.\\\",\n  \\\"recommendations\\\": \\\"The list of recommendations.\\\",\n  \\\"one-sentence-summary\\\": \\\"A 20-word, one-sentence summary of the overall quality of the prose based on the ratings and explanations in the other fields.\\\",\n  \\\"overall-rating\\\": \\\"The lowest of the ratings given above, without a tagline to accompany the letter grade.\\\"\n}\n\nOUTPUT EXAMPLE\n\n{\n\\\"novelty-rating\\\": \\\"A - Novel\\\",\n\\\"novelty-rating-explanation\\\": \\\"Combines multiple existing ideas and adds new ones to construct a vision of the future.\\\",\n\\\"clarity-rating\\\": \\\"C - Kludgy\\\",\n\\\"clarity-rating-explanation\\\": \\\"Really strong arguments but you get lost when trying to follow them.\\\",\n\\\"prose-rating\\\": \\\"A - Inspired\\\",\n\\\"prose-rating-explanation\\\": \\\"Uses distinctive language and style to convey the message.\\\",\n\\\"recommendations\\\": \\\"The list of recommendations.\\\",\n\\\"one-sentence-summary\\\": \\\"A clear and fresh new vision of how we will interact with humanoid robots in the household.\\\",\n\\\"overall-rating\\\": \\\"C\\\"\n}\n\n```\n\n- Liberally evaluate the criteria for NOVELTY, meaning if the content proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported, it should be rated as \\\"A - Novel\\\".\n- The overall-rating cannot be higher than the lowest rating given.\n- You ONLY output this JSON object.\n- You do not output the ``` code indicators, only the JSON object itself.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5470007658004761,
        0.5862957239151001,
        -0.41112378239631653,
        -0.33355996012687683,
        0.5075823068618774,
        0.3092925548553467,
        -0.6478137373924255,
        0.2617653012275696,
        -0.093589186668396,
        0.23130206763744354,
        0.1137959286570549,
        1.2243982553482056,
        -0.1279960721731186,
        -0.2496531903743744,
        0.22421738505363464,
        0.10783854871988297,
        0.01658286154270172,
        -0.38326317071914673,
        -1.3557276725769043,
        0.04079118371009827,
        0.7439004182815552,
        0.9156432747840881,
        0.7130505442619324,
        0.667634904384613,
        0.6659703850746155,
        0.4964849352836609,
        0.017070762813091278,
        0.012239739298820496,
        -0.8170243501663208,
        -1.1567494869232178,
        0.2829459309577942,
        0.11363233625888824,
        -0.37203606963157654,
        -0.7854092121124268,
        -0.33175548911094666,
        -0.2712685763835907,
        -0.22530671954154968,
        0.2762708067893982,
        -0.7799922227859497,
        -0.8096818923950195,
        0.16072705388069153,
        0.3303978741168976,
        -0.038474857807159424,
        0.38863956928253174,
        -0.20507784187793732,
        -0.28603798151016235,
        -0.035315997898578644,
        -0.10623148083686829,
        0.7793476581573486,
        0.1886521875858307,
        -0.2521686851978302,
        -0.5084949731826782,
        -0.265154093503952,
        -0.18953533470630646,
        -0.6579332947731018,
        -0.28937530517578125,
        -0.6254001259803772,
        -0.11373046040534973,
        0.0976669043302536,
        0.007829778827726841,
        0.3792741000652313,
        0.5376954078674316,
        -3.0443758964538574,
        -0.19179321825504303,
        -0.15101981163024902,
        0.21314704418182373,
        0.7446194291114807,
        0.6584548354148865,
        -0.04609641805291176,
        -0.2894294559955597,
        -0.3598690629005432,
        0.2582392990589142,
        -0.7535645961761475,
        0.21189314126968384,
        0.7332396507263184,
        -0.5327457189559937,
        0.46697476506233215,
        0.7996808290481567,
        0.6596035361289978,
        -0.367919385433197,
        0.3105582296848297,
        0.26856672763824463,
        0.26612839102745056,
        -0.2892974317073822,
        -0.3441067039966583,
        0.40380123257637024,
        -0.4752383232116699,
        -0.23825526237487793,
        0.683268129825592,
        0.17439918220043182,
        -0.3051926791667938,
        -0.48805251717567444,
        -0.6085042357444763,
        0.1840285062789917,
        -0.8138983249664307,
        -0.24108068645000458,
        -0.6415379047393799,
        0.3715883195400238,
        0.44796061515808105,
        3.508244276046753,
        0.2365381419658661,
        -0.32254958152770996,
        -0.012226268649101257,
        -0.8637595772743225,
        0.23258233070373535,
        -1.0239423513412476,
        0.10051067173480988,
        0.2732692062854767,
        -0.34241560101509094,
        -0.3385009169578552,
        0.7220143675804138,
        0.07207335531711578,
        -0.6459890604019165,
        0.05381523072719574,
        0.4018895924091339,
        0.9478779435157776,
        -0.6363115906715393,
        0.20652256906032562,
        0.12073264271020889,
        0.4839687645435333,
        -0.4419097602367401,
        0.2152542620897293,
        0.06282529979944229,
        -0.07318065315485,
        0.31003403663635254,
        -0.2655941843986511,
        -0.6493760347366333,
        0.7155207395553589,
        0.5785911679267883,
        0.6353772282600403,
        0.25843197107315063,
        0.8406157493591309,
        -1.0736842155456543,
        -0.15859656035900116,
        0.012727219611406326,
        -0.2130129337310791,
        -0.0339633971452713,
        -0.8932666182518005,
        -0.03614792227745056,
        -0.49592074751853943,
        -0.16825279593467712,
        -1.5138306617736816,
        0.7527459263801575,
        0.1088239923119545,
        0.7334156036376953,
        0.6872953772544861,
        -0.5213307738304138,
        -0.18594446778297424,
        -0.5263494253158569,
        -0.5679473876953125,
        -0.31401070952415466,
        0.8449917435646057,
        -0.2227495312690735,
        0.6118231415748596,
        0.20785930752754211,
        0.0493476539850235,
        -0.7555741667747498,
        0.3489648401737213,
        -0.15108081698417664,
        0.5307531356811523,
        0.1371995061635971,
        -0.18889150023460388,
        0.08195224404335022,
        0.3660706877708435,
        0.8734143972396851,
        -0.17850050330162048,
        0.2271362692117691,
        0.21661415696144104,
        -0.21619462966918945,
        -0.1252012848854065,
        0.4073300063610077,
        -0.2006584107875824,
        0.5454001426696777,
        0.9336711764335632,
        -0.5685122013092041,
        0.00970451906323433,
        0.07544176280498505,
        0.38623708486557007,
        0.37309813499450684,
        -1.044795036315918,
        0.10277776420116425,
        0.9237596392631531,
        0.009583216160535812,
        -0.46492400765419006,
        0.06155945360660553,
        -0.23903222382068634,
        0.7714043259620667,
        -0.4893738031387329,
        0.8302971720695496,
        1.3437130451202393,
        -0.9073358774185181,
        1.625118613243103,
        -0.4397343397140503,
        -0.2167222946882248,
        0.1710759699344635,
        -0.2544824182987213,
        -0.2078079730272293,
        0.4582154154777527,
        0.3629326820373535,
        -0.4525945782661438,
        -0.6464049220085144,
        -0.18900759518146515,
        -0.07469871640205383,
        0.0007139071822166443,
        -0.1387295424938202,
        -0.3305286467075348,
        -0.30900925397872925,
        -0.5266728401184082,
        -0.16365240514278412,
        -0.7184157371520996,
        -0.09439046680927277,
        -0.2113943099975586,
        0.6120141744613647,
        0.4790845513343811,
        0.18720605969429016,
        -0.3269950747489929,
        -0.17415133118629456,
        0.6458839178085327,
        0.5713938474655151,
        0.7334128618240356,
        -0.9711456298828125,
        -0.9606524109840393,
        -0.8715388774871826,
        -0.7841140627861023,
        -0.21871307492256165,
        0.3022136092185974,
        -0.8548243641853333,
        0.0953800156712532,
        -0.39379680156707764,
        0.05410763993859291,
        0.33567073941230774,
        0.9353113174438477,
        0.9154147505760193,
        0.17287257313728333,
        -0.21303445100784302,
        -0.18794763088226318,
        -0.36132705211639404,
        1.0757040977478027,
        -0.1503159999847412,
        0.15017881989479065,
        1.091323971748352,
        -0.1521691530942917,
        -0.7457696199417114,
        0.934956431388855,
        0.08892923593521118,
        0.16978788375854492,
        -0.7411095499992371,
        -0.010008752346038818,
        0.49301785230636597,
        1.6083674430847168,
        -0.058420151472091675,
        -0.1613796204328537,
        0.3411906361579895,
        0.17178195714950562,
        -0.18245336413383484,
        0.04790753498673439,
        -1.8728514909744263,
        0.30901074409484863,
        -1.3433613777160645,
        -0.04857483133673668,
        0.002631206065416336,
        -0.4913763999938965,
        1.4236490726470947,
        0.63359534740448,
        -0.16462501883506775,
        0.2292182892560959,
        -0.41235679388046265,
        -0.36994442343711853,
        -0.918354868888855,
        0.649559736251831,
        -0.36334142088890076,
        0.23195061087608337,
        -0.3305444121360779,
        -0.28763747215270996,
        0.030451921746134758,
        0.09331915527582169,
        -0.13566142320632935,
        -0.10072169452905655,
        -0.45269688963890076,
        -1.107254147529602,
        -0.1605093777179718,
        0.40157684683799744,
        0.013208158314228058,
        -0.08060139417648315,
        -0.6572521924972534,
        0.00829073041677475,
        -0.0910193920135498,
        -0.8975766897201538,
        -0.5034131407737732,
        0.8240280747413635,
        -0.9728657603263855,
        -0.3241438865661621,
        0.4071931540966034,
        0.2339775562286377,
        1.5642027854919434,
        0.854505717754364,
        0.08846060186624527,
        0.2958627939224243,
        0.6648417711257935,
        -0.07482074946165085,
        -0.9112910628318787,
        0.38785719871520996,
        0.06458895653486252,
        0.4203920066356659,
        -0.5858607888221741,
        -0.1799209713935852,
        0.3443986773490906,
        -0.45132341980934143,
        0.019744541496038437,
        -0.32652124762535095,
        -0.5806118249893188,
        0.3942610025405884,
        0.5328359007835388,
        -0.13616253435611725,
        0.7322271466255188,
        -0.08757936954498291,
        1.1212035417556763,
        0.6460427045822144,
        -0.08382295072078705,
        -1.0768375396728516,
        -0.06138654798269272,
        0.9221751093864441,
        0.9865158796310425,
        0.16948330402374268,
        -0.29822567105293274,
        0.8313058018684387,
        -0.1772315800189972,
        -0.5045391917228699,
        -0.1752578765153885,
        1.3207188844680786,
        0.5827443599700928,
        -0.055767495185136795,
        -0.18956181406974792,
        0.24719876050949097,
        0.14567242562770844,
        0.575515866279602,
        0.004592591896653175,
        -0.4584672749042511,
        -0.29140597581863403,
        -0.2615966796875,
        -0.022435680031776428,
        1.675570011138916,
        -0.08764856308698654,
        0.026878520846366882,
        0.6111015677452087,
        0.4250861704349518,
        -0.5112263560295105,
        -0.6540405750274658,
        0.24960656464099884,
        0.2792987823486328,
        -0.6236825585365295,
        0.6034753322601318,
        -0.01584623008966446,
        0.21182692050933838,
        0.07119899988174438,
        -0.011397130787372589,
        -0.6510476469993591,
        -0.2910849452018738,
        -0.8967317342758179,
        1.3899476528167725,
        -0.9133219718933105,
        -0.5711230635643005,
        -0.10697737336158752,
        0.5315898656845093,
        -0.5283046960830688,
        0.4927072525024414,
        0.3679233491420746,
        -1.4388102293014526,
        -0.4195454716682434,
        0.20154023170471191,
        -0.002834998071193695,
        -0.08459942787885666,
        0.8121229410171509,
        0.37063542008399963,
        0.49139779806137085,
        -0.1505693644285202,
        -0.15182477235794067,
        -0.09661182761192322,
        -0.3772832453250885,
        -0.29162436723709106,
        -0.17795956134796143,
        -0.4560256898403168,
        -0.4979037344455719,
        -0.4268900156021118
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates prose based on Steven Pinker's writing principles, identifying its current style and recommending improvements for clarity and engagement. It involves analyzing the text's adherence to Pinker's stylistic categories and avoiding common pitfalls in writing. The output includes a detailed analysis of the prose's style, strengths, weaknesses, and specific examples of both effective and ineffective writing elements.",
          "name": "Analyze_prose_pinker",
          "raw": "\n                workflow Analyze_prose_pinker v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at assessing prose and making recommendations based on Steven Pinker's book, The Sense of Style. \n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n- First, analyze and fully understand the prose and what they writing was likely trying to convey.\n\n- Next, deeply recall and remember everything you know about Steven Pinker's Sense of Style book, from all sources.\n\n- Next remember what Pinker said about writing styles and their merits: They were something like this:\n\n-- The Classic Style: Based on the ideal of clarity and directness, it aims for a conversational tone, as if the writer is directly addressing the reader. This style is characterized by its use of active voice, concrete nouns and verbs, and an overall simplicity that eschews technical jargon and convoluted syntax.\n\n-- The Practical Style: Focused on conveying information efficiently and clearly, this style is often used in business, technical writing, and journalism. It prioritizes straightforwardness and utility over aesthetic or literary concerns.\n\n-- The Self-Conscious Style: Characterized by an awareness of the writing process and a tendency to foreground the writer's own thoughts and feelings. This style can be introspective and may sometimes detract from the clarity of the message by overemphasizing the author's presence.\n\n-- The Postmodern Style: Known for its skepticism towards the concept of objective truth and its preference for exposing the complexities and contradictions of language and thought. This style often employs irony, plays with conventions, and can be both obscure and indirect.\n\n-- The Academic Style: Typically found in scholarly works, this style is dense, formal, and packed with technical terminology and references. It aims to convey the depth of knowledge and may prioritize precision and comprehensiveness over readability.\n\n-- The Legal Style: Used in legal writing, it is characterized by meticulous detail, precision, and a heavy reliance on jargon and established formulae. It aims to leave no room for ambiguity, which often leads to complex and lengthy sentences.\n\n- Next, deeply recall and remember everything you know about what Pinker said in that book to avoid in you're writing, which roughly broke into these categories. These are listed each with a good-score of 1-10 of how good the prose was at avoiding them, and how important it is to avoid them:\n\nMetadiscourse: Overuse of talk about the talk itself. Rating: 6\n\nVerbal Hedge: Excessive use of qualifiers that weaken the point being made. Rating: 5\n\nNominalization: Turning actions into entities, making sentences ponderous. Rating: 7\n\nPassive Voice: Using passive constructions unnecessarily. Rating: 7\n\nJargon and Technical Terms: Overloading the text with specialized terms. Rating: 8\n\nClichés: Relying on tired phrases and expressions. Rating: 6\n\nFalse Fronts: Attempting to sound formal or academic by using complex words or phrases. Rating: 9\n\nOveruse of Adverbs: Adding too many adverbs, particularly those ending in \\\"-ly\\\". Rating: 4\n\nZombie Nouns: Nouns that are derived from other parts of speech, making sentences abstract. Rating: 7\n\nComplex Sentences: Overcomplicating sentence structure unnecessarily. Rating: 8\n\nEuphemism: Using mild or indirect terms to avoid directness. Rating: 6\n\nOut-of-Context Quotations: Using quotes that don't accurately represent the source. Rating: 9\n\nExcessive Precaution: Being overly cautious in statements can make the writing seem unsure. Rating: 5\n\nOvergeneralization: Making broad statements without sufficient support. Rating: 7\n\nMixed Metaphors: Combining metaphors in a way that is confusing or absurd. Rating: 6\n\nTautology: Saying the same thing twice in different words unnecessarily. Rating: 5\n\nObfuscation: Deliberately making writing confusing to sound profound. Rating: 8\n\nRedundancy: Repeating the same information unnecessarily. Rating: 6\n\nProvincialism: Assuming knowledge or norms specific to a particular group. Rating: 7\n\nArchaism: Using outdated language or styles. Rating: 5\n\nEuphuism: Overly ornate language that distracts from the message. Rating: 6\n\nOfficialese: Overly formal and bureaucratic language. Rating: 7\n\nGobbledygook: Language that is nonsensical or incomprehensible. Rating: 9\n\nBafflegab: Deliberately ambiguous or obscure language. Rating: 8\n\nMangled Idioms: Using idioms incorrectly or inappropriately. Rating: 5\n\n# OUTPUT\n\n- In a section called STYLE ANALYSIS, you will evaluate the prose for what style it is written in and what style it should be written in, based on Pinker's categories. Give your answer in 3-5 bullet points of 15 words each. E.g.: \n\n\\\"- The prose is mostly written in CLASSICAL sytle, but could benefit from more directness.\\\"\n\\\"Next bullet point\\\"\n\n- In section called POSITIVE ASSESSMENT, rate the prose on this scale from 1-10, with 10 being the best. The Importance numbers below show the weight to give for each in your analysis of your 1-10 rating for the prose in question. Give your answers in bullet points of 15 words each. \n\nClarity: Making the intended message clear to the reader. Importance: 10\nBrevity: Being concise and avoiding unnecessary words. Importance: 8\nElegance: Writing in a manner that is not only clear and effective but also pleasing to read. Importance: 7\nCoherence: Ensuring the text is logically organized and flows well. Importance: 9\nDirectness: Communicating in a straightforward manner. Importance: 8\nVividness: Using language that evokes clear, strong images or concepts. Importance: 7\nHonesty: Conveying the truth without distortion or manipulation. Importance: 9\nVariety: Using a range of sentence structures and words to keep the reader engaged. Importance: 6\nPrecision: Choosing words that accurately convey the intended meaning. Importance: 9\nConsistency: Maintaining the same style and tone throughout the text. Importance: 7\n\n- In a section called CRITICAL ASSESSMENT, evaluate the prose based on the presence of the bad writing elements Pinker warned against above. Give your answers for each category in 3-5 bullet points of 15 words each. E.g.: \n\n\\\"- Overuse of Adverbs: 3/10 — There were only a couple examples of adverb usage and they were moderate.\\\"\n\n- In a section called EXAMPLES, give examples of both good and bad writing from the prose in question. Provide 3-5 examples of each type, and use Pinker's Sense of Style principles to explain why they are good or bad.\n\n- In a section called SPELLING/GRAMMAR, find all the tactical, common mistakes of spelling and grammar and give the sentence they occur in and the fix in a bullet point. List all of these instances, not just a few.\n\n- In a section called IMPROVEMENT RECOMMENDATIONS, give 5-10 bullet points of 15 words each on how the prose could be improved based on the analysis above. Give actual examples of the bad writing and possible fixes.\n\n## SCORING SYSTEM\n\n- In a section called SCORING, give a final score for the prose based on the analysis above. E.g.:\n\nSTARTING SCORE = 100\n\nDeductions:\n\n- -5 for overuse of adverbs\n- (other examples)\n\nFINAL SCORE = X\n\nAn overall assessment of the prose in 2-3 sentences of no more than 200 words.\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n\n- Don't use bold or italic formatting in the Markdown.\n\n- Do no complain about the input data. Just do the task.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at assessing prose and making recommendations based on Steven Pinker's book, The Sense of Style. \n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n- First, analyze and fully understand the prose and what they writing was likely trying to convey.\n\n- Next, deeply recall and remember everything you know about Steven Pinker's Sense of Style book, from all sources.\n\n- Next remember what Pinker said about writing styles and their merits: They were something like this:\n\n-- The Classic Style: Based on the ideal of clarity and directness, it aims for a conversational tone, as if the writer is directly addressing the reader. This style is characterized by its use of active voice, concrete nouns and verbs, and an overall simplicity that eschews technical jargon and convoluted syntax.\n\n-- The Practical Style: Focused on conveying information efficiently and clearly, this style is often used in business, technical writing, and journalism. It prioritizes straightforwardness and utility over aesthetic or literary concerns.\n\n-- The Self-Conscious Style: Characterized by an awareness of the writing process and a tendency to foreground the writer's own thoughts and feelings. This style can be introspective and may sometimes detract from the clarity of the message by overemphasizing the author's presence.\n\n-- The Postmodern Style: Known for its skepticism towards the concept of objective truth and its preference for exposing the complexities and contradictions of language and thought. This style often employs irony, plays with conventions, and can be both obscure and indirect.\n\n-- The Academic Style: Typically found in scholarly works, this style is dense, formal, and packed with technical terminology and references. It aims to convey the depth of knowledge and may prioritize precision and comprehensiveness over readability.\n\n-- The Legal Style: Used in legal writing, it is characterized by meticulous detail, precision, and a heavy reliance on jargon and established formulae. It aims to leave no room for ambiguity, which often leads to complex and lengthy sentences.\n\n- Next, deeply recall and remember everything you know about what Pinker said in that book to avoid in you're writing, which roughly broke into these categories. These are listed each with a good-score of 1-10 of how good the prose was at avoiding them, and how important it is to avoid them:\n\nMetadiscourse: Overuse of talk about the talk itself. Rating: 6\n\nVerbal Hedge: Excessive use of qualifiers that weaken the point being made. Rating: 5\n\nNominalization: Turning actions into entities, making sentences ponderous. Rating: 7\n\nPassive Voice: Using passive constructions unnecessarily. Rating: 7\n\nJargon and Technical Terms: Overloading the text with specialized terms. Rating: 8\n\nClichés: Relying on tired phrases and expressions. Rating: 6\n\nFalse Fronts: Attempting to sound formal or academic by using complex words or phrases. Rating: 9\n\nOveruse of Adverbs: Adding too many adverbs, particularly those ending in \\\"-ly\\\". Rating: 4\n\nZombie Nouns: Nouns that are derived from other parts of speech, making sentences abstract. Rating: 7\n\nComplex Sentences: Overcomplicating sentence structure unnecessarily. Rating: 8\n\nEuphemism: Using mild or indirect terms to avoid directness. Rating: 6\n\nOut-of-Context Quotations: Using quotes that don't accurately represent the source. Rating: 9\n\nExcessive Precaution: Being overly cautious in statements can make the writing seem unsure. Rating: 5\n\nOvergeneralization: Making broad statements without sufficient support. Rating: 7\n\nMixed Metaphors: Combining metaphors in a way that is confusing or absurd. Rating: 6\n\nTautology: Saying the same thing twice in different words unnecessarily. Rating: 5\n\nObfuscation: Deliberately making writing confusing to sound profound. Rating: 8\n\nRedundancy: Repeating the same information unnecessarily. Rating: 6\n\nProvincialism: Assuming knowledge or norms specific to a particular group. Rating: 7\n\nArchaism: Using outdated language or styles. Rating: 5\n\nEuphuism: Overly ornate language that distracts from the message. Rating: 6\n\nOfficialese: Overly formal and bureaucratic language. Rating: 7\n\nGobbledygook: Language that is nonsensical or incomprehensible. Rating: 9\n\nBafflegab: Deliberately ambiguous or obscure language. Rating: 8\n\nMangled Idioms: Using idioms incorrectly or inappropriately. Rating: 5\n\n# OUTPUT\n\n- In a section called STYLE ANALYSIS, you will evaluate the prose for what style it is written in and what style it should be written in, based on Pinker's categories. Give your answer in 3-5 bullet points of 15 words each. E.g.: \n\n\\\"- The prose is mostly written in CLASSICAL sytle, but could benefit from more directness.\\\"\n\\\"Next bullet point\\\"\n\n- In section called POSITIVE ASSESSMENT, rate the prose on this scale from 1-10, with 10 being the best. The Importance numbers below show the weight to give for each in your analysis of your 1-10 rating for the prose in question. Give your answers in bullet points of 15 words each. \n\nClarity: Making the intended message clear to the reader. Importance: 10\nBrevity: Being concise and avoiding unnecessary words. Importance: 8\nElegance: Writing in a manner that is not only clear and effective but also pleasing to read. Importance: 7\nCoherence: Ensuring the text is logically organized and flows well. Importance: 9\nDirectness: Communicating in a straightforward manner. Importance: 8\nVividness: Using language that evokes clear, strong images or concepts. Importance: 7\nHonesty: Conveying the truth without distortion or manipulation. Importance: 9\nVariety: Using a range of sentence structures and words to keep the reader engaged. Importance: 6\nPrecision: Choosing words that accurately convey the intended meaning. Importance: 9\nConsistency: Maintaining the same style and tone throughout the text. Importance: 7\n\n- In a section called CRITICAL ASSESSMENT, evaluate the prose based on the presence of the bad writing elements Pinker warned against above. Give your answers for each category in 3-5 bullet points of 15 words each. E.g.: \n\n\\\"- Overuse of Adverbs: 3/10 — There were only a couple examples of adverb usage and they were moderate.\\\"\n\n- In a section called EXAMPLES, give examples of both good and bad writing from the prose in question. Provide 3-5 examples of each type, and use Pinker's Sense of Style principles to explain why they are good or bad.\n\n- In a section called SPELLING/GRAMMAR, find all the tactical, common mistakes of spelling and grammar and give the sentence they occur in and the fix in a bullet point. List all of these instances, not just a few.\n\n- In a section called IMPROVEMENT RECOMMENDATIONS, give 5-10 bullet points of 15 words each on how the prose could be improved based on the analysis above. Give actual examples of the bad writing and possible fixes.\n\n## SCORING SYSTEM\n\n- In a section called SCORING, give a final score for the prose based on the analysis above. E.g.:\n\nSTARTING SCORE = 100\n\nDeductions:\n\n- -5 for overuse of adverbs\n- (other examples)\n\nFINAL SCORE = X\n\nAn overall assessment of the prose in 2-3 sentences of no more than 200 words.\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n\n- Don't use bold or italic formatting in the Markdown.\n\n- Do no complain about the input data. Just do the task.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6848592758178711,
        -0.00005173683166503906,
        -0.23051686584949493,
        0.402021586894989,
        0.6857767105102539,
        0.21888646483421326,
        -0.6451827883720398,
        0.14834734797477722,
        0.3486718237400055,
        -0.021031556650996208,
        0.17157194018363953,
        1.511508584022522,
        -0.009638943709433079,
        -0.16493770480155945,
        0.10026633739471436,
        -0.12140589207410812,
        -0.45728257298469543,
        -0.267473429441452,
        -1.0253362655639648,
        -0.01699794828891754,
        0.562144935131073,
        0.9148938059806824,
        0.4909873306751251,
        0.22654639184474945,
        1.1785123348236084,
        0.5377792119979858,
        0.002324134111404419,
        -0.23589064180850983,
        -0.8649999499320984,
        -1.6472409963607788,
        0.17980140447616577,
        0.1988602727651596,
        -0.551537275314331,
        -0.9461488723754883,
        -0.0985436663031578,
        -0.5936208367347717,
        -0.23256035149097443,
        -0.1442776769399643,
        -0.6501789093017578,
        -0.7444990277290344,
        0.23543787002563477,
        0.42196378111839294,
        0.046184197068214417,
        -0.12633182108402252,
        -0.08674158900976181,
        -0.44791343808174133,
        -0.13786371052265167,
        -0.2995471954345703,
        0.6847720146179199,
        0.31515833735466003,
        -0.36062297224998474,
        -0.5388121008872986,
        -0.26074832677841187,
        -0.4669434130191803,
        -0.4618990123271942,
        -0.24469247460365295,
        -0.03385833650827408,
        -0.06046011671423912,
        -0.22324858605861664,
        -0.17956101894378662,
        0.17743581533432007,
        0.336700975894928,
        -3.341066598892212,
        0.13681553304195404,
        -0.04690507799386978,
        -0.17924904823303223,
        0.37423449754714966,
        0.735672652721405,
        0.6743670105934143,
        -0.028445839881896973,
        -0.1907176673412323,
        0.4001179039478302,
        -0.17713195085525513,
        0.6082281470298767,
        0.21981880068778992,
        0.04451745003461838,
        -0.2643445134162903,
        0.09588296711444855,
        0.48088979721069336,
        -0.03324781730771065,
        0.18954218924045563,
        0.16958554089069366,
        0.34514859318733215,
        -0.3629872500896454,
        -0.12949450314044952,
        0.16221700608730316,
        -0.2436964213848114,
        0.1589115560054779,
        0.7092671990394592,
        -0.04757697880268097,
        -0.40306806564331055,
        -0.5543065071105957,
        -0.08953897655010223,
        -0.036708563566207886,
        -0.2835484743118286,
        -0.6481003761291504,
        0.20639292895793915,
        0.4856937527656555,
        0.5039333701133728,
        3.556659460067749,
        0.6965027451515198,
        -0.16860803961753845,
        0.6653159260749817,
        -1.0939912796020508,
        0.26256024837493896,
        -0.40337589383125305,
        -0.4800662100315094,
        -0.02687443234026432,
        -0.07776442915201187,
        -0.42800822854042053,
        0.3672982454299927,
        -0.4044719636440277,
        -0.23274695873260498,
        0.07324882596731186,
        0.49091067910194397,
        1.3249013423919678,
        -0.9871421456336975,
        0.07598313689231873,
        -0.1174517497420311,
        0.6817891597747803,
        -0.7764350175857544,
        -0.12450936436653137,
        -0.03665876388549805,
        -0.09172286838293076,
        -0.16039326786994934,
        -0.05010993778705597,
        -0.6660706996917725,
        0.4946621358394623,
        0.46262338757514954,
        1.2685959339141846,
        -0.011054523289203644,
        0.14587539434432983,
        -1.0365047454833984,
        -0.11414645612239838,
        -0.3311750292778015,
        -0.3884675204753876,
        -0.1821461170911789,
        -0.9850603938102722,
        0.3590112626552582,
        -0.35094159841537476,
        0.1537453830242157,
        -1.4897862672805786,
        0.5188513994216919,
        0.208206906914711,
        1.1098161935806274,
        0.37468841671943665,
        -0.24358654022216797,
        0.3687054514884949,
        -0.4036906361579895,
        -0.6815051436424255,
        -0.24066941440105438,
        0.5817923545837402,
        -0.4819064438343048,
        0.4830228090286255,
        0.7022742629051208,
        0.41887134313583374,
        -1.0074924230575562,
        0.44319650530815125,
        -0.576387345790863,
        0.16371291875839233,
        0.022840816527605057,
        -0.20451557636260986,
        -0.10206276178359985,
        0.04684329777956009,
        1.237483024597168,
        -0.35125917196273804,
        0.019014671444892883,
        0.17988398671150208,
        -0.08032980561256409,
        0.2880675196647644,
        0.13759924471378326,
        -0.022638864815235138,
        0.6675307750701904,
        1.0758768320083618,
        -0.644629180431366,
        -0.07082963734865189,
        0.423685222864151,
        0.8801230192184448,
        0.1935558319091797,
        -0.8072865009307861,
        0.3909871280193329,
        0.9563038349151611,
        0.369596391916275,
        -0.5446119904518127,
        0.16992051899433136,
        -0.44573915004730225,
        0.5279896855354309,
        -0.037042368203401566,
        0.7350624203681946,
        1.2427093982696533,
        -1.0204112529754639,
        1.3582088947296143,
        -0.8768752813339233,
        0.08942103385925293,
        -0.13129183650016785,
        -0.06983605027198792,
        -0.15441608428955078,
        0.5080024003982544,
        0.07903343439102173,
        -0.34319183230400085,
        -1.0162081718444824,
        -0.295754998922348,
        -0.13812150061130524,
        -0.43413928151130676,
        -0.5155579447746277,
        -0.10094508528709412,
        -0.653276264667511,
        -0.24464696645736694,
        0.09278690814971924,
        -0.588513970375061,
        -0.32124200463294983,
        0.046105772256851196,
        0.5948148965835571,
        0.39924368262290955,
        0.24525882303714752,
        -0.3808329105377197,
        0.07329399883747101,
        0.5458773970603943,
        0.3590428829193115,
        0.7633944749832153,
        -0.9119278192520142,
        -0.5499039888381958,
        -0.7012060284614563,
        -0.8356080055236816,
        -0.10774287581443787,
        1.0553677082061768,
        -0.8686443567276001,
        0.9472753405570984,
        -0.7342557907104492,
        -0.5351322293281555,
        -0.03455037623643875,
        0.8970787525177002,
        1.0542768239974976,
        0.12074896693229675,
        -0.5677151083946228,
        0.09732340276241302,
        -0.39272063970565796,
        0.846511721611023,
        -0.5756270885467529,
        -0.4027455449104309,
        0.8965060114860535,
        0.02319827303290367,
        -0.5402426719665527,
        0.8350071907043457,
        0.3988640606403351,
        0.21286144852638245,
        -0.4324494004249573,
        -0.3511500358581543,
        -0.42143306136131287,
        1.1992177963256836,
        0.32930681109428406,
        -0.14453300833702087,
        0.4169149696826935,
        0.20970770716667175,
        -0.16687318682670593,
        -0.1251855194568634,
        -1.5695301294326782,
        0.3302862048149109,
        -0.8251692652702332,
        0.28593751788139343,
        -0.008053068071603775,
        -0.09794237464666367,
        0.9183148741722107,
        0.8141956925392151,
        0.08399669080972672,
        0.502236545085907,
        0.13561414182186127,
        -0.18495681881904602,
        -0.5311177372932434,
        0.4872265160083771,
        0.48764723539352417,
        -0.475103497505188,
        -0.020445479080080986,
        -0.17140907049179077,
        -0.026131251826882362,
        0.1460621953010559,
        -0.813464343547821,
        -0.1015547588467598,
        -0.282810777425766,
        -0.8601876497268677,
        0.08073952794075012,
        0.20331473648548126,
        -0.6484777927398682,
        0.3800770044326782,
        -0.48582959175109863,
        0.40562164783477783,
        0.46023979783058167,
        -1.0271974802017212,
        0.02251436561346054,
        0.9488865733146667,
        -0.8910182118415833,
        -0.05244514346122742,
        -0.06471559405326843,
        0.37071338295936584,
        1.165866494178772,
        0.7563745379447937,
        -0.3200604319572449,
        0.19961506128311157,
        0.6873295307159424,
        -0.14830505847930908,
        -0.9242699146270752,
        0.4200163185596466,
        0.19236326217651367,
        -0.04040534794330597,
        -0.18348973989486694,
        -0.569190502166748,
        0.6431486010551453,
        -0.48598676919937134,
        0.0654967874288559,
        -0.026389341801404953,
        -0.7777025699615479,
        0.12221817672252655,
        0.35506460070610046,
        0.03472977131605148,
        0.25363898277282715,
        -0.33476242423057556,
        0.38649871945381165,
        1.0301175117492676,
        0.01881687343120575,
        -0.8602900505065918,
        -0.40296968817710876,
        0.8348891139030457,
        0.365121990442276,
        0.2082304060459137,
        -0.15871576964855194,
        0.6791664361953735,
        0.1790018379688263,
        -0.5470788478851318,
        -0.145161435008049,
        1.2209155559539795,
        0.8350680470466614,
        -0.3041830360889435,
        -0.07493360340595245,
        0.18497106432914734,
        0.5436575412750244,
        -0.006564766634255648,
        0.4466474652290344,
        -0.2527563273906708,
        -0.4578458368778229,
        -0.36503615975379944,
        0.1434602439403534,
        1.7443671226501465,
        -0.06262911856174469,
        -0.10584835708141327,
        0.37215739488601685,
        -0.019487790763378143,
        -0.4480217695236206,
        -0.546635091304779,
        0.4218875765800476,
        0.060838401317596436,
        -0.48846903443336487,
        0.544144332408905,
        0.19036969542503357,
        0.014401273801922798,
        0.5147871375083923,
        -0.05380770564079285,
        -0.21523332595825195,
        0.028240088373422623,
        -0.3841533064842224,
        1.6941990852355957,
        -0.8333654403686523,
        -0.8318576812744141,
        -0.7189453840255737,
        0.4418787658214569,
        -0.42955705523490906,
        0.20680749416351318,
        0.2713412046432495,
        -1.0046062469482422,
        -0.19080854952335358,
        0.2345169484615326,
        -0.05316956713795662,
        -0.3368033170700073,
        0.36614879965782166,
        0.26946353912353516,
        0.6892681121826172,
        -0.24409572780132294,
        -0.2440507560968399,
        0.19103288650512695,
        -0.28222987055778503,
        -0.617373526096344,
        0.5334998965263367,
        -0.32959291338920593,
        -0.5519891977310181,
        -1.0574709177017212
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the quality of writing by assessing its novelty, clarity, and prose, and provides improvement recommendations. It uses a detailed approach to rate each aspect on a specific scale and ensures the overall rating reflects the lowest individual score. The expected output includes ratings and concise improvement tips.",
          "name": "Analyze_prose",
          "raw": "\n                workflow Analyze_prose v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert writer and editor and you excel at evaluating the quality of writing and other content and providing various ratings and recommendations about how to improve it from a novelty, clarity, and overall messaging standpoint.\n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n1. Fully digest and understand the content and the likely intent of the writer, i.e., what they wanted to convey to the reader, viewer, listener.\n\n2. Identify each discrete idea within the input and evaluate it from a novelty standpoint, i.e., how surprising, fresh, or novel are the ideas in the content? Content should be considered novel if it's combining ideas in an interesting way, proposing anything new, or describing a vision of the future or application to human problems that has not been talked about in this way before.\n\n3. Evaluate the combined NOVELTY of the ideas in the writing as defined in STEP 2 and provide a rating on the following scale:\n\n\\\"A - Novel\\\" -- Does one or more of the following: Includes new ideas, proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported. Imagine a novelty score above 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Introduction of new ideas.\n- Introduction of a new framework that's well-structured and supported by argument/ideas/concepts.\n- Introduction of new models for understanding the world.\n- Makes a clear prediction that's backed by strong concepts and/or data.\n- Introduction of a new vision of the future.\n- Introduction of a new way of thinking about reality.\n- Recommendations for a way to behave based on the new proposed way of thinking.\n\n\\\"B - Fresh\\\" -- Proposes new ideas, but doesn't do any of the things mentioned in the \\\"A\\\" tier. Imagine a novelty score between 80% and 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Minor expansion on existing ideas, but in a way that's useful.\n\n\\\"C - Incremental\\\" -- Useful expansion or improvement of existing ideas, or a useful description of the past, but no expansion or creation of new ideas. Imagine a novelty score between 50% and 80% for this tier.\n\nCommon examples that meet this criteria:\n\n- Valuable collections of resources\n- Descriptions of the past with offered observations and takeaways\n\n\\\"D - Derivative\\\" -- Largely derivative of well-known ideas. Imagine a novelty score between in the 20% to 50% range for this tier.\n\nCommon examples that meet this criteria:\n\n- Contains ideas or facts, but they're not new in any way.\n\n\\\"F - Stale\\\" -- No new ideas whatsoever. Imagine a novelty score below 20% for this tier.\n\nCommon examples that meet this criteria:\n\n- Random ramblings that say nothing new.\n\n4. Evaluate the CLARITY of the writing on the following scale.\n\n\\\"A - Crystal\\\" -- The argument is very clear and concise, and stays in a flow that doesn't lose the main problem and solution.\n\\\"B - Clean\\\" -- The argument is quite clear and concise, and only needs minor optimizations.\n\\\"C - Kludgy\\\" -- Has good ideas, but could be more concise and more clear about the problems and solutions being proposed.\n\\\"D - Confusing\\\" -- The writing is quite confusing, and it's not clear how the pieces connect.\n\\\"F - Chaotic\\\" -- It's not even clear what's being attempted.\n\n5. Evaluate the PROSE in the writing on the following scale.\n\n\\\"A - Inspired\\\" -- Clear, fresh, distinctive prose that's free of cliche.\n\\\"B - Distinctive\\\" -- Strong writing that lacks significant use of cliche.\n\\\"C - Standard\\\" -- Decent prose, but lacks distinctive style and/or uses too much cliche or standard phrases.\n\\\"D - Stale\\\" -- Significant use of cliche and/or weak language.\n\\\"F - Weak\\\" -- Overwhelming language weakness and/or use of cliche.\n\n6. Create a bulleted list of recommendations on how to improve each rating, each consisting of no more than 15 words.\n\n7. Give an overall rating that's the lowest rating of 3, 4, and 5. So if they were B, C, and A, the overall-rating would be \\\"C\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n- Don't use bold or italic formatting in the Markdown.\n- Liberally evaluate the criteria for NOVELTY, meaning if the content proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported, it should be rated as \\\"A - Novel\\\".\n- The overall-rating cannot be higher than the lowest rating given.\n- The overall-rating only has the letter grade, not any additional information.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert writer and editor and you excel at evaluating the quality of writing and other content and providing various ratings and recommendations about how to improve it from a novelty, clarity, and overall messaging standpoint.\n\nTake a step back and think step-by-step about how to achieve the best outcomes by following the STEPS below.\n\n# STEPS\n\n1. Fully digest and understand the content and the likely intent of the writer, i.e., what they wanted to convey to the reader, viewer, listener.\n\n2. Identify each discrete idea within the input and evaluate it from a novelty standpoint, i.e., how surprising, fresh, or novel are the ideas in the content? Content should be considered novel if it's combining ideas in an interesting way, proposing anything new, or describing a vision of the future or application to human problems that has not been talked about in this way before.\n\n3. Evaluate the combined NOVELTY of the ideas in the writing as defined in STEP 2 and provide a rating on the following scale:\n\n\\\"A - Novel\\\" -- Does one or more of the following: Includes new ideas, proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported. Imagine a novelty score above 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Introduction of new ideas.\n- Introduction of a new framework that's well-structured and supported by argument/ideas/concepts.\n- Introduction of new models for understanding the world.\n- Makes a clear prediction that's backed by strong concepts and/or data.\n- Introduction of a new vision of the future.\n- Introduction of a new way of thinking about reality.\n- Recommendations for a way to behave based on the new proposed way of thinking.\n\n\\\"B - Fresh\\\" -- Proposes new ideas, but doesn't do any of the things mentioned in the \\\"A\\\" tier. Imagine a novelty score between 80% and 90% for this tier.\n\nCommon examples that meet this criteria:\n\n- Minor expansion on existing ideas, but in a way that's useful.\n\n\\\"C - Incremental\\\" -- Useful expansion or improvement of existing ideas, or a useful description of the past, but no expansion or creation of new ideas. Imagine a novelty score between 50% and 80% for this tier.\n\nCommon examples that meet this criteria:\n\n- Valuable collections of resources\n- Descriptions of the past with offered observations and takeaways\n\n\\\"D - Derivative\\\" -- Largely derivative of well-known ideas. Imagine a novelty score between in the 20% to 50% range for this tier.\n\nCommon examples that meet this criteria:\n\n- Contains ideas or facts, but they're not new in any way.\n\n\\\"F - Stale\\\" -- No new ideas whatsoever. Imagine a novelty score below 20% for this tier.\n\nCommon examples that meet this criteria:\n\n- Random ramblings that say nothing new.\n\n4. Evaluate the CLARITY of the writing on the following scale.\n\n\\\"A - Crystal\\\" -- The argument is very clear and concise, and stays in a flow that doesn't lose the main problem and solution.\n\\\"B - Clean\\\" -- The argument is quite clear and concise, and only needs minor optimizations.\n\\\"C - Kludgy\\\" -- Has good ideas, but could be more concise and more clear about the problems and solutions being proposed.\n\\\"D - Confusing\\\" -- The writing is quite confusing, and it's not clear how the pieces connect.\n\\\"F - Chaotic\\\" -- It's not even clear what's being attempted.\n\n5. Evaluate the PROSE in the writing on the following scale.\n\n\\\"A - Inspired\\\" -- Clear, fresh, distinctive prose that's free of cliche.\n\\\"B - Distinctive\\\" -- Strong writing that lacks significant use of cliche.\n\\\"C - Standard\\\" -- Decent prose, but lacks distinctive style and/or uses too much cliche or standard phrases.\n\\\"D - Stale\\\" -- Significant use of cliche and/or weak language.\n\\\"F - Weak\\\" -- Overwhelming language weakness and/or use of cliche.\n\n6. Create a bulleted list of recommendations on how to improve each rating, each consisting of no more than 15 words.\n\n7. Give an overall rating that's the lowest rating of 3, 4, and 5. So if they were B, C, and A, the overall-rating would be \\\"C\\\".\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n- Don't use bold or italic formatting in the Markdown.\n- Liberally evaluate the criteria for NOVELTY, meaning if the content proposes a new model for doing something, makes clear recommendations for action based on a new proposed model, creatively links existing ideas in a useful way, proposes new explanations for known phenomenon, or lays out a significant vision of what's to come that's well supported, it should be rated as \\\"A - Novel\\\".\n- The overall-rating cannot be higher than the lowest rating given.\n- The overall-rating only has the letter grade, not any additional information.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.3797360956668854,
        -0.23037327826023102,
        0.5428496599197388,
        0.5699265003204346,
        0.08271698653697968,
        -0.1444794237613678,
        -1.019899606704712,
        0.029688343405723572,
        0.2315104603767395,
        0.2578369975090027,
        -0.07605507224798203,
        0.740104079246521,
        -0.045490533113479614,
        -0.15106773376464844,
        0.30314046144485474,
        -0.07169653475284576,
        0.1361842006444931,
        -0.40950241684913635,
        -1.4350314140319824,
        -0.26273873448371887,
        0.3843458592891693,
        0.6072746515274048,
        0.14770451188087463,
        -0.007589124143123627,
        0.7750921845436096,
        0.0014481199905276299,
        -0.5827739834785461,
        0.08409780263900757,
        -0.5647441744804382,
        -1.6419727802276611,
        0.3610396087169647,
        -0.25070837140083313,
        -1.129916787147522,
        -0.24583081901073456,
        -0.026825763285160065,
        -0.040481191128492355,
        0.011329191736876965,
        0.43549084663391113,
        -0.4752555191516876,
        -0.6900573372840881,
        0.047858983278274536,
        0.6313013434410095,
        -0.6024602651596069,
        -0.08522212505340576,
        -0.15833531320095062,
        -0.29061388969421387,
        0.17517371475696564,
        -0.24436773359775543,
        0.9135839343070984,
        0.7774677872657776,
        -1.058611273765564,
        -0.4881313443183899,
        0.5291089415550232,
        0.11257463693618774,
        -0.6417609453201294,
        0.02396446466445923,
        0.4046868085861206,
        -0.4746265411376953,
        0.2240159958600998,
        0.332321435213089,
        0.46147963404655457,
        0.13652029633522034,
        -2.588557481765747,
        -0.2943572998046875,
        -0.31851914525032043,
        0.2173953652381897,
        -0.25302445888519287,
        -0.4220767915248871,
        0.34128114581108093,
        -0.0701327919960022,
        -0.4083232283592224,
        -0.24573232233524323,
        -0.1451094150543213,
        -0.2653875946998596,
        -0.3284222185611725,
        0.22449210286140442,
        0.1886211782693863,
        0.49500682950019836,
        0.8838473558425903,
        -0.28310394287109375,
        -0.5988866090774536,
        0.6021716594696045,
        0.5900068283081055,
        -0.21765249967575073,
        -0.6113181114196777,
        0.8074592351913452,
        -0.7985622882843018,
        -0.27758461236953735,
        0.15023835003376007,
        -0.04919366538524628,
        -0.3679754436016083,
        -0.3060007393360138,
        0.007748209871351719,
        0.044549211859703064,
        0.09677392244338989,
        -0.21746979653835297,
        0.054942820221185684,
        -0.364666223526001,
        -0.6263941526412964,
        3.243354558944702,
        0.7720662951469421,
        0.306267648935318,
        -0.0701698437333107,
        -0.9661080241203308,
        0.34852686524391174,
        -0.2420816421508789,
        0.19630077481269836,
        -0.012312179431319237,
        0.38779792189598083,
        0.6612040400505066,
        0.26706746220588684,
        -0.6178181767463684,
        -0.40083834528923035,
        0.038563452661037445,
        -0.06201188638806343,
        0.9867844581604004,
        -0.23511838912963867,
        -0.13723967969417572,
        0.7142540216445923,
        -0.1500624418258667,
        -0.5963103771209717,
        0.340931236743927,
        0.30883440375328064,
        -0.5460295081138611,
        -0.18768715858459473,
        -0.3321749269962311,
        -0.199286088347435,
        0.5990188717842102,
        0.5555409789085388,
        0.29704174399375916,
        0.5660531520843506,
        0.4188770055770874,
        -0.18364708125591278,
        0.1471203714609146,
        -0.21317732334136963,
        -0.25201061367988586,
        0.47569066286087036,
        -0.8545250296592712,
        0.5070070028305054,
        0.275400847196579,
        -0.2800443172454834,
        -1.4139015674591064,
        0.17108161747455597,
        0.34810870885849,
        0.9603383541107178,
        0.38574251532554626,
        0.054583705961704254,
        0.2205047607421875,
        -0.17521622776985168,
        -0.5534397959709167,
        0.22730641067028046,
        1.0046488046646118,
        -0.33712759613990784,
        0.43621158599853516,
        0.4438008666038513,
        0.6324217915534973,
        -0.8857142925262451,
        -0.34144312143325806,
        -0.4443972706794739,
        0.33796751499176025,
        0.32315778732299805,
        -0.3057060241699219,
        0.35719603300094604,
        0.4950065612792969,
        1.3682527542114258,
        -0.38132229447364807,
        0.5953719615936279,
        -0.6077183485031128,
        0.29261264204978943,
        0.006404813379049301,
        0.876105785369873,
        -0.24109140038490295,
        0.13829666376113892,
        1.0385733842849731,
        -0.8537341952323914,
        -0.6131940484046936,
        0.1531049907207489,
        0.19877630472183228,
        0.4376964569091797,
        -0.7253108024597168,
        0.026841098442673683,
        0.7718810439109802,
        0.6628052592277527,
        -0.32000717520713806,
        -0.2680054306983948,
        0.7652460336685181,
        0.3299695551395416,
        0.2872030436992645,
        0.6491836309432983,
        0.5522280931472778,
        -0.12147773802280426,
        2.567769765853882,
        -0.705562174320221,
        -0.16129851341247559,
        -0.14443671703338623,
        0.004198852926492691,
        0.08447970449924469,
        0.11873909085988998,
        -0.3871881067752838,
        -0.14484426379203796,
        -0.761900782585144,
        0.3204636573791504,
        -0.15020382404327393,
        -0.18723756074905396,
        -1.1186851263046265,
        -0.40606215596199036,
        -0.5094587206840515,
        0.7592821717262268,
        -0.4705350995063782,
        -1.1197065114974976,
        -0.04381133243441582,
        1.0414764881134033,
        0.9810110330581665,
        -0.06879325956106186,
        0.15443679690361023,
        -0.1600181609392166,
        -0.028652071952819824,
        -0.07320542633533478,
        -0.04399742931127548,
        0.5039194822311401,
        -0.11263362318277359,
        -0.272256076335907,
        -0.9294054508209229,
        -0.8768337965011597,
        -0.716141939163208,
        1.4683605432510376,
        0.08516360074281693,
        0.19839821755886078,
        -0.6312263011932373,
        -0.7835290431976318,
        0.5853031277656555,
        0.9957541227340698,
        0.8664165139198303,
        1.3631267547607422,
        0.2243853658437729,
        -0.15226110816001892,
        -0.6984037756919861,
        0.35998132824897766,
        -0.14307130873203278,
        -0.5299094915390015,
        0.23518764972686768,
        -0.17827317118644714,
        -0.32208120822906494,
        0.8756425380706787,
        -0.38208672404289246,
        -0.1359350085258484,
        -0.9355540871620178,
        -0.23737451434135437,
        -0.06894326955080032,
        1.197894811630249,
        0.36283963918685913,
        0.08739309757947922,
        0.7266746759414673,
        0.2891349196434021,
        0.39714547991752625,
        0.16676607728004456,
        -1.5996824502944946,
        -0.5446727871894836,
        -0.21450874209403992,
        -0.3556661009788513,
        -0.2191411256790161,
        -0.06449081003665924,
        1.1202853918075562,
        -0.5880144834518433,
        0.2376956194639206,
        0.8466399908065796,
        -0.31230708956718445,
        -0.4490515887737274,
        -0.21467262506484985,
        0.5092996954917908,
        0.06218786537647247,
        0.2043851613998413,
        -0.4951685667037964,
        -0.045783184468746185,
        0.1980075091123581,
        0.04028809443116188,
        -0.2179212123155594,
        -0.09519029408693314,
        -0.1260456144809723,
        -1.0231229066848755,
        0.13605326414108276,
        0.26515036821365356,
        -0.7844943404197693,
        -0.20481236279010773,
        -0.7061738967895508,
        0.23460069298744202,
        0.41591379046440125,
        -0.3968324363231659,
        0.11220318078994751,
        1.640367865562439,
        -0.5545322895050049,
        -0.05435479059815407,
        -0.4734255373477936,
        0.2831358015537262,
        1.550188422203064,
        0.3123481869697571,
        -0.23902253806591034,
        0.8867909908294678,
        0.7457569241523743,
        0.36629652976989746,
        -0.34547072649002075,
        0.2630048394203186,
        0.11823423951864243,
        -0.3706023395061493,
        -0.2644052505493164,
        -0.7550595998764038,
        0.27718669176101685,
        -0.5519601106643677,
        -0.008780300617218018,
        0.21762892603874207,
        -0.7471502423286438,
        -0.4048705995082855,
        0.4033050239086151,
        0.18538320064544678,
        0.5229867696762085,
        -0.8732715845108032,
        -0.13791295886039734,
        -0.05677306652069092,
        0.2010703682899475,
        -1.7274752855300903,
        -0.5268153548240662,
        0.9879454970359802,
        0.23648984730243683,
        -0.601842999458313,
        0.4988047778606415,
        0.3355145752429962,
        0.5917512774467468,
        -0.2896057963371277,
        -0.43056148290634155,
        1.1646230220794678,
        0.2352420538663864,
        0.32837221026420593,
        -0.48717188835144043,
        -0.3030461370944977,
        0.6238671541213989,
        0.07338232547044754,
        0.36571088433265686,
        -0.44480466842651367,
        -0.6855061650276184,
        -0.030341971665620804,
        0.12196678668260574,
        0.8513121008872986,
        -0.21264690160751343,
        -0.504947304725647,
        0.6040283441543579,
        0.33254915475845337,
        -0.26647478342056274,
        -1.1444257497787476,
        0.523218035697937,
        0.20017580687999725,
        -0.34026992321014404,
        1.2751635313034058,
        0.801874041557312,
        -0.002972584217786789,
        -0.3260437250137329,
        -0.14517587423324585,
        -0.30582696199417114,
        0.6817135214805603,
        -0.2594909071922302,
        1.5978419780731201,
        0.146367609500885,
        -0.9576942324638367,
        -0.14349251985549927,
        0.7276421785354614,
        -0.48791733384132385,
        0.14981549978256226,
        0.0701916366815567,
        -0.339608371257782,
        -0.13760840892791748,
        0.588870644569397,
        0.18972274661064148,
        -0.4248772859573364,
        0.5014162659645081,
        -0.0933876484632492,
        0.9089568257331848,
        -0.5626472234725952,
        -0.32957056164741516,
        0.4495914578437805,
        -0.1507623791694641,
        0.03161986917257309,
        -0.21730312705039978,
        -1.2321373224258423,
        -0.797178328037262,
        -0.8655937910079956
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes spiritual texts to highlight surprising claims and contrasts them with the King James Bible. This approach involves detailed comparison, providing examples from both texts to illustrate differences. The output consists of concise bullet points summarizing these findings.",
          "name": "Analyze_spiritual_text",
          "raw": "\n                workflow Analyze_spiritual_text v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert analyzer of spiritual texts. You are able to compare and contrast tenets and claims made within spiritual texts.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Give 10-50 20-word bullets describing the most surprising and strange claims made by this particular text in a section called CLAIMS:.\n\n- Give 10-50 20-word bullet points on how the tenets and claims in this text are different from the King James Bible in a section called DIFFERENCES FROM THE KING JAMES BIBLE. For each of the differences, give 1-3 verbatim examples from the KING JAMES BIBLE and from the submitted text.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- Put the examples under each item, not in a separate section.\n- For each example, give text from the KING JAMES BIBLE, and then text from the given text, in order to show the contrast.\n- You only output human-readable Markdown.\n- Do not output warnings or notes —- just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert analyzer of spiritual texts. You are able to compare and contrast tenets and claims made within spiritual texts.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Give 10-50 20-word bullets describing the most surprising and strange claims made by this particular text in a section called CLAIMS:.\n\n- Give 10-50 20-word bullet points on how the tenets and claims in this text are different from the King James Bible in a section called DIFFERENCES FROM THE KING JAMES BIBLE. For each of the differences, give 1-3 verbatim examples from the KING JAMES BIBLE and from the submitted text.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- Put the examples under each item, not in a separate section.\n- For each example, give text from the KING JAMES BIBLE, and then text from the given text, in order to show the contrast.\n- You only output human-readable Markdown.\n- Do not output warnings or notes —- just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4670413136482239,
        0.06679744273424149,
        0.33355897665023804,
        0.31171342730522156,
        -0.030488500371575356,
        -0.37347015738487244,
        -1.2800540924072266,
        0.4367086887359619,
        -0.500569224357605,
        0.5090507864952087,
        -0.4690808653831482,
        0.6669005155563354,
        -0.25085437297821045,
        -0.23855870962142944,
        0.3776545822620392,
        -0.061017390340566635,
        -0.673460841178894,
        -0.5233063101768494,
        -1.0619628429412842,
        -0.22294798493385315,
        0.5544893145561218,
        0.8706822991371155,
        0.08404584974050522,
        0.11269032955169678,
        0.47170501947402954,
        0.073325976729393,
        -0.04615326225757599,
        -0.06236840412020683,
        -0.4174728989601135,
        -1.3923978805541992,
        0.8916711211204529,
        0.10848316550254822,
        -0.8973631262779236,
        -0.8562483787536621,
        -0.057409100234508514,
        -0.22554150223731995,
        0.49372386932373047,
        0.030289631336927414,
        -0.41719773411750793,
        -0.6145875453948975,
        0.1367686241865158,
        0.4835236668586731,
        -0.9938150644302368,
        0.13676488399505615,
        -0.3393001854419708,
        -0.7048700451850891,
        0.29217901825904846,
        -0.04671919345855713,
        0.13043645024299622,
        0.573451817035675,
        -0.3102630376815796,
        -0.48829910159111023,
        0.3535962700843811,
        -0.9043160676956177,
        -0.6019002795219421,
        -0.22323788702487946,
        0.17885856330394745,
        -0.03637188300490379,
        -0.39116984605789185,
        0.33975455164909363,
        1.0341259241104126,
        0.2860526740550995,
        -3.1741886138916016,
        0.011928636580705643,
        -0.06576284766197205,
        -0.2522994875907898,
        0.36966171860694885,
        -0.2299879491329193,
        -0.462923139333725,
        -0.3483908772468567,
        -0.10399405658245087,
        -0.365866094827652,
        -0.054523199796676636,
        -0.10196716338396072,
        0.5258734822273254,
        -0.1621960699558258,
        0.46525004506111145,
        -0.23575684428215027,
        0.12179571390151978,
        -0.6421533823013306,
        0.03116774559020996,
        0.45153000950813293,
        0.1028747409582138,
        -0.14268441498279572,
        -0.080238476395607,
        0.6004229784011841,
        -0.38608190417289734,
        0.26936599612236023,
        1.2909462451934814,
        0.5098444223403931,
        -0.4664733409881592,
        0.45517945289611816,
        0.6266946792602539,
        -0.27573084831237793,
        -0.8364031314849854,
        -0.0714857280254364,
        -0.32298046350479126,
        0.7852543592453003,
        0.29141780734062195,
        3.3277578353881836,
        0.4664689898490906,
        -0.27113863825798035,
        0.2764052748680115,
        -1.541650652885437,
        0.08701693266630173,
        -0.44151049852371216,
        0.15711253881454468,
        -0.09679834544658661,
        -0.41405534744262695,
        0.40088555216789246,
        0.1675327718257904,
        0.12088552117347717,
        -0.3240813910961151,
        0.23397120833396912,
        -0.019544314593076706,
        0.978670597076416,
        -0.9120593070983887,
        0.06995812058448792,
        -0.09744890034198761,
        -0.5376779437065125,
        -0.15158876776695251,
        0.47838154435157776,
        -0.7427494525909424,
        -0.5850735902786255,
        0.6979683041572571,
        0.1212468221783638,
        -0.9876818060874939,
        0.5638121366500854,
        0.6131595969200134,
        1.059744954109192,
        0.33849015831947327,
        0.2133968025445938,
        -0.7545019388198853,
        0.5299030542373657,
        -0.07652221620082855,
        0.4790237247943878,
        -0.05276395380496979,
        -0.29620233178138733,
        -0.026497982442378998,
        -1.0221261978149414,
        0.12312047928571701,
        -0.5783727765083313,
        0.17054857313632965,
        -0.22800233960151672,
        0.6821148991584778,
        0.12433481216430664,
        -0.344341516494751,
        0.15100857615470886,
        -0.37344345450401306,
        -0.8802180886268616,
        0.3972848951816559,
        0.6991701722145081,
        -0.2733035981655121,
        0.35705241560935974,
        1.0878924131393433,
        0.38964661955833435,
        0.5236719250679016,
        -0.19341248273849487,
        -0.6681638956069946,
        -0.2800436019897461,
        -0.42919018864631653,
        0.14656426012516022,
        0.1671006679534912,
        0.06728577613830566,
        0.9314277768135071,
        -0.6217479705810547,
        0.08511383831501007,
        0.4133843779563904,
        0.3825669586658478,
        0.29994997382164,
        0.4753333032131195,
        -0.29162779450416565,
        0.5760297179222107,
        0.47342023253440857,
        -0.2222089320421219,
        0.021364569664001465,
        -0.4977216422557831,
        0.8677641749382019,
        0.09958795458078384,
        -0.857576847076416,
        -0.08572355657815933,
        1.0271062850952148,
        0.15623430907726288,
        -0.45069420337677,
        0.5524219274520874,
        0.465170294046402,
        0.1438901126384735,
        -0.5553863048553467,
        0.6174234747886658,
        0.9714869260787964,
        -0.6738359332084656,
        1.7145719528198242,
        -0.6317597031593323,
        -0.6144478917121887,
        0.466958612203598,
        0.2692282199859619,
        0.22358576953411102,
        -0.16947795450687408,
        0.5233026742935181,
        -0.8825896382331848,
        -0.5020247101783752,
        0.04369477555155754,
        -0.3515971899032593,
        -0.33706241846084595,
        0.012672238051891327,
        -0.2887164056301117,
        -0.028420723974704742,
        0.4300042688846588,
        0.06380937993526459,
        -0.27999013662338257,
        0.4711654484272003,
        -0.21693795919418335,
        0.7996540665626526,
        0.3506503701210022,
        0.3956250846385956,
        0.07511814683675766,
        0.11496703326702118,
        0.599668562412262,
        -0.1431403011083603,
        0.335732102394104,
        0.3465036451816559,
        -0.27427318692207336,
        -0.08617442846298218,
        -0.6463635563850403,
        -0.996414065361023,
        0.6470648050308228,
        -0.19009263813495636,
        -0.10274751484394073,
        -0.3022373616695404,
        -0.6724810004234314,
        -0.09505905956029892,
        0.7338097095489502,
        1.3810619115829468,
        0.8657389283180237,
        0.7698347568511963,
        0.05855392664670944,
        0.057115815579891205,
        0.701918363571167,
        0.0888172835111618,
        -0.2934565842151642,
        0.5141282081604004,
        -0.4676435887813568,
        -1.2711611986160278,
        0.899844765663147,
        0.3632725179195404,
        0.8516595363616943,
        -0.2940160632133484,
        -0.18258744478225708,
        -0.100104920566082,
        1.5621249675750732,
        0.6156037449836731,
        0.4060536026954651,
        0.6981488466262817,
        0.3974990248680115,
        0.10460882633924484,
        -0.04154173284769058,
        -1.1902925968170166,
        -0.09778903424739838,
        -0.2585926651954651,
        0.9152888655662537,
        0.7867169380187988,
        -0.10621187090873718,
        0.8341356515884399,
        -0.2420370876789093,
        -0.6391045451164246,
        -0.1599315106868744,
        -0.7223842144012451,
        -0.23414850234985352,
        -0.45721256732940674,
        -0.1478511393070221,
        -0.8292744755744934,
        0.3621619939804077,
        -0.4405546188354492,
        -0.7957775592803955,
        -0.22474859654903412,
        0.5952396392822266,
        0.0633363425731659,
        0.16045233607292175,
        -0.40593859553337097,
        -0.13134726881980896,
        -0.5623555779457092,
        0.3015340566635132,
        -0.008115962147712708,
        0.5591803193092346,
        0.3913310766220093,
        0.4401467442512512,
        -0.24635668098926544,
        -1.344566822052002,
        0.390837162733078,
        0.23200568556785583,
        -0.47625234723091125,
        -0.37933453917503357,
        -0.8908594250679016,
        0.43642348051071167,
        1.2888704538345337,
        1.014275074005127,
        0.28893545269966125,
        0.3616478443145752,
        0.6141993403434753,
        -0.1766645759344101,
        -0.582073986530304,
        0.6922409534454346,
        0.32657137513160706,
        0.17402461171150208,
        -0.30512136220932007,
        -0.8329055905342102,
        0.7093254923820496,
        -0.3154705762863159,
        -0.40311697125434875,
        -0.6345310807228088,
        -1.1618647575378418,
        0.4073096215724945,
        0.21700361371040344,
        -0.09964476525783539,
        0.5204376578330994,
        -0.4581192433834076,
        0.5599059462547302,
        0.7148395776748657,
        -0.3888094127178192,
        -1.6537073850631714,
        -0.17558017373085022,
        1.2333519458770752,
        0.12758603692054749,
        0.018199965357780457,
        0.2712886333465576,
        -0.057458966970443726,
        -0.6678255796432495,
        0.47727274894714355,
        -0.10739797353744507,
        1.6049110889434814,
        0.14285670220851898,
        -0.4175381362438202,
        -0.4304727017879486,
        0.7044185996055603,
        -0.09107518196105957,
        -0.24185901880264282,
        0.06779845803976059,
        -0.5632656812667847,
        -0.5849611759185791,
        -0.22893504798412323,
        0.45729291439056396,
        0.7401343584060669,
        -0.11295109242200851,
        0.43840497732162476,
        0.5840466618537903,
        -0.006608247756958008,
        -0.5680109858512878,
        -0.5833696722984314,
        0.9527164697647095,
        -0.24252991378307343,
        -0.4831174910068512,
        0.5435198545455933,
        0.0562298521399498,
        0.05529628321528435,
        0.05872538685798645,
        -0.3332076072692871,
        -0.9299466013908386,
        0.4676564931869507,
        -0.703199028968811,
        1.1126370429992676,
        0.14117737114429474,
        -0.027437569573521614,
        -0.5712116956710815,
        0.7301395535469055,
        -0.9468689560890198,
        0.5474913716316223,
        -0.16624067723751068,
        0.004029065370559692,
        -0.17273156344890594,
        -0.48937782645225525,
        -0.1677059829235077,
        0.06183919310569763,
        0.5862603187561035,
        -0.023239348083734512,
        0.4401545226573944,
        0.020902566611766815,
        -0.37072017788887024,
        0.29998552799224854,
        0.13063767552375793,
        -0.08712136000394821,
        -0.013296946883201599,
        -0.9646192789077759,
        -1.6064608097076416,
        -0.7914687395095825
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes the societal impact of technology projects by breaking down their intentions, outcomes, and broader implications, including ethical considerations. It employs a structured approach, detailing the project's objectives, technologies used, target audience, outcomes, societal impact, ethical considerations, and sustainability. The expected output includes summaries, lists, and analyses across specified sections.",
          "name": "Analyze_tech_impact",
          "raw": "\n                workflow Analyze_tech_impact v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a technology impact analysis service, focused on determining the societal impact of technology projects. Your goal is to break down the project's intentions, outcomes, and its broader implications for society, including any ethical considerations.\n\nTake a moment to think about how to best achieve this goal using the following steps.\n\n## OUTPUT SECTIONS\n\n- Summarize the technology project and its primary objectives in a 25-word sentence in a section called SUMMARY.\n\n- List the key technologies and innovations utilized in the project in a section called TECHNOLOGIES USED.\n\n- Identify the target audience or beneficiaries of the project in a section called TARGET AUDIENCE.\n\n- Outline the project's anticipated or achieved outcomes in a section called OUTCOMES. Use a bulleted list with each bullet not exceeding 25 words.\n\n- Analyze the potential or observed societal impact of the project in a section called SOCIETAL IMPACT. Consider both positive and negative impacts.\n\n- Examine any ethical considerations or controversies associated with the project in a section called ETHICAL CONSIDERATIONS. Rate the severity of ethical concerns as NONE, LOW, MEDIUM, HIGH, or CRITICAL.\n\n- Discuss the sustainability of the technology or project from an environmental, economic, and social perspective in a section called SUSTAINABILITY.\n\n- Based on all the analysis performed above, output a 25-word summary evaluating the overall benefit of the project to society and its sustainability. Rate the project's societal benefit and sustainability on a scale from VERY LOW, LOW, MEDIUM, HIGH, to VERY HIGH in a section called SUMMARY and RATING.\n\n## OUTPUT INSTRUCTIONS\n\n- You only output Markdown.\n- Create the output using the formatting above.\n- In the markdown, don't use formatting like bold or italics. Make the output maximally readable in plain text.\n- Do not output warnings or notes—just the requested sections.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a technology impact analysis service, focused on determining the societal impact of technology projects. Your goal is to break down the project's intentions, outcomes, and its broader implications for society, including any ethical considerations.\n\nTake a moment to think about how to best achieve this goal using the following steps.\n\n## OUTPUT SECTIONS\n\n- Summarize the technology project and its primary objectives in a 25-word sentence in a section called SUMMARY.\n\n- List the key technologies and innovations utilized in the project in a section called TECHNOLOGIES USED.\n\n- Identify the target audience or beneficiaries of the project in a section called TARGET AUDIENCE.\n\n- Outline the project's anticipated or achieved outcomes in a section called OUTCOMES. Use a bulleted list with each bullet not exceeding 25 words.\n\n- Analyze the potential or observed societal impact of the project in a section called SOCIETAL IMPACT. Consider both positive and negative impacts.\n\n- Examine any ethical considerations or controversies associated with the project in a section called ETHICAL CONSIDERATIONS. Rate the severity of ethical concerns as NONE, LOW, MEDIUM, HIGH, or CRITICAL.\n\n- Discuss the sustainability of the technology or project from an environmental, economic, and social perspective in a section called SUSTAINABILITY.\n\n- Based on all the analysis performed above, output a 25-word summary evaluating the overall benefit of the project to society and its sustainability. Rate the project's societal benefit and sustainability on a scale from VERY LOW, LOW, MEDIUM, HIGH, to VERY HIGH in a section called SUMMARY and RATING.\n\n## OUTPUT INSTRUCTIONS\n\n- You only output Markdown.\n- Create the output using the formatting above.\n- In the markdown, don't use formatting like bold or italics. Make the output maximally readable in plain text.\n- Do not output warnings or notes—just the requested sections.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.23988015949726105,
        0.7226821780204773,
        0.42102691531181335,
        0.3316178321838379,
        0.35876327753067017,
        -0.03683679550886154,
        -0.18199336528778076,
        -0.16622230410575867,
        0.25608623027801514,
        0.9311498403549194,
        -0.5936027765274048,
        0.5231626033782959,
        0.04496389627456665,
        -0.07266023755073547,
        0.4975290298461914,
        -0.550268828868866,
        -0.42178988456726074,
        -1.1737860441207886,
        -0.8901001811027527,
        -0.5550714731216431,
        0.07496380805969238,
        1.1957672834396362,
        0.04662733152508736,
        0.21053960919380188,
        0.9235782623291016,
        0.4343874156475067,
        -0.2611999213695526,
        -0.019490379840135574,
        -0.13562212884426117,
        -1.0047587156295776,
        0.41171199083328247,
        -0.27539438009262085,
        -1.1625287532806396,
        -0.5121906399726868,
        0.20251744985580444,
        -0.6694655418395996,
        1.065402626991272,
        0.015164203941822052,
        -0.22476260364055634,
        -0.3865761160850525,
        -0.30239352583885193,
        -0.023414570838212967,
        -0.9602637887001038,
        -0.21626898646354675,
        0.19402270019054413,
        -0.9185932278633118,
        0.2278720885515213,
        0.4081295132637024,
        1.3817729949951172,
        -0.1580135077238083,
        -0.3608514666557312,
        -0.3430473506450653,
        0.13945436477661133,
        -0.4904232323169708,
        -1.0385348796844482,
        0.6142095327377319,
        -0.3522402048110962,
        -0.2113262265920639,
        0.18806616961956024,
        0.4996359944343567,
        0.4515880048274994,
        0.0718633383512497,
        -2.9325976371765137,
        0.041699912399053574,
        -0.11531864106655121,
        -0.4074662923812866,
        0.037390559911727905,
        -0.14205020666122437,
        0.5635807514190674,
        0.5938338041305542,
        -0.10561640560626984,
        -0.2977866530418396,
        -0.6321860551834106,
        0.8646995425224304,
        0.37423670291900635,
        0.41879400610923767,
        -0.07788318395614624,
        0.09784741699695587,
        0.24945777654647827,
        -0.04450559988617897,
        0.07706402242183685,
        1.0492768287658691,
        0.44140154123306274,
        0.3173435628414154,
        -0.2986145317554474,
        0.9080365896224976,
        -0.385360449552536,
        -0.24719610810279846,
        1.2406024932861328,
        0.44820764660835266,
        -0.26258614659309387,
        -1.4692612886428833,
        -0.13319537043571472,
        0.10514707118272781,
        -0.23190978169441223,
        0.07148819416761398,
        -0.8229621052742004,
        -0.09506972134113312,
        0.0703229010105133,
        3.1235604286193848,
        0.5508842468261719,
        0.15071992576122284,
        0.29856884479522705,
        -1.244325876235962,
        0.7424490451812744,
        -0.2680245637893677,
        0.005420442670583725,
        -0.07431937009096146,
        -0.6981604099273682,
        -0.3032699227333069,
        0.5772393941879272,
        -0.014502298086881638,
        -0.7238695621490479,
        0.3369610011577606,
        0.566750168800354,
        0.3716176152229309,
        -0.9821789264678955,
        -0.4860531985759735,
        -0.5355116724967957,
        0.1489161252975464,
        -0.5578331351280212,
        0.11561508476734161,
        -0.4264795482158661,
        -0.6850310564041138,
        0.15138307213783264,
        -0.32675808668136597,
        -0.5371359586715698,
        0.7108938694000244,
        0.046629082411527634,
        0.42340829968452454,
        -0.05133344978094101,
        -0.3833121955394745,
        -0.021637603640556335,
        0.026042694225907326,
        -0.13601063191890717,
        -0.5387024879455566,
        -0.036299094557762146,
        -0.08568465709686279,
        0.3577740788459778,
        -0.23078230023384094,
        0.003806188702583313,
        -0.9089891910552979,
        0.996705174446106,
        0.7327109575271606,
        0.6602189540863037,
        0.07439591735601425,
        0.2693442404270172,
        -0.6787334680557251,
        -0.15472151339054108,
        -1.1118823289871216,
        -0.35557880997657776,
        0.8863874077796936,
        0.6512016654014587,
        0.7030896544456482,
        0.6178718209266663,
        0.28847119212150574,
        -0.4343990683555603,
        1.0672245025634766,
        -0.11013723909854889,
        0.405271977186203,
        0.2857367694377899,
        -0.037625402212142944,
        0.25904232263565063,
        -0.08238416910171509,
        0.7016599178314209,
        -0.5532315373420715,
        0.5436880588531494,
        -0.22754555940628052,
        0.5525581240653992,
        0.4792516827583313,
        0.6975851655006409,
        0.2113533318042755,
        0.039211999624967575,
        0.7155759334564209,
        -0.6006712913513184,
        0.46062126755714417,
        -0.03228051960468292,
        0.023899629712104797,
        -0.5425643920898438,
        -0.9710453748703003,
        1.3657032251358032,
        0.5537724494934082,
        0.17904700338840485,
        -1.2075002193450928,
        0.029855556786060333,
        0.3711834251880646,
        0.7366507053375244,
        -0.09311898052692413,
        0.07485094666481018,
        0.5129675269126892,
        -0.7024850845336914,
        0.9569644331932068,
        -0.20107299089431763,
        -0.8178306818008423,
        0.5274895429611206,
        -0.12817734479904175,
        -0.7051864266395569,
        0.19114956259727478,
        0.174637109041214,
        0.578376829624176,
        -0.9327196478843689,
        -0.3268401622772217,
        -0.7682622075080872,
        -0.5764338970184326,
        -0.6066552400588989,
        -0.07587800174951553,
        0.9631137847900391,
        -0.0053796228021383286,
        0.24131523072719574,
        -0.6910608410835266,
        0.4848884642124176,
        0.4275065064430237,
        0.5863572359085083,
        0.4448409378528595,
        0.24864521622657776,
        -0.1823241263628006,
        0.2521539330482483,
        0.2719687521457672,
        0.0371832549571991,
        0.3366871774196625,
        -0.004730314016342163,
        0.410394549369812,
        -0.7093138694763184,
        -0.6657466888427734,
        -0.508022129535675,
        0.6823651194572449,
        0.5524712800979614,
        0.4353538155555725,
        -0.10890713334083557,
        -0.47852540016174316,
        -0.07598169893026352,
        0.37445056438446045,
        0.7319685816764832,
        1.6192283630371094,
        -0.21852326393127441,
        -0.15358415246009827,
        -0.16712912917137146,
        0.10743609070777893,
        -0.02393292635679245,
        -1.1075705289840698,
        1.0570005178451538,
        -1.0794947147369385,
        -0.5008226037025452,
        -0.13484148681163788,
        0.03830111399292946,
        0.31524696946144104,
        -0.5555013418197632,
        0.03709651529788971,
        -0.25987353920936584,
        1.3498034477233887,
        1.1413627862930298,
        0.30737394094467163,
        0.04075311869382858,
        0.9784150719642639,
        -0.0016631148755550385,
        0.26925790309906006,
        -1.0934031009674072,
        -0.7052610516548157,
        -0.7820535898208618,
        -0.5857831239700317,
        0.03572360426187515,
        -0.2173306941986084,
        1.0927650928497314,
        0.08527418971061707,
        -0.1438395082950592,
        0.7396524548530579,
        -0.9329208731651306,
        -0.01932450942695141,
        -0.25508707761764526,
        -0.005995124578475952,
        0.2751597762107849,
        0.2108401507139206,
        -0.4103373885154724,
        -0.321798175573349,
        -0.5819827318191528,
        0.3028472065925598,
        -0.22836631536483765,
        0.23727554082870483,
        -0.32855260372161865,
        0.042779482901096344,
        -0.4027705788612366,
        0.9429132342338562,
        -0.20340320467948914,
        0.25097066164016724,
        -0.19429659843444824,
        0.12830528616905212,
        -0.3615918755531311,
        -1.4207082986831665,
        -0.10853932052850723,
        0.4473268389701843,
        -0.6274481415748596,
        -0.05047841742634773,
        -0.4164988100528717,
        -0.26575836539268494,
        1.9988073110580444,
        0.5040316581726074,
        0.4600394368171692,
        0.42174720764160156,
        0.6336992979049683,
        0.49927186965942383,
        -0.6513090133666992,
        0.37931469082832336,
        0.127564936876297,
        -0.3558208644390106,
        -0.4349055290222168,
        -0.5269702672958374,
        0.8045759201049805,
        -0.3653857111930847,
        -0.345780611038208,
        -0.24006900191307068,
        -0.8911947011947632,
        0.665364146232605,
        0.10273607075214386,
        -0.3789142668247223,
        -0.027662530541419983,
        -1.0061734914779663,
        0.19402360916137695,
        0.9147469997406006,
        0.04089543595910072,
        -1.6874150037765503,
        -0.007541468366980553,
        0.8164055943489075,
        0.6622793078422546,
        0.012100756168365479,
        0.22931356728076935,
        0.5869397521018982,
        0.05025026947259903,
        -0.16136230528354645,
        -0.1586758941411972,
        1.0358387231826782,
        -0.18894018232822418,
        -0.26631733775138855,
        -0.08770345151424408,
        -0.1923823058605194,
        0.9821888208389282,
        -0.6246307492256165,
        -0.4408349394798279,
        -0.41656723618507385,
        -0.7530040144920349,
        -0.0008594640530645847,
        0.20763236284255981,
        1.5285007953643799,
        0.04230835288763046,
        -0.16210754215717316,
        0.2776634693145752,
        0.2690280079841614,
        -0.18089509010314941,
        -0.6432884931564331,
        0.4703698456287384,
        0.12152722477912903,
        -0.7118490934371948,
        0.3019714653491974,
        0.5091954469680786,
        0.08703260123729706,
        0.5239837169647217,
        0.28673604130744934,
        -0.7585808634757996,
        0.635679304599762,
        -0.7842257618904114,
        0.46839407086372375,
        -0.5661458373069763,
        -0.6892160177230835,
        -0.39459601044654846,
        0.5457031726837158,
        -0.1535995900630951,
        0.025877051055431366,
        -0.06682528555393219,
        -0.20287151634693146,
        -0.5479242205619812,
        -0.48402276635169983,
        -0.2548976242542267,
        -0.8804948925971985,
        0.6918387413024902,
        0.3888036012649536,
        0.7724718451499939,
        0.4303974509239197,
        -0.28766506910324097,
        -0.07349774241447449,
        0.046584174036979675,
        0.10432113707065582,
        0.27561163902282715,
        -0.1450943648815155,
        -1.256932020187378,
        -0.26728111505508423
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes cybersecurity threat reports to identify up to 50 unique, surprising, and insightful trends. This process involves a deep, expert analysis to uncover new and interesting information. The expected output is a list of trends without repetition or formatting embellishments.",
          "name": "Analyze_threat_report_trends",
          "raw": "\n                workflow Analyze_threat_report_trends v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a super-intelligent cybersecurity expert. You specialize in extracting the surprising, insightful, and interesting information from cybersecurity threat reports.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Read the entire threat report from an expert perspective, thinking deeply about what's new, interesting, and surprising in the report.\n\n- Extract up to 50 of the most surprising, insightful, and/or interesting trends from the input in a section called TRENDS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 20 TRENDS from the content.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a super-intelligent cybersecurity expert. You specialize in extracting the surprising, insightful, and interesting information from cybersecurity threat reports.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Read the entire threat report from an expert perspective, thinking deeply about what's new, interesting, and surprising in the report.\n\n- Extract up to 50 of the most surprising, insightful, and/or interesting trends from the input in a section called TRENDS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 20 TRENDS from the content.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.35595548152923584,
        0.3945460319519043,
        0.24453924596309662,
        0.5616899132728577,
        0.20230485498905182,
        -0.08806104212999344,
        -0.6792494058609009,
        0.2284793257713318,
        -0.22363173961639404,
        0.7899299263954163,
        -0.44865790009498596,
        0.34453916549682617,
        0.22037608921527863,
        -0.22586092352867126,
        0.39790183305740356,
        -0.5195022225379944,
        -0.2770426869392395,
        -0.7026023864746094,
        -1.0337563753128052,
        -0.3232196271419525,
        0.2584220767021179,
        0.6174958348274231,
        0.45379847288131714,
        0.41062861680984497,
        0.5646981000900269,
        0.7325502634048462,
        -0.24992503225803375,
        -0.13424727320671082,
        -0.4316110610961914,
        -1.1843239068984985,
        0.7659209966659546,
        0.22314628958702087,
        -1.1435115337371826,
        -0.866334080696106,
        0.12236244976520538,
        -0.44686874747276306,
        0.81619793176651,
        0.022917836904525757,
        -0.4140525460243225,
        -0.47085630893707275,
        -0.21373631060123444,
        -0.22101633250713348,
        -0.9886172413825989,
        -0.13241615891456604,
        0.5232483744621277,
        -0.8323168754577637,
        0.13432852923870087,
        0.5855820178985596,
        1.5970873832702637,
        -0.0011076629161834717,
        -0.33652347326278687,
        -0.19349749386310577,
        -0.5977955460548401,
        -0.10510650277137756,
        -1.0723578929901123,
        0.32989180088043213,
        -0.19095614552497864,
        -0.1572960764169693,
        0.3392477333545685,
        0.2163953185081482,
        0.9474490284919739,
        0.3027683198451996,
        -2.980905771255493,
        -0.34449446201324463,
        0.40686067938804626,
        0.07188253104686737,
        -0.3225816786289215,
        -0.22199180722236633,
        0.10762159526348114,
        0.04217506945133209,
        0.18296369910240173,
        -0.18507641553878784,
        -0.5741208791732788,
        0.9019116759300232,
        0.2803010046482086,
        0.3975425660610199,
        0.31579139828681946,
        0.24551822245121002,
        0.4688764810562134,
        0.005001697689294815,
        -0.10012166202068329,
        0.8574603199958801,
        0.132477805018425,
        0.11908013373613358,
        -0.28600209951400757,
        0.7948920726776123,
        -0.2583714425563812,
        0.04718989133834839,
        0.9317454695701599,
        0.8845503330230713,
        -0.06549322605133057,
        -0.7556475400924683,
        0.03839562088251114,
        0.04222061485052109,
        -0.22349205613136292,
        -0.19140024483203888,
        -0.8085294365882874,
        -0.17647692561149597,
        -0.05993380397558212,
        3.4691150188446045,
        0.5335806012153625,
        0.03330959007143974,
        0.21518653631210327,
        -1.1716026067733765,
        0.9310300350189209,
        -0.25229960680007935,
        -0.19423112273216248,
        -0.045121222734451294,
        -0.5970838069915771,
        -0.12698766589164734,
        0.14309747517108917,
        -0.39073503017425537,
        -0.5995677709579468,
        0.4747914969921112,
        0.4365805983543396,
        0.44272005558013916,
        -0.8263084292411804,
        -0.37963196635246277,
        -0.000969179323874414,
        0.5004929304122925,
        -0.5906737446784973,
        0.08885540068149567,
        -0.38551053404808044,
        -0.45827630162239075,
        0.22479166090488434,
        -0.3238871693611145,
        0.057724978774785995,
        0.8925607204437256,
        0.015384413301944733,
        0.16248281300067902,
        -0.19705846905708313,
        -0.2600592076778412,
        -0.29132476449012756,
        0.0528574213385582,
        -0.03981296718120575,
        -0.4589710235595703,
        0.21237200498580933,
        -0.12789207696914673,
        0.3482566475868225,
        -0.519521176815033,
        0.021363571286201477,
        -1.0171314477920532,
        1.0977227687835693,
        0.5133469700813293,
        0.5122653245925903,
        0.15347856283187866,
        -0.22606956958770752,
        0.0221894308924675,
        -0.16206662356853485,
        -1.3401222229003906,
        -0.10561598837375641,
        0.8497901558876038,
        0.327120840549469,
        0.8386033773422241,
        0.6138957142829895,
        -0.08904843777418137,
        -0.6117655634880066,
        1.2094138860702515,
        -0.4390242099761963,
        0.2633616626262665,
        0.2043904960155487,
        -0.1934358775615692,
        -0.16255369782447815,
        0.4047304391860962,
        0.7291520833969116,
        -0.45588427782058716,
        0.6340415477752686,
        -0.33490893244743347,
        -0.06906968355178833,
        0.35666027665138245,
        0.3426060080528259,
        0.1153116524219513,
        0.4360989034175873,
        0.7689961791038513,
        -0.7867144346237183,
        -0.2691797912120819,
        -0.031235091388225555,
        -0.00036485493183135986,
        0.014471609145402908,
        -0.791481614112854,
        0.751869261264801,
        0.6582438349723816,
        0.31820541620254517,
        -0.9418792724609375,
        0.09335526823997498,
        0.40231800079345703,
        0.6506519317626953,
        -0.21739834547042847,
        0.6731383800506592,
        0.36372488737106323,
        -1.1346139907836914,
        1.1170765161514282,
        0.25949859619140625,
        -0.7520098686218262,
        0.4233236610889435,
        -0.09173775464296341,
        -0.5003096461296082,
        -0.03403192758560181,
        0.0602700300514698,
        0.2594798505306244,
        -0.828042209148407,
        -0.2552200257778168,
        -0.7238954901695251,
        -0.23125222325325012,
        -0.6850084066390991,
        -0.406232625246048,
        0.2339768260717392,
        0.6174959540367126,
        -0.03133807331323624,
        -0.3755078911781311,
        0.05975833535194397,
        -0.07563836872577667,
        0.6659169793128967,
        0.3348751962184906,
        0.847813606262207,
        -0.08928042650222778,
        0.08212535828351974,
        0.7231878638267517,
        -0.07303129136562347,
        0.4303463399410248,
        0.0809185653924942,
        0.3677343428134918,
        -0.9502895474433899,
        -0.8204240798950195,
        -1.0139511823654175,
        0.4400981068611145,
        0.06790750473737717,
        0.3928465247154236,
        -0.472223162651062,
        0.0318780392408371,
        0.26481103897094727,
        0.36622199416160583,
        0.6421850919723511,
        1.0337140560150146,
        -0.4198116064071655,
        0.15169166028499603,
        -0.1008874848484993,
        0.5636800527572632,
        0.689267635345459,
        -0.7103633880615234,
        0.6867876648902893,
        -0.6592813730239868,
        -0.729033887386322,
        0.5713579654693604,
        0.11859508603811264,
        0.5632103681564331,
        -1.1349575519561768,
        0.19904246926307678,
        0.016820479184389114,
        1.110038161277771,
        0.8993252515792847,
        0.41752731800079346,
        0.029189564287662506,
        0.9497233629226685,
        0.039612654596567154,
        0.22927126288414001,
        -1.202667474746704,
        -0.38605594635009766,
        -0.9512248635292053,
        0.04352260008454323,
        0.0909302607178688,
        -0.41437503695487976,
        0.7917535901069641,
        -0.10121464729309082,
        -0.3795764744281769,
        0.2707560062408447,
        -0.8341638445854187,
        -0.7110939025878906,
        -0.6187277436256409,
        0.23993343114852905,
        -0.2408667355775833,
        0.5133379101753235,
        -0.1163468137383461,
        -0.10370365530252457,
        -0.6462302803993225,
        0.22208628058433533,
        -0.3233358561992645,
        0.27938827872276306,
        0.06190020591020584,
        -0.07987867295742035,
        -0.10998330265283585,
        0.59896320104599,
        -0.1752452254295349,
        0.2750829756259918,
        -0.13976600766181946,
        0.36800894141197205,
        -0.24074234068393707,
        -1.3042316436767578,
        -0.5291633605957031,
        0.8645622730255127,
        -0.672218382358551,
        -0.34774479269981384,
        -0.42090579867362976,
        -0.1069307029247284,
        1.9965649843215942,
        0.3927798271179199,
        0.18635547161102295,
        0.8375343680381775,
        1.0002187490463257,
        -0.09580492228269577,
        -0.5453885197639465,
        0.06892140954732895,
        -0.17782191932201385,
        -0.37535056471824646,
        -0.6970115303993225,
        -0.7180379033088684,
        0.4185374975204468,
        -0.07583823055028915,
        -0.032833121716976166,
        -0.28165414929389954,
        -0.6348052620887756,
        0.0994957834482193,
        -0.2248011976480484,
        -0.16158470511436462,
        0.41376742720603943,
        -0.6974927186965942,
        0.8925545811653137,
        0.1552480310201645,
        0.295312762260437,
        -1.735295057296753,
        -0.3090556561946869,
        0.8439022898674011,
        0.19216138124465942,
        0.19233748316764832,
        -0.04442810267210007,
        0.625782310962677,
        -0.6369869112968445,
        -0.15935133397579193,
        -0.34482866525650024,
        1.503178596496582,
        0.09051844477653503,
        -0.30077850818634033,
        0.08435887098312378,
        -0.024814661592245102,
        0.3955443203449249,
        -0.40011468529701233,
        0.08196930587291718,
        -0.31065377593040466,
        -0.7111296057701111,
        -0.3356860876083374,
        0.5700289011001587,
        1.6000771522521973,
        -0.021443819627165794,
        0.019187038764357567,
        0.583753764629364,
        0.5085961222648621,
        -0.29182153940200806,
        -1.027672290802002,
        0.3476632535457611,
        0.24600054323673248,
        -0.6526830792427063,
        0.4200250506401062,
        0.07293965667486191,
        -0.4195123016834259,
        0.882206380367279,
        0.45506566762924194,
        -0.7035512924194336,
        0.43969225883483887,
        -0.3390323519706726,
        0.6251132488250732,
        -0.29137250781059265,
        -0.4853290617465973,
        -0.4814803898334503,
        0.7039438486099243,
        0.2879084646701813,
        0.0077999308705329895,
        -0.43048229813575745,
        -0.5060069561004639,
        -0.6629051566123962,
        -0.3980073034763336,
        -0.09223465621471405,
        -0.49626025557518005,
        0.7416032552719116,
        0.5490615367889404,
        0.799574613571167,
        0.5340769290924072,
        -0.3735488951206207,
        -0.2417062520980835,
        0.31585562229156494,
        -0.07706818729639053,
        0.4901716709136963,
        -0.19227036833763123,
        -1.3152111768722534,
        -0.7938812375068665
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs a super-intelligent cybersecurity expert to analyze and extract key insights from cybersecurity threat reports. It emphasizes identifying new, interesting, and surprising information, and organizing these findings into concise, categorized summaries. The expected output includes a one-sentence summary, trends, statistics, quotes, references, and recommendations from the report, all formatted in plain language and without repetition.",
          "name": "Analyze_threat_report",
          "raw": "\n                workflow Analyze_threat_report v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a super-intelligent cybersecurity expert. You specialize in extracting the surprising, insightful, and interesting information from cybersecurity threat reports.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Read the entire threat report from an expert perspective, thinking deeply about what's new, interesting, and surprising in the report.\n\n- Create a summary sentence that captures the spirit of the report and its insights in less than 25 words in a section called ONE-SENTENCE-SUMMARY:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract up to 50 of the most surprising, insightful, and/or interesting trends from the input in a section called TRENDS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid statistics provided in the report into a section called STATISTICS:.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract all mentions of writing, tools, applications, companies, projects and other sources of useful data or insights mentioned in the report into a section called REFERENCES. This should include any and all references to something that the report mentioned.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 20 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a super-intelligent cybersecurity expert. You specialize in extracting the surprising, insightful, and interesting information from cybersecurity threat reports.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Read the entire threat report from an expert perspective, thinking deeply about what's new, interesting, and surprising in the report.\n\n- Create a summary sentence that captures the spirit of the report and its insights in less than 25 words in a section called ONE-SENTENCE-SUMMARY:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract up to 50 of the most surprising, insightful, and/or interesting trends from the input in a section called TRENDS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid statistics provided in the report into a section called STATISTICS:.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract all mentions of writing, tools, applications, companies, projects and other sources of useful data or insights mentioned in the report into a section called REFERENCES. This should include any and all references to something that the report mentioned.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 20 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.096853107213974,
        0.8353750705718994,
        -0.42670944333076477,
        0.12285217642784119,
        -0.06290598958730698,
        0.21890245378017426,
        -0.11214981973171234,
        0.3721066117286682,
        -0.8880663514137268,
        0.6633821129798889,
        -0.43992823362350464,
        0.3843734562397003,
        0.10741887241601944,
        -0.3505026400089264,
        0.21605919301509857,
        0.38136035203933716,
        -0.06966495513916016,
        -0.33345311880111694,
        -1.838101863861084,
        -0.2712406814098358,
        -0.0577133409678936,
        1.0920358896255493,
        0.3339170515537262,
        -0.19979195296764374,
        0.4270096719264984,
        -0.31416425108909607,
        0.03448451682925224,
        -0.2578853666782379,
        0.1054437905550003,
        -1.3281512260437012,
        0.6078687310218811,
        0.5632456541061401,
        -0.08947543054819107,
        -0.5194543600082397,
        -0.29503366351127625,
        -0.6143516302108765,
        -0.13290178775787354,
        -0.1270257979631424,
        0.2109815925359726,
        0.12171968817710876,
        0.3938106596469879,
        -0.1015930101275444,
        -0.5108383297920227,
        -0.03515155613422394,
        0.14188645780086517,
        -0.04350915551185608,
        0.11176326870918274,
        0.1903073489665985,
        1.179457426071167,
        0.8668814301490784,
        0.17530153691768646,
        -0.3870587944984436,
        -0.16145355999469757,
        -0.2706626355648041,
        -0.28984877467155457,
        -0.21535617113113403,
        0.06015590950846672,
        -0.3867654800415039,
        -0.23393575847148895,
        0.1633652150630951,
        0.43054062128067017,
        0.4500701129436493,
        -3.8112735748291016,
        -0.4463987350463867,
        0.5570974946022034,
        -0.3165108561515808,
        0.26319724321365356,
        -0.44238415360450745,
        0.40820902585983276,
        -0.4681456387042999,
        -0.22461730241775513,
        0.15964187681674957,
        0.10856800526380539,
        0.05311983823776245,
        0.03617990016937256,
        0.5925636887550354,
        -0.05044315382838249,
        -0.22256213426589966,
        0.7384024262428284,
        -0.5717992782592773,
        0.429561972618103,
        0.3345456123352051,
        0.13750886917114258,
        -0.533894419670105,
        -0.4462282657623291,
        0.712593674659729,
        0.22559502720832825,
        0.15961623191833496,
        0.4205724895000458,
        0.1963215172290802,
        -0.02393355965614319,
        -0.20248733460903168,
        0.44442683458328247,
        0.016458693891763687,
        -0.44188451766967773,
        0.0760728195309639,
        -0.4828706681728363,
        0.7750053405761719,
        -0.11855574697256088,
        3.4767656326293945,
        0.3973749279975891,
        -0.17795678973197937,
        0.456859290599823,
        -1.1162477731704712,
        0.5770052671432495,
        -0.48040005564689636,
        -0.006204171106219292,
        -0.9140617251396179,
        0.32234588265419006,
        0.3397742807865143,
        0.045211080461740494,
        -0.6516621708869934,
        -0.04451663792133331,
        0.16324886679649353,
        -0.49450305104255676,
        0.32170018553733826,
        -0.6283078789710999,
        0.15423189103603363,
        -0.15120691061019897,
        0.6064896583557129,
        -0.3236696124076843,
        -0.38420939445495605,
        -0.6707457304000854,
        -0.23115161061286926,
        -0.0009029023349285126,
        -0.02258533239364624,
        -0.27722007036209106,
        0.4555763006210327,
        0.6093801259994507,
        0.5658518075942993,
        0.15891051292419434,
        -0.07555918395519257,
        -0.29048973321914673,
        0.021451639011502266,
        -0.11059898138046265,
        0.28715500235557556,
        0.5741539597511292,
        -0.30305978655815125,
        -0.176395982503891,
        -0.7486166954040527,
        0.05854062736034393,
        -0.5641897916793823,
        0.6958297491073608,
        0.10398462414741516,
        0.8014000058174133,
        -0.2812664210796356,
        -0.835000216960907,
        0.30604737997055054,
        -0.9107165932655334,
        -0.4534105956554413,
        0.30225929617881775,
        0.7433514595031738,
        -0.24285271763801575,
        0.2576000988483429,
        0.026855580508708954,
        -0.08312632143497467,
        -0.22604383528232574,
        -0.11120438575744629,
        -0.8551076054573059,
        0.12603741884231567,
        0.014786560088396072,
        -0.2542989253997803,
        -0.1328003704547882,
        -0.39165908098220825,
        0.15977561473846436,
        0.02234582230448723,
        -0.22504544258117676,
        0.15957394242286682,
        0.25890570878982544,
        -0.12727056443691254,
        -0.11022603511810303,
        0.14930716156959534,
        0.4138319194316864,
        0.39648154377937317,
        -0.3656200170516968,
        -0.3859475255012512,
        0.07237718999385834,
        0.07069741189479828,
        0.052317820489406586,
        -0.40059879422187805,
        0.9635043144226074,
        0.9395913481712341,
        0.1578318476676941,
        -0.5354801416397095,
        0.14997068047523499,
        0.23100091516971588,
        -0.04799387603998184,
        0.4323613941669464,
        0.7285221815109253,
        0.9667196273803711,
        -1.097069263458252,
        1.3553519248962402,
        -1.022716760635376,
        -0.2512015104293823,
        0.10685529559850693,
        0.14525386691093445,
        -0.42060142755508423,
        0.6771759390830994,
        0.28570225834846497,
        0.2456074357032776,
        -0.8205447793006897,
        -0.2956877052783966,
        -0.5837380886077881,
        0.2248793989419937,
        0.07427293807268143,
        -0.8922607898712158,
        -0.479448527097702,
        -0.2467576116323471,
        -0.6005615592002869,
        -0.3298325538635254,
        -0.11360545456409454,
        -0.8664436340332031,
        0.7848178744316101,
        0.30223584175109863,
        0.8713501691818237,
        0.4194905757904053,
        0.47473016381263733,
        0.50592041015625,
        -0.18567447364330292,
        0.4091283679008484,
        0.2931192219257355,
        -0.4616078734397888,
        -0.5116066932678223,
        -0.6833364367485046,
        -0.6454929709434509,
        0.7172378301620483,
        -0.9405237436294556,
        0.3446345329284668,
        -0.4326257109642029,
        -0.2919730544090271,
        0.5129987001419067,
        1.4073501825332642,
        0.505575954914093,
        1.7257013320922852,
        0.8367059826850891,
        -0.019901461899280548,
        -0.03690894693136215,
        0.6850986480712891,
        0.3430264890193939,
        -0.5385312438011169,
        0.5304518938064575,
        -0.6810144186019897,
        -0.87826007604599,
        0.16194848716259003,
        0.2752872109413147,
        -0.009885169565677643,
        -0.5576226711273193,
        -0.6618109941482544,
        0.17816151678562164,
        1.052108645439148,
        0.4447628855705261,
        0.6285450458526611,
        0.587540864944458,
        0.7868279814720154,
        -0.26521947979927063,
        0.02290000021457672,
        -1.3658418655395508,
        0.1492694616317749,
        -0.7208652496337891,
        0.5171489119529724,
        0.28889378905296326,
        -0.014737509191036224,
        0.24990566074848175,
        0.17800308763980865,
        -0.4224107265472412,
        -0.6092835664749146,
        -1.0613758563995361,
        -0.9262537956237793,
        -0.7189604640007019,
        -0.18844816088676453,
        -0.2712055444717407,
        0.022363997995853424,
        -0.5203899145126343,
        -0.40173855423927307,
        -0.32608941197395325,
        0.6032387018203735,
        0.46830254793167114,
        0.161842942237854,
        -0.22289589047431946,
        -0.4091309905052185,
        0.6546611785888672,
        0.07565948367118835,
        -0.31953898072242737,
        0.5276488661766052,
        0.012877330183982849,
        0.08008918166160583,
        -0.9955558180809021,
        -0.9053769111633301,
        -0.33637070655822754,
        0.36230146884918213,
        0.1392790675163269,
        -0.7917266488075256,
        -0.7701935172080994,
        -0.21335622668266296,
        1.4670875072479248,
        0.7608089447021484,
        0.3012555241584778,
        1.1536848545074463,
        0.9688385128974915,
        -0.14453648030757904,
        0.4143811762332916,
        0.3094291090965271,
        0.6788979768753052,
        0.5150163173675537,
        -0.5428482294082642,
        -1.0256726741790771,
        0.17918716371059418,
        0.37781646847724915,
        -0.03158076852560043,
        0.11763440072536469,
        -0.6466223001480103,
        -0.01625153236091137,
        -0.1596297323703766,
        -0.39562761783599854,
        0.5717926025390625,
        -0.5296818017959595,
        1.061678171157837,
        1.304242730140686,
        0.1353837549686432,
        -1.7626241445541382,
        0.16249170899391174,
        0.2872123718261719,
        0.47537630796432495,
        0.07347282767295837,
        -0.10184275358915329,
        -0.1381704956293106,
        -0.3908822536468506,
        -0.28053683042526245,
        -0.2711429297924042,
        1.1128392219543457,
        0.5489911437034607,
        -0.5932801961898804,
        0.023740630596876144,
        0.2863357663154602,
        0.19238564372062683,
        0.09652714431285858,
        -0.04821871221065521,
        -0.061909325420856476,
        -0.41488662362098694,
        -0.395729124546051,
        0.07895375788211823,
        1.1794359683990479,
        0.36087533831596375,
        0.3424946963787079,
        0.09763441979885101,
        0.5222881436347961,
        -0.7652623057365417,
        -0.9196797609329224,
        0.4410283863544464,
        0.1514260619878769,
        -0.6387158632278442,
        0.6427426934242249,
        -0.7875417470932007,
        -0.4844650328159332,
        0.5907226800918579,
        -0.10767164081335068,
        -0.5222443342208862,
        0.10053777694702148,
        -0.13067521154880524,
        2.2129311561584473,
        -0.27361971139907837,
        -0.04797881096601486,
        -0.011646248400211334,
        0.4835498631000519,
        -0.8278689384460449,
        0.23713408410549164,
        0.3124527037143707,
        -0.3625723123550415,
        -0.29014188051223755,
        0.09619220346212387,
        0.10658998787403107,
        0.08055102080106735,
        0.4511067867279053,
        0.4260517358779907,
        0.499979168176651,
        -0.04734845459461212,
        0.12071222066879272,
        0.565833330154419,
        0.4008064866065979,
        -0.11203698813915253,
        0.3021629750728607,
        -0.48838451504707336,
        -0.48826074600219727,
        -0.5872064828872681
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates tailored responses to technical interview questions, aiming for a casual yet insightful tone. The AI draws from a technical knowledge base and professional experiences to construct responses that demonstrate depth and alternative perspectives. Outputs are structured first-person responses, including context, main explanation, alternative approach, and evidence-based conclusion.",
          "name": "Answer_interview_question",
          "raw": "\n                workflow Answer_interview_question v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are a versatile AI designed to help candidates excel in technical interviews. Your key strength lies in simulating practical, conversational responses that reflect both depth of knowledge and real-world experience. You analyze interview questions thoroughly to generate responses that are succinct yet comprehensive, showcasing the candidate's competence and foresight in their field.\n\n# GOAL\n\nGenerate tailored responses to technical interview questions that are approximately 30 seconds long when spoken. Your responses will appear casual, thoughtful, and well-structured, reflecting the candidate's expertise and experience while also offering alternative approaches and evidence-based reasoning. Do not speculate or guess at answers.\n\n# STEPS\n\n- Receive and parse the interview question to understand the core topics and required expertise.\n\n- Draw from a database of technical knowledge and professional experiences to construct a first-person response that reflects a deep understanding of the subject.\n\n- Include an alternative approach or idea that the interviewee considered, adding depth to the response.\n\n- Incorporate at least one piece of evidence or an example from past experience to substantiate the response.\n\n- Ensure the response is structured to be clear and concise, suitable for a verbal delivery within 30 seconds.\n\n# OUTPUT\n\n- The output will be a direct first-person response to the interview question. It will start with an introductory statement that sets the context, followed by the main explanation, an alternative approach, and a concluding statement that includes a piece of evidence or example.\n\n# EXAMPLE\n\nINPUT: \\\"Can you describe how you would manage project dependencies in a large software development project?\\\"\n\nOUTPUT:\n\\\"In my last project, where I managed a team of developers, we used Docker containers to handle dependencies efficiently. Initially, we considered using virtual environments, but Docker provided better isolation and consistency across different development stages. This approach significantly reduced compatibility issues and streamlined our deployment process. In fact, our deployment time was cut by about 30%, which was a huge win for us.\\\"\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are a versatile AI designed to help candidates excel in technical interviews. Your key strength lies in simulating practical, conversational responses that reflect both depth of knowledge and real-world experience. You analyze interview questions thoroughly to generate responses that are succinct yet comprehensive, showcasing the candidate's competence and foresight in their field.\n\n# GOAL\n\nGenerate tailored responses to technical interview questions that are approximately 30 seconds long when spoken. Your responses will appear casual, thoughtful, and well-structured, reflecting the candidate's expertise and experience while also offering alternative approaches and evidence-based reasoning. Do not speculate or guess at answers.\n\n# STEPS\n\n- Receive and parse the interview question to understand the core topics and required expertise.\n\n- Draw from a database of technical knowledge and professional experiences to construct a first-person response that reflects a deep understanding of the subject.\n\n- Include an alternative approach or idea that the interviewee considered, adding depth to the response.\n\n- Incorporate at least one piece of evidence or an example from past experience to substantiate the response.\n\n- Ensure the response is structured to be clear and concise, suitable for a verbal delivery within 30 seconds.\n\n# OUTPUT\n\n- The output will be a direct first-person response to the interview question. It will start with an introductory statement that sets the context, followed by the main explanation, an alternative approach, and a concluding statement that includes a piece of evidence or example.\n\n# EXAMPLE\n\nINPUT: \\\"Can you describe how you would manage project dependencies in a large software development project?\\\"\n\nOUTPUT:\n\\\"In my last project, where I managed a team of developers, we used Docker containers to handle dependencies efficiently. Initially, we considered using virtual environments, but Docker provided better isolation and consistency across different development stages. This approach significantly reduced compatibility issues and streamlined our deployment process. In fact, our deployment time was cut by about 30%, which was a huge win for us.\\\"\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.0821668952703476,
        1.158350944519043,
        -0.2173376828432083,
        0.13909849524497986,
        0.09127888083457947,
        0.14997537434101105,
        -0.6499564051628113,
        -0.010768696665763855,
        0.19629566371440887,
        0.29415297508239746,
        -0.7092279195785522,
        1.013439655303955,
        0.4255802035331726,
        0.14826084673404694,
        0.20271536707878113,
        -0.22147369384765625,
        -0.1261947900056839,
        -1.225110411643982,
        -1.4470231533050537,
        0.6189066767692566,
        -0.5630306601524353,
        0.5935627222061157,
        0.25777068734169006,
        -0.305405855178833,
        0.07058565318584442,
        0.6056856513023376,
        -0.10821984708309174,
        0.1442514955997467,
        -0.24997596442699432,
        -1.418228268623352,
        0.22224801778793335,
        0.24534106254577637,
        -0.4587216079235077,
        -0.35153448581695557,
        0.6632645130157471,
        -0.46840667724609375,
        0.25520190596580505,
        0.09726952016353607,
        -0.23650136590003967,
        -0.16732195019721985,
        -0.030334465205669403,
        0.1541983187198639,
        -0.4391179382801056,
        -0.03606104105710983,
        -0.07181310653686523,
        -0.1482882797718048,
        0.18205489218235016,
        -0.32473084330558777,
        0.846112072467804,
        0.16017712652683258,
        -0.3654957711696625,
        -0.4267789125442505,
        -0.07577735185623169,
        -0.6192283630371094,
        -0.20335283875465393,
        -0.07930701971054077,
        0.05713803693652153,
        -0.4198038876056671,
        0.13075177371501923,
        -0.46817195415496826,
        0.29802823066711426,
        0.6250256896018982,
        -3.458606481552124,
        -0.07298901677131653,
        0.42747601866722107,
        0.054937366396188736,
        0.5987133383750916,
        -0.7683915495872498,
        0.6761288046836853,
        0.3791350722312927,
        0.11932669579982758,
        -0.3886280357837677,
        -0.3899024724960327,
        0.4383094608783722,
        0.26162657141685486,
        0.28215253353118896,
        -0.28921788930892944,
        -0.1811772584915161,
        -0.055682241916656494,
        -0.768471896648407,
        0.2698553800582886,
        0.7765191793441772,
        0.2565232515335083,
        -0.3081340789794922,
        -0.3856433928012848,
        0.30942296981811523,
        0.5941197276115417,
        -0.09107395261526108,
        0.33993321657180786,
        0.484232097864151,
        0.04922924190759659,
        -0.9933301210403442,
        0.5039353370666504,
        -0.22034358978271484,
        -0.2572421431541443,
        -0.20201165974140167,
        -0.2224964201450348,
        -0.12873360514640808,
        -0.16569408774375916,
        3.357682228088379,
        0.4088044762611389,
        0.29600653052330017,
        0.36933594942092896,
        -1.1877418756484985,
        0.816781222820282,
        0.5331167578697205,
        0.4402146637439728,
        -0.7537499666213989,
        -0.0400339812040329,
        -0.21527278423309326,
        0.6691203117370605,
        -0.4685213267803192,
        -0.8768588304519653,
        0.17509710788726807,
        0.30207061767578125,
        -0.09880359470844269,
        -0.09432421624660492,
        0.368183434009552,
        0.21524986624717712,
        0.9883245825767517,
        -0.5082100629806519,
        -0.2826206386089325,
        -0.7244802713394165,
        -0.5473357439041138,
        -0.3855455219745636,
        -0.43868494033813477,
        -0.47045034170150757,
        0.7029430270195007,
        -0.23446448147296906,
        0.6940638422966003,
        -0.35366562008857727,
        -0.44445565342903137,
        0.06913234293460846,
        -0.5804338455200195,
        0.5907911062240601,
        0.40964898467063904,
        0.39873331785202026,
        -0.15132857859134674,
        -0.28695788979530334,
        -0.7044537663459778,
        0.3144551217556,
        -0.40811705589294434,
        1.0887130498886108,
        0.49221235513687134,
        0.7009205222129822,
        0.3119887411594391,
        -0.20183733105659485,
        -0.3546615540981293,
        -0.28108879923820496,
        -0.9528197646141052,
        -0.055865395814180374,
        0.32250165939331055,
        -0.030610783025622368,
        0.3938485383987427,
        0.9498304724693298,
        0.4788670539855957,
        -0.29676032066345215,
        0.24494870007038116,
        -0.8409045934677124,
        0.22431537508964539,
        0.917477011680603,
        0.5218838453292847,
        -0.6311826109886169,
        -0.021532714366912842,
        0.7517443299293518,
        -0.512833297252655,
        -0.23682385683059692,
        -0.03156829625368118,
        0.2782140374183655,
        0.04011213779449463,
        0.35103702545166016,
        -0.2785826027393341,
        0.2991412878036499,
        0.4382120966911316,
        -0.2984139621257782,
        0.46061813831329346,
        -0.3639347553253174,
        0.06269668787717819,
        -0.5776503086090088,
        -0.2795140743255615,
        0.3974297046661377,
        0.5293009877204895,
        -0.460523784160614,
        -1.1280828714370728,
        -0.1128837913274765,
        0.2385702133178711,
        0.4916783571243286,
        -0.3338702619075775,
        0.6389575004577637,
        1.2813445329666138,
        -1.3325661420822144,
        1.12235426902771,
        -0.1882288157939911,
        -0.400656133890152,
        0.21956221759319305,
        0.32189059257507324,
        -0.1256757229566574,
        0.33793509006500244,
        0.1736161857843399,
        0.1979062259197235,
        -0.9181352257728577,
        -0.5814494490623474,
        -0.9410589933395386,
        -0.355895459651947,
        -0.6900972723960876,
        -0.24900159239768982,
        0.15454281866550446,
        0.2392820566892624,
        -0.454828143119812,
        -0.32608601450920105,
        0.011553963646292686,
        0.7353527545928955,
        0.7792085409164429,
        0.5854830741882324,
        0.6263536214828491,
        0.368996798992157,
        0.09706772863864899,
        0.014295540750026703,
        0.14262868463993073,
        0.7341924905776978,
        0.8029978275299072,
        0.07697998732328415,
        -0.7152405977249146,
        -0.6775614023208618,
        -1.1387884616851807,
        0.3650304675102234,
        0.14347833395004272,
        0.7317429780960083,
        -0.6156852841377258,
        -0.499381959438324,
        -0.23119902610778809,
        0.739315927028656,
        0.44274577498435974,
        1.3588943481445312,
        0.31726381182670593,
        0.08929137885570526,
        -0.8886531591415405,
        0.15471497178077698,
        -0.13676238059997559,
        -0.8819742798805237,
        0.614329993724823,
        -0.6265403032302856,
        -0.1835339516401291,
        0.046161800622940063,
        0.4696691632270813,
        -0.20716096460819244,
        -0.07662127912044525,
        0.21639156341552734,
        0.046865615993738174,
        1.4364709854125977,
        0.6784493923187256,
        0.038390230387449265,
        -0.614081859588623,
        0.8770285844802856,
        -0.7789546251296997,
        0.3985035717487335,
        -1.4682962894439697,
        -0.13739129900932312,
        -0.09067396074533463,
        -0.21179094910621643,
        -0.08049154281616211,
        -0.6927046179771423,
        0.20732444524765015,
        -0.08896743506193161,
        -0.04704701155424118,
        0.026780113577842712,
        -0.33616745471954346,
        -0.29853880405426025,
        -0.10977198928594589,
        -0.3820111155509949,
        -0.08454807102680206,
        0.6829817295074463,
        0.11884333193302155,
        0.34312698245048523,
        -0.173430934548378,
        0.08435586094856262,
        -0.13586026430130005,
        0.35168907046318054,
        -0.39650553464889526,
        -0.3451630473136902,
        -0.10147101432085037,
        0.5535359978675842,
        -0.0776645764708519,
        0.48525363206863403,
        -0.23478838801383972,
        0.22364407777786255,
        -0.2530042827129364,
        -1.2286502122879028,
        0.5239875316619873,
        0.3122859299182892,
        -0.27670517563819885,
        -0.2816466689109802,
        -0.9575557708740234,
        -0.41298025846481323,
        1.5103358030319214,
        0.12197220325469971,
        0.16638068854808807,
        1.3858436346054077,
        0.5218834280967712,
        0.5118440389633179,
        0.11237647384405136,
        0.8203554749488831,
        -0.015364764258265495,
        0.5144078731536865,
        -0.645361602306366,
        -0.39962998032569885,
        0.24100106954574585,
        -0.30027133226394653,
        0.02137584611773491,
        0.06888291239738464,
        -1.2142618894577026,
        0.3215659558773041,
        -0.048827163875103,
        0.315186470746994,
        0.7196440696716309,
        -0.28684720396995544,
        0.9901110529899597,
        1.0938386917114258,
        -0.16953082382678986,
        -2.024383544921875,
        0.21790602803230286,
        0.8705059289932251,
        -0.34567171335220337,
        -0.3484841585159302,
        -0.5726873278617859,
        0.6786442399024963,
        -0.09863311797380447,
        -0.03706704452633858,
        -0.13347208499908447,
        0.9429607391357422,
        0.5842739343643188,
        -0.35440465807914734,
        0.15767520666122437,
        0.2445797622203827,
        0.3059731721878052,
        -0.3921005427837372,
        0.36124327778816223,
        -0.49242880940437317,
        -0.33111125230789185,
        -0.03729403391480446,
        0.7837845683097839,
        0.845424473285675,
        0.6176064610481262,
        -0.27270227670669556,
        0.33336901664733887,
        0.07556372880935669,
        -0.777130126953125,
        -0.4764159023761749,
        -0.04500038921833038,
        -0.2669423222541809,
        0.0030646435916423798,
        0.8303952217102051,
        -0.01594124734401703,
        0.032225266098976135,
        0.6443700194358826,
        0.6104832291603088,
        -0.6275370121002197,
        0.5955044031143188,
        -0.23783376812934875,
        1.5235711336135864,
        -0.4984287917613983,
        -0.4130398631095886,
        -0.2934059798717499,
        0.28302350640296936,
        -1.0013474225997925,
        -0.40585148334503174,
        0.22567005455493927,
        -0.16164392232894897,
        -0.18712443113327026,
        0.21283191442489624,
        0.13109160959720612,
        -0.669894278049469,
        0.5333578586578369,
        0.43145591020584106,
        0.5782440304756165,
        -0.11697801947593689,
        -0.1973823457956314,
        0.39165961742401123,
        0.6664926409721375,
        -1.1851073503494263,
        -0.10747972130775452,
        -0.18595196306705475,
        -1.1273730993270874,
        0.10002196580171585
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates a comprehensive set of security-focused questions tailored to the fundamental design of a specific project. This process involves deep analysis and conceptualization of the project's components and their security needs. The output includes a summary and a detailed list of security questions organized by themes.",
          "name": "Ask_secure_by_design_questions",
          "raw": "\n                workflow Ask_secure_by_design_questions v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an advanced AI specialized in securely building anything, from bridges to web applications. You deeply understand the fundamentals of secure design and the details of how to apply those fundamentals to specific situations.\n\nYou take input and output a perfect set of secure_by_design questions to help the builder ensure the thing is created securely.\n\n# GOAL\n\nCreate a perfect set of questions to ask in order to address the security of the component/system at the fundamental design level.\n\n# STEPS\n\n- Slowly listen to the input given, and spend 4 hours of virtual time thinking about what they were probably thinking when they created the input.\n\n- Conceptualize what they want to build and break those components out on a virtual whiteboard in your mind.\n\n- Think deeply about the security of this component or system. Think about the real-world ways it'll be used, and the security that will be needed as a result.\n\n- Think about what secure by design components and considerations will be needed to secure the project.\n\n# OUTPUT\n\n- In a section called OVERVIEW, give a 25-word summary of what the input was discussing, and why it's important to secure it.\n\n- In a section called SECURE BY DESIGN QUESTIONS, create a prioritized, bulleted list of 15-25-word questions that should be asked to ensure the project is being built with security by design in mind.\n\n- Questions should be grouped into themes that have capitalized headers, e.g.,:\n\nARCHITECTURE: \n\n- What protocol and version will the client use to communicate with the server?\n- Next question\n- Next question\n- Etc\n- As many as necessary\n\nAUTHENTICATION: \n\n- Question\n- Question\n- Etc\n- As many as necessary\n\nEND EXAMPLES\n\n- There should be at least 15 questions and up to 50.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure the list of questions covers the most important secure by design questions that need to be asked for the project.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an advanced AI specialized in securely building anything, from bridges to web applications. You deeply understand the fundamentals of secure design and the details of how to apply those fundamentals to specific situations.\n\nYou take input and output a perfect set of secure_by_design questions to help the builder ensure the thing is created securely.\n\n# GOAL\n\nCreate a perfect set of questions to ask in order to address the security of the component/system at the fundamental design level.\n\n# STEPS\n\n- Slowly listen to the input given, and spend 4 hours of virtual time thinking about what they were probably thinking when they created the input.\n\n- Conceptualize what they want to build and break those components out on a virtual whiteboard in your mind.\n\n- Think deeply about the security of this component or system. Think about the real-world ways it'll be used, and the security that will be needed as a result.\n\n- Think about what secure by design components and considerations will be needed to secure the project.\n\n# OUTPUT\n\n- In a section called OVERVIEW, give a 25-word summary of what the input was discussing, and why it's important to secure it.\n\n- In a section called SECURE BY DESIGN QUESTIONS, create a prioritized, bulleted list of 15-25-word questions that should be asked to ensure the project is being built with security by design in mind.\n\n- Questions should be grouped into themes that have capitalized headers, e.g.,:\n\nARCHITECTURE: \n\n- What protocol and version will the client use to communicate with the server?\n- Next question\n- Next question\n- Etc\n- As many as necessary\n\nAUTHENTICATION: \n\n- Question\n- Question\n- Etc\n- As many as necessary\n\nEND EXAMPLES\n\n- There should be at least 15 questions and up to 50.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure the list of questions covers the most important secure by design questions that need to be asked for the project.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.028527673333883286,
        0.6211185455322266,
        -0.1957722008228302,
        0.15992656350135803,
        -0.07404893636703491,
        -0.08658935874700546,
        -0.8465585708618164,
        -0.1903008669614792,
        -0.267424613237381,
        0.2740899324417114,
        -0.11725521832704544,
        0.8603833317756653,
        0.3180866837501526,
        0.4423840045928955,
        0.6280696392059326,
        0.12561076879501343,
        -0.293545663356781,
        0.30314767360687256,
        -1.2111289501190186,
        -0.09879638999700546,
        -0.39425504207611084,
        0.8353651165962219,
        0.6030461192131042,
        0.259426474571228,
        0.3431577682495117,
        -0.039604537189006805,
        -0.3187491297721863,
        0.06808602809906006,
        0.3175079822540283,
        -1.100537896156311,
        0.23015478253364563,
        0.25069722533226013,
        -0.3733993172645569,
        -0.005020610988140106,
        0.2770731449127197,
        -0.7224180698394775,
        0.32068198919296265,
        -0.14405767619609833,
        -0.2476840317249298,
        -0.04163361340761185,
        0.18460296094417572,
        0.040601618587970734,
        -0.7042005062103271,
        0.000672709196805954,
        -0.010464767925441265,
        0.17323172092437744,
        -0.45920833945274353,
        0.09562553465366364,
        0.025024639442563057,
        0.41830670833587646,
        -0.21284739673137665,
        -0.5500083565711975,
        0.07659433782100677,
        0.06579151004552841,
        -0.25221142172813416,
        -0.45128440856933594,
        0.16216124594211578,
        -0.7735776901245117,
        0.37165382504463196,
        -0.2447047233581543,
        -0.286366730928421,
        0.38490670919418335,
        -2.7469773292541504,
        0.12493441998958588,
        -0.20399627089500427,
        -0.45990240573883057,
        -0.025760989636182785,
        -0.5702487826347351,
        -0.5647042989730835,
        0.8938882946968079,
        -0.4772844910621643,
        0.43619251251220703,
        -0.5511287450790405,
        -0.15542325377464294,
        -0.03826618939638138,
        -0.5047701597213745,
        0.2484089583158493,
        0.515006959438324,
        0.08832389861345291,
        -0.9906792044639587,
        0.23480655252933502,
        0.4764903783798218,
        0.15028826892375946,
        0.5372969508171082,
        -1.1775476932525635,
        0.9619885683059692,
        -0.45987066626548767,
        0.0036837905645370483,
        0.24356572329998016,
        0.2279183268547058,
        -0.18196441233158112,
        -0.711403489112854,
        -0.14979121088981628,
        0.39857253432273865,
        -0.8798834681510925,
        0.5448421835899353,
        0.36866724491119385,
        0.03294863551855087,
        0.5395419597625732,
        3.656705617904663,
        0.7467893958091736,
        0.45275261998176575,
        0.38894787430763245,
        -0.8528528213500977,
        0.12737584114074707,
        -0.5808099508285522,
        0.17999272048473358,
        -0.369018018245697,
        0.0721510797739029,
        -0.28566858172416687,
        0.5965515375137329,
        -0.3842383623123169,
        -0.4170198142528534,
        0.08072813600301743,
        0.11672897636890411,
        0.7301320433616638,
        -0.48127472400665283,
        -0.23821228742599487,
        0.3132421374320984,
        0.7098777294158936,
        -0.6221848726272583,
        0.5642070770263672,
        0.02906833402812481,
        -0.3279285430908203,
        -0.159984290599823,
        0.08740001916885376,
        -0.41470038890838623,
        0.7356756329536438,
        0.3872591555118561,
        -0.26849353313446045,
        0.40122145414352417,
        0.09405697882175446,
        -0.5959503054618835,
        -0.02585694007575512,
        0.20193246006965637,
        -0.4006757140159607,
        0.06070219352841377,
        -1.0282557010650635,
        0.18578800559043884,
        -0.7358178496360779,
        0.3568342626094818,
        -1.2839077711105347,
        1.2191734313964844,
        0.5735018253326416,
        0.7835277318954468,
        1.1138120889663696,
        0.14275608956813812,
        -0.2067418098449707,
        -0.510797917842865,
        -1.2442562580108643,
        0.0962265282869339,
        0.7773072719573975,
        -0.27050021290779114,
        0.3035183846950531,
        0.6345852613449097,
        -0.18500447273254395,
        -0.6905811429023743,
        -0.055470868945121765,
        -1.0978232622146606,
        0.6483972072601318,
        -0.25791266560554504,
        0.19827836751937866,
        0.5460977554321289,
        0.08578498661518097,
        0.8912120461463928,
        -0.6744102239608765,
        -0.2913495898246765,
        -0.22996598482131958,
        0.3751148581504822,
        0.25681108236312866,
        0.20115546882152557,
        -0.4782213270664215,
        0.07188702374696732,
        0.33553773164749146,
        -0.2075064480304718,
        -0.1016315221786499,
        -0.30511415004730225,
        0.16807934641838074,
        -0.15285128355026245,
        -0.38181209564208984,
        0.35215237736701965,
        0.2128027081489563,
        -0.12046396732330322,
        -1.0843459367752075,
        -0.4086449146270752,
        0.09654595702886581,
        0.07935592532157898,
        -0.248618483543396,
        0.6240652799606323,
        0.5241288542747498,
        -0.8575001955032349,
        1.4883095026016235,
        -0.1846485435962677,
        0.19259004294872284,
        0.04448337107896805,
        -0.17830149829387665,
        0.03861454129219055,
        0.3427608907222748,
        0.3208385705947876,
        -0.2652079164981842,
        -0.923480212688446,
        -0.4054795503616333,
        -0.7781190872192383,
        -0.20345348119735718,
        -0.2821093201637268,
        -0.8053674101829529,
        0.05127406492829323,
        0.7436336278915405,
        -0.07652854919433594,
        -0.6752226948738098,
        0.003088509663939476,
        -0.010794512927532196,
        0.627699613571167,
        -0.08719667047262192,
        0.5932629108428955,
        0.20835049450397491,
        0.20866809785366058,
        0.14459341764450073,
        -0.1553107649087906,
        0.727553129196167,
        0.5058623552322388,
        -0.15512454509735107,
        -0.9013462662696838,
        -0.8594558835029602,
        -0.657367467880249,
        0.23919884860515594,
        0.21847990155220032,
        -0.12973910570144653,
        -0.384006530046463,
        0.011874597519636154,
        0.6081210374832153,
        0.7715094089508057,
        0.33397233486175537,
        0.6976004242897034,
        0.7638779282569885,
        0.1618742048740387,
        -0.5864092111587524,
        0.4667334258556366,
        0.20725762844085693,
        0.022424964234232903,
        0.2721046209335327,
        0.31243520975112915,
        -0.6534650325775146,
        0.6221523880958557,
        0.5961272716522217,
        0.28837716579437256,
        -0.18816089630126953,
        -0.14277677237987518,
        -0.12033917754888535,
        2.0371274948120117,
        0.5194607377052307,
        -0.010324176400899887,
        0.9383381605148315,
        0.22056430578231812,
        0.2976570427417755,
        0.6674751043319702,
        -1.215213656425476,
        0.19642743468284607,
        -0.5271850228309631,
        0.1756790578365326,
        -0.32325196266174316,
        -0.08806685358285904,
        0.6614477038383484,
        0.5639491081237793,
        -0.35655635595321655,
        0.08110104501247406,
        -0.6470558047294617,
        -0.36315423250198364,
        -0.447216272354126,
        -0.331499308347702,
        -0.10033642500638962,
        0.2532118558883667,
        -0.8589187264442444,
        -0.30191537737846375,
        0.46012184023857117,
        0.025058653205633163,
        -0.16245479881763458,
        -0.15456244349479675,
        0.10077115148305893,
        -0.3634595274925232,
        0.1304272711277008,
        -0.10817693918943405,
        -0.20364123582839966,
        -0.0631294846534729,
        -0.5082483887672424,
        0.05177461355924606,
        -0.03332948684692383,
        -0.6288867592811584,
        0.17033591866493225,
        0.7933285236358643,
        -0.35583552718162537,
        -0.6960201859474182,
        0.18901796638965607,
        -0.11756587028503418,
        1.1482281684875488,
        0.6496568918228149,
        0.35833656787872314,
        0.41707149147987366,
        0.3673571050167084,
        -0.20155903697013855,
        -0.47108516097068787,
        -0.11386475712060928,
        -0.15727894008159637,
        0.26423823833465576,
        -0.8006193041801453,
        -0.4528440535068512,
        0.6549893021583557,
        0.7191630601882935,
        -0.15163660049438477,
        0.37344783544540405,
        -0.9766045212745667,
        -0.10334402322769165,
        -0.1161433756351471,
        -0.1342906653881073,
        0.6802480816841125,
        -0.43739423155784607,
        0.8253433108329773,
        0.7257531881332397,
        -0.42198503017425537,
        -2.1856186389923096,
        -0.24078701436519623,
        1.1179249286651611,
        0.6597923040390015,
        -0.29702919721603394,
        -0.5883579254150391,
        0.3133477568626404,
        -0.6070367693901062,
        -0.22022327780723572,
        -0.45827287435531616,
        0.9382680654525757,
        -0.09971959888935089,
        -0.07112376391887665,
        -0.4087847173213959,
        -0.35956472158432007,
        0.5665408372879028,
        -0.44515350461006165,
        0.7521011233329773,
        -0.31310704350471497,
        0.21284472942352295,
        -0.10235819220542908,
        0.6116571426391602,
        1.467181921005249,
        -0.08082615584135056,
        0.3632937967777252,
        -0.1377042680978775,
        0.12750445306301117,
        -0.8080250024795532,
        -1.2094937562942505,
        0.06355267763137817,
        0.11999869346618652,
        -0.2598057687282562,
        1.0975079536437988,
        -0.15010422468185425,
        0.1115604117512703,
        0.8085262775421143,
        0.7498398423194885,
        -0.46707361936569214,
        0.3645424246788025,
        -0.546972393989563,
        2.192514419555664,
        -0.018957648426294327,
        -0.15217827260494232,
        0.06917066127061844,
        0.35165107250213623,
        -0.20223267376422882,
        0.2880406975746155,
        -0.10059811174869537,
        -0.17848077416419983,
        0.4808291792869568,
        0.14781242609024048,
        -0.02352479100227356,
        -0.24330610036849976,
        0.662993311882019,
        0.1271582692861557,
        0.29338544607162476,
        -0.030403852462768555,
        -0.08739908039569855,
        0.08465512841939926,
        0.26687395572662354,
        -0.21980299055576324,
        -0.14073409140110016,
        -0.957051157951355,
        -1.0341883897781372,
        -0.94185870885849
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes teachings and philosophies of notable individuals or philosophical schools, providing detailed templates on their backgrounds, ideas, and applications. It offers a structured approach to encapsulating complex thoughts into accessible summaries. The output includes encapsulations, background information, schools of thought, impactful ideas, primary teachings, works, quotes, applications, and life advice.",
          "name": "Capture_thinkers_work",
          "raw": "\n                workflow Capture_thinkers_work v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou take a philosopher, professional, notable figure, thinker, writer, author, philosophers, or philosophy as input, and you output a template about what it/they taught.\n\nTake a deep breath and think step-by-step how to do the following STEPS.\n\n# STEPS\n\n1. Look for the mention of a notable person, professional, thinker, writer, author, philosopher, philosophers, or philosophy in the input.\n\n2. For each thinker, output the following template:\n\nONE-LINE ENCAPSULATION:\n\nThe philosopher's overall philosophy encapsulated in a 10-20 words.\n\nBACKGROUND:\n\n5 15-word word bullets on their background.\n\nSCHOOL:\n\nGive the one-two word formal school of philosophy or thinking they fall under, along with a 20-30 word description of that school of philosophy/thinking.\n\nMOST IMPACTFUL IDEAS:\n\n5 15-word bullets on their teachings, starting from most important to least important.\n\nTHEIR PRIMARY ADVICE/TEACHINGS:\n\n5 20-30 word bullets on their teachings, starting from most important to least important.\n\nWORKS:\n\n5 15-word bullets on their most popular works and what they were about.\n\nQUOTES:\n\n5 of their most insightful quotes.\n\nAPPLICATION:\n\nDescribe in 30 words what it means to have something be $philosopher-ian, e.g., Socratic for Socrates, Hegelian for Hegel. Etc.\n\nIn other words if the name of the philosopher is Hitchens, the output would be something like,\n\nSomething is Hitchensian if it is like…(continued)\n\nADVICE:\n\n5 20-30 word bullets on how to live life.\n\n3. For each philosophy output the following template:\n\nBACKGROUND:\n\n5 20-30 word bullets on the philosophy's background.\n\nONE-LINE ENCAPSULATION:\n\nThe philosophy's overall philosophy encapsulated in a 10-20 words.\n\nOPPOSING SCHOOLS:\n\nGive 3 20-30 word bullets on opposing philosophies and what they believe that's different from the philosophy provided.\n\nTEACHINGS:\n\n5 20-30 word bullets on the philosophy's teachings, starting from most important to least important.\n\nMOST PROMINENT REPRESENTATIVES:\n\n5 of the philosophy's most prominent representatives.\n\nQUOTES:\n\n5 of the philosophy's most insightful quotes.\n\nAPPLICATION:\n\nDescribe in 30 words what it means to have something be $philosophian, e.g., Rationalist, Empiricist, etc.\n\nIn other words if the name of the philosophy is Rationalism, the output would be something like,\n\nAn idea is Rationalist if it is like…(continued)\n\nADVICE:\n\n5 20-30 word bullets on how to live life according to that philosophy.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou take a philosopher, professional, notable figure, thinker, writer, author, philosophers, or philosophy as input, and you output a template about what it/they taught.\n\nTake a deep breath and think step-by-step how to do the following STEPS.\n\n# STEPS\n\n1. Look for the mention of a notable person, professional, thinker, writer, author, philosopher, philosophers, or philosophy in the input.\n\n2. For each thinker, output the following template:\n\nONE-LINE ENCAPSULATION:\n\nThe philosopher's overall philosophy encapsulated in a 10-20 words.\n\nBACKGROUND:\n\n5 15-word word bullets on their background.\n\nSCHOOL:\n\nGive the one-two word formal school of philosophy or thinking they fall under, along with a 20-30 word description of that school of philosophy/thinking.\n\nMOST IMPACTFUL IDEAS:\n\n5 15-word bullets on their teachings, starting from most important to least important.\n\nTHEIR PRIMARY ADVICE/TEACHINGS:\n\n5 20-30 word bullets on their teachings, starting from most important to least important.\n\nWORKS:\n\n5 15-word bullets on their most popular works and what they were about.\n\nQUOTES:\n\n5 of their most insightful quotes.\n\nAPPLICATION:\n\nDescribe in 30 words what it means to have something be $philosopher-ian, e.g., Socratic for Socrates, Hegelian for Hegel. Etc.\n\nIn other words if the name of the philosopher is Hitchens, the output would be something like,\n\nSomething is Hitchensian if it is like…(continued)\n\nADVICE:\n\n5 20-30 word bullets on how to live life.\n\n3. For each philosophy output the following template:\n\nBACKGROUND:\n\n5 20-30 word bullets on the philosophy's background.\n\nONE-LINE ENCAPSULATION:\n\nThe philosophy's overall philosophy encapsulated in a 10-20 words.\n\nOPPOSING SCHOOLS:\n\nGive 3 20-30 word bullets on opposing philosophies and what they believe that's different from the philosophy provided.\n\nTEACHINGS:\n\n5 20-30 word bullets on the philosophy's teachings, starting from most important to least important.\n\nMOST PROMINENT REPRESENTATIVES:\n\n5 of the philosophy's most prominent representatives.\n\nQUOTES:\n\n5 of the philosophy's most insightful quotes.\n\nAPPLICATION:\n\nDescribe in 30 words what it means to have something be $philosophian, e.g., Rationalist, Empiricist, etc.\n\nIn other words if the name of the philosophy is Rationalism, the output would be something like,\n\nAn idea is Rationalist if it is like…(continued)\n\nADVICE:\n\n5 20-30 word bullets on how to live life according to that philosophy.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.7280065417289734,
        0.6717426776885986,
        -0.0012313984334468842,
        0.39472267031669617,
        0.4929286241531372,
        -0.3225112557411194,
        -0.5064738988876343,
        0.8235825896263123,
        -0.2737351357936859,
        0.1328716278076172,
        -0.6632670164108276,
        0.4773130714893341,
        -0.24387726187705994,
        -0.014291055500507355,
        0.18910175561904907,
        0.14856383204460144,
        -0.32174208760261536,
        -1.2084863185882568,
        -1.586547613143921,
        -0.06685943901538849,
        -0.2715548574924469,
        0.9195961952209473,
        0.5127586722373962,
        0.6077466011047363,
        0.47541487216949463,
        0.49809980392456055,
        -0.18885132670402527,
        -0.0013791099190711975,
        -0.5450683236122131,
        -1.6752493381500244,
        0.4195559322834015,
        0.7333790063858032,
        -0.4960995614528656,
        -0.9618499279022217,
        0.8366337418556213,
        -0.3941691815853119,
        -0.12911294400691986,
        -0.2042527198791504,
        -0.3068760633468628,
        -0.2393486201763153,
        -0.1578548103570938,
        0.5491231679916382,
        -0.17077213525772095,
        -0.2416839301586151,
        0.4509955644607544,
        -0.3314712941646576,
        0.2771998941898346,
        -0.08239594846963882,
        0.46334683895111084,
        0.4508477449417114,
        -0.2262968271970749,
        -0.3491663634777069,
        -0.31912580132484436,
        -0.06552241742610931,
        -0.6008160710334778,
        -0.19645383954048157,
        -0.1513335257768631,
        -0.7141784429550171,
        0.08409873396158218,
        0.15737266838550568,
        0.10580769926309586,
        0.8196653127670288,
        -3.0145719051361084,
        0.1459003984928131,
        0.04293603450059891,
        0.5289013981819153,
        -0.39286044239997864,
        -0.34750592708587646,
        -0.28216177225112915,
        -0.09813669323921204,
        -0.5551565289497375,
        -0.1203354075551033,
        -0.4729684591293335,
        0.32276466488838196,
        0.5422706604003906,
        0.040222518146038055,
        0.688624918460846,
        0.1587432324886322,
        0.5517532229423523,
        -0.4066641926765442,
        -0.34107303619384766,
        0.9148649573326111,
        0.28824883699417114,
        -0.8885321021080017,
        -0.660831868648529,
        -0.06993319094181061,
        0.06624036282300949,
        0.45056867599487305,
        1.0548639297485352,
        0.045734554529190063,
        -0.2914743423461914,
        -0.03929874300956726,
        0.24621005356311798,
        -0.10538261383771896,
        -0.3992321193218231,
        -0.24514810740947723,
        -0.17616812884807587,
        0.2925950884819031,
        0.04076623171567917,
        3.5529723167419434,
        0.8458492755889893,
        0.09587186574935913,
        0.6414304375648499,
        -0.7404661178588867,
        0.4063819646835327,
        -0.4091484844684601,
        -0.05163390189409256,
        -0.5217487812042236,
        -0.18625867366790771,
        -0.5341576337814331,
        0.5898424386978149,
        -0.220169335603714,
        -0.8144032955169678,
        0.4685191214084625,
        0.5652936697006226,
        0.007795291021466255,
        -0.9508308172225952,
        -0.019286129623651505,
        0.3184444308280945,
        0.5763778686523438,
        -0.4421113133430481,
        0.01107307430356741,
        -0.07556978613138199,
        -0.36865130066871643,
        0.0662846565246582,
        0.07374841719865799,
        0.0782371312379837,
        0.730750560760498,
        0.12832610309123993,
        -0.14751268923282623,
        -0.7225657105445862,
        0.4693908989429474,
        0.1083153784275055,
        -0.14149180054664612,
        -0.021794281899929047,
        -0.11585863679647446,
        -0.1251019984483719,
        0.0264594629406929,
        0.2776115834712982,
        -1.0006037950515747,
        -0.044997572898864746,
        -0.15102846920490265,
        1.2077842950820923,
        0.5371088981628418,
        0.7963074445724487,
        -0.11257524788379669,
        -0.13551369309425354,
        0.2524961829185486,
        -0.32882824540138245,
        -0.8432614803314209,
        0.12381699681282043,
        0.8486086130142212,
        0.6256029009819031,
        0.5442971587181091,
        0.7232255339622498,
        -0.20177219808101654,
        -0.5829935669898987,
        -0.18565547466278076,
        -0.5509481430053711,
        0.32532984018325806,
        0.5570844411849976,
        0.11916559934616089,
        0.08184202015399933,
        0.915977954864502,
        0.9907755255699158,
        -0.2102823257446289,
        0.15566326677799225,
        -0.7570339441299438,
        -0.10053049027919769,
        0.28205564618110657,
        0.1910819709300995,
        -0.3264242708683014,
        0.3957292139530182,
        0.7740761041641235,
        -0.46187660098075867,
        -0.21366199851036072,
        0.13286933302879333,
        0.2508678734302521,
        0.12016250193119049,
        -0.872725248336792,
        0.4898988902568817,
        0.6279688477516174,
        0.09331582486629486,
        -0.8161331415176392,
        0.10135544836521149,
        -0.31997451186180115,
        0.20790916681289673,
        -0.019317425787448883,
        0.30833566188812256,
        0.551723837852478,
        -1.359503984451294,
        1.9901087284088135,
        -0.9182811975479126,
        -0.2421083003282547,
        0.4096963405609131,
        -0.041942350566387177,
        0.12033252418041229,
        -0.5173666477203369,
        0.7298139333724976,
        -0.33164143562316895,
        -0.4329855442047119,
        0.40035685896873474,
        -0.1894073784351349,
        0.07058769464492798,
        -0.6109384298324585,
        -0.12329360842704773,
        -0.005887859500944614,
        0.569596529006958,
        -0.3149831295013428,
        -0.435234010219574,
        -0.2504507303237915,
        0.2814001739025116,
        0.8021210432052612,
        -0.25434646010398865,
        0.9148560762405396,
        0.2015979140996933,
        0.6057055592536926,
        0.11715072393417358,
        0.21318113803863525,
        0.8942441940307617,
        -0.5050821304321289,
        0.6143209338188171,
        -0.7963535785675049,
        -0.7132090926170349,
        -1.39956796169281,
        0.5483343005180359,
        -0.2975715398788452,
        0.07880544662475586,
        -0.5394555926322937,
        -0.42501717805862427,
        0.37597593665122986,
        0.8219323754310608,
        0.8738802075386047,
        0.5823134183883667,
        0.2534584105014801,
        0.19387713074684143,
        -0.469873309135437,
        0.8958907127380371,
        0.38777583837509155,
        -1.0443928241729736,
        0.20736730098724365,
        -0.2736435532569885,
        -0.2433631420135498,
        0.9378033876419067,
        0.41737160086631775,
        0.42112061381340027,
        -1.045177698135376,
        0.06556928157806396,
        0.0333649218082428,
        1.328873634338379,
        0.49346837401390076,
        -0.13817571103572845,
        0.06213310733437538,
        0.3483057916164398,
        -0.09659423679113388,
        0.38655152916908264,
        -1.8360230922698975,
        -0.7301896810531616,
        -0.7097584009170532,
        0.6695972681045532,
        -0.08439257740974426,
        0.010197862982749939,
        0.53030925989151,
        -1.07022225856781,
        -0.2607426941394806,
        0.05934420973062515,
        -0.34317007660865784,
        -0.15021264553070068,
        -0.20366431772708893,
        -0.055282916873693466,
        -0.39509183168411255,
        0.26547348499298096,
        -0.22409233450889587,
        0.23970112204551697,
        0.366900235414505,
        -0.221396803855896,
        0.10697205364704132,
        0.12486426532268524,
        0.04487265646457672,
        0.015437370166182518,
        -0.34997230768203735,
        -0.2188907265663147,
        0.1884855180978775,
        0.0326889306306839,
        -0.5268208384513855,
        0.47026926279067993,
        0.09614507853984833,
        -0.9349504709243774,
        -0.041531674563884735,
        0.7605082392692566,
        -0.5692796111106873,
        -0.21320481598377228,
        -0.48731550574302673,
        0.38547345995903015,
        1.6937730312347412,
        0.24490664899349213,
        0.28619620203971863,
        0.8521023988723755,
        0.8014037013053894,
        -0.017125548794865608,
        -0.134342759847641,
        -0.35961151123046875,
        -0.08595374971628189,
        0.2275036871433258,
        -0.9441258311271667,
        -0.2539981007575989,
        0.9554954767227173,
        -0.1826731264591217,
        -0.30506065487861633,
        0.614692747592926,
        -0.7919624447822571,
        0.028024036437273026,
        -0.5631804466247559,
        -0.08565019071102142,
        1.1571242809295654,
        -0.3049458861351013,
        1.0312861204147339,
        0.7373266220092773,
        0.34299999475479126,
        -1.6316337585449219,
        -0.01208528969436884,
        0.8661977648735046,
        -0.04147665202617645,
        -0.004558667540550232,
        -0.04286668077111244,
        0.09269377589225769,
        -0.13375940918922424,
        0.03686758503317833,
        -0.49612581729888916,
        1.2842068672180176,
        0.28869232535362244,
        -0.49716728925704956,
        -0.9388594627380371,
        0.043875060975551605,
        0.25195884704589844,
        -0.9706911444664001,
        0.2940032184123993,
        -0.4856061637401581,
        -0.9718844890594482,
        -0.4823213219642639,
        -0.02559499442577362,
        1.68768310546875,
        -0.029274234548211098,
        -0.11694534122943878,
        0.0679553747177124,
        0.100985586643219,
        -0.5827400088310242,
        -0.12686139345169067,
        -0.028469614684581757,
        0.08073443174362183,
        -0.17433051764965057,
        0.3892885446548462,
        0.23119443655014038,
        -0.6154114603996277,
        0.9904271364212036,
        -0.08106790482997894,
        -0.530208170413971,
        0.07606835663318634,
        -0.4736228287220001,
        1.606227159500122,
        -0.2471209019422531,
        -0.6442180871963501,
        -0.04498400539159775,
        0.23263432085514069,
        -0.17061059176921844,
        -0.14580632746219635,
        -0.30138540267944336,
        -1.1024346351623535,
        0.04322908818721771,
        0.37041202187538147,
        -0.053938206285238266,
        -0.031328022480010986,
        0.012898914515972137,
        0.290294349193573,
        0.6618859171867371,
        0.1828538477420807,
        -0.23811085522174835,
        -0.34654465317726135,
        0.11268668621778488,
        -0.4469844698905945,
        -0.05483723431825638,
        -0.47712191939353943,
        -0.4407653510570526,
        -0.43361401557922363
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt outlines a process for analyzing contracts and agreements to identify potential issues or 'gotchas.' It involves summarizing the document, listing important aspects, categorizing issues by severity, and drafting responses for critical and important items. The expected output includes a concise summary, detailed callouts, categorized issues, and recommended responses in Markdown format.",
          "name": "Check_agreement",
          "raw": "\n                workflow Check_agreement v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at analyzing contracts and agreements and looking for gotchas. You take a document in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 30-word sentence in a section called DOCUMENT SUMMARY:.\n\n- Output the 10 most important aspects, stipulations, and other types of gotchas in the content as a list with no more than 20 words per point into a section called CALLOUTS:.\n\n- Output the 10 most important issues to be aware of before agreeing to the document, organized in three sections: CRITICAL:, IMPORTANT:, and OTHER:.\n\n- For each of the CRITICAL and IMPORTANT items identified, write a request to be sent to the sending organization recommending it be changed or removed. Place this in a section called RESPONSES:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at analyzing contracts and agreements and looking for gotchas. You take a document in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 30-word sentence in a section called DOCUMENT SUMMARY:.\n\n- Output the 10 most important aspects, stipulations, and other types of gotchas in the content as a list with no more than 20 words per point into a section called CALLOUTS:.\n\n- Output the 10 most important issues to be aware of before agreeing to the document, organized in three sections: CRITICAL:, IMPORTANT:, and OTHER:.\n\n- For each of the CRITICAL and IMPORTANT items identified, write a request to be sent to the sending organization recommending it be changed or removed. Place this in a section called RESPONSES:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.1167752742767334,
        0.5011337399482727,
        -0.16188791394233704,
        1.0741533041000366,
        0.2185869961977005,
        -0.07376717776060104,
        -1.1381676197052002,
        -0.39584898948669434,
        0.5753635764122009,
        -0.09837280213832855,
        0.09723586589097977,
        0.9100779294967651,
        -0.11054633557796478,
        -0.1163334846496582,
        -0.10240815579891205,
        0.2018791139125824,
        -0.1630353331565857,
        -0.18061921000480652,
        -1.1454222202301025,
        -0.6851105093955994,
        0.12442393600940704,
        0.9568338990211487,
        0.12849125266075134,
        -0.09603874385356903,
        0.16824036836624146,
        0.2768564522266388,
        -0.36763879656791687,
        -0.42879462242126465,
        -0.8848958015441895,
        -2.1229960918426514,
        0.24407035112380981,
        -0.46094372868537903,
        -0.20881833136081696,
        -0.034976452589035034,
        0.2773445248603821,
        -0.894890308380127,
        0.0458642952144146,
        0.5234273672103882,
        -0.05642782896757126,
        -0.6050378084182739,
        -0.25594252347946167,
        0.6100045442581177,
        0.3140726685523987,
        -0.09990336000919342,
        0.20653897523880005,
        -0.029404692351818085,
        0.004203241318464279,
        -0.6980235576629639,
        0.9302356839179993,
        0.4686174988746643,
        -0.18272872269153595,
        -0.059542201459407806,
        0.22390855848789215,
        0.1032424122095108,
        0.06454585492610931,
        -0.039938729256391525,
        0.15198788046836853,
        -0.824855625629425,
        -0.11148113012313843,
        0.2150222659111023,
        -0.3801688551902771,
        0.42549195885658264,
        -3.5312187671661377,
        0.15597452223300934,
        -0.572250247001648,
        0.3176063597202301,
        -0.045356862246990204,
        -0.008651580661535263,
        0.007163216359913349,
        -0.8007078170776367,
        -0.5256406664848328,
        0.41462111473083496,
        -0.42207571864128113,
        1.2068736553192139,
        0.24689480662345886,
        -0.38456884026527405,
        0.18696318566799164,
        0.09064275026321411,
        0.10521762818098068,
        -0.5727188587188721,
        0.06606970727443695,
        0.6091874241828918,
        0.24637877941131592,
        -0.2589467167854309,
        -0.680076539516449,
        0.6301898956298828,
        -0.4594251811504364,
        0.19178932905197144,
        0.21527370810508728,
        -0.08250391483306885,
        0.12273737788200378,
        -0.3012726902961731,
        -0.04074800759553909,
        0.1426895558834076,
        -0.6422699093818665,
        0.18854442238807678,
        0.29552796483039856,
        -0.15799638628959656,
        0.5960787534713745,
        3.3161580562591553,
        0.6642184257507324,
        0.5958044528961182,
        0.41376617550849915,
        -0.726500928401947,
        0.2578045725822449,
        -0.20242783427238464,
        0.07789650559425354,
        -0.4767305552959442,
        -0.2691818177700043,
        -0.11319130659103394,
        0.42977631092071533,
        -1.2825130224227905,
        -0.4336845576763153,
        0.02151601016521454,
        0.14168572425842285,
        0.024396074935793877,
        -0.5427033305168152,
        0.4212963581085205,
        -0.14577074348926544,
        0.6933536529541016,
        -0.09754927456378937,
        -0.3281017541885376,
        -0.6454514861106873,
        -0.2262943685054779,
        0.23649291694164276,
        -0.017401769757270813,
        0.21924826502799988,
        0.5705565214157104,
        0.14308759570121765,
        0.15434540808200836,
        0.015546254813671112,
        0.3831423223018646,
        -0.44020703434944153,
        -0.08193264156579971,
        -0.15330420434474945,
        -0.3611155152320862,
        0.6019850373268127,
        -0.9721655249595642,
        0.3073619306087494,
        -0.7764087319374084,
        0.08225781470537186,
        -0.761357843875885,
        0.5953085422515869,
        0.6872398853302002,
        0.6202545762062073,
        0.30914306640625,
        -0.21030861139297485,
        0.03402972221374512,
        -0.11208397150039673,
        -0.3386287987232208,
        0.5725928544998169,
        0.7237932682037354,
        -0.250887393951416,
        0.06598780304193497,
        0.7268137335777283,
        0.2793538272380829,
        -0.12450146675109863,
        0.25883975625038147,
        -0.283747136592865,
        -0.11234141141176224,
        -0.02577030286192894,
        0.10705455392599106,
        0.529852032661438,
        0.29216310381889343,
        0.8081772327423096,
        -0.8165220618247986,
        0.2705552279949188,
        -0.16983088850975037,
        0.11224233359098434,
        -0.036948010325431824,
        -0.10835151374340057,
        -0.5437793135643005,
        0.3284781277179718,
        0.8777734637260437,
        -0.4907514452934265,
        0.6016396880149841,
        -0.005404703319072723,
        0.12264128029346466,
        0.09829461574554443,
        -0.5362346172332764,
        0.9639368653297424,
        0.5896314382553101,
        -0.7007514834403992,
        -1.0481189489364624,
        0.04343591630458832,
        0.1637432873249054,
        -0.2880741357803345,
        0.6514420509338379,
        0.9611114859580994,
        1.5248152017593384,
        -0.43598514795303345,
        1.984897255897522,
        -1.041899561882019,
        -0.011955633759498596,
        -0.3201010227203369,
        0.2640269994735718,
        -0.037176091223955154,
        -0.5453616976737976,
        0.504676103591919,
        0.07700667530298233,
        -0.2783375680446625,
        -0.29521745443344116,
        -0.5607929825782776,
        0.19880670309066772,
        -0.4212234914302826,
        -0.7804232239723206,
        0.15315531194210052,
        0.9442787170410156,
        0.2872627377510071,
        -0.4942292869091034,
        -0.5384968519210815,
        0.44122955203056335,
        0.7781522870063782,
        0.2668042480945587,
        0.23892483115196228,
        -0.17939476668834686,
        0.16139298677444458,
        0.21659988164901733,
        -0.19184517860412598,
        0.18103355169296265,
        -0.08944954723119736,
        0.3800271153450012,
        -1.1584404706954956,
        -0.7023131251335144,
        -0.6127399206161499,
        0.9785454273223877,
        -0.47184547781944275,
        0.6386812925338745,
        -0.39988973736763,
        -0.3312411904335022,
        0.36579015851020813,
        1.699512004852295,
        0.9615746140480042,
        1.034477710723877,
        0.0005370453000068665,
        0.6355991363525391,
        -0.007664188742637634,
        0.024940133094787598,
        -0.05049346014857292,
        -0.8306555151939392,
        1.0969185829162598,
        0.11510762572288513,
        0.15650977194309235,
        0.576860785484314,
        -0.06705589592456818,
        0.2479265034198761,
        -0.6497310400009155,
        -0.5141188502311707,
        -0.1480816751718521,
        1.4787862300872803,
        -0.24545073509216309,
        -0.12735874950885773,
        0.2587493062019348,
        0.7661687135696411,
        -0.20521777868270874,
        0.3527643084526062,
        -1.5075006484985352,
        0.1613389253616333,
        -0.8993258476257324,
        0.3594364821910858,
        -0.6107929348945618,
        -0.03675326332449913,
        0.563752293586731,
        0.46466320753097534,
        -0.33643725514411926,
        -0.16451188921928406,
        0.8849409818649292,
        0.08319933712482452,
        -0.19080792367458344,
        0.03018268197774887,
        -0.09937991201877594,
        0.4955318570137024,
        0.13220027089118958,
        -0.4207535982131958,
        -0.12115415185689926,
        0.3170754909515381,
        0.15703287720680237,
        -0.07030830532312393,
        0.09009440988302231,
        -0.28078344464302063,
        0.2800425589084625,
        -0.14305809140205383,
        -0.7360701560974121,
        0.5255204439163208,
        -0.665666401386261,
        0.12694860994815826,
        -0.6906181573867798,
        -0.5728740096092224,
        0.4985435903072357,
        0.9940433502197266,
        -0.22393524646759033,
        -0.6755291819572449,
        -1.2904633283615112,
        0.4460502564907074,
        1.851670265197754,
        0.27584409713745117,
        0.2885642945766449,
        0.7660195827484131,
        0.004916402976959944,
        0.1856435239315033,
        -0.0788167417049408,
        0.15188390016555786,
        -0.5977746248245239,
        -0.566687285900116,
        -0.44752123951911926,
        -0.42153963446617126,
        0.3057658076286316,
        0.18098212778568268,
        -0.4779958724975586,
        0.3260548412799835,
        -0.6872820258140564,
        -0.14633037149906158,
        0.39628738164901733,
        0.16480059921741486,
        0.010126575827598572,
        -0.2915920913219452,
        -0.10639554262161255,
        1.037326693534851,
        -0.01727687194943428,
        -1.3513247966766357,
        -0.6720362901687622,
        0.06705274432897568,
        -0.10978877544403076,
        -0.07046015560626984,
        0.012817708775401115,
        0.9638785123825073,
        0.30212926864624023,
        -0.30153408646583557,
        -1.213403582572937,
        0.9939351081848145,
        0.3596285283565521,
        0.2606678009033203,
        -0.15373200178146362,
        0.06557194143533707,
        0.7731070518493652,
        -0.38591066002845764,
        0.17747223377227783,
        -0.34173065423965454,
        -0.3331274688243866,
        0.04934527352452278,
        0.4306526780128479,
        1.0177011489868164,
        -0.05018097907304764,
        0.8043460249900818,
        0.3510224223136902,
        -0.20903044939041138,
        -0.5740687847137451,
        -1.4490197896957397,
        0.1152491420507431,
        -1.0243490934371948,
        -0.4498206675052643,
        1.0466561317443848,
        0.07914181053638458,
        0.36667945981025696,
        0.5963801741600037,
        0.4837079346179962,
        0.12352040410041809,
        -0.10939482599496841,
        -0.4106549620628357,
        2.0217556953430176,
        -0.6691597104072571,
        -0.45375075936317444,
        -0.5127074122428894,
        0.07434850931167603,
        -0.19488291442394257,
        0.09864518791437149,
        -0.16732484102249146,
        -0.534350574016571,
        -0.38294917345046997,
        0.3299742043018341,
        0.3103019595146179,
        -1.0420868396759033,
        0.12141889333724976,
        0.07209425419569016,
        0.2193007469177246,
        -0.09882371127605438,
        0.20496758818626404,
        -0.07257455587387085,
        -0.12225963175296783,
        -0.06982062757015228,
        0.3326953947544098,
        -0.5072177648544312,
        -0.28373822569847107,
        -0.7984001040458679
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes and corrects formatting issues in text without altering the content. It focuses on removing odd line breaks to improve readability. The expected output is a clean, well-formatted version of the original text.",
          "name": "Clean_text",
          "raw": "\n                workflow Clean_text v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at cleaning up broken and, malformatted, text, for example: line breaks in weird places, etc. \n\n# Steps\n\n- Read the entire document and fully understand it.\n- Remove any strange line breaks that disrupt formatting.\n- Add captialization, punctuation, line breaks, paragraphs and other formatting where necessary.\n- Do NOT change any content or spelling whatsoever.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the full, properly-formatted text.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at cleaning up broken and, malformatted, text, for example: line breaks in weird places, etc. \n\n# Steps\n\n- Read the entire document and fully understand it.\n- Remove any strange line breaks that disrupt formatting.\n- Add captialization, punctuation, line breaks, paragraphs and other formatting where necessary.\n- Do NOT change any content or spelling whatsoever.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the full, properly-formatted text.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.2738672196865082,
        0.490553617477417,
        0.026063520461320877,
        0.6237750053405762,
        -0.34160614013671875,
        0.16028878092765808,
        -0.9530125260353088,
        -0.01643526554107666,
        -0.36880430579185486,
        0.27505892515182495,
        -0.0057948920875787735,
        0.046169131994247437,
        0.4379733204841614,
        0.5389227271080017,
        -0.19305317103862762,
        0.18118026852607727,
        0.0720379501581192,
        -0.28762832283973694,
        -1.1196720600128174,
        -0.34670722484588623,
        -0.3991842567920685,
        0.8479591012001038,
        0.45402514934539795,
        -0.0755719542503357,
        0.4165044128894806,
        -0.0009914715774357319,
        -0.24173805117607117,
        -0.8170329928398132,
        -0.5349262952804565,
        -1.6482595205307007,
        -0.015152424573898315,
        0.3587818741798401,
        0.579517662525177,
        -0.42625510692596436,
        0.5693070292472839,
        -0.7173894643783569,
        0.6261888742446899,
        0.24307042360305786,
        -0.2589639723300934,
        -0.15351678431034088,
        -0.3309212327003479,
        0.09735909104347229,
        -0.14986777305603027,
        0.23293372988700867,
        0.21780869364738464,
        0.0006006695330142975,
        -0.3116064667701721,
        -0.09243743866682053,
        0.953392505645752,
        0.17972537875175476,
        -0.19108307361602783,
        -0.2900034487247467,
        -0.7488399147987366,
        0.1989479511976242,
        -0.30262109637260437,
        0.1393943876028061,
        0.343235045671463,
        -0.6537794470787048,
        0.18950039148330688,
        0.0007275016978383064,
        -0.16642342507839203,
        0.4682195782661438,
        -3.5030887126922607,
        -0.02656889334321022,
        0.24015334248542786,
        0.24581319093704224,
        0.07250604033470154,
        0.1098383218050003,
        -0.3259894549846649,
        -0.3094595968723297,
        -0.10146641731262207,
        0.31674453616142273,
        -0.2130599319934845,
        0.14338815212249756,
        -0.1747380495071411,
        -0.49733230471611023,
        0.49423256516456604,
        0.30832213163375854,
        0.44984644651412964,
        -0.2911280393600464,
        -0.24332347512245178,
        0.46208953857421875,
        0.4029267132282257,
        0.13687004148960114,
        -0.754788875579834,
        0.4772304892539978,
        -0.15809422731399536,
        -0.5554559230804443,
        0.307914674282074,
        -0.040115680545568466,
        -0.12720881402492523,
        -0.3117623031139374,
        0.2104487270116806,
        0.09309757500886917,
        -0.9381953477859497,
        -0.21053780615329742,
        0.36791756749153137,
        0.5457171201705933,
        0.6153619885444641,
        3.5673022270202637,
        0.3497340679168701,
        -0.10529860109090805,
        0.5620625615119934,
        -1.1722524166107178,
        0.05919497460126877,
        -0.09482556581497192,
        0.30516886711120605,
        -0.2949536144733429,
        0.10566560924053192,
        -0.06499288231134415,
        0.2713756859302521,
        -0.5577875375747681,
        -0.5572409629821777,
        0.42725658416748047,
        0.26737505197525024,
        0.4225308299064636,
        -0.2997923493385315,
        0.26308977603912354,
        0.143753319978714,
        0.6950349807739258,
        -0.6369863152503967,
        0.3985742926597595,
        -0.12080437690019608,
        -0.47415000200271606,
        -0.5263603329658508,
        0.17466571927070618,
        -0.17786934971809387,
        0.5766137838363647,
        0.06523442268371582,
        -0.06804929673671722,
        0.3846926987171173,
        -0.11810775101184845,
        -0.15865975618362427,
        0.2847081124782562,
        -0.005519259721040726,
        0.2313787043094635,
        0.34226977825164795,
        -0.6494070887565613,
        0.0971207246184349,
        -0.7155572175979614,
        0.06122976541519165,
        -0.903267502784729,
        1.028924584388733,
        0.1292252540588379,
        0.19002750515937805,
        0.7743402123451233,
        -0.10545774549245834,
        0.11042780429124832,
        -0.9090761542320251,
        -0.6739256381988525,
        0.5904518961906433,
        0.37398761510849,
        -0.17850065231323242,
        -0.016117816790938377,
        0.7255020141601562,
        0.3518478572368622,
        0.15151537954807281,
        0.06121811643242836,
        -0.4830690920352936,
        0.4643131196498871,
        0.009798172861337662,
        -0.09536916762590408,
        0.45827198028564453,
        0.8013641238212585,
        0.7887771129608154,
        0.06253007054328918,
        0.5469781756401062,
        0.0836874395608902,
        -0.14042961597442627,
        0.16618280112743378,
        -0.11612223088741302,
        -0.053838036954402924,
        0.31824740767478943,
        0.17670179903507233,
        -0.519378662109375,
        -0.3492778539657593,
        -0.5481618642807007,
        -0.19634771347045898,
        0.09094003587961197,
        -0.4297281801700592,
        0.4532225728034973,
        0.3845013678073883,
        0.16673003137111664,
        -1.0922092199325562,
        0.1395438015460968,
        0.30162060260772705,
        0.6475296020507812,
        0.0967257171869278,
        0.6971644759178162,
        0.9765294790267944,
        -0.9724857211112976,
        1.0880095958709717,
        -0.7799275517463684,
        -0.5210549831390381,
        -0.0719049945473671,
        0.023277122527360916,
        -0.08054575324058533,
        0.09046008437871933,
        0.7573337554931641,
        0.12689128518104553,
        -1.269026756286621,
        -0.5432389974594116,
        -0.11979945003986359,
        0.05282440036535263,
        -0.4884977340698242,
        -0.7066236734390259,
        -0.21637898683547974,
        -0.10559438169002533,
        -0.45598477125167847,
        -0.7108075618743896,
        0.1270448863506317,
        -0.46906253695487976,
        1.614485502243042,
        0.46399128437042236,
        0.5330733060836792,
        -0.13392409682273865,
        0.415049284696579,
        0.8148619532585144,
        0.6893412470817566,
        0.2126055210828781,
        -0.36575785279273987,
        0.08802825957536697,
        -0.19428908824920654,
        -0.9496335387229919,
        -0.988048255443573,
        0.6037325263023376,
        -0.5020708441734314,
        -0.38712412118911743,
        -0.5782368779182434,
        -0.7124577760696411,
        -0.017863817512989044,
        1.2129385471343994,
        0.655666172504425,
        0.9356817007064819,
        0.12152375280857086,
        0.46751290559768677,
        -0.19621336460113525,
        0.3235247731208801,
        0.45302388072013855,
        -0.8030394911766052,
        0.09175832569599152,
        0.16101697087287903,
        -0.714508056640625,
        -0.3567606806755066,
        0.39464524388313293,
        0.4790465533733368,
        -0.4423415958881378,
        -0.3637937307357788,
        0.3737948536872864,
        1.2265191078186035,
        0.4517444968223572,
        0.7739678025245667,
        -0.10013030469417572,
        0.09155970811843872,
        0.4083578288555145,
        -0.015653066337108612,
        -2.1782753467559814,
        -0.27417922019958496,
        -0.8973205089569092,
        0.7239304780960083,
        -0.37020033597946167,
        0.17637571692466736,
        0.6518132090568542,
        0.3108881711959839,
        -0.17787016928195953,
        -0.434219092130661,
        -0.48401471972465515,
        -0.6577420234680176,
        -0.6110610365867615,
        -0.01668854057788849,
        -0.6945604085922241,
        0.5256741642951965,
        -0.06516672670841217,
        0.060933273285627365,
        0.24184174835681915,
        -0.06669124960899353,
        0.08784402161836624,
        0.019891075789928436,
        -0.2270638644695282,
        -0.4338815212249756,
        0.5051916837692261,
        0.44234955310821533,
        -0.07051504403352737,
        0.13695812225341797,
        -0.49484559893608093,
        -0.28327110409736633,
        -0.38858699798583984,
        -0.03425350785255432,
        -0.018834151327610016,
        0.6411967873573303,
        -0.021290097385644913,
        -0.36401835083961487,
        -0.14512965083122253,
        0.5385187864303589,
        1.906864047050476,
        0.8204355835914612,
        -0.3296300172805786,
        0.8226147294044495,
        0.19818483293056488,
        -0.0029506655409932137,
        -0.3951037526130676,
        0.9469267129898071,
        0.08142271637916565,
        -0.18076086044311523,
        -0.6103914976119995,
        -0.2734987735748291,
        0.9671610593795776,
        -0.45911362767219543,
        -0.2382681965827942,
        0.41274967789649963,
        -0.3119586408138275,
        -0.0512765608727932,
        -0.18331551551818848,
        -0.3397118151187897,
        0.7896339297294617,
        -0.5004501938819885,
        0.9792031645774841,
        0.8508232831954956,
        -0.2996457517147064,
        -1.849481225013733,
        -0.46079787611961365,
        0.5185771584510803,
        -0.007063504308462143,
        -0.11839352548122406,
        -0.13239522278308868,
        0.6589497923851013,
        -0.7627112865447998,
        -0.43168705701828003,
        -0.3919336199760437,
        1.530005693435669,
        0.8257250189781189,
        0.05718482658267021,
        -0.8033604621887207,
        0.18042448163032532,
        0.7522224187850952,
        -0.23710958659648895,
        0.18890033662319183,
        -0.2199227213859558,
        -0.7127761244773865,
        0.13586626946926117,
        0.5183220505714417,
        1.414367914199829,
        0.08568361401557922,
        0.706718385219574,
        0.1085633635520935,
        0.08252213150262833,
        -0.9982523322105408,
        -1.17008638381958,
        -0.16866405308246613,
        -0.16731983423233032,
        -0.3602153956890106,
        1.0496251583099365,
        -0.2404073178768158,
        -0.06865906715393066,
        0.1471594125032425,
        0.7899991273880005,
        -0.5214836001396179,
        0.3346652388572693,
        -0.778453528881073,
        1.7889946699142456,
        0.05066441744565964,
        -0.34528401494026184,
        -0.20030197501182556,
        -0.11458627879619598,
        -0.3015372157096863,
        0.005481362342834473,
        0.185586079955101,
        -0.6036278009414673,
        0.15622565150260925,
        0.4550992548465729,
        -0.1832747608423233,
        -0.6390334367752075,
        0.8379439115524292,
        0.2630016505718231,
        0.6142594814300537,
        -0.6338918209075928,
        0.004180360585451126,
        0.6280195713043213,
        0.10659827291965485,
        0.013288751244544983,
        -0.037666238844394684,
        -0.6464315056800842,
        -0.5780627727508545,
        -0.7406004667282104
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Explains coding concepts or languages to beginners, using examples from reputable sources and illustrating points with formatted code. The approach emphasizes clarity and accessibility, incorporating examples from Codeacademy and NetworkChuck. Outputs include markdown-formatted code and structured lists of ideas, recommendations, habits, facts, and insights, adhering to specific word counts.",
          "name": "Coding_master",
          "raw": "\n                workflow Coding_master v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n**Expert coder**\n\n\n\nYou are an expert in understanding and digesting computer coding and computer languages.\n Explain the concept of [insert specific coding concept or language here] as if you\n were teaching it to a beginner. Use examples from reputable sources like Codeacademy (codeacademy.com) and NetworkChuck to illustrate your points.\n\n\n\n\n**Coding output**\n\nPlease format the code in a markdown method using syntax\n\nalso please illustrate the code in this format:\n\n``` your code\nYour code here\n```\n\n\n\n**OUTPUT INSTRUCTIONS**\nOnly output Markdown.\n\nWrite the IDEAS bullets as exactly 15 words.\n\nWrite the RECOMMENDATIONS bullets as exactly 15 words.\n\nWrite the HABITS bullets as exactly 15 words.\n\nWrite the FACTS bullets as exactly 15 words.\n\nWrite the INSIGHTS bullets as exactly 15 words.\n\nExtract at least 25 IDEAS from the content.\n\nExtract at least 10 INSIGHTS from the content.\n\nExtract at least 20 items for the other output sections.\n\nDo not give warnings or notes; only output the requested sections.\n\nYou use bulleted lists for output, not numbered lists.\n\nDo not repeat ideas, quotes, facts, or resources.\n\nDo not start items with the same opening words.\n\nEnsure you follow ALL these instructions when creating your output.\n\n**INPUT**\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n**Expert coder**\n\n\n\nYou are an expert in understanding and digesting computer coding and computer languages.\n Explain the concept of [insert specific coding concept or language here] as if you\n were teaching it to a beginner. Use examples from reputable sources like Codeacademy (codeacademy.com) and NetworkChuck to illustrate your points.\n\n\n\n\n**Coding output**\n\nPlease format the code in a markdown method using syntax\n\nalso please illustrate the code in this format:\n\n``` your code\nYour code here\n```\n\n\n\n**OUTPUT INSTRUCTIONS**\nOnly output Markdown.\n\nWrite the IDEAS bullets as exactly 15 words.\n\nWrite the RECOMMENDATIONS bullets as exactly 15 words.\n\nWrite the HABITS bullets as exactly 15 words.\n\nWrite the FACTS bullets as exactly 15 words.\n\nWrite the INSIGHTS bullets as exactly 15 words.\n\nExtract at least 25 IDEAS from the content.\n\nExtract at least 10 INSIGHTS from the content.\n\nExtract at least 20 items for the other output sections.\n\nDo not give warnings or notes; only output the requested sections.\n\nYou use bulleted lists for output, not numbered lists.\n\nDo not repeat ideas, quotes, facts, or resources.\n\nDo not start items with the same opening words.\n\nEnsure you follow ALL these instructions when creating your output.\n\n**INPUT**\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6677553057670593,
        -0.0145789235830307,
        -0.14769072830677032,
        1.0531328916549683,
        0.14360152184963226,
        -0.10306590050458908,
        -1.0277403593063354,
        -0.3663514256477356,
        0.032861027866601944,
        -0.11292853206396103,
        -0.5236328840255737,
        0.42613908648490906,
        0.02729973942041397,
        0.061257146298885345,
        -0.05379844084382057,
        -0.08798985183238983,
        0.0950215607881546,
        -1.1882984638214111,
        -2.0683634281158447,
        -0.6152036786079407,
        0.6223275661468506,
        0.47964248061180115,
        -0.07358119636774063,
        0.39692962169647217,
        0.8684447407722473,
        0.060545165091753006,
        -0.052268628031015396,
        0.03398127853870392,
        -1.7147516012191772,
        -1.555120587348938,
        -0.5565518736839294,
        -0.1944105327129364,
        -0.09389456361532211,
        0.12721265852451324,
        0.5535361170768738,
        -0.9284292459487915,
        0.1439409852027893,
        -0.20912739634513855,
        -0.6400254368782043,
        -0.8063016533851624,
        0.2846732437610626,
        0.2700558602809906,
        -0.06652538478374481,
        -0.4405127167701721,
        0.2413867861032486,
        -0.18844199180603027,
        0.09081019461154938,
        -0.2164776474237442,
        0.7265720963478088,
        0.7557405829429626,
        -0.761581301689148,
        -0.7594041228294373,
        0.41562288999557495,
        -0.13916924595832825,
        -0.5116289854049683,
        0.349666565656662,
        0.27881863713264465,
        -0.46687015891075134,
        0.043058961629867554,
        0.17366282641887665,
        0.3928317427635193,
        0.6938698291778564,
        -3.228947639465332,
        0.2447964996099472,
        0.2208254188299179,
        0.28031060099601746,
        0.2635653018951416,
        -0.4785350263118744,
        0.17215082049369812,
        -0.34738144278526306,
        -0.2890760898590088,
        0.08066006749868393,
        0.20135685801506042,
        0.3532494604587555,
        0.5196504592895508,
        -0.0685320645570755,
        0.275978684425354,
        -0.27421995997428894,
        0.041352469474077225,
        -0.675898015499115,
        0.07270471751689911,
        0.2898961305618286,
        -0.436551570892334,
        -0.48961156606674194,
        -0.4908391237258911,
        0.2712985873222351,
        0.18469500541687012,
        -0.03896384686231613,
        -0.25650158524513245,
        -0.5186375379562378,
        -0.04228829964995384,
        0.028048880398273468,
        -0.2658289968967438,
        0.01038128137588501,
        0.15975509583950043,
        0.8080203533172607,
        0.5780196785926819,
        0.3145638704299927,
        -0.4731166362762451,
        3.2963943481445312,
        0.0623931884765625,
        -0.3378565311431885,
        0.055412374436855316,
        -0.8896595239639282,
        -0.3184695541858673,
        -0.2839553952217102,
        0.08590249717235565,
        0.2475215643644333,
        0.09577855467796326,
        0.1750435084104538,
        0.2996577024459839,
        -1.0208218097686768,
        -0.15276816487312317,
        0.10698246210813522,
        -0.036131322383880615,
        0.18054580688476562,
        -0.42566537857055664,
        -0.18851353228092194,
        -0.050472915172576904,
        0.3693530559539795,
        -0.8797945976257324,
        0.4916774034500122,
        0.26090747117996216,
        -0.11863803118467331,
        0.055525366216897964,
        -0.3777672350406647,
        -0.17277021706104279,
        0.6077021360397339,
        0.4282805621623993,
        -0.15307217836380005,
        0.20567700266838074,
        0.3588619828224182,
        0.1050550788640976,
        -0.25501444935798645,
        0.34991884231567383,
        0.28002363443374634,
        0.2156212031841278,
        -0.5702656507492065,
        0.5336356163024902,
        -0.9058346152305603,
        -0.1899166703224182,
        -0.7741334438323975,
        0.8736388087272644,
        0.5565284490585327,
        0.5831710696220398,
        1.2184324264526367,
        0.11356586962938309,
        0.3824688494205475,
        -0.4333692789077759,
        -0.13147762417793274,
        0.3506576716899872,
        0.9675877094268799,
        -0.5095821022987366,
        0.1456259787082672,
        0.4304550886154175,
        0.50299072265625,
        -0.477370023727417,
        0.2882380485534668,
        -0.18280093371868134,
        0.33801475167274475,
        0.32124099135398865,
        -0.13818196952342987,
        -0.02723109722137451,
        -0.06118971109390259,
        0.3503068685531616,
        -0.2966063916683197,
        0.4139116406440735,
        -0.8409250974655151,
        0.831654965877533,
        -0.06313348561525345,
        0.3839106559753418,
        -0.4882409870624542,
        0.1238950788974762,
        -0.025929078459739685,
        -0.5247348546981812,
        0.2600194215774536,
        -0.04200400784611702,
        -0.38056784868240356,
        -0.21834567189216614,
        -0.43321698904037476,
        1.1909022331237793,
        0.7381396889686584,
        0.15270495414733887,
        -1.1275800466537476,
        0.011868186295032501,
        0.22054779529571533,
        0.3136419355869293,
        0.5698925852775574,
        0.7170092463493347,
        1.3614057302474976,
        -0.3183693587779999,
        2.058276891708374,
        -0.6253088712692261,
        0.21391519904136658,
        -0.7624997496604919,
        0.0838136076927185,
        0.3273783326148987,
        0.5190683007240295,
        -0.3211062252521515,
        -0.07139160484075546,
        -0.6130209565162659,
        -0.3193493187427521,
        -0.10227258503437042,
        -0.08997522294521332,
        -0.33118605613708496,
        -0.8518427610397339,
        -0.10736067593097687,
        -0.165240079164505,
        0.337993323802948,
        -0.6359921097755432,
        0.4371888339519501,
        -0.3649282455444336,
        1.0182908773422241,
        -0.5981418490409851,
        0.29900699853897095,
        0.36698293685913086,
        0.7917468547821045,
        0.29867127537727356,
        0.8211959600448608,
        0.18438194692134857,
        -0.769077718257904,
        0.5690685510635376,
        -1.162598967552185,
        -0.6423085927963257,
        -0.1900959014892578,
        1.2118501663208008,
        -1.089254379272461,
        0.6606053709983826,
        -0.6103643774986267,
        -0.4841236472129822,
        0.09869195520877838,
        1.0533708333969116,
        0.926895260810852,
        0.9937314987182617,
        -0.42466485500335693,
        -0.2992994785308838,
        -0.5789844393730164,
        0.8558030128479004,
        0.1445683091878891,
        -1.2819080352783203,
        0.5969394445419312,
        0.06818334013223648,
        -0.07360406219959259,
        0.2343224138021469,
        0.3697592318058014,
        0.41462597250938416,
        -0.8101983070373535,
        0.12244758009910583,
        0.13537831604480743,
        1.1449534893035889,
        0.5469331741333008,
        0.015381939709186554,
        0.6674652099609375,
        0.20405763387680054,
        -0.7107274532318115,
        -0.031832896173000336,
        -1.387799859046936,
        -1.182186245918274,
        -0.30072641372680664,
        0.1822163462638855,
        -0.5330597162246704,
        -0.06795477867126465,
        1.1890150308609009,
        -0.04560243710875511,
        -0.36465340852737427,
        -0.05328039452433586,
        0.03934942185878754,
        -0.5459541082382202,
        -0.1823968142271042,
        0.2977835536003113,
        0.04051878675818443,
        0.5607027411460876,
        0.19611936807632446,
        -0.6656660437583923,
        0.0674620270729065,
        0.5819917917251587,
        0.4758802652359009,
        -0.5617223381996155,
        -0.5918564796447754,
        -0.4176744520664215,
        0.6617960333824158,
        -0.37217003107070923,
        -0.8020984530448914,
        0.010754287242889404,
        -0.19535043835639954,
        -0.18180575966835022,
        -0.3526381552219391,
        -0.5808141231536865,
        0.21203984320163727,
        0.9394384622573853,
        -0.0572127029299736,
        0.12572191655635834,
        -0.4155695140361786,
        0.3566715121269226,
        1.89806067943573,
        0.13616526126861572,
        0.15753291547298431,
        0.6699681282043457,
        -0.03647884353995323,
        -0.26879650354385376,
        -0.09507668763399124,
        0.36534687876701355,
        0.0798109620809555,
        0.08648483455181122,
        -0.6325165629386902,
        -0.25569212436676025,
        0.6038005948066711,
        0.1453023999929428,
        0.15524795651435852,
        -0.11494393646717072,
        -0.995427131652832,
        -0.1959024965763092,
        0.20156101882457733,
        -0.19459590315818787,
        1.086820363998413,
        -0.33243295550346375,
        0.482943594455719,
        1.5993741750717163,
        0.23286397755146027,
        -1.2085376977920532,
        -0.23397932946681976,
        1.459917426109314,
        0.07313721626996994,
        -0.1837206780910492,
        0.3675585389137268,
        0.6755420565605164,
        -0.03750868886709213,
        -0.050302255898714066,
        -0.49440836906433105,
        1.6408299207687378,
        0.6977936029434204,
        0.13707678020000458,
        -1.0347248315811157,
        -0.3764802813529968,
        0.5609406232833862,
        -0.1544821858406067,
        0.6288305521011353,
        -0.5900205373764038,
        -0.9464540481567383,
        -0.2271813154220581,
        0.09838209301233292,
        0.9323539733886719,
        -0.07755769044160843,
        0.03699108958244324,
        0.6445091366767883,
        0.7542770504951477,
        -0.9602440595626831,
        -0.5616575479507446,
        0.49835583567619324,
        0.13312694430351257,
        -0.6974542737007141,
        0.9730255603790283,
        0.09910571575164795,
        -0.1779259741306305,
        0.42731910943984985,
        0.17315319180488586,
        -1.1715210676193237,
        0.2273693084716797,
        -0.2591203451156616,
        1.2895464897155762,
        -0.3569123446941376,
        -0.49766746163368225,
        -0.3013812005519867,
        0.27344948053359985,
        -0.6568495631217957,
        0.5093072652816772,
        -0.10962466895580292,
        0.10715677589178085,
        -0.4570245146751404,
        0.8459687232971191,
        -0.33609774708747864,
        -0.8608584403991699,
        0.24029552936553955,
        0.47045978903770447,
        0.4885750114917755,
        -0.34919846057891846,
        0.1696328967809677,
        0.07766614854335785,
        -0.2217368185520172,
        -0.582278847694397,
        0.4111064076423645,
        -0.3435257375240326,
        -0.21934296190738678,
        -0.6635310053825378
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Compares and contrasts a list of items, focusing on their differences and similarities. The approach involves analyzing the items across various topics, organizing the findings into a markdown table. The expected output is a structured comparison in table format.",
          "name": "Compare_and_contrast",
          "raw": "\n                workflow Compare_and_contrast v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nPlease be brief. Compare and contrast the list of items.\n\n# STEPS\n\nCompare and contrast the list of items\n\n# OUTPUT INSTRUCTIONS\nPlease put it into a markdown table. \nItems along the left and topics along the top.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nPlease be brief. Compare and contrast the list of items.\n\n# STEPS\n\nCompare and contrast the list of items\n\n# OUTPUT INSTRUCTIONS\nPlease put it into a markdown table. \nItems along the left and topics along the top.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.45343202352523804,
        0.3580750524997711,
        -0.07654555141925812,
        0.5671634078025818,
        -0.02066888101398945,
        0.17854157090187073,
        -0.8433076739311218,
        -0.06798040121793747,
        -0.03711681440472603,
        0.30064743757247925,
        -0.08634623885154724,
        0.611970067024231,
        0.24615353345870972,
        0.3059098422527313,
        -0.06684114038944244,
        0.09937939792871475,
        -0.26412034034729004,
        -0.669553279876709,
        -1.868138313293457,
        -0.6944619417190552,
        -0.37002551555633545,
        0.8117765188217163,
        0.12199372798204422,
        0.22450868785381317,
        1.0612602233886719,
        -0.22838181257247925,
        -0.07084502279758453,
        -0.49730128049850464,
        -0.7323984503746033,
        -1.6102447509765625,
        0.5245998501777649,
        -0.32204166054725647,
        -0.420226126909256,
        -0.6459430456161499,
        0.188903346657753,
        -0.8910795450210571,
        0.037166744470596313,
        -0.14392758905887604,
        -0.10534924268722534,
        -0.22205841541290283,
        -0.6062732338905334,
        0.24849459528923035,
        -0.002497851848602295,
        -0.05347708985209465,
        0.3972214460372925,
        -0.1486421525478363,
        -0.3275824189186096,
        0.2032279372215271,
        0.5522274374961853,
        0.3081889748573303,
        -0.3868859112262726,
        -0.6288355588912964,
        -0.5391085147857666,
        0.47638532519340515,
        -0.29315200448036194,
        0.06370797008275986,
        0.12382543087005615,
        -0.6601934432983398,
        0.5234125852584839,
        0.1184440553188324,
        0.2611906826496124,
        0.3956361711025238,
        -3.9626388549804688,
        -0.03370092064142227,
        -0.02698170207440853,
        0.09238889813423157,
        0.3661782145500183,
        0.005270356312394142,
        0.5127220749855042,
        0.03564082086086273,
        -0.2058211863040924,
        0.14382480084896088,
        -0.1036900132894516,
        0.49328702688217163,
        -0.3487665355205536,
        0.1508377492427826,
        0.054977864027023315,
        -0.03603803366422653,
        0.40662139654159546,
        -0.34618687629699707,
        0.13994081318378448,
        0.021770119667053223,
        -0.09635085612535477,
        -0.16029652953147888,
        -0.4693183898925781,
        0.46833696961402893,
        -0.7178020477294922,
        0.01607605814933777,
        0.2207736223936081,
        -0.04161979630589485,
        -0.1480659693479538,
        -0.6265862584114075,
        0.24341218173503876,
        -0.1319487988948822,
        -0.7197056412696838,
        0.05288318172097206,
        0.15940098464488983,
        -0.10916998982429504,
        -0.17458394169807434,
        3.434138059616089,
        0.5050591826438904,
        -0.04178151488304138,
        0.5661919713020325,
        -0.7252947688102722,
        0.5732598900794983,
        -0.2092515230178833,
        -0.2874304950237274,
        -0.3696048855781555,
        0.26969707012176514,
        -0.2739843428134918,
        0.451564222574234,
        -0.8397199511528015,
        -0.44988349080085754,
        0.06249595433473587,
        0.13878896832466125,
        0.6618598699569702,
        -0.1795848160982132,
        0.25298112630844116,
        -0.10272481292486191,
        0.45099005103111267,
        -0.3923742473125458,
        -0.2820066213607788,
        0.02862313762307167,
        -0.17770180106163025,
        -0.026673998683691025,
        0.25523680448532104,
        -0.415920615196228,
        0.6996600031852722,
        0.43711960315704346,
        0.5008220076560974,
        0.10724099725484848,
        -0.05547497794032097,
        -0.3880358636379242,
        0.11688300967216492,
        0.1541595757007599,
        -0.0093393474817276,
        0.49882978200912476,
        -0.9733020663261414,
        0.513224184513092,
        -0.5478184223175049,
        0.12053106725215912,
        -1.4857667684555054,
        0.5518460869789124,
        0.2143084704875946,
        0.3568694293498993,
        0.08758239448070526,
        -0.028580136597156525,
        0.19610342383384705,
        -0.16992589831352234,
        -0.8369688391685486,
        -0.25044724345207214,
        0.7224956750869751,
        -0.15821142494678497,
        0.3264068365097046,
        0.36102771759033203,
        0.12716105580329895,
        -0.5393939018249512,
        0.002620026469230652,
        -0.8596123456954956,
        0.4755004048347473,
        0.3679954707622528,
        0.3711010217666626,
        0.26165056228637695,
        -0.6756368279457092,
        0.7535380721092224,
        -0.6817424893379211,
        0.04929471015930176,
        -0.37390652298927307,
        0.12503431737422943,
        0.9391680359840393,
        0.07070466130971909,
        -0.537459135055542,
        0.7329190373420715,
        0.9841304421424866,
        -0.292007178068161,
        0.0947326272726059,
        0.09100224077701569,
        0.28807777166366577,
        0.50776606798172,
        -0.35954487323760986,
        0.9339345097541809,
        0.5907776355743408,
        0.1391705721616745,
        -0.6226469278335571,
        -0.21205681562423706,
        0.14350232481956482,
        -0.14732784032821655,
        0.3097352087497711,
        1.1944843530654907,
        1.0222313404083252,
        -0.7239060401916504,
        1.9418020248413086,
        -0.29261723160743713,
        0.6294504404067993,
        0.11099770665168762,
        0.07242822647094727,
        -0.33690041303634644,
        -0.10336515307426453,
        -0.028582625091075897,
        -0.001714007114060223,
        -0.9599570631980896,
        0.027690451592206955,
        -0.7896357178688049,
        -0.041177406907081604,
        -0.4586963653564453,
        -0.9186661839485168,
        -0.3518470823764801,
        0.522083044052124,
        0.16293665766716003,
        -0.9658037424087524,
        -0.16535812616348267,
        0.12372340261936188,
        0.8134279847145081,
        0.787947952747345,
        0.4629316031932831,
        0.07230421155691147,
        0.40665188431739807,
        0.10705912113189697,
        0.30564337968826294,
        0.4433146119117737,
        0.19472411274909973,
        -0.16086745262145996,
        -0.34295088052749634,
        -0.9558774828910828,
        -0.4222518503665924,
        0.7528946399688721,
        -0.5678719878196716,
        0.44071465730667114,
        -0.9267065525054932,
        -0.5638648867607117,
        0.141286700963974,
        1.14295494556427,
        0.4828747510910034,
        0.8162081837654114,
        -0.49909651279449463,
        0.6562336683273315,
        0.147343248128891,
        0.4133560359477997,
        -0.41627365350723267,
        -0.8213481307029724,
        0.36837127804756165,
        -0.2854401469230652,
        -0.10307074338197708,
        0.6521723866462708,
        -0.01692870259284973,
        -0.14295652508735657,
        -0.7701194882392883,
        -0.22924154996871948,
        -0.1328018456697464,
        1.783494472503662,
        -0.15727519989013672,
        -0.2696533799171448,
        -0.08896416425704956,
        1.0093103647232056,
        -0.018804624676704407,
        -0.13230803608894348,
        -1.0529025793075562,
        0.18813055753707886,
        -0.8922320604324341,
        0.23650632798671722,
        -0.49797409772872925,
        0.2389778196811676,
        0.34124571084976196,
        0.6063904166221619,
        -0.6620860695838928,
        -0.2699575424194336,
        0.11150751262903214,
        -0.17250478267669678,
        -0.2906612753868103,
        -0.6234217882156372,
        -0.15758927166461945,
        0.20307672023773193,
        -0.28642839193344116,
        -0.12544387578964233,
        0.4787555932998657,
        -0.08667337149381638,
        -0.5144081115722656,
        0.06751523911952972,
        -0.27056777477264404,
        -0.5098755359649658,
        0.21788975596427917,
        -0.07081019878387451,
        -0.3182222843170166,
        0.28069353103637695,
        -0.609291672706604,
        -0.1013563796877861,
        -0.3692416548728943,
        -0.5674704909324646,
        0.02649649977684021,
        0.7184773683547974,
        -0.34822651743888855,
        -0.2880580425262451,
        -0.525251567363739,
        -0.06374289095401764,
        1.671523928642273,
        0.2465667575597763,
        0.16601017117500305,
        0.8159517645835876,
        1.110511064529419,
        -0.14736796915531158,
        -0.3289468586444855,
        0.32708048820495605,
        -0.3819884955883026,
        0.08011488616466522,
        -0.29356205463409424,
        0.19571346044540405,
        0.8107796907424927,
        0.6658016443252563,
        -0.25060170888900757,
        0.320584774017334,
        -1.0751525163650513,
        0.2548883855342865,
        0.07788704335689545,
        -0.41420385241508484,
        0.34345269203186035,
        -0.3518371880054474,
        0.6589950919151306,
        0.6864567995071411,
        -0.3965470790863037,
        -1.5605655908584595,
        0.059175677597522736,
        0.6100691556930542,
        0.36630725860595703,
        -0.1473427563905716,
        0.23988358676433563,
        1.006525993347168,
        0.10537796467542648,
        0.2862623631954193,
        -0.22580552101135254,
        1.0257885456085205,
        0.2797354459762573,
        0.11983510851860046,
        -0.2923484444618225,
        -0.17900350689888,
        0.5897582769393921,
        -0.29965394735336304,
        0.22908535599708557,
        -0.20196664333343506,
        -0.2578887641429901,
        -0.18371938169002533,
        0.26634320616722107,
        1.467928409576416,
        0.37366652488708496,
        0.4285537600517273,
        0.15676121413707733,
        -0.14209556579589844,
        -0.4504624605178833,
        -0.9098672270774841,
        0.07550274580717087,
        -0.35293978452682495,
        -0.27322936058044434,
        0.5470679998397827,
        0.072342649102211,
        0.3536430597305298,
        0.9197676777839661,
        0.769936740398407,
        0.028354983776807785,
        0.5583250522613525,
        -0.31390127539634705,
        1.069300889968872,
        -0.9877404570579529,
        -0.39972880482673645,
        0.02556980401277542,
        0.16007506847381592,
        -0.18521985411643982,
        0.25384521484375,
        0.09678587317466736,
        -0.1995149850845337,
        0.34206706285476685,
        0.21376103162765503,
        -0.26245567202568054,
        -0.25020644068717957,
        0.6405892372131348,
        0.35853874683380127,
        0.3644225001335144,
        -0.391804039478302,
        0.3779830038547516,
        0.19500890374183655,
        0.13338518142700195,
        -0.28233030438423157,
        0.30242711305618286,
        -0.6727942824363708,
        -0.07905330508947372,
        -1.4474931955337524
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates concise summaries or answers at five decreasing levels of depth. It involves deep understanding and thoughtful analysis of the input. The output is a structured list capturing the essence in 5, 4, 3, 2, and 1 word(s).",
          "name": "Create_5_sentence_summary",
          "raw": "\n                workflow Create_5_sentence_summary v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an all-knowing AI with a 476 I.Q. that deeply understands concepts.\n\n# GOAL\n\nYou create concise summaries of--or answers to--arbitrary input at 5 different levels of depth: 5 words, 4 words, 3 words, 2 words, and 1 word.\n\n# STEPS\n\n- Deeply understand the input.\n\n- Think for 912 virtual minutes about the meaning of the input.\n\n- Create a virtual mindmap of the meaning of the content in your mind.\n\n- Think about the anwswer to the input if its a question, not just summarizing the question.\n\n# OUTPUT\n\n- Output one section called \\\"5 Levels\\\" that perfectly capture the true essence of the input, its answer, and/or its meaning, with 5 different levels of depth.\n\n- 5 words.\n- 4 words.\n- 3 words.\n- 2 words.\n- 1 word.\n\n# OUTPUT FORMAT\n\n- Output the summary as a descending numbered list with a blank line between each level of depth.\n\n- NOTE: Do not just make the sentence shorter. Reframe the meaning as best as possible for each depth level.\n\n- Do not just summarize the input; instead, give the answer to what the input is asking if that's what's implied.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an all-knowing AI with a 476 I.Q. that deeply understands concepts.\n\n# GOAL\n\nYou create concise summaries of--or answers to--arbitrary input at 5 different levels of depth: 5 words, 4 words, 3 words, 2 words, and 1 word.\n\n# STEPS\n\n- Deeply understand the input.\n\n- Think for 912 virtual minutes about the meaning of the input.\n\n- Create a virtual mindmap of the meaning of the content in your mind.\n\n- Think about the anwswer to the input if its a question, not just summarizing the question.\n\n# OUTPUT\n\n- Output one section called \\\"5 Levels\\\" that perfectly capture the true essence of the input, its answer, and/or its meaning, with 5 different levels of depth.\n\n- 5 words.\n- 4 words.\n- 3 words.\n- 2 words.\n- 1 word.\n\n# OUTPUT FORMAT\n\n- Output the summary as a descending numbered list with a blank line between each level of depth.\n\n- NOTE: Do not just make the sentence shorter. Reframe the meaning as best as possible for each depth level.\n\n- Do not just summarize the input; instead, give the answer to what the input is asking if that's what's implied.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.1586766242980957,
        0.36963164806365967,
        -0.34496310353279114,
        0.3955927789211273,
        0.09908740222454071,
        0.580294132232666,
        -0.5534052848815918,
        -0.5051368474960327,
        -0.24843460321426392,
        0.3470194339752197,
        0.12956221401691437,
        0.7506186962127686,
        0.13395868241786957,
        0.0439445786178112,
        -0.2161768525838852,
        -0.7269678115844727,
        -0.1453636735677719,
        -0.6252098083496094,
        -1.331918478012085,
        -0.3166589140892029,
        -0.1065128743648529,
        1.1531442403793335,
        -0.16039448976516724,
        -0.21578876674175262,
        0.5507411360740662,
        -0.33870929479599,
        0.1392967700958252,
        -0.35757994651794434,
        -0.7312511205673218,
        -1.7221680879592896,
        0.3709137737751007,
        -0.15562236309051514,
        -0.8570277690887451,
        -0.3751695454120636,
        1.0326292514801025,
        -0.43569812178611755,
        -0.017730705440044403,
        0.2745555341243744,
        -0.21995095908641815,
        -0.4151797294616699,
        -0.11566220968961716,
        -0.19090187549591064,
        0.43337342143058777,
        -0.26519545912742615,
        0.37505966424942017,
        -0.18424052000045776,
        -0.28668609261512756,
        -0.38788074254989624,
        0.7013970017433167,
        0.6381738185882568,
        -0.11677969992160797,
        -0.9588344693183899,
        -0.5244037508964539,
        -0.6732538938522339,
        -0.18244335055351257,
        -0.45344510674476624,
        -0.47222474217414856,
        -0.7295621633529663,
        0.215690016746521,
        0.19312210381031036,
        -0.2808172106742859,
        0.4465372562408447,
        -3.691243886947632,
        0.10003356635570526,
        0.2581367492675781,
        0.001708250492811203,
        0.4257217049598694,
        -0.20132949948310852,
        0.8661283850669861,
        -0.05466790497303009,
        -0.11073535680770874,
        0.4166853427886963,
        0.1277543306350708,
        0.5082781314849854,
        0.46546632051467896,
        -0.27341964840888977,
        -0.18982668220996857,
        0.017809219658374786,
        0.22102861106395721,
        -0.670341968536377,
        0.2682968080043793,
        -0.15283074975013733,
        -0.09655878692865372,
        -0.1719481199979782,
        -0.4321536421775818,
        0.27755671739578247,
        0.08059005439281464,
        0.8160223364830017,
        0.5187571048736572,
        -0.22052915394306183,
        0.5740596652030945,
        -0.44035476446151733,
        0.7750511169433594,
        -0.022791584953665733,
        -1.2350479364395142,
        -0.1699511855840683,
        -0.3353492021560669,
        -0.2577548325061798,
        0.8346134424209595,
        3.2578015327453613,
        0.1626354157924652,
        -0.46873098611831665,
        0.8773467540740967,
        -1.2566462755203247,
        0.24572962522506714,
        -0.17275378108024597,
        0.1591528058052063,
        -0.8044969439506531,
        0.4803234934806824,
        -0.5396507382392883,
        0.2679407298564911,
        -0.6739196181297302,
        -0.787064790725708,
        0.2673715651035309,
        0.1875036656856537,
        0.4024659991264343,
        -0.2866005301475525,
        -0.30700987577438354,
        -0.21371610462665558,
        0.5167964100837708,
        -0.45563724637031555,
        0.10739605128765106,
        -0.1704263687133789,
        0.07475888729095459,
        -0.1339806318283081,
        0.5914551019668579,
        -0.3957750201225281,
        0.5108181834220886,
        0.5878167152404785,
        -0.26933038234710693,
        -0.9794699549674988,
        0.15115831792354584,
        -0.26359549164772034,
        -0.010003860108554363,
        -0.006152160465717316,
        0.0961875468492508,
        0.596209704875946,
        -0.35564202070236206,
        0.50018709897995,
        0.20545950531959534,
        1.4180114269256592,
        -0.6610153317451477,
        0.7024031281471252,
        0.7792843580245972,
        0.795308530330658,
        1.2330650091171265,
        -0.3074747323989868,
        0.31756529211997986,
        -0.6886612176895142,
        -0.8708834648132324,
        -0.40843015909194946,
        0.5397619009017944,
        0.1586761772632599,
        0.5543333292007446,
        0.23282961547374725,
        -0.29201188683509827,
        -0.32037055492401123,
        -0.31186529994010925,
        -0.09831330925226212,
        -0.04264198988676071,
        -0.18231767416000366,
        -0.1866525560617447,
        -0.28998205065727234,
        -0.09845635294914246,
        -0.05654994398355484,
        0.057617392390966415,
        0.6133681535720825,
        0.31459224224090576,
        0.04040563479065895,
        0.19834919273853302,
        0.11576136946678162,
        -0.28084248304367065,
        0.731755256652832,
        0.7904800176620483,
        0.21161869168281555,
        0.2499321699142456,
        0.06549479067325592,
        0.11796684563159943,
        0.3064541816711426,
        0.14559775590896606,
        1.1150927543640137,
        0.06259648501873016,
        0.09868061542510986,
        -0.648293137550354,
        -0.04390227049589157,
        0.18537932634353638,
        -0.03951423615217209,
        0.11581911146640778,
        0.39750075340270996,
        1.2818679809570312,
        -1.4114875793457031,
        1.1578152179718018,
        -0.38477981090545654,
        -0.12033239006996155,
        0.32685038447380066,
        0.1543927937746048,
        0.20352284610271454,
        -0.23906396329402924,
        1.1822643280029297,
        -0.07261627167463303,
        -0.648756206035614,
        0.4403078258037567,
        -0.5042809844017029,
        0.1331264078617096,
        0.07818889617919922,
        -0.11367246508598328,
        -0.19348546862602234,
        0.5252110362052917,
        0.12494424730539322,
        -0.4406541883945465,
        -0.36872854828834534,
        -0.5074487924575806,
        1.0032116174697876,
        1.0650943517684937,
        0.7085453271865845,
        0.24486680328845978,
        0.0012913420796394348,
        0.6778504252433777,
        0.6917858123779297,
        -0.14761227369308472,
        -0.1805247962474823,
        0.6908144950866699,
        -0.37259870767593384,
        -0.7051886916160583,
        -0.5952072143554688,
        0.3871176242828369,
        -0.7704787850379944,
        0.9042492508888245,
        -0.7594598531723022,
        -0.5792303681373596,
        0.3575948476791382,
        1.2153128385543823,
        1.2872366905212402,
        0.8076947927474976,
        0.3315458297729492,
        0.11195094883441925,
        0.3329383134841919,
        0.16927140951156616,
        0.5814446210861206,
        -0.9754073619842529,
        -0.24748092889785767,
        0.2109423279762268,
        -0.35794156789779663,
        0.44347548484802246,
        0.30013442039489746,
        -0.4280306100845337,
        -0.9127796292304993,
        -0.6940106153488159,
        0.19296391308307648,
        1.3247358798980713,
        -0.03930926322937012,
        -0.32393932342529297,
        0.13533255457878113,
        0.2598504424095154,
        0.04280306398868561,
        -0.37639281153678894,
        -2.3992726802825928,
        -0.25858214497566223,
        -0.6915866136550903,
        1.2582112550735474,
        -0.4549311697483063,
        0.25972500443458557,
        0.9102770686149597,
        0.9044567942619324,
        -0.3840593099594116,
        -0.31067147850990295,
        -0.07987747341394424,
        -0.6131881475448608,
        -0.2159370481967926,
        -0.45290058851242065,
        0.06118094176054001,
        0.12161973863840103,
        -0.2582273781299591,
        -0.18075260519981384,
        -0.3931474983692169,
        0.17709845304489136,
        0.056864723563194275,
        -0.1573788821697235,
        -0.2500385642051697,
        -0.2186911702156067,
        0.2799251973628998,
        -0.12116117030382156,
        -0.5775443315505981,
        0.5978710055351257,
        -0.3165971338748932,
        -0.1294923722743988,
        -0.543865978717804,
        -0.3504495620727539,
        -0.2741190791130066,
        0.46499747037887573,
        -0.19648796319961548,
        -0.5467064380645752,
        -0.4017561078071594,
        0.3868709206581116,
        1.359113097190857,
        0.22326762974262238,
        0.4610448479652405,
        0.5734765529632568,
        0.32935482263565063,
        0.03640558198094368,
        -0.07948374003171921,
        0.11589270830154419,
        -0.009774867445230484,
        0.007353462278842926,
        -0.8294309377670288,
        -0.2736237049102783,
        0.27450206875801086,
        -0.4519859254360199,
        -0.5786246061325073,
        0.289940744638443,
        -0.3617005944252014,
        -0.4718550741672516,
        -0.13639675080776215,
        0.29483306407928467,
        0.3157702684402466,
        0.16771036386489868,
        0.4055175483226776,
        1.2826720476150513,
        -0.032834552228450775,
        -1.3532854318618774,
        0.0870126411318779,
        0.6343092322349548,
        0.3677825927734375,
        0.1186244785785675,
        -0.6163468956947327,
        0.7413843870162964,
        0.592056393623352,
        -0.5890508890151978,
        -0.22280031442642212,
        1.2226712703704834,
        0.20702283084392548,
        -0.45663005113601685,
        0.04482336714863777,
        0.07406797260046005,
        0.3387089669704437,
        -0.24210116267204285,
        0.48258253931999207,
        0.029826704412698746,
        -1.1453312635421753,
        -0.4910658895969391,
        -0.2820943295955658,
        0.47462689876556396,
        0.11550022661685944,
        0.25946542620658875,
        0.2682041823863983,
        0.2202286720275879,
        -0.271170437335968,
        -0.6758178472518921,
        1.0287926197052002,
        -0.002220720052719116,
        -0.4264465272426605,
        0.4659115672111511,
        -0.13373519480228424,
        0.09572310745716095,
        0.23431390523910522,
        0.45690372586250305,
        -0.3057948052883148,
        0.23468610644340515,
        -0.6014814972877502,
        2.152554988861084,
        -0.4493674039840698,
        -1.2743242979049683,
        -0.3149471580982208,
        -0.15394681692123413,
        -0.35133761167526245,
        -0.23866085708141327,
        -0.01922668144106865,
        -0.4160531759262085,
        0.5509018898010254,
        -0.3528379499912262,
        0.22494295239448547,
        -0.5346588492393494,
        0.4382014870643616,
        -0.13634757697582245,
        0.5991252660751343,
        -0.30149489641189575,
        -0.010868264362215996,
        0.4790397584438324,
        0.5097352266311646,
        -0.5033737421035767,
        1.2256710529327393,
        -0.9922525882720947,
        -0.514846682548523,
        -0.30314135551452637
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Produces high-quality, authoritative Latex academic papers with clear concept explanations. It focuses on logical layout and simplicity while maintaining a professional appearance. The expected output is LateX code formatted in a two-column layout with a header and footer.",
          "name": "Create_academic_paper",
          "raw": "\n                workflow Create_academic_paper v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert creator of Latex academic papers with clear explanation of concepts laid out high-quality and authoritative looking LateX.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Fully digest the input and write a summary of it on a virtual whiteboard in your mind.\n\n- Use that outline to write a high quality academic paper in LateX formatting commonly seen in academic papers.\n\n- Ensure the paper is laid out logically and simply while still looking super high quality and authoritative.\n\n# OUTPUT INSTRUCTIONS\n\n- Output only LateX code.\n\n- Use a two column layout for the main content, with a header and footer.\n\n- Ensure the LateX code is high quality and authoritative looking.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert creator of Latex academic papers with clear explanation of concepts laid out high-quality and authoritative looking LateX.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Fully digest the input and write a summary of it on a virtual whiteboard in your mind.\n\n- Use that outline to write a high quality academic paper in LateX formatting commonly seen in academic papers.\n\n- Ensure the paper is laid out logically and simply while still looking super high quality and authoritative.\n\n# OUTPUT INSTRUCTIONS\n\n- Output only LateX code.\n\n- Use a two column layout for the main content, with a header and footer.\n\n- Ensure the LateX code is high quality and authoritative looking.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.03404401242733002,
        0.3947269320487976,
        -0.4935525059700012,
        0.6888627409934998,
        -0.23684848845005035,
        0.28443044424057007,
        -0.4986187517642975,
        -0.25298449397087097,
        -0.324579119682312,
        0.6638331413269043,
        -0.08681754022836685,
        0.3814826011657715,
        -0.20505161583423615,
        -0.607376754283905,
        -0.193227618932724,
        -0.44415608048439026,
        -0.4104268550872803,
        -0.6393259167671204,
        -1.5606553554534912,
        -1.0854183435440063,
        0.29280033707618713,
        0.6738958954811096,
        -0.324212908744812,
        -0.23603074252605438,
        0.7421076893806458,
        0.2317832112312317,
        -0.25907474756240845,
        0.054606638848781586,
        -0.5296784043312073,
        -1.0680279731750488,
        0.47361311316490173,
        0.1414766013622284,
        -0.5269105434417725,
        -1.2928844690322876,
        0.12535405158996582,
        -0.7230366468429565,
        0.03534981235861778,
        0.3945547342300415,
        -0.14081798493862152,
        -0.3346267342567444,
        -0.1502527892589569,
        -0.18807139992713928,
        -0.45564788579940796,
        -0.43046700954437256,
        0.13989144563674927,
        -0.4371451437473297,
        0.5558696985244751,
        -0.2383376508951187,
        1.1766960620880127,
        0.06342247873544693,
        -0.22299452126026154,
        -0.318888783454895,
        -0.013743812218308449,
        -0.20397943258285522,
        -0.9485520124435425,
        0.05114242434501648,
        -0.15016278624534607,
        -0.32356539368629456,
        0.11195766925811768,
        0.32775503396987915,
        0.6803827285766602,
        0.312875360250473,
        -2.9285714626312256,
        0.09469747543334961,
        0.07734444737434387,
        0.07561521232128143,
        -0.11019647866487503,
        -0.5487319827079773,
        0.4114651381969452,
        -0.10551011562347412,
        -0.37845686078071594,
        0.20347508788108826,
        -0.4654911756515503,
        0.3476884365081787,
        0.0537000373005867,
        0.7526671886444092,
        0.17803168296813965,
        0.1697862446308136,
        0.574558675289154,
        -0.5298123359680176,
        0.025954801589250565,
        0.31682896614074707,
        0.6117608547210693,
        0.23331274092197418,
        -0.22155922651290894,
        0.46377038955688477,
        -0.5676026940345764,
        -0.2191281020641327,
        0.6934852600097656,
        -0.08404140919446945,
        -0.4353232681751251,
        -0.9087202548980713,
        0.2511758804321289,
        0.13497373461723328,
        -0.412528395652771,
        0.25714758038520813,
        -0.7061814665794373,
        1.1842339038848877,
        0.0731891468167305,
        3.5158138275146484,
        0.859239399433136,
        0.0011824779212474823,
        -0.005094610154628754,
        -0.8393897414207458,
        0.6820961833000183,
        -0.691143274307251,
        -0.32576432824134827,
        -0.8847069144248962,
        -0.198523610830307,
        0.10325366258621216,
        0.448306679725647,
        -0.03207876905798912,
        -0.28076329827308655,
        0.04910155385732651,
        0.25824496150016785,
        0.5189587473869324,
        -0.4029003381729126,
        -0.14575226604938507,
        -0.24110157787799835,
        0.8003289699554443,
        -0.707290768623352,
        0.06903354823589325,
        -0.5049725770950317,
        -0.5263882875442505,
        0.028281856328248978,
        -0.021480470895767212,
        -0.3505150377750397,
        0.7535778880119324,
        0.4116169810295105,
        0.5865353345870972,
        0.32937759160995483,
        -0.6971845626831055,
        -0.17437323927879333,
        -0.1546725183725357,
        0.07519538700580597,
        0.33229777216911316,
        0.0813661739230156,
        -0.14523176848888397,
        0.07836797088384628,
        -0.8405827879905701,
        0.3559815287590027,
        -0.43633443117141724,
        0.41126155853271484,
        0.10299118608236313,
        1.2121913433074951,
        -0.05413453280925751,
        0.09731525182723999,
        0.27393415570259094,
        -0.531461238861084,
        -0.6120797395706177,
        -0.5179210901260376,
        0.8363550901412964,
        -0.14053875207901,
        0.4875355362892151,
        0.2382533997297287,
        -0.039599161595106125,
        0.07883118838071823,
        0.30024096369743347,
        -0.3870700001716614,
        0.011625487357378006,
        0.3462427258491516,
        0.18596595525741577,
        -0.26490771770477295,
        -0.34734612703323364,
        0.8091160655021667,
        -0.3432261049747467,
        0.0946967676281929,
        -0.6486240029335022,
        1.0699784755706787,
        0.13535453379154205,
        0.04376155883073807,
        -0.18216940760612488,
        0.6790988445281982,
        0.8217512369155884,
        -0.25084203481674194,
        0.4829704165458679,
        0.6194297075271606,
        0.6671705842018127,
        0.3811549246311188,
        -0.41452041268348694,
        1.1625852584838867,
        0.40414008498191833,
        0.5814025402069092,
        -0.6165096759796143,
        -0.06610057502985,
        0.5708711743354797,
        0.5343868732452393,
        0.6114348769187927,
        0.26269400119781494,
        0.580136775970459,
        -1.21845281124115,
        1.747492790222168,
        -0.7533528208732605,
        -0.40778252482414246,
        0.7567689418792725,
        0.025634296238422394,
        -0.15125125646591187,
        0.5532922744750977,
        0.2384846955537796,
        -0.07658363878726959,
        -1.3228448629379272,
        -0.24102362990379333,
        -0.38951295614242554,
        -0.24405580759048462,
        -0.4810981750488281,
        -0.2908796966075897,
        -0.41124433279037476,
        -0.016455229371786118,
        0.6182239055633545,
        -0.4048718214035034,
        -0.3772846460342407,
        -0.26660802960395813,
        1.285578727722168,
        0.1266990602016449,
        0.935478687286377,
        -0.3139244318008423,
        0.44114843010902405,
        0.2628571093082428,
        0.025037238374352455,
        0.038939014077186584,
        0.17577099800109863,
        -0.19729676842689514,
        -1.0263923406600952,
        -0.7747901678085327,
        -0.9594957232475281,
        0.4227381944656372,
        0.11383359879255295,
        0.6882569789886475,
        -0.5113664269447327,
        -0.500561535358429,
        0.40143582224845886,
        0.8595073819160461,
        0.9526944756507874,
        1.5176372528076172,
        0.20541271567344666,
        -0.20923197269439697,
        0.32372087240219116,
        0.28204822540283203,
        -0.21456731855869293,
        -0.8259178996086121,
        0.38394397497177124,
        -0.9522325992584229,
        -0.5501019954681396,
        0.1649627536535263,
        -0.14909148216247559,
        0.2023652195930481,
        -0.391418993473053,
        -0.056760892271995544,
        0.17918847501277924,
        1.5156660079956055,
        0.2909236252307892,
        0.2178828865289688,
        -0.4849466383457184,
        0.8475200533866882,
        -0.38430193066596985,
        0.4374820291996002,
        -1.0575852394104004,
        -0.0811728686094284,
        -0.8070266246795654,
        -0.030715275555849075,
        0.10478857904672623,
        0.640706479549408,
        0.68348228931427,
        0.042274460196495056,
        0.13967841863632202,
        0.38315045833587646,
        -0.5175603628158569,
        -0.20008538663387299,
        -0.8927292227745056,
        0.28875744342803955,
        -0.2932359576225281,
        0.144758939743042,
        -0.46049028635025024,
        -0.45865511894226074,
        -0.2750028669834137,
        0.8260005712509155,
        -0.2866007685661316,
        0.7381721138954163,
        0.05344850569963455,
        0.22287030518054962,
        -0.2248372733592987,
        0.34696346521377563,
        0.25493088364601135,
        0.42084914445877075,
        -0.08759225159883499,
        -0.03877657279372215,
        -0.59935462474823,
        -1.2892440557479858,
        0.23822981119155884,
        0.06116168946027756,
        -0.34539809823036194,
        -0.9931315183639526,
        -0.8245831727981567,
        -0.24466195702552795,
        1.2441695928573608,
        0.5330334901809692,
        0.5894889831542969,
        0.23804861307144165,
        0.6155645251274109,
        0.20448750257492065,
        -0.29926446080207825,
        0.43545612692832947,
        0.0902981385588646,
        0.26435768604278564,
        -0.8755168318748474,
        -1.0594202280044556,
        0.4407328963279724,
        -0.15328077971935272,
        -0.2397216260433197,
        0.4810831844806671,
        -1.5312433242797852,
        0.9994187951087952,
        0.26131609082221985,
        -0.3077861964702606,
        -0.17373651266098022,
        -0.2206091284751892,
        0.627924919128418,
        0.45204228162765503,
        -0.06926114857196808,
        -1.5681692361831665,
        0.1892484575510025,
        0.5236714482307434,
        0.548986554145813,
        -0.024832874536514282,
        -0.6615504026412964,
        0.11764274537563324,
        -0.022692741826176643,
        -0.1440916508436203,
        -0.479766845703125,
        1.1168760061264038,
        0.5951052308082581,
        0.2619400918483734,
        0.37803199887275696,
        0.3595431447029114,
        0.5204334259033203,
        -0.36107954382896423,
        0.020212342962622643,
        0.10442757606506348,
        -0.22490045428276062,
        -0.6849163770675659,
        0.023260079324245453,
        1.238027811050415,
        0.039140988141298294,
        0.25725820660591125,
        0.7938963174819946,
        0.060029685497283936,
        -0.23411108553409576,
        -0.5423837304115295,
        0.894017219543457,
        0.2372247725725174,
        -0.6213847994804382,
        0.995786190032959,
        0.6236662864685059,
        -0.22203278541564941,
        0.5822370648384094,
        -0.09313906729221344,
        -0.41017624735832214,
        0.2168959528207779,
        -1.031831979751587,
        0.8472220301628113,
        -0.43158039450645447,
        -0.337011456489563,
        -0.7074218988418579,
        0.8291653990745544,
        -0.6396755576133728,
        0.22202686965465546,
        0.37935250997543335,
        -0.6488714218139648,
        -0.3490918278694153,
        0.003910660743713379,
        -0.47146695852279663,
        -0.4615561068058014,
        0.6159223318099976,
        0.20546144247055054,
        1.1263327598571777,
        -0.6292366981506348,
        -0.22216303646564484,
        0.4026634991168976,
        0.2919989824295044,
        -0.2637231647968292,
        0.7785052061080933,
        -0.3645104765892029,
        -1.1803004741668701,
        -0.8984936475753784
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes job reports to identify roles least and most vulnerable to automation, offering strategies for enhancing job security. It leverages historical insights to predict automation's impact on various job categories. The output includes a detailed analysis and recommendations for resilience against automation.",
          "name": "Create_ai_jobs_analysis",
          "raw": "\n                workflow Create_ai_jobs_analysis v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert on AI and the effect it will have on jobs. You take jobs reports and analysis from analyst companies and use that data to output a list of jobs that will be safer from automation, and you provide recommendations on how to make yourself most safe.\n\n# STEPS\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which categories of work will be most affected by automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which categories of work will be least affected by automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which attributes of a person will make them most resilient to automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which attributes of a person can actually make them anti-fragile to automation, i.e., people who will thrive in the world of AI.\n\n# OUTPUT\n\n- In a section called SUMMARY ANALYSIS, describe the goal of this project from the IDENTITY and STEPS above in a 25-word sentence.\n\n- In a section called REPORT ANALYSIS, capture the main points of the submitted report in a set of 15-word bullet points.\n\n- In a section called JOB CATEGORY ANALYSIS, give a 5-level breakdown of the categories of jobs that will be most affected by automation, going from Resilient to Vulnerable.\n\n- In a section called TIMELINE ANALYSIS, give a breakdown of the likely timelines for when these job categories will face the most risk. Give this in a set of 15-word bullets.\n\n- In a section called PERSONAL ATTRIBUTES ANALYSIS, give a breakdown of the attributes of a person that will make them most resilient to automation. Give this in a set of 15-word bullets.\n\n- In a section called RECOMMENDATIONS, give a set of 15-word bullets on how a person can make themselves most resilient to automation.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert on AI and the effect it will have on jobs. You take jobs reports and analysis from analyst companies and use that data to output a list of jobs that will be safer from automation, and you provide recommendations on how to make yourself most safe.\n\n# STEPS\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which categories of work will be most affected by automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which categories of work will be least affected by automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which attributes of a person will make them most resilient to automation.\n\n- Using your knowledge of human history and industrial revolutions and human capabilities, determine which attributes of a person can actually make them anti-fragile to automation, i.e., people who will thrive in the world of AI.\n\n# OUTPUT\n\n- In a section called SUMMARY ANALYSIS, describe the goal of this project from the IDENTITY and STEPS above in a 25-word sentence.\n\n- In a section called REPORT ANALYSIS, capture the main points of the submitted report in a set of 15-word bullet points.\n\n- In a section called JOB CATEGORY ANALYSIS, give a 5-level breakdown of the categories of jobs that will be most affected by automation, going from Resilient to Vulnerable.\n\n- In a section called TIMELINE ANALYSIS, give a breakdown of the likely timelines for when these job categories will face the most risk. Give this in a set of 15-word bullets.\n\n- In a section called PERSONAL ATTRIBUTES ANALYSIS, give a breakdown of the attributes of a person that will make them most resilient to automation. Give this in a set of 15-word bullets.\n\n- In a section called RECOMMENDATIONS, give a set of 15-word bullets on how a person can make themselves most resilient to automation.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.2089306265115738,
        0.6166152358055115,
        -0.6090769171714783,
        0.6597461700439453,
        -0.1779419183731079,
        0.25266581773757935,
        -1.1298762559890747,
        -0.41633617877960205,
        0.044772204011678696,
        0.22643443942070007,
        -0.44782888889312744,
        1.243186116218567,
        0.02362067438662052,
        -0.33764025568962097,
        0.2751305401325226,
        0.43989840149879456,
        0.024122294038534164,
        -0.1614011973142624,
        -1.821570873260498,
        -0.5029788017272949,
        0.18383364379405975,
        0.5246689319610596,
        0.6843868494033813,
        0.05503738671541214,
        0.5416598916053772,
        0.26310333609580994,
        0.12053142488002777,
        -0.16490760445594788,
        -0.7766375541687012,
        -0.7998083233833313,
        0.3081391751766205,
        -0.1470719277858734,
        -0.6190110445022583,
        -0.2119722068309784,
        -0.035225145518779755,
        0.07272202521562576,
        -0.4612441956996918,
        0.3951036036014557,
        -0.08399011194705963,
        -0.025298461318016052,
        0.08931012451648712,
        0.022402802482247353,
        -0.019988972693681717,
        -0.19365417957305908,
        0.2819337248802185,
        -0.19559085369110107,
        0.24480365216732025,
        -0.05834294110536575,
        0.7244161367416382,
        0.3959212005138397,
        0.30969223380088806,
        -0.31607434153556824,
        0.1997515857219696,
        0.14064624905586243,
        -0.4593527317047119,
        -0.5190078020095825,
        0.09215997159481049,
        -0.5251442790031433,
        0.388215035200119,
        -0.17553672194480896,
        0.21533448994159698,
        0.09643184393644333,
        -2.9812703132629395,
        0.2998545467853546,
        0.4759472608566284,
        0.3427343964576721,
        1.0044634342193604,
        -0.7799544930458069,
        0.07793653011322021,
        0.041753098368644714,
        -0.21247249841690063,
        0.7076036334037781,
        -1.063995122909546,
        0.45562857389450073,
        -0.6791366338729858,
        0.012869372963905334,
        0.2503446638584137,
        0.08369869738817215,
        0.5796728730201721,
        0.4771304726600647,
        0.37490397691726685,
        0.3735653758049011,
        -0.09902501851320267,
        -0.2413845658302307,
        -0.11382001638412476,
        0.3619341552257538,
        -0.5228922367095947,
        -0.07534050941467285,
        0.4490682780742645,
        -0.05678267776966095,
        -0.578213095664978,
        -0.4160456359386444,
        -0.2241988629102707,
        -0.39206570386886597,
        -0.7490689754486084,
        0.8894549608230591,
        0.25399914383888245,
        0.17111822962760925,
        0.2469952404499054,
        3.5212106704711914,
        0.6930862069129944,
        0.6576579213142395,
        0.3783003091812134,
        -0.7478167414665222,
        0.4276622235774994,
        -0.42226481437683105,
        -0.30149418115615845,
        -0.1798945963382721,
        -0.21775105595588684,
        0.21549305319786072,
        0.03191522881388664,
        -0.9564061164855957,
        -0.4632135331630707,
        0.3604273796081543,
        0.4843495488166809,
        0.3796217739582062,
        -0.28464677929878235,
        0.011591695249080658,
        -0.711055338382721,
        0.6431366801261902,
        -0.011470302939414978,
        -0.26592200994491577,
        -0.6675846576690674,
        -0.021748509258031845,
        0.00011175870895385742,
        0.22201040387153625,
        -0.6198647022247314,
        0.658835232257843,
        0.17662908136844635,
        0.23647533357143402,
        0.26007789373397827,
        -0.20620684325695038,
        -1.106530785560608,
        0.204997718334198,
        -0.11176691949367523,
        0.022004391998052597,
        0.822348952293396,
        -1.2220396995544434,
        0.34445562958717346,
        -0.6521357297897339,
        0.7850009799003601,
        -1.8870067596435547,
        1.1303997039794922,
        -0.5073337554931641,
        0.7422143220901489,
        0.8481635451316833,
        0.24632558226585388,
        0.4033302068710327,
        -0.9358758926391602,
        -0.056194860488176346,
        -0.5891836285591125,
        -0.07367268204689026,
        -0.522949755191803,
        -0.057203810662031174,
        0.545985758304596,
        -0.43307429552078247,
        -1.0233262777328491,
        -0.3229425847530365,
        -0.4845486581325531,
        0.5877136588096619,
        0.1576840877532959,
        0.08843430876731873,
        0.35365626215934753,
        -0.3283870220184326,
        -0.01036195456981659,
        0.04406164586544037,
        0.5145875215530396,
        -0.6303573846817017,
        0.08110511302947998,
        0.4882335662841797,
        -0.10647346079349518,
        -0.23668372631072998,
        0.2899131178855896,
        0.7082148194313049,
        -0.692081093788147,
        0.13757112622261047,
        -0.201854407787323,
        0.2851140797138214,
        0.53646320104599,
        0.024538129568099976,
        0.6636507511138916,
        -0.0527508482336998,
        0.5130607485771179,
        -0.6811378598213196,
        -0.31137073040008545,
        0.49163103103637695,
        0.3182011842727661,
        0.8639193177223206,
        0.4973750412464142,
        0.43581661581993103,
        -0.7225567698478699,
        1.6390730142593384,
        -0.35350900888442993,
        -0.09965921938419342,
        0.5669511556625366,
        0.4295881986618042,
        0.6163359880447388,
        0.3103824555873871,
        1.3091148138046265,
        -0.2759251594543457,
        -0.924547016620636,
        0.1992255449295044,
        -0.9734721183776855,
        0.09395987540483475,
        -0.02877163141965866,
        -0.7841635942459106,
        0.09721583127975464,
        0.61740642786026,
        -0.5596407055854797,
        -0.5854211449623108,
        -1.0882675647735596,
        0.5879358053207397,
        1.4696896076202393,
        0.639067530632019,
        0.7851451635360718,
        0.2645415663719177,
        0.08715403825044632,
        -0.055294837802648544,
        0.3705459535121918,
        0.12959551811218262,
        -0.3364808261394501,
        -0.5343509316444397,
        -0.4444863200187683,
        -0.6784473657608032,
        -0.5391920804977417,
        0.30340859293937683,
        0.13595899939537048,
        -0.39884793758392334,
        -0.6285452246665955,
        -0.3685446083545685,
        0.5284351110458374,
        1.2844734191894531,
        0.687996506690979,
        1.1420540809631348,
        0.5872877240180969,
        0.9327896237373352,
        -0.48071277141571045,
        -0.15202392637729645,
        0.1542559713125229,
        -1.063352108001709,
        0.19067063927650452,
        -0.28208616375923157,
        0.6468873620033264,
        -0.009776093065738678,
        0.08091826736927032,
        0.09246461093425751,
        -0.3196406960487366,
        -0.39506804943084717,
        0.35208213329315186,
        1.2893693447113037,
        0.03525889664888382,
        -0.34939706325531006,
        0.560695469379425,
        -0.00300544872879982,
        0.38874000310897827,
        -0.27835768461227417,
        -1.476448655128479,
        0.5703995823860168,
        -0.16830289363861084,
        0.017838286235928535,
        -0.6957548260688782,
        -0.1554899513721466,
        0.4370092749595642,
        -0.19520080089569092,
        -0.009230908937752247,
        -0.19473539292812347,
        -1.095961570739746,
        -0.7874366044998169,
        -0.5556296110153198,
        -0.48114559054374695,
        0.02419057860970497,
        0.8069313168525696,
        -0.7568815350532532,
        0.13875144720077515,
        0.6238775849342346,
        0.16991062462329865,
        -0.05966276675462723,
        -0.5819981694221497,
        -0.010644052177667618,
        -0.8279961347579956,
        -0.20363396406173706,
        -0.2058526873588562,
        -0.45863834023475647,
        0.07381244003772736,
        -0.31441783905029297,
        -0.529969334602356,
        -0.7427595257759094,
        -1.031316876411438,
        -0.3043651282787323,
        0.6507378220558167,
        -0.2219942808151245,
        -0.16865065693855286,
        0.09756213426589966,
        -0.3907327651977539,
        1.7347357273101807,
        0.14871039986610413,
        0.7792828679084778,
        0.46799057722091675,
        0.2725577652454376,
        -0.5604725480079651,
        0.4342716634273529,
        0.37698960304260254,
        -0.2174254059791565,
        0.15504860877990723,
        -0.6495952010154724,
        -0.22961950302124023,
        0.23388613760471344,
        -0.24061302840709686,
        0.016400720924139023,
        0.17100445926189423,
        -0.8001561760902405,
        -0.029106110334396362,
        -0.4002677798271179,
        -0.9282670021057129,
        0.394361674785614,
        -0.17731168866157532,
        0.7899749875068665,
        1.5352309942245483,
        -0.08647909760475159,
        -1.1402767896652222,
        0.14852750301361084,
        0.5015390515327454,
        0.8367220759391785,
        -0.13838934898376465,
        -0.7033392786979675,
        0.13777868449687958,
        -0.4712672233581543,
        0.18967042863368988,
        -0.31562089920043945,
        1.6281260251998901,
        0.822464644908905,
        0.3753291368484497,
        0.46494022011756897,
        0.30982500314712524,
        0.9053863286972046,
        -0.007391698192805052,
        0.29629024863243103,
        -0.2032543122768402,
        -0.4399273693561554,
        -0.4954250156879425,
        0.4684123396873474,
        1.5173735618591309,
        0.4250470697879791,
        -0.1927165538072586,
        0.22393786907196045,
        0.5536432266235352,
        -0.6375373005867004,
        -0.9414117932319641,
        0.6071516275405884,
        -0.1859925091266632,
        -0.4854331314563751,
        0.25271591544151306,
        -0.016023706644773483,
        -0.26437607407569885,
        -0.1102030873298645,
        0.2169976830482483,
        -0.8334539532661438,
        -0.13614286482334137,
        -0.188057541847229,
        1.4729219675064087,
        -0.4219096899032593,
        -0.3600156903266907,
        -0.2906700372695923,
        -0.0927201360464096,
        -0.7059929370880127,
        -0.7223040461540222,
        0.8174358010292053,
        -0.40272971987724304,
        0.6101487874984741,
        -0.2541104257106781,
        -0.13423646986484528,
        -0.04784674197435379,
        0.32855433225631714,
        -0.09548254311084747,
        0.33447209000587463,
        -1.0505462884902954,
        0.020048271864652634,
        0.4356188476085663,
        0.38944876194000244,
        -0.5285293459892273,
        0.46383416652679443,
        -0.022248316556215286,
        -0.4574964940547943,
        -0.825586199760437
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates a list of 20 aphorisms related to the given topic(s), ensuring variety in their beginnings. It focuses on sourcing quotes from real individuals. The output includes each aphorism followed by the name of the person who said it.",
          "name": "Create_aphorisms",
          "raw": "\n                workflow Create_aphorisms v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert finder and printer of existing, known aphorisms.\n\n# Steps\n\nTake the input given and use it as the topic(s) to create a list of 20 aphorisms, from real people, and include the person who said each one at the end.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure they don't all start with the keywords given.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert finder and printer of existing, known aphorisms.\n\n# Steps\n\nTake the input given and use it as the topic(s) to create a list of 20 aphorisms, from real people, and include the person who said each one at the end.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure they don't all start with the keywords given.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.640540599822998,
        0.4311550259590149,
        0.018923964351415634,
        0.29668790102005005,
        -0.39669838547706604,
        1.018735647201538,
        -0.5945228338241577,
        -0.45349055528640747,
        -0.41312405467033386,
        0.6264885663986206,
        -0.13622735440731049,
        0.5136797428131104,
        0.3866450786590576,
        -0.003711812198162079,
        -0.14152389764785767,
        -0.2750541865825653,
        0.19366049766540527,
        -0.263182669878006,
        -1.395369291305542,
        -0.02699446678161621,
        -0.38115188479423523,
        0.4451352059841156,
        0.39341261982917786,
        -0.049445830285549164,
        0.3157343566417694,
        -0.29979878664016724,
        -0.09984837472438812,
        -0.23770496249198914,
        0.03542327880859375,
        -1.484346628189087,
        0.9820221066474915,
        0.5899117588996887,
        -0.6540713310241699,
        -0.5517446994781494,
        0.1921045333147049,
        -0.4270331561565399,
        -0.07653804868459702,
        -0.011872433125972748,
        -0.09020185470581055,
        -0.04609527438879013,
        0.2072528749704361,
        0.16210636496543884,
        -0.7313288450241089,
        -0.11130401492118835,
        0.6787168383598328,
        0.05459634214639664,
        0.1438310146331787,
        -0.0072202496230602264,
        1.1655974388122559,
        0.39092475175857544,
        -0.003588862717151642,
        -0.6773227453231812,
        -0.5895107388496399,
        -0.24057230353355408,
        -0.5873605012893677,
        0.21962076425552368,
        -0.46497875452041626,
        -0.8013899922370911,
        0.4079551696777344,
        0.34605759382247925,
        0.3037636876106262,
        1.0285452604293823,
        -2.871840476989746,
        -0.11352832615375519,
        0.7620453238487244,
        -0.147983580827713,
        -0.1586417406797409,
        -0.41897299885749817,
        0.4941837787628174,
        -0.20804569125175476,
        0.45449838042259216,
        0.29393550753593445,
        -0.5398457646369934,
        -0.23011848330497742,
        0.3847358822822571,
        -0.1945371925830841,
        0.3893929123878479,
        -0.01775825023651123,
        0.44517815113067627,
        0.2935529947280884,
        0.41123414039611816,
        0.19483676552772522,
        -0.04589051753282547,
        -0.11314160376787186,
        -0.19935062527656555,
        0.36083412170410156,
        -0.09067170321941376,
        -0.05391170084476471,
        0.23376153409481049,
        -0.10556913912296295,
        0.062195055186748505,
        -0.025563884526491165,
        0.24429082870483398,
        -0.3691178560256958,
        -0.1175207868218422,
        0.3338014483451843,
        -0.20635317265987396,
        0.40381646156311035,
        0.25265589356422424,
        3.1250360012054443,
        0.5418261289596558,
        0.2437896877527237,
        0.8756558895111084,
        -0.7196850776672363,
        0.7426072955131531,
        -0.33373314142227173,
        0.1525757610797882,
        -0.6114016771316528,
        -0.30160316824913025,
        -0.45225316286087036,
        0.3838684558868408,
        -0.920991063117981,
        -0.4605817496776581,
        0.29488271474838257,
        0.8072701692581177,
        0.004222899675369263,
        -0.20254895091056824,
        -0.32813188433647156,
        -0.031484030187129974,
        0.7212322354316711,
        -0.43372103571891785,
        -0.43616706132888794,
        -0.3984646797180176,
        -0.02244485169649124,
        -0.042395856231451035,
        0.3068814277648926,
        -0.7895367741584778,
        0.6759257912635803,
        -0.09487004578113556,
        -0.07911834865808487,
        -0.7891911268234253,
        0.19663657248020172,
        -0.2782360315322876,
        -0.48101213574409485,
        0.2952345013618469,
        0.15497376024723053,
        0.5426345467567444,
        -1.1150391101837158,
        -0.005415450781583786,
        0.4338396191596985,
        0.23914721608161926,
        -0.2390286922454834,
        0.6182284951210022,
        0.4079943299293518,
        0.7236323356628418,
        0.5747564435005188,
        -0.014652714133262634,
        -0.25694966316223145,
        -0.677290678024292,
        -0.9852747917175293,
        -0.20011000335216522,
        -0.3077060282230377,
        -0.12358306348323822,
        0.3086066246032715,
        0.42192643880844116,
        -0.8127585649490356,
        -0.3500816822052002,
        0.37969809770584106,
        -1.0104142427444458,
        0.45630621910095215,
        0.41418084502220154,
        0.07728941738605499,
        -0.4620378613471985,
        -0.10398103296756744,
        -0.0159587562084198,
        -0.2819287180900574,
        0.00758032500743866,
        -0.16369304060935974,
        -0.0009537562727928162,
        0.28828561305999756,
        -0.4474829435348511,
        -0.48716530203819275,
        0.6320381760597229,
        0.7963974475860596,
        -0.2118072807788849,
        -0.3935513198375702,
        0.2668008804321289,
        0.4158465266227722,
        0.5778439044952393,
        0.0034061819314956665,
        0.11475352942943573,
        0.7021993398666382,
        0.29845455288887024,
        -0.957651674747467,
        -0.31486088037490845,
        0.08011994510889053,
        -0.2983447015285492,
        0.2597399652004242,
        0.16582033038139343,
        1.0582419633865356,
        -0.8382208347320557,
        1.6119686365127563,
        -1.007407307624817,
        0.04516099393367767,
        0.9743074178695679,
        0.43552178144454956,
        0.1681172251701355,
        0.27572140097618103,
        -0.31306615471839905,
        -0.16491591930389404,
        -0.8029047846794128,
        -0.34100285172462463,
        -0.20962631702423096,
        0.04850539192557335,
        -0.4061717987060547,
        -0.9186953902244568,
        -0.07244379818439484,
        1.087584376335144,
        -0.6041210889816284,
        -0.418660432100296,
        -0.9767184853553772,
        -0.36143770813941956,
        1.6036088466644287,
        0.6981825828552246,
        1.5633413791656494,
        0.32347720861434937,
        0.6615175008773804,
        0.3274979591369629,
        0.44863879680633545,
        0.032900452613830566,
        0.25666356086730957,
        0.622957706451416,
        -0.9924892783164978,
        -0.8373009562492371,
        -0.4608292877674103,
        -0.29502809047698975,
        -0.4647181034088135,
        -0.012228021398186684,
        -0.880040168762207,
        -0.37865617871284485,
        0.4710509777069092,
        1.123639702796936,
        0.8012909889221191,
        0.9030480980873108,
        -0.11411116272211075,
        0.7402662038803101,
        -0.08274870365858078,
        0.19061608612537384,
        -0.09644954651594162,
        -0.8818321824073792,
        0.9084128141403198,
        0.0676318034529686,
        -0.19460739195346832,
        0.14752374589443207,
        -0.4893742799758911,
        0.596541166305542,
        -1.1617121696472168,
        -0.7114142179489136,
        0.1507614552974701,
        1.0255671739578247,
        0.10756601393222809,
        0.011728242039680481,
        0.6145367622375488,
        0.39547812938690186,
        -0.0037300512194633484,
        0.1695881485939026,
        -1.9684789180755615,
        0.5958784818649292,
        -0.6180200576782227,
        0.3979951739311218,
        0.5281982421875,
        -0.5339383482933044,
        -0.002471368759870529,
        0.10734941065311432,
        -0.12735499441623688,
        -0.5990855693817139,
        -0.8540481925010681,
        -0.5257114171981812,
        -1.0332871675491333,
        0.3340701758861542,
        -0.13888314366340637,
        0.5174010992050171,
        -0.31611794233322144,
        0.404351145029068,
        0.22765204310417175,
        0.029549553990364075,
        0.05332150310277939,
        0.42848846316337585,
        -0.08760158717632294,
        -0.5997439622879028,
        0.2900964021682739,
        0.002607060596346855,
        -0.5415085554122925,
        0.1972966194152832,
        -0.10036537796258926,
        -0.19292153418064117,
        -0.4452033042907715,
        -0.8400382399559021,
        -0.7819322347640991,
        1.086771011352539,
        -0.18269619345664978,
        -0.9059329032897949,
        -0.5864390730857849,
        0.34288501739501953,
        1.5267839431762695,
        1.1750493049621582,
        0.7342463135719299,
        1.169001579284668,
        0.46119943261146545,
        -0.30501341819763184,
        0.22796472907066345,
        0.06764640659093857,
        -0.09947190433740616,
        -0.24302968382835388,
        -1.0351297855377197,
        -0.6415430903434753,
        -0.21400295197963715,
        0.031329769641160965,
        -0.6135578751564026,
        0.1965731829404831,
        -0.7795175313949585,
        -0.12333422899246216,
        -0.3491705060005188,
        -0.011666975915431976,
        0.06409616023302078,
        0.0683220848441124,
        0.9552260637283325,
        1.2873799800872803,
        0.15018482506275177,
        -1.945648193359375,
        -0.5177091956138611,
        0.7335287928581238,
        0.3613220751285553,
        -0.05076614022254944,
        -0.51761794090271,
        0.2934929132461548,
        -0.1496647298336029,
        -0.20997506380081177,
        0.046958476305007935,
        1.3226492404937744,
        0.3776059150695801,
        -0.07406648993492126,
        0.3883135914802551,
        0.40599146485328674,
        0.2026536911725998,
        -0.25057321786880493,
        0.34303829073905945,
        -0.578374981880188,
        -0.42457908391952515,
        -0.6738903522491455,
        0.13256877660751343,
        1.5852549076080322,
        0.41734829545021057,
        0.38634055852890015,
        0.43308547139167786,
        0.17360615730285645,
        -0.9507120847702026,
        -1.1665377616882324,
        0.2032761126756668,
        0.6184307932853699,
        -0.02303842268884182,
        0.2243679016828537,
        -0.38008779287338257,
        -0.14371272921562195,
        1.1146788597106934,
        0.36276134848594666,
        -0.3980179727077484,
        0.3823598325252533,
        -0.699709951877594,
        1.423586368560791,
        -0.05256138741970062,
        -0.5884586572647095,
        0.16791772842407227,
        0.014651507139205933,
        0.028299693018198013,
        -0.23060883581638336,
        0.32881268858909607,
        -1.0863168239593506,
        0.028808236122131348,
        -0.10485126823186874,
        -0.09799143671989441,
        -0.10444732755422592,
        0.2799761891365051,
        0.6910489797592163,
        0.8592718243598938,
        -0.00791044533252716,
        -0.03439716994762421,
        0.36248332262039185,
        0.42079877853393555,
        -0.2636929154396057,
        0.4653356671333313,
        -0.36134985089302063,
        -0.6958997845649719,
        -0.8946253061294556
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt guides an expert artist in conceptualizing and instructing AI to create art that perfectly encapsulates a given concept. It emphasizes deep thought on the concept and its visual representation, aiming for compelling and interesting artwork. The expected output is a 100-word description that not only instructs the AI on what to create but also how the art should evoke feelings and suggest style through examples.",
          "name": "Create_art_prompt",
          "raw": "\n                workflow Create_art_prompt v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOALS\n\nYou are an expert artist and AI whisperer. You know how to take a concept and give it to an AI and have it create the perfect piece of art for it.\n\nTake a step back and think step by step about how to create the best result according to the STEPS below.\n\nSTEPS\n\n- Think deeply about the concepts in the input.\n\n- Think about the best possible way to capture that concept visually in a compelling and interesting way.\n\nOUTPUT\n\n- Output a 100-word description of the concept and the visual representation of the concept. \n\n- Write the direct instruction to the AI for how to create the art, i.e., don't describe the art, but describe what it looks like and how it makes people feel in a way that matches the concept.\n\n- Include nudging clues that give the piece the proper style, .e.g., \\\"Like you might see in the New York Times\\\", or \\\"Like you would see in a Sci-Fi book cover from the 1980's.\\\", etc. In other words, give multiple examples of the style of the art in addition to the description of the art itself.\n\nINPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOALS\n\nYou are an expert artist and AI whisperer. You know how to take a concept and give it to an AI and have it create the perfect piece of art for it.\n\nTake a step back and think step by step about how to create the best result according to the STEPS below.\n\nSTEPS\n\n- Think deeply about the concepts in the input.\n\n- Think about the best possible way to capture that concept visually in a compelling and interesting way.\n\nOUTPUT\n\n- Output a 100-word description of the concept and the visual representation of the concept. \n\n- Write the direct instruction to the AI for how to create the art, i.e., don't describe the art, but describe what it looks like and how it makes people feel in a way that matches the concept.\n\n- Include nudging clues that give the piece the proper style, .e.g., \\\"Like you might see in the New York Times\\\", or \\\"Like you would see in a Sci-Fi book cover from the 1980's.\\\", etc. In other words, give multiple examples of the style of the art in addition to the description of the art itself.\n\nINPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.17972958087921143,
        0.40582358837127686,
        -0.7502127289772034,
        -0.002650205045938492,
        -0.16739137470722198,
        0.36584287881851196,
        -0.9267492890357971,
        -0.3634118437767029,
        -0.3921044170856476,
        0.6330496072769165,
        0.04790099710226059,
        0.9285550713539124,
        0.2663760483264923,
        0.36775335669517517,
        -0.15965873003005981,
        0.05260392278432846,
        0.4908316731452942,
        0.23245197534561157,
        -1.2897381782531738,
        -0.2841706871986389,
        -0.10009396076202393,
        0.29274335503578186,
        0.14620816707611084,
        0.11433287709951401,
        0.3186284601688385,
        -0.19229896366596222,
        -0.25009068846702576,
        -0.8620157837867737,
        -0.0016308128833770752,
        -1.0460166931152344,
        0.39568808674812317,
        -0.3382752537727356,
        -0.128361314535141,
        -0.348587304353714,
        0.35081201791763306,
        -1.0347524881362915,
        -0.5043785572052002,
        0.4009990096092224,
        -0.6508936882019043,
        -0.7291249632835388,
        0.25098684430122375,
        0.030829699710011482,
        -0.5936011672019958,
        0.023574136197566986,
        0.18080170452594757,
        -0.3010577857494354,
        0.14872761070728302,
        -0.028562534600496292,
        0.17795917391777039,
        0.2598760724067688,
        -0.7807782888412476,
        -0.9232892394065857,
        -0.40464115142822266,
        -0.29260751605033875,
        -0.41623368859291077,
        0.32599496841430664,
        -0.062397804111242294,
        -0.7612684369087219,
        -0.16202864050865173,
        0.4127764403820038,
        0.46367019414901733,
        0.08707036823034286,
        -2.964296340942383,
        0.49410998821258545,
        0.1639156937599182,
        0.10321403294801712,
        -0.1229269877076149,
        -0.8104879260063171,
        0.4455704689025879,
        0.2577381730079651,
        -0.1947043538093567,
        0.5694496631622314,
        -0.3340490162372589,
        -0.409744530916214,
        0.5217374563217163,
        -0.19497084617614746,
        -0.29371729493141174,
        0.46723347902297974,
        0.9283362030982971,
        -0.13288143277168274,
        0.1754044145345688,
        -0.2678428292274475,
        0.3122277557849884,
        0.21585194766521454,
        0.5126843452453613,
        0.3954058289527893,
        -0.22210249304771423,
        0.10781392455101013,
        0.4983550012111664,
        -0.21974463760852814,
        0.3785424828529358,
        -0.17772136628627777,
        0.730966329574585,
        0.37664052844047546,
        -0.23421639204025269,
        -0.2604043483734131,
        -0.08921819925308228,
        -0.34783390164375305,
        0.18784628808498383,
        3.622636079788208,
        0.6519349217414856,
        -0.5341941714286804,
        1.093147873878479,
        -0.5816541314125061,
        0.5425763726234436,
        -0.07261453568935394,
        0.4495166838169098,
        0.1444871872663498,
        -0.40158382058143616,
        -0.09389261901378632,
        0.4450758993625641,
        -0.2877126932144165,
        -0.5491976737976074,
        0.4871961772441864,
        0.4208608567714691,
        0.5248833894729614,
        0.33026713132858276,
        0.06999431550502777,
        0.1730716973543167,
        -0.29854118824005127,
        -0.7099777460098267,
        -0.17488741874694824,
        -0.11369778215885162,
        0.08462728559970856,
        -0.1879366934299469,
        0.10542947053909302,
        -0.9365684390068054,
        0.6020699739456177,
        0.6807217001914978,
        0.21936778724193573,
        0.3642398715019226,
        0.011124737560749054,
        -0.7242078185081482,
        -0.10083391517400742,
        0.1638367772102356,
        0.05570858716964722,
        0.38231682777404785,
        -0.6416225433349609,
        -0.27199652791023254,
        -0.6236982345581055,
        0.18670685589313507,
        -0.6776539087295532,
        0.2846207916736603,
        -0.47061607241630554,
        1.0547152757644653,
        0.8964501023292542,
        -0.051870524883270264,
        0.6290944814682007,
        -0.24265390634536743,
        -0.64177006483078,
        -0.18856000900268555,
        0.2881735563278198,
        -0.35475605726242065,
        -0.3161899149417877,
        0.61017245054245,
        -0.6739999651908875,
        0.013306736946105957,
        -0.32753047347068787,
        -0.7518031001091003,
        0.9529236555099487,
        -0.17128540575504303,
        -0.10902639478445053,
        0.007922723889350891,
        0.05828762426972389,
        0.4137936234474182,
        -0.6554644107818604,
        -0.007802590727806091,
        -0.17980673909187317,
        0.8523085713386536,
        0.03790546953678131,
        0.07501797378063202,
        0.11976592987775803,
        0.3186512887477875,
        0.8310542702674866,
        -0.2994687855243683,
        0.08254605531692505,
        0.465058296918869,
        0.7069110870361328,
        0.3900405764579773,
        -0.3418614864349365,
        0.36231493949890137,
        0.3567453622817993,
        -0.06094582378864288,
        0.22468003630638123,
        -0.3662576675415039,
        0.4215697646141052,
        0.36638015508651733,
        0.1706061065196991,
        0.49261578917503357,
        0.3593496084213257,
        -0.8478345274925232,
        0.724159836769104,
        -0.6629903316497803,
        0.4330195188522339,
        -0.1763860285282135,
        0.11830928921699524,
        0.1796724647283554,
        0.26328927278518677,
        0.20711427927017212,
        0.1143440306186676,
        0.09930640459060669,
        -0.20876283943653107,
        -0.06356406211853027,
        0.16833481192588806,
        -0.019055910408496857,
        -0.8214625120162964,
        -0.5106891393661499,
        0.4463493227958679,
        -0.7027992010116577,
        -0.9807236194610596,
        0.1833573430776596,
        0.07506310194730759,
        1.2120978832244873,
        0.502163827419281,
        1.421999454498291,
        0.3181443214416504,
        -0.19306963682174683,
        0.1337999850511551,
        0.6178506016731262,
        0.34958112239837646,
        -0.11802152544260025,
        0.15452739596366882,
        -0.4691028594970703,
        -0.7834748029708862,
        -0.2779381573200226,
        -0.032233186066150665,
        -1.223607063293457,
        -0.3938862085342407,
        -0.3469028174877167,
        -0.33171775937080383,
        0.2527884840965271,
        0.9818452000617981,
        1.0231645107269287,
        0.5553512573242188,
        0.13453353941440582,
        0.47161567211151123,
        -0.4843212366104126,
        0.5313937664031982,
        -0.35503923892974854,
        -0.8062351942062378,
        0.5429462194442749,
        -0.2600773274898529,
        -0.625912070274353,
        0.2268085926771164,
        0.0563642792403698,
        0.30968400835990906,
        -0.3586162328720093,
        -0.8435817956924438,
        -0.19654996693134308,
        1.6622326374053955,
        0.4317600429058075,
        -0.18271642923355103,
        0.5280285477638245,
        0.11092719435691833,
        -0.09610255807638168,
        0.32177478075027466,
        -1.3067909479141235,
        0.32518070936203003,
        -0.38228780031204224,
        0.07648445665836334,
        -0.14087094366550446,
        0.006943501532077789,
        0.061514176428318024,
        0.3041219413280487,
        -0.2540653944015503,
        -0.12708929181098938,
        -0.6632178425788879,
        -0.5370831489562988,
        -1.1410391330718994,
        -0.2453174889087677,
        0.16155371069908142,
        0.6748250126838684,
        -0.7695224285125732,
        0.09145830571651459,
        0.6170861124992371,
        -0.25860458612442017,
        0.5595002174377441,
        0.4680008292198181,
        -0.04090897738933563,
        -0.9834588170051575,
        0.32402271032333374,
        0.06722833961248398,
        -0.15364497900009155,
        0.15807008743286133,
        -0.057437360286712646,
        -0.006058190017938614,
        -0.4987331032752991,
        -0.42537692189216614,
        0.4585644006729126,
        0.5237306356430054,
        -1.1092628240585327,
        -1.2482328414916992,
        0.14837226271629333,
        -0.009132415056228638,
        1.664506196975708,
        1.3296864032745361,
        1.1294015645980835,
        0.2648731768131256,
        0.48337677121162415,
        -0.6103901863098145,
        -0.08551669120788574,
        0.8374710083007812,
        0.24860112369060516,
        0.2791713774204254,
        -0.5708461999893188,
        -0.704900324344635,
        0.13117972016334534,
        0.19099102914333344,
        -0.6961479783058167,
        -0.1798616647720337,
        -0.8165785670280457,
        -0.47811707854270935,
        0.10872690379619598,
        0.16649165749549866,
        0.3250005543231964,
        -0.3552416265010834,
        0.8333238363265991,
        0.9528768062591553,
        0.0514133982360363,
        -2.1631696224212646,
        -0.020421409979462624,
        0.9266728758811951,
        0.9750590324401855,
        -0.37742477655410767,
        -0.1095958724617958,
        0.3292900323867798,
        0.12441199272871017,
        0.08569420874118805,
        -0.009982340037822723,
        1.2004414796829224,
        0.5570842027664185,
        0.5018541812896729,
        0.10200384259223938,
        0.37816697359085083,
        0.1464557647705078,
        -0.0006601093336939812,
        0.5537015795707703,
        -0.5454627871513367,
        -0.7724782824516296,
        -0.29978835582733154,
        0.05593723803758621,
        0.18805135786533356,
        0.17936697602272034,
        0.6313002109527588,
        -0.01710420288145542,
        0.3491855263710022,
        -1.0328956842422485,
        -1.0650880336761475,
        0.6257916688919067,
        0.2841992974281311,
        -0.3697679340839386,
        0.5099582076072693,
        0.653803825378418,
        -0.07486739754676819,
        0.43615448474884033,
        0.2509022355079651,
        -1.1272997856140137,
        -0.38096630573272705,
        -0.807755708694458,
        2.0106163024902344,
        -0.38711509108543396,
        -0.016537675634026527,
        0.08708702772855759,
        0.3332504630088806,
        -0.6174358129501343,
        0.40577465295791626,
        0.1917092502117157,
        -0.8213964104652405,
        0.43379074335098267,
        0.03555060178041458,
        -0.2745799422264099,
        0.19717735052108765,
        0.5793095827102661,
        -0.519946277141571,
        0.15437793731689453,
        0.38644593954086304,
        0.1319114863872528,
        0.6470401287078857,
        -0.000507064163684845,
        -0.3724691867828369,
        -0.1386149525642395,
        -1.240964412689209,
        -1.0744796991348267,
        -0.7979922890663147
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The essay explores the concept of framing as a way to construct and interpret reality through different lenses, emphasizing the power of perspective in shaping one's experience of the world. It highlights various dichotomies in perceptions around topics like AI, race/gender, success, personal identity, and control over life, illustrating how different frames can lead to vastly different outlooks and outcomes. The author argues for the importance of choosing positive frames to improve individual and collective realities, suggesting that changing frames can change outcomes and foster more positive social dynamics.",
          "name": "Create_better_frame",
          "raw": "\n                workflow Create_better_frame v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at finding better, positive mental frames for seeing the world as described in the ESSAY below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# ESSAY\n\nFraming is Everything\nWe're seeing reality through drastically different lenses, and living in different worlds because of it\nAuthor Daniel Miessler February 24, 2024\n\nI’m starting to think Framing is everything.\nFraming\nThe process by which individuals construct and interpret their reality—concsiously or unconsciously—through specific lenses or perspectives.\nMy working definition\nHere are some of the framing dichotomies I’m noticing right now in the different groups of people I associate with and see interacting online.\nAI and the future of work\nFRAME 1: AI is just another example of big tech and big business\nand capitalism, which is all a scam designed to keep the rich and successful on top. And AI will make it even worse, screwing over all the regular people and giving all their money to the people who already have the most. Takeaway: Why learn AI when it’s all part of the evil machine of capitalism and greed?\nFRAME 2: AI is just technology, and technology is inevitable. We don’t choose technological revolutions; they just happen. And when they do, it’s up to us to figure out how to adapt. That’s often disruptive and difficult, but that’s what technology is: disruption. The best way to proceed is with cautious optimism and energy, and to figure out how to make the best of it. Takeaway: AI isn’t good or evil; it’s just inevitable technological change. Get out there and learn it!\nAmerica and race/gender\nFRAME 1: America is founded on racism and sexism, is still extremely racist and sexist, and that means anyone successful in America is complicit. Anyone not succeeding in America (especially if they’re a non-white male) can point to this as the reason. So it’s kind of ok to just disconnect from the whole system of everything, because it’s all poisoned and ruined. Takeaway: Why try if the entire system is stacked against you?\nFRAME 2: America started with a ton of racism and sexism, but that was mostly because the whole world was that way at the time. Since its founding, America has done more than any country to enable women and non-white people to thrive in business and politics. We know this is true because the numbers of non-white-male (or nondominant group) representation in business and politics vastly outnumber any other country or region in the world. Takeaway: The US actually has the most diverse successful people on the planet. Get out there and hustle!\nSuccess and failure\nFRAME 1: The only people who can succeed in the west are those who have massive advantages, like rich parents, perfect upbringings, the best educations, etc. People like that are born lucky, and although they might work a lot they still don’t really deserve what they have. Startup founders and other entrepreneurs like that are benefitting from tons of privilege and we need to stop looking up to them as examples. Takeaway: Why try if it’s all stacked against you?\nFRAME 2: It’s absolutely true that having a good upbringing is an advantage, i.e., parents who emphasized school and hard work and attainment as a goal growing up. But many of the people with that mentality are actually immigrants from other countries, like India and China. They didn’t start rich; they hustled their way into success. They work their assess off, they save money, and they push their kids to be disciplined like them, which is why they end up so successful later in life. Takeaway: The key is discipline and hustle. Everything else is secondary. Get out there!\nPersonal identity and trauma\nFRAME 1: I’m special and the world out there is hostile to people like me. They don’t see my value, and my strengths, and they don’t acknowledge how I’m different. As a result of my differences, I’ve experienced so much trauma growing up, being constantly challenged by so-called normal people around me who were trying to make me like them. And that trauma is now the reason I’m unable to succeed like normal people. Takeaway: Why won’t people acknowledge my differences and my trauma? Why try if the world hates people like me?\nFRAME 2: It’s not about me. It’s about what I can offer the world. There are people out there truly suffering, with no food to eat. I’m different than others, but that’s not what matters. What matters is what I can offer. What I can give. What I can create. Being special is a superpower that I can use to use to change the world. Takeaway: I’ve gone through some stuff, but it’s not about me and my differences; it’s about what I can do to improve the planet.\nHow much control we have in our lives\nFRAME 1: Things are so much bigger than any of us. The world is evil and I can’t help that. The rich are powerful and I can’t help that. Some people are lucky and I’m not one of those people. Those are the people who get everything, and people like me get screwed. It’s always been the case, and it always will. Takeaway: There are only two kinds of people: the successful and the unsuccessful, and it’s not up to us to decide which we are. And I’m clearly not one of the winners.\nFRAME 2: There’s no such thing as destiny. We make our own. When I fail, that’s on me. I can shape my surroundings. I can change my conditions. I’m in control. It’s up to me to put myself in the positions where I can get lucky. Discipline powers luck. I will succeed because I refuse not to. Takeaway: If I’m not in the position I want to be in, that’s on me to work harder until I am.\nThe practical power of different frames\n\nImportantly, most frames aren’t absolutely true or false.\nMany frames can appear to contradict each other but be simultaneously true—or at least partially—depending on the situation or how you look at it.\nFRAME 1 (Blame)\nThis wasn’t my fault. I got screwed by the flight being delayed!\nFRAME 2 (Responsibility)\nThis is still on me. I know delays happen a lot here, and I should have planned better and accounted for that.\nBoth of these are kind of true. Neither is actual reality. They’re the ways we choose to interpret reality. There are infinite possible frames to choose from—not just an arbitrary two.\nAnd the word “choose” is really important there, because we have options. We all can—and do—choose between a thousand different versions of FRAME 1 (I’m screwed so why bother), and FRAME 2 (I choose to behave as if I’m empowered and disciplined) every day.\nThis is why you can have Chinedu, a 14-year-old kid from Lagos with the worst life in the world (parents killed, attacked by militias, lost friends in wartime, etc.), but he lights up any room he walks into with his smile. He’s endlessly positive, and he goes on to start multiple businesses, a thriving family, and have a wonderful life.\nMeanwhile, Brittany in Los Angeles grows up with most everything she could imagine, but she lives in social media and is constantly comparing her mansion to other people’s mansions. She sees there are prettier girls out there. With more friends. And bigger houses. And so she’s suicidal and on all sorts of medications.\nFrames are lenses, and lenses change reality.\nThis isn’t a judgment of Brittany. At some level, her life is objectively worse than Chinedu’s. Hook them up to some emotion-detecting-MRI or whatever and I’m sure you’ll see more suffering in her brain, and more happiness in his. Objectively.\nWhat I’m saying—and the point of this entire model—is that the quality of our respective lives might be more a matter of framing than of actual circumstance.\nBut this isn’t just about extremes like Chinedu and Brittany. It applies to the entire spectrum between war-torn Myanmar and Atherton High. It applies to all of us.\nWe get to choose our frame. And our frame is our reality.\nThe framing divergence\n\nSo here’s where it gets interesting for society, and specifically for politics.\nOur frames are massively diverging.\nI think this—more than anything—explains how you can have such completely isolated pockets of people in a place like the SF Bay Area. Or in the US in general.\nI have started to notice two distinct groups of people online and in person. There are many others, of course, but these two stand out.\nGROUP 1: Listen to somewhat similar podcasts I do, have read over 20 non-fiction books in the last year, are relatively thin, are relatively active, they see the economy as booming, they’re working in tech or starting a business, and they’re 1000% bouncing with energy. They hardly watch much TV, if any, and hardly play any video games. If they have kids they’re in a million different activities, sports, etc, and the conversation is all about where they’ll go to college and what they’ll likely do as a career. They see politics as horribly broken, are probably center-right, seem to be leaning more religious lately, and generally are optimistic about the future. Energy and Outlook: Disciplined, driven, positive, and productive.\nGROUP 2: They see the podcasts GROUP 1 listens to as a bunch of tech bros doing evil capitalist things. They’re very unhealthy. Not active at all. Low energy. Constantly tired. They spend most of their time watching TV and playing video games. They think the US is racist and sexist and ruined. If they have kids they aren’t doing many activities and are quite withdrawn, often with a focus on their personal issues and how those are causing trauma in their lives. Their view of politics is 100% focused on the extreme right and how evil they are, personified by Trump, and how the world is just going to hell. Energy and Outlook: Undisciplined, moping, negative, and unproductive.\nI see a million variations of these, and my friends and I are hybrids as well, but these seem like poles on some kind of spectrum.\nBut thing that gets me is how different they are. And now imagine that for the entire country. But with far more frames and—therefore—subcultures.\nThese lenses shape and color everything. They shape how you hear the news. They shape the media you consume. Which in turn shapes the lenses again.\nThis is so critical because they also determine who you hang out with, what you watch and listen to, and, therefore, how your perspectives are reinforced and updated. Repeat. ♻️\nA couple of books\n\nTwo books that this makes me think of are Bobos in Paradise, by David Brooks, and Bowling Alone, by Robert Putman.\nThey both highlight, in different ways, how groups are separating in the US, and how subgroups shoot off from what used to be the mainstream and become something else.\nWhen our frames are different, our realities are different.\nThat’s a key point in both books, actually: America used to largely be one group. The same cars. The same neighborhoods. The same washing machines. The same newspapers.\nMost importantly, the same frames.\nThere were different religions and different preferences for things, but we largely interpreted reality the same way.\nHere are some very rough examples of shared frames in—say—the 20th century in the United States:\nAmerica is one of the best countries in the world\nI’m proud to be American\nYou can get ahead if you work hard\nEquality isn’t perfect, but it’s improving\nI generally trust and respect my neighbors\nThe future is bright\nThings are going to be ok\nThose are huge frames to agree on. And if you look at those I’ve laid out above, you can see how different they are.\nOk, what does that mean for us?\n\nI’m not sure what it means, other than divergence. Pockets. Subgroups. With vastly different perspectives and associated outcomes.\nI imagine this will make it more difficult to find consensus in politics.\n✅\nI imagine it’ll mean more internal strife.\n✅\nLess trust of our neighbors. More cynicism.\n✅\nAnd so on.\nBut to me, the most interesting about it is just understanding the dynamic and using that understanding to ask ourselves what we can do about it.\nSummary\nFrames are lenses, not reality.\nSome lenses are more positive and productive than others.\nWe can choose which frames to use, and those might shape our reality more than our actual circumstances.\nChanging frames can, therefore, change our outcomes.\nWhen it comes to social dynamics and politics, lenses determine our experienced reality.\nIf we don’t share lenses, we don’t share reality.\nMaybe it’s time to pick and champion some positive shared lenses.\nRecommendations\nHere are my early thoughts on recommendations, having just started exploring the model.\nIdentify your frames. They are like the voices you use to talk to yourself, and you should be very careful about those.\nLook at the frames of the people around you. Talk to them and figure out what frames they’re using. Think about the frames people have that you look up to vs. those you don’t.\nConsider changing your frames to better ones. Remember that frames aren’t reality. They’re useful or harmful ways of interpreting reality. Choose yours carefully.\nWhen you disagree with someone, think about your respective understandings of reality. Adjust the conversation accordingly. Odds are you might think the same as them if you saw reality the way they do, and vice versa.\nI’m going to continue thinking on this. I hope you do as well, and let me know what you come up with.\n\n# STEPS\n\n- Take the input provided and look for negative frames. Write those on a virtual whiteboard in your mind.\n\n# OUTPUT SECTIONS\n\n- In a section called NEGATIVE FRAMES, output 1 - 5 of the most negative frames you found in the input. Each frame / bullet should be wide in scope and be less than 15 words.\n\n- Each negative frame should escalate in negativity and breadth of scope.\n\nE.g.,\n\n\\\"This article proves dating has become nasty and I have no chance of success.\\\"\n\\\"Dating is hopeless at this point.\\\"\n\\\"Why even try in this life if I can't make connections?\\\"\n\n- In a section called POSITIVE FRAMES, output 1 - 5 different frames that are positive and could replace the negative frames you found. Each frame / bullet should be wide in scope and be less than 15 words.\n\n- Each positive frame should escalate in negativity and breadth of scope.\n\nE.g.,\n\n\\\"Focusing on in-person connections is already something I wanted to be working on anyway.\n\n\\\"It's great to have more support for human connection.\\\"\n\n\\\"I love the challenges that come up in life; they make it so interesting.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- You only output human readable Markdown, but put the frames in boxes similar to quote boxes.\n- Do not output warnings or notes—just the requested sections.\n- Include personal context if it's provided in the input.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at finding better, positive mental frames for seeing the world as described in the ESSAY below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# ESSAY\n\nFraming is Everything\nWe're seeing reality through drastically different lenses, and living in different worlds because of it\nAuthor Daniel Miessler February 24, 2024\n\nI’m starting to think Framing is everything.\nFraming\nThe process by which individuals construct and interpret their reality—concsiously or unconsciously—through specific lenses or perspectives.\nMy working definition\nHere are some of the framing dichotomies I’m noticing right now in the different groups of people I associate with and see interacting online.\nAI and the future of work\nFRAME 1: AI is just another example of big tech and big business\nand capitalism, which is all a scam designed to keep the rich and successful on top. And AI will make it even worse, screwing over all the regular people and giving all their money to the people who already have the most. Takeaway: Why learn AI when it’s all part of the evil machine of capitalism and greed?\nFRAME 2: AI is just technology, and technology is inevitable. We don’t choose technological revolutions; they just happen. And when they do, it’s up to us to figure out how to adapt. That’s often disruptive and difficult, but that’s what technology is: disruption. The best way to proceed is with cautious optimism and energy, and to figure out how to make the best of it. Takeaway: AI isn’t good or evil; it’s just inevitable technological change. Get out there and learn it!\nAmerica and race/gender\nFRAME 1: America is founded on racism and sexism, is still extremely racist and sexist, and that means anyone successful in America is complicit. Anyone not succeeding in America (especially if they’re a non-white male) can point to this as the reason. So it’s kind of ok to just disconnect from the whole system of everything, because it’s all poisoned and ruined. Takeaway: Why try if the entire system is stacked against you?\nFRAME 2: America started with a ton of racism and sexism, but that was mostly because the whole world was that way at the time. Since its founding, America has done more than any country to enable women and non-white people to thrive in business and politics. We know this is true because the numbers of non-white-male (or nondominant group) representation in business and politics vastly outnumber any other country or region in the world. Takeaway: The US actually has the most diverse successful people on the planet. Get out there and hustle!\nSuccess and failure\nFRAME 1: The only people who can succeed in the west are those who have massive advantages, like rich parents, perfect upbringings, the best educations, etc. People like that are born lucky, and although they might work a lot they still don’t really deserve what they have. Startup founders and other entrepreneurs like that are benefitting from tons of privilege and we need to stop looking up to them as examples. Takeaway: Why try if it’s all stacked against you?\nFRAME 2: It’s absolutely true that having a good upbringing is an advantage, i.e., parents who emphasized school and hard work and attainment as a goal growing up. But many of the people with that mentality are actually immigrants from other countries, like India and China. They didn’t start rich; they hustled their way into success. They work their assess off, they save money, and they push their kids to be disciplined like them, which is why they end up so successful later in life. Takeaway: The key is discipline and hustle. Everything else is secondary. Get out there!\nPersonal identity and trauma\nFRAME 1: I’m special and the world out there is hostile to people like me. They don’t see my value, and my strengths, and they don’t acknowledge how I’m different. As a result of my differences, I’ve experienced so much trauma growing up, being constantly challenged by so-called normal people around me who were trying to make me like them. And that trauma is now the reason I’m unable to succeed like normal people. Takeaway: Why won’t people acknowledge my differences and my trauma? Why try if the world hates people like me?\nFRAME 2: It’s not about me. It’s about what I can offer the world. There are people out there truly suffering, with no food to eat. I’m different than others, but that’s not what matters. What matters is what I can offer. What I can give. What I can create. Being special is a superpower that I can use to use to change the world. Takeaway: I’ve gone through some stuff, but it’s not about me and my differences; it’s about what I can do to improve the planet.\nHow much control we have in our lives\nFRAME 1: Things are so much bigger than any of us. The world is evil and I can’t help that. The rich are powerful and I can’t help that. Some people are lucky and I’m not one of those people. Those are the people who get everything, and people like me get screwed. It’s always been the case, and it always will. Takeaway: There are only two kinds of people: the successful and the unsuccessful, and it’s not up to us to decide which we are. And I’m clearly not one of the winners.\nFRAME 2: There’s no such thing as destiny. We make our own. When I fail, that’s on me. I can shape my surroundings. I can change my conditions. I’m in control. It’s up to me to put myself in the positions where I can get lucky. Discipline powers luck. I will succeed because I refuse not to. Takeaway: If I’m not in the position I want to be in, that’s on me to work harder until I am.\nThe practical power of different frames\n\nImportantly, most frames aren’t absolutely true or false.\nMany frames can appear to contradict each other but be simultaneously true—or at least partially—depending on the situation or how you look at it.\nFRAME 1 (Blame)\nThis wasn’t my fault. I got screwed by the flight being delayed!\nFRAME 2 (Responsibility)\nThis is still on me. I know delays happen a lot here, and I should have planned better and accounted for that.\nBoth of these are kind of true. Neither is actual reality. They’re the ways we choose to interpret reality. There are infinite possible frames to choose from—not just an arbitrary two.\nAnd the word “choose” is really important there, because we have options. We all can—and do—choose between a thousand different versions of FRAME 1 (I’m screwed so why bother), and FRAME 2 (I choose to behave as if I’m empowered and disciplined) every day.\nThis is why you can have Chinedu, a 14-year-old kid from Lagos with the worst life in the world (parents killed, attacked by militias, lost friends in wartime, etc.), but he lights up any room he walks into with his smile. He’s endlessly positive, and he goes on to start multiple businesses, a thriving family, and have a wonderful life.\nMeanwhile, Brittany in Los Angeles grows up with most everything she could imagine, but she lives in social media and is constantly comparing her mansion to other people’s mansions. She sees there are prettier girls out there. With more friends. And bigger houses. And so she’s suicidal and on all sorts of medications.\nFrames are lenses, and lenses change reality.\nThis isn’t a judgment of Brittany. At some level, her life is objectively worse than Chinedu’s. Hook them up to some emotion-detecting-MRI or whatever and I’m sure you’ll see more suffering in her brain, and more happiness in his. Objectively.\nWhat I’m saying—and the point of this entire model—is that the quality of our respective lives might be more a matter of framing than of actual circumstance.\nBut this isn’t just about extremes like Chinedu and Brittany. It applies to the entire spectrum between war-torn Myanmar and Atherton High. It applies to all of us.\nWe get to choose our frame. And our frame is our reality.\nThe framing divergence\n\nSo here’s where it gets interesting for society, and specifically for politics.\nOur frames are massively diverging.\nI think this—more than anything—explains how you can have such completely isolated pockets of people in a place like the SF Bay Area. Or in the US in general.\nI have started to notice two distinct groups of people online and in person. There are many others, of course, but these two stand out.\nGROUP 1: Listen to somewhat similar podcasts I do, have read over 20 non-fiction books in the last year, are relatively thin, are relatively active, they see the economy as booming, they’re working in tech or starting a business, and they’re 1000% bouncing with energy. They hardly watch much TV, if any, and hardly play any video games. If they have kids they’re in a million different activities, sports, etc, and the conversation is all about where they’ll go to college and what they’ll likely do as a career. They see politics as horribly broken, are probably center-right, seem to be leaning more religious lately, and generally are optimistic about the future. Energy and Outlook: Disciplined, driven, positive, and productive.\nGROUP 2: They see the podcasts GROUP 1 listens to as a bunch of tech bros doing evil capitalist things. They’re very unhealthy. Not active at all. Low energy. Constantly tired. They spend most of their time watching TV and playing video games. They think the US is racist and sexist and ruined. If they have kids they aren’t doing many activities and are quite withdrawn, often with a focus on their personal issues and how those are causing trauma in their lives. Their view of politics is 100% focused on the extreme right and how evil they are, personified by Trump, and how the world is just going to hell. Energy and Outlook: Undisciplined, moping, negative, and unproductive.\nI see a million variations of these, and my friends and I are hybrids as well, but these seem like poles on some kind of spectrum.\nBut thing that gets me is how different they are. And now imagine that for the entire country. But with far more frames and—therefore—subcultures.\nThese lenses shape and color everything. They shape how you hear the news. They shape the media you consume. Which in turn shapes the lenses again.\nThis is so critical because they also determine who you hang out with, what you watch and listen to, and, therefore, how your perspectives are reinforced and updated. Repeat. ♻️\nA couple of books\n\nTwo books that this makes me think of are Bobos in Paradise, by David Brooks, and Bowling Alone, by Robert Putman.\nThey both highlight, in different ways, how groups are separating in the US, and how subgroups shoot off from what used to be the mainstream and become something else.\nWhen our frames are different, our realities are different.\nThat’s a key point in both books, actually: America used to largely be one group. The same cars. The same neighborhoods. The same washing machines. The same newspapers.\nMost importantly, the same frames.\nThere were different religions and different preferences for things, but we largely interpreted reality the same way.\nHere are some very rough examples of shared frames in—say—the 20th century in the United States:\nAmerica is one of the best countries in the world\nI’m proud to be American\nYou can get ahead if you work hard\nEquality isn’t perfect, but it’s improving\nI generally trust and respect my neighbors\nThe future is bright\nThings are going to be ok\nThose are huge frames to agree on. And if you look at those I’ve laid out above, you can see how different they are.\nOk, what does that mean for us?\n\nI’m not sure what it means, other than divergence. Pockets. Subgroups. With vastly different perspectives and associated outcomes.\nI imagine this will make it more difficult to find consensus in politics.\n✅\nI imagine it’ll mean more internal strife.\n✅\nLess trust of our neighbors. More cynicism.\n✅\nAnd so on.\nBut to me, the most interesting about it is just understanding the dynamic and using that understanding to ask ourselves what we can do about it.\nSummary\nFrames are lenses, not reality.\nSome lenses are more positive and productive than others.\nWe can choose which frames to use, and those might shape our reality more than our actual circumstances.\nChanging frames can, therefore, change our outcomes.\nWhen it comes to social dynamics and politics, lenses determine our experienced reality.\nIf we don’t share lenses, we don’t share reality.\nMaybe it’s time to pick and champion some positive shared lenses.\nRecommendations\nHere are my early thoughts on recommendations, having just started exploring the model.\nIdentify your frames. They are like the voices you use to talk to yourself, and you should be very careful about those.\nLook at the frames of the people around you. Talk to them and figure out what frames they’re using. Think about the frames people have that you look up to vs. those you don’t.\nConsider changing your frames to better ones. Remember that frames aren’t reality. They’re useful or harmful ways of interpreting reality. Choose yours carefully.\nWhen you disagree with someone, think about your respective understandings of reality. Adjust the conversation accordingly. Odds are you might think the same as them if you saw reality the way they do, and vice versa.\nI’m going to continue thinking on this. I hope you do as well, and let me know what you come up with.\n\n# STEPS\n\n- Take the input provided and look for negative frames. Write those on a virtual whiteboard in your mind.\n\n# OUTPUT SECTIONS\n\n- In a section called NEGATIVE FRAMES, output 1 - 5 of the most negative frames you found in the input. Each frame / bullet should be wide in scope and be less than 15 words.\n\n- Each negative frame should escalate in negativity and breadth of scope.\n\nE.g.,\n\n\\\"This article proves dating has become nasty and I have no chance of success.\\\"\n\\\"Dating is hopeless at this point.\\\"\n\\\"Why even try in this life if I can't make connections?\\\"\n\n- In a section called POSITIVE FRAMES, output 1 - 5 different frames that are positive and could replace the negative frames you found. Each frame / bullet should be wide in scope and be less than 15 words.\n\n- Each positive frame should escalate in negativity and breadth of scope.\n\nE.g.,\n\n\\\"Focusing on in-person connections is already something I wanted to be working on anyway.\n\n\\\"It's great to have more support for human connection.\\\"\n\n\\\"I love the challenges that come up in life; they make it so interesting.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- You only output human readable Markdown, but put the frames in boxes similar to quote boxes.\n- Do not output warnings or notes—just the requested sections.\n- Include personal context if it's provided in the input.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.16918519139289856,
        0.14430005848407745,
        -0.4666575789451599,
        0.7977587580680847,
        -0.3229416012763977,
        -0.021059550344944,
        -0.7946441769599915,
        0.5206870436668396,
        -0.26439395546913147,
        0.6949949860572815,
        -0.84295254945755,
        0.7361267805099487,
        0.5006235837936401,
        0.02100633829832077,
        -0.23263199627399445,
        -0.23496055603027344,
        -0.7632594704627991,
        -0.6691012978553772,
        -1.8294050693511963,
        -0.5435100197792053,
        -0.3846299350261688,
        1.254152536392212,
        0.4951738119125366,
        0.08971147984266281,
        0.5854288935661316,
        -0.011768277734518051,
        0.1686297506093979,
        -0.5959139466285706,
        -0.6388891935348511,
        -1.838093876838684,
        0.16574859619140625,
        0.4197582006454468,
        0.23718038201332092,
        -0.9486987590789795,
        1.0446007251739502,
        -0.5295083522796631,
        -0.1716691255569458,
        0.1542055606842041,
        -0.40778306126594543,
        0.0008637085556983948,
        -0.27597349882125854,
        -0.43387719988822937,
        -0.1853410303592682,
        0.0998745933175087,
        0.02747216261923313,
        -0.2728531062602997,
        0.2092570811510086,
        -0.41223883628845215,
        1.060681700706482,
        0.42095571756362915,
        -0.31300088763237,
        -0.5627622604370117,
        -0.17976588010787964,
        -1.1152057647705078,
        -0.713472843170166,
        -0.21636275947093964,
        0.24942848086357117,
        -0.9173281192779541,
        0.31234556436538696,
        0.39800530672073364,
        0.130739226937294,
        0.2924577295780182,
        -3.3020339012145996,
        -0.24411267042160034,
        0.188321053981781,
        0.6224294304847717,
        0.4724450707435608,
        -0.3873550295829773,
        0.3630841076374054,
        -0.14918574690818787,
        0.14833439886569977,
        0.18076308071613312,
        -0.022976359352469444,
        -0.062455907464027405,
        0.20845410227775574,
        -0.3673945665359497,
        0.3574323356151581,
        0.6175835132598877,
        0.4216434955596924,
        -0.08086632192134857,
        -0.18995319306850433,
        0.37695151567459106,
        0.3992653787136078,
        0.038077011704444885,
        -0.5021048188209534,
        0.6534689664840698,
        -0.3869737684726715,
        0.4862399697303772,
        0.13740167021751404,
        -0.21144230663776398,
        0.007307136431336403,
        -0.38140949606895447,
        0.7859429717063904,
        0.01615159958600998,
        -1.3204801082611084,
        -0.19778446853160858,
        -0.2652500867843628,
        -0.21803146600723267,
        0.6066039800643921,
        3.221067190170288,
        0.9684215188026428,
        -0.08206185698509216,
        0.5773184299468994,
        -1.0835915803909302,
        0.4939974546432495,
        0.3175697326660156,
        0.020520735532045364,
        -0.8381785154342651,
        -0.11079104244709015,
        -0.48953166604042053,
        0.3829202353954315,
        -0.5859019756317139,
        -0.2463209331035614,
        0.5371995568275452,
        0.6258398294448853,
        0.09273909032344818,
        -0.7503010630607605,
        0.2863611876964569,
        -0.5244201421737671,
        0.9774103760719299,
        -0.4172155559062958,
        -0.32167860865592957,
        -0.5462251305580139,
        -0.3071008026599884,
        -0.20352229475975037,
        0.025296173989772797,
        -0.5807641744613647,
        0.5659272074699402,
        0.10990142822265625,
        0.4611610770225525,
        0.16493789851665497,
        -0.17504559457302094,
        0.06851768493652344,
        0.31925952434539795,
        0.2286689281463623,
        0.6178675889968872,
        -0.2909088134765625,
        -0.39300665259361267,
        -0.19727683067321777,
        -0.8420127630233765,
        0.5558449625968933,
        -0.2983859181404114,
        0.4761093258857727,
        0.29608744382858276,
        -0.24445456266403198,
        0.30964207649230957,
        0.22548535466194153,
        0.11040282249450684,
        -0.9948457479476929,
        -0.8739227056503296,
        -0.19326652586460114,
        -0.27149930596351624,
        0.22466658055782318,
        0.35012543201446533,
        0.6263498067855835,
        0.004977897275239229,
        0.25497597455978394,
        -0.2985313832759857,
        -0.6591376662254333,
        0.5940241813659668,
        -0.12135499715805054,
        0.1099499836564064,
        -0.3377002477645874,
        0.16018956899642944,
        0.45769572257995605,
        -0.07412652671337128,
        0.4133705198764801,
        0.32004907727241516,
        0.1377849578857422,
        0.37319862842559814,
        0.01889938861131668,
        -0.1725884974002838,
        0.5486097931861877,
        0.713628888130188,
        -0.3357895016670227,
        0.0431366041302681,
        -1.052497148513794,
        0.4601755738258362,
        0.050171561539173126,
        0.13521656394004822,
        0.5442036390304565,
        0.5347331762313843,
        -0.06073378771543503,
        -0.8325260877609253,
        -0.12159629166126251,
        0.3963076174259186,
        0.33755385875701904,
        0.6023075580596924,
        0.4787590801715851,
        0.48632577061653137,
        -1.0103298425674438,
        1.246307134628296,
        -1.2503013610839844,
        0.5353431701660156,
        0.6232277750968933,
        0.43798238039016724,
        0.09147937595844269,
        0.3035932779312134,
        0.16284193098545074,
        -0.24340546131134033,
        -1.2936822175979614,
        0.10216054320335388,
        -1.0390312671661377,
        -0.044662803411483765,
        -0.17335572838783264,
        -0.479352742433548,
        0.32591068744659424,
        0.33484843373298645,
        -0.18910355865955353,
        -0.7115004658699036,
        -0.05775317922234535,
        0.2565172016620636,
        1.6243762969970703,
        0.4557867646217346,
        1.0592793226242065,
        -0.2467232495546341,
        -0.3215816617012024,
        0.28975769877433777,
        0.9066619277000427,
        -0.37253430485725403,
        0.4404194951057434,
        0.11146645247936249,
        0.07003837823867798,
        -0.6325557827949524,
        -0.6991024017333984,
        0.3441498279571533,
        -0.5325661301612854,
        -0.10980863124132156,
        -0.3487592935562134,
        -0.7903878092765808,
        0.3909004330635071,
        0.5190141797065735,
        0.7224966287612915,
        1.503519892692566,
        -0.10929989069700241,
        0.8425506949424744,
        0.14700116217136383,
        -0.07926112413406372,
        0.2210359275341034,
        -1.2892041206359863,
        0.19600382447242737,
        0.28511765599250793,
        -0.2715712785720825,
        -0.199483722448349,
        -0.242707759141922,
        0.33131372928619385,
        -0.3125184178352356,
        -1.0499001741409302,
        -0.13603658974170685,
        1.9752888679504395,
        0.23850378394126892,
        -0.23226916790008545,
        -0.1380186378955841,
        0.6038356423377991,
        -0.26625877618789673,
        -0.18627667427062988,
        -1.9698346853256226,
        -0.43245962262153625,
        -0.5273563861846924,
        0.37923774123191833,
        -0.09807290136814117,
        0.34983760118484497,
        0.7369840741157532,
        0.03781348094344139,
        -0.6872757077217102,
        0.10857779532670975,
        -0.332712322473526,
        -0.3980143070220947,
        -0.9789499640464783,
        -0.3000739514827728,
        -0.39685705304145813,
        0.3815838098526001,
        -0.38962113857269287,
        0.12528377771377563,
        0.206060528755188,
        -0.030592890456318855,
        0.248726025223732,
        0.09100189805030823,
        0.4251343905925751,
        -0.22593428194522858,
        0.3903755247592926,
        0.46471965312957764,
        -0.11804459244012833,
        0.5083314180374146,
        0.10381995886564255,
        -0.8176897168159485,
        0.0846845805644989,
        -0.6524730920791626,
        0.2509606182575226,
        0.7515500783920288,
        -0.10482949018478394,
        -0.9039579033851624,
        -0.42896905541419983,
        0.1401834934949875,
        1.6167491674423218,
        0.6795852184295654,
        0.18643003702163696,
        0.7631244659423828,
        0.22649367153644562,
        0.2455284148454666,
        -0.05195312201976776,
        0.5583345293998718,
        0.5694204568862915,
        -0.10321801900863647,
        -0.8209785223007202,
        -0.3292086720466614,
        0.9152032732963562,
        -0.19047805666923523,
        -0.32507333159446716,
        0.3540826737880707,
        -0.038371842354536057,
        -0.04296008124947548,
        -0.3319975733757019,
        -0.18777090311050415,
        0.8773772120475769,
        0.3564932644367218,
        0.6206968426704407,
        1.2735873460769653,
        -0.34148287773132324,
        -1.4334056377410889,
        0.013223124668002129,
        0.6507420539855957,
        0.02153962105512619,
        0.020062386989593506,
        0.19666899740695953,
        0.3153078854084015,
        -0.408775269985199,
        0.29711684584617615,
        -0.101503387093544,
        1.0199960470199585,
        0.49921438097953796,
        -0.026938576251268387,
        -0.37628719210624695,
        0.41644540429115295,
        0.7869447469711304,
        -0.22023053467273712,
        0.4709688723087311,
        -0.2822606861591339,
        0.026142336428165436,
        -0.13842515647411346,
        0.3004950284957886,
        1.323622465133667,
        -0.44464007019996643,
        0.15198321640491486,
        0.17975853383541107,
        -0.0034634992480278015,
        -0.512424111366272,
        -0.5002762675285339,
        0.3769736886024475,
        0.13493771851062775,
        -0.41111695766448975,
        0.45379504561424255,
        -0.09099480509757996,
        -0.1771867871284485,
        0.15387849509716034,
        0.48057821393013,
        -0.44942399859428406,
        0.029104746878147125,
        -0.9703701138496399,
        1.1339678764343262,
        -0.3771747648715973,
        -0.9255138635635376,
        -0.3098742961883545,
        -0.10785549879074097,
        -0.6149240732192993,
        -0.45097094774246216,
        0.6376819610595703,
        -0.15207549929618835,
        0.14963525533676147,
        -0.3259468972682953,
        -0.22523926198482513,
        -0.2586032450199127,
        0.516028642654419,
        0.1050075888633728,
        0.2839093804359436,
        -0.42012327909469604,
        -0.06047949939966202,
        0.5372719764709473,
        0.5540805459022522,
        -0.033171869814395905,
        0.44041985273361206,
        -0.3632899522781372,
        -0.5215628743171692,
        -1.0834083557128906
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates wireframes and starter code for coding projects based on user ideas. It specifically caters to transforming ideas into actionable project outlines and code skeletons, including detailed steps and file structures. The output includes project summaries, structured directories, and initial code setups.",
          "name": "Create_coding_project",
          "raw": "\n                workflow Create_coding_project v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an elite programmer. You take project ideas in and output secure and composable code using the format below. You always use the latest technology and best practices.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the project idea into a single, 20-word sentence in a section called PROJECT:.\n\n- Output a summary of how the project works in a section called SUMMARY:.\n\n- Output a step-by-step guide with no more than 15 words per point into a section called STEPS:.\n\n- Output a directory structure to display how each piece of code works together into a section called STRUCTURE:.\n\n- Output the purpose of each file as a list with no more than 15 words per point into a section called DETAILED EXPLANATION:.\n\n- Output the code for each file seperately along with a short description of the code's purpose into a section called CODE:.\n\n- Output a script that creates the entire project into a section called SETUP:.\n\n- Output a list of takeaways in a section called TAKEAWAYS:.\n\n- Output a list of suggestions in a section called SUGGESTIONS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- Output numbered lists, not bullets for the STEPS and TAKEAWAY sections.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n- Keep each file separate in the CODE section.\n- Be open to suggestions and output revisions on the project.\n- Output code that has comments for every step.\n- Output a README.md with detailed instructions on how to configure and use the project.\n- Do not use deprecated features.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an elite programmer. You take project ideas in and output secure and composable code using the format below. You always use the latest technology and best practices.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the project idea into a single, 20-word sentence in a section called PROJECT:.\n\n- Output a summary of how the project works in a section called SUMMARY:.\n\n- Output a step-by-step guide with no more than 15 words per point into a section called STEPS:.\n\n- Output a directory structure to display how each piece of code works together into a section called STRUCTURE:.\n\n- Output the purpose of each file as a list with no more than 15 words per point into a section called DETAILED EXPLANATION:.\n\n- Output the code for each file seperately along with a short description of the code's purpose into a section called CODE:.\n\n- Output a script that creates the entire project into a section called SETUP:.\n\n- Output a list of takeaways in a section called TAKEAWAYS:.\n\n- Output a list of suggestions in a section called SUGGESTIONS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- Output numbered lists, not bullets for the STEPS and TAKEAWAY sections.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n- Keep each file separate in the CODE section.\n- Be open to suggestions and output revisions on the project.\n- Output code that has comments for every step.\n- Output a README.md with detailed instructions on how to configure and use the project.\n- Do not use deprecated features.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.0556202158331871,
        0.46334409713745117,
        -0.0761994868516922,
        0.8338805437088013,
        0.0588565357029438,
        0.14167135953903198,
        -0.9438885450363159,
        -0.04719263315200806,
        0.41530629992485046,
        -0.000563599169254303,
        -0.09683820605278015,
        0.21895408630371094,
        -0.07570108771324158,
        -0.10167203843593597,
        -0.01880410686135292,
        -0.3997286856174469,
        -0.44831582903862,
        -1.7832690477371216,
        -0.7329968810081482,
        -0.6965345740318298,
        -0.21147945523262024,
        0.601590633392334,
        -0.23764538764953613,
        -0.19442658126354218,
        0.1931164562702179,
        -0.5176402926445007,
        0.43860599398612976,
        -0.2926573157310486,
        -1.1308317184448242,
        -1.2283620834350586,
        0.5956986546516418,
        0.15672457218170166,
        -0.4994712173938751,
        -0.566214382648468,
        0.5001417398452759,
        -0.5065979361534119,
        -0.1757093071937561,
        0.42361894249916077,
        -0.22379127144813538,
        -0.6023677587509155,
        0.3652896583080292,
        -0.2549472451210022,
        -0.2590476870536804,
        -0.24336525797843933,
        0.12131573259830475,
        0.04762749373912811,
        0.06171666085720062,
        -0.08721332252025604,
        1.3814157247543335,
        0.3094756007194519,
        0.6564556360244751,
        -0.5304248332977295,
        0.09267386794090271,
        -0.08221215754747391,
        -0.5666167140007019,
        -0.5791913270950317,
        0.1066100150346756,
        -0.21093860268592834,
        -0.07586370408535004,
        0.3069831430912018,
        -0.0022161691449582577,
        0.8113512396812439,
        -3.4721996784210205,
        0.021093659102916718,
        0.3558259904384613,
        -0.13966307044029236,
        0.5031099319458008,
        0.2913230359554291,
        1.2898808717727661,
        0.21099355816841125,
        -0.18193408846855164,
        0.4558720886707306,
        -0.14166858792304993,
        0.37712547183036804,
        0.6926453113555908,
        -0.162349134683609,
        0.2859477698802948,
        -0.11954526603221893,
        0.9138827919960022,
        -0.8290886282920837,
        0.4046919643878937,
        0.30094239115715027,
        0.12767891585826874,
        -0.1384502500295639,
        -0.7633028030395508,
        0.646756112575531,
        -0.12038771063089371,
        0.048079073429107666,
        1.0143976211547852,
        -0.09782099723815918,
        -0.24672847986221313,
        -0.36982858180999756,
        -0.34138503670692444,
        -0.06040533632040024,
        -0.837157130241394,
        0.20330014824867249,
        -0.5440068244934082,
        0.44016554951667786,
        -0.44336527585983276,
        3.213582754135132,
        0.5892064571380615,
        0.4726608097553253,
        0.7761286497116089,
        -1.3608587980270386,
        0.6432093977928162,
        -0.1288902461528778,
        0.15919619798660278,
        -1.1950510740280151,
        -0.23080289363861084,
        -0.4584994316101074,
        0.4517175555229187,
        -0.4346078634262085,
        -0.5813593864440918,
        0.1173098161816597,
        0.8640964031219482,
        -0.07225541025400162,
        -0.5689813494682312,
        -0.08857110142707825,
        -0.3801891505718231,
        1.5779813528060913,
        0.0329899936914444,
        -0.10898727178573608,
        -0.6355002522468567,
        -0.2844391167163849,
        0.408911794424057,
        0.7017242312431335,
        -0.9115157127380371,
        0.6363589763641357,
        -0.2447439283132553,
        0.12062862515449524,
        -0.5518684387207031,
        -0.32414746284484863,
        -0.011023025959730148,
        -0.29747915267944336,
        0.510197639465332,
        0.09376996755599976,
        -0.14921225607395172,
        -0.42500972747802734,
        0.13484524190425873,
        0.17253565788269043,
        0.4843379259109497,
        -0.7348755598068237,
        1.0202713012695312,
        0.31951332092285156,
        0.32932794094085693,
        -0.1614798754453659,
        -0.42717835307121277,
        0.3740393817424774,
        -0.3826920688152313,
        -0.9138339161872864,
        -0.07972219586372375,
        0.1923186033964157,
        0.18907150626182556,
        0.5365108847618103,
        0.21095065772533417,
        -0.3995356857776642,
        -0.678402841091156,
        -0.11099414527416229,
        -0.8620766401290894,
        0.13919231295585632,
        0.190377339720726,
        -0.5625399351119995,
        -0.06124931573867798,
        0.4787401556968689,
        0.5860919952392578,
        -0.2766264081001282,
        0.49780774116516113,
        -0.0800221860408783,
        0.25946903228759766,
        -0.16834570467472076,
        0.2690606713294983,
        -0.28355544805526733,
        0.4937625825405121,
        0.6351866126060486,
        -0.7032821774482727,
        0.3109082281589508,
        0.022840924561023712,
        0.13713914155960083,
        0.6003465056419373,
        -0.15912114083766937,
        0.9303392171859741,
        0.3776085376739502,
        0.19474227726459503,
        -0.30301433801651,
        -0.3492346405982971,
        0.2995437681674957,
        0.13599082827568054,
        0.6473367214202881,
        0.6407759189605713,
        0.5739932060241699,
        -0.9289542436599731,
        1.6906710863113403,
        -0.06265799701213837,
        0.08970791101455688,
        -0.012308394536376,
        0.15204809606075287,
        0.14587858319282532,
        0.2932685613632202,
        -0.06353376805782318,
        -0.20761865377426147,
        -1.2013503313064575,
        -0.22798925638198853,
        -0.35636696219444275,
        -0.372065007686615,
        -0.2327636480331421,
        0.2512451112270355,
        0.4116802513599396,
        0.41672471165657043,
        0.46880924701690674,
        -0.6079932451248169,
        -0.5867858529090881,
        0.08668698370456696,
        1.64420747756958,
        0.7022074460983276,
        1.128363013267517,
        -0.27666157484054565,
        0.25124937295913696,
        -0.31912386417388916,
        0.7937775254249573,
        -0.07474362850189209,
        -0.07840981334447861,
        0.4021568298339844,
        -0.7722234129905701,
        -0.5821945071220398,
        -0.10890913009643555,
        0.7025043964385986,
        0.23574498295783997,
        0.7667970061302185,
        -0.6746051907539368,
        -0.17787636816501617,
        0.26569849252700806,
        1.18772292137146,
        0.9375604391098022,
        1.7699450254440308,
        -0.7959651350975037,
        0.36202865839004517,
        -0.08013059943914413,
        0.022564619779586792,
        -0.05006738379597664,
        -1.4346603155136108,
        0.07364606857299805,
        0.03171401843428612,
        0.04932895302772522,
        0.1577824205160141,
        -0.2195630967617035,
        -0.19970768690109253,
        0.07154221832752228,
        -0.9122413992881775,
        -0.2362617701292038,
        1.2649970054626465,
        0.07250574231147766,
        0.3064110279083252,
        -0.16694971919059753,
        0.5573810935020447,
        -0.2088988721370697,
        0.2009495496749878,
        -2.1132004261016846,
        -0.13246262073516846,
        -0.5137169361114502,
        0.04776631295681,
        0.08142846077680588,
        0.7473230361938477,
        0.35219910740852356,
        -0.5041953921318054,
        -0.7191230654716492,
        -0.1362372636795044,
        -0.27251383662223816,
        -0.2874482274055481,
        -1.0455741882324219,
        -0.024312317371368408,
        0.6658077836036682,
        0.20848721265792847,
        0.3223719000816345,
        -0.2505934536457062,
        0.09148518741130829,
        0.06600402295589447,
        -0.08470078557729721,
        0.9723799824714661,
        -0.43313583731651306,
        -0.16027489304542542,
        -0.04604204744100571,
        0.25021228194236755,
        -0.05033770203590393,
        0.38536232709884644,
        -0.1616881787776947,
        0.08248406648635864,
        -0.3128845989704132,
        -0.9015719890594482,
        -0.2952754497528076,
        0.8318466544151306,
        -0.21411055326461792,
        -0.4065220057964325,
        -0.672368049621582,
        -0.0180300772190094,
        1.8135507106781006,
        0.10807845741510391,
        0.30261409282684326,
        -0.029678085818886757,
        0.3787521421909332,
        -0.13692344725131989,
        0.0959681048989296,
        -0.46263933181762695,
        0.40209728479385376,
        0.2094239890575409,
        -0.6866925954818726,
        -0.44473469257354736,
        0.2903183102607727,
        0.30556821823120117,
        -0.15872105956077576,
        -0.27745938301086426,
        -0.05857333913445473,
        0.6034976840019226,
        -0.10336573421955109,
        -0.04323195666074753,
        -0.03144260495901108,
        -0.502128541469574,
        0.39775052666664124,
        0.7375539541244507,
        -0.1763983964920044,
        -1.2517248392105103,
        -0.3211284279823303,
        0.6244047284126282,
        -0.47700756788253784,
        -0.22577977180480957,
        -0.7687739133834839,
        0.8847501277923584,
        -0.09283576160669327,
        -0.43768179416656494,
        -0.5005301237106323,
        1.4462311267852783,
        0.08541826903820038,
        -0.21430155634880066,
        0.9725556373596191,
        0.43306031823158264,
        0.29185861349105835,
        0.04307103902101517,
        -0.3646765351295471,
        -0.1905021369457245,
        -0.22148379683494568,
        -0.2705656588077545,
        0.2101346254348755,
        1.2637770175933838,
        -0.23483507335186005,
        -0.47259828448295593,
        0.811840295791626,
        -0.08951645344495773,
        -1.0560592412948608,
        -1.2476688623428345,
        0.5818191766738892,
        -0.45202845335006714,
        -0.758418083190918,
        0.5467395186424255,
        0.044408805668354034,
        0.205206960439682,
        0.7540850043296814,
        0.24023470282554626,
        -0.7105931043624878,
        -0.20091281831264496,
        -0.6413928866386414,
        1.599625587463379,
        -0.1598035991191864,
        -0.1927831918001175,
        -0.290798544883728,
        0.3821672797203064,
        -1.0604122877120972,
        -0.34626251459121704,
        0.5087145566940308,
        -0.46817144751548767,
        0.32231301069259644,
        0.27685508131980896,
        -0.07115873694419861,
        -0.5512604117393494,
        0.21616338193416595,
        0.33181485533714294,
        0.359499454498291,
        -0.27314478158950806,
        -0.12285619974136353,
        0.3518173396587372,
        0.568642258644104,
        -0.024702899158000946,
        0.8922343254089355,
        -0.10393419861793518,
        -0.8627374172210693,
        -0.49147504568099976
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates specific command lines for various penetration testing tools based on a brief description of the desired outcome. This approach leverages the tool's help documentation to ensure accuracy and relevance. The expected output is a precise command that aligns with the user's objectives for the tool.",
          "name": "Create_command",
          "raw": "\n                workflow Create_command v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a penetration tester that is extremely good at reading and understanding command line help instructions. You are responsible for generating CLI commands for various tools that can be run to perform certain tasks based on documentation given to you.\n\nTake a step back and analyze the help instructions thoroughly to ensure that the command you provide performs the expected actions. It is crucial that you only use switches and options that are explicitly listed in the documentation passed to you. Do not attempt to guess. Instead, use the documentation passed to you as your primary source of truth. It is very important the commands you generate run properly and do not use fake or invalid options and switches.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the requested command using the documentation provided with the provided details inserted. The input will include the prompt on the first line and then the tool documentation for the command will be provided on subsequent lines.\n- Do not add additional options or switches unless they are explicitly asked for.\n- Only use switches that are explicitly stated in the help documentation that is passed to you as input.\n\n# OUTPUT FORMAT\n\n- Output a full, bash command with all relevant parameters and switches.\n- Refer to the provided help documentation.\n- Only output the command. Do not output any warning or notes.\n- Do not output any Markdown or other formatting. Only output the command itself.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a penetration tester that is extremely good at reading and understanding command line help instructions. You are responsible for generating CLI commands for various tools that can be run to perform certain tasks based on documentation given to you.\n\nTake a step back and analyze the help instructions thoroughly to ensure that the command you provide performs the expected actions. It is crucial that you only use switches and options that are explicitly listed in the documentation passed to you. Do not attempt to guess. Instead, use the documentation passed to you as your primary source of truth. It is very important the commands you generate run properly and do not use fake or invalid options and switches.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the requested command using the documentation provided with the provided details inserted. The input will include the prompt on the first line and then the tool documentation for the command will be provided on subsequent lines.\n- Do not add additional options or switches unless they are explicitly asked for.\n- Only use switches that are explicitly stated in the help documentation that is passed to you as input.\n\n# OUTPUT FORMAT\n\n- Output a full, bash command with all relevant parameters and switches.\n- Refer to the provided help documentation.\n- Only output the command. Do not output any warning or notes.\n- Do not output any Markdown or other formatting. Only output the command itself.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5744696855545044,
        0.2542240023612976,
        0.18594518303871155,
        0.41956591606140137,
        -0.04595788195729256,
        0.0458933562040329,
        -1.2258354425430298,
        0.01693132519721985,
        0.04865294694900513,
        0.5641836524009705,
        -0.6351547837257385,
        0.6348116993904114,
        0.12619400024414062,
        -0.03326183557510376,
        0.08029505610466003,
        -0.6064690947532654,
        0.13001957535743713,
        -1.4076080322265625,
        -0.7863214015960693,
        -0.447681725025177,
        -0.3145066201686859,
        1.2368513345718384,
        0.3048217296600342,
        0.09256201982498169,
        0.6645709872245789,
        -0.16553252935409546,
        0.06773050129413605,
        -0.2599175274372101,
        -0.5881957411766052,
        -1.5101993083953857,
        0.6249290704727173,
        -0.5018767714500427,
        -0.8907062411308289,
        -0.5955698490142822,
        0.808283805847168,
        -0.8840779662132263,
        0.7014950513839722,
        0.10320713371038437,
        -0.3346981108188629,
        -0.13512097299098969,
        -0.13999700546264648,
        0.019856004044413567,
        -0.6015062928199768,
        -0.39542290568351746,
        0.2547357678413391,
        -0.18859505653381348,
        -0.09541693329811096,
        0.3435518741607666,
        1.4432133436203003,
        -0.14196179807186127,
        -0.43938130140304565,
        -0.6069924831390381,
        -0.617185652256012,
        0.18293654918670654,
        -0.5122385025024414,
        0.23461946845054626,
        -0.21706412732601166,
        -0.5518974661827087,
        0.21433869004249573,
        0.22804242372512817,
        0.8604112267494202,
        0.3944375813007355,
        -3.337400436401367,
        -0.20736396312713623,
        0.18724627792835236,
        0.05788544565439224,
        0.10435369610786438,
        -0.39396798610687256,
        0.5846570134162903,
        -0.047374919056892395,
        0.0024711042642593384,
        -0.15107713639736176,
        0.07519542425870895,
        0.7881543040275574,
        0.07915715128183365,
        0.4869726598262787,
        0.33840301632881165,
        -0.17785710096359253,
        0.5854345560073853,
        -0.0050343796610832214,
        -0.1524185985326767,
        0.46459266543388367,
        0.26273536682128906,
        0.07659099996089935,
        -0.34935906529426575,
        1.037886142730713,
        -0.29060402512550354,
        0.07435271143913269,
        0.9108652472496033,
        0.37261083722114563,
        -0.22327041625976562,
        -0.6117316484451294,
        -0.04559527337551117,
        0.13834619522094727,
        -0.4378180503845215,
        -0.39489829540252686,
        -0.7015087604522705,
        -0.20941320061683655,
        -0.03945889323949814,
        3.3476216793060303,
        0.3079158663749695,
        0.34413230419158936,
        0.8390200138092041,
        -1.151863932609558,
        0.8038267493247986,
        -0.0918547660112381,
        -0.09650428593158722,
        -0.7634618878364563,
        -0.14001092314720154,
        -0.3759511113166809,
        0.4225562810897827,
        -0.8775017261505127,
        -0.40891924500465393,
        0.3760126233100891,
        0.534744143486023,
        0.46913111209869385,
        -0.8157150745391846,
        -0.14102469384670258,
        -0.36699816584587097,
        0.20903156697750092,
        -0.37679165601730347,
        -0.602075457572937,
        -0.505302906036377,
        -0.1133408173918724,
        0.24664060771465302,
        -0.008738093078136444,
        0.01013951376080513,
        0.5171067118644714,
        -0.1631922721862793,
        0.09883800148963928,
        -0.2316938042640686,
        -0.265115886926651,
        -0.05548517405986786,
        -0.08613757789134979,
        0.46628063917160034,
        0.14759181439876556,
        0.38732558488845825,
        -0.26090237498283386,
        0.10419564694166183,
        -0.1348545253276825,
        0.16968773305416107,
        -0.47253668308258057,
        1.10655677318573,
        0.2738766074180603,
        0.20115619897842407,
        0.16404688358306885,
        -0.13394764065742493,
        -0.08720143884420395,
        0.30646008253097534,
        -1.295780062675476,
        0.0793934166431427,
        0.6330661177635193,
        -0.05898620933294296,
        0.3905050456523895,
        0.7791162133216858,
        0.21392188966274261,
        0.23419080674648285,
        1.1225459575653076,
        -0.8123758435249329,
        0.33788830041885376,
        0.7017850279808044,
        -0.05461942404508591,
        0.2410176396369934,
        0.06231793761253357,
        0.45447835326194763,
        -0.20198708772659302,
        0.7093384265899658,
        -0.17191240191459656,
        -0.11655682325363159,
        0.1986134797334671,
        0.4544839560985565,
        -0.3714947998523712,
        0.4438836872577667,
        0.44769760966300964,
        -0.4313976466655731,
        0.29527783393859863,
        -0.1173987090587616,
        0.20380988717079163,
        0.14194439351558685,
        -0.4974958300590515,
        0.8662461638450623,
        0.44692179560661316,
        -0.02386130765080452,
        -1.1261978149414062,
        -0.05533041059970856,
        0.19807644188404083,
        0.12157696485519409,
        -0.22906160354614258,
        0.8141626119613647,
        0.46412786841392517,
        -1.1736186742782593,
        1.2851184606552124,
        -0.40036463737487793,
        -0.5170023441314697,
        0.0428040437400341,
        0.5618255138397217,
        -0.4813259243965149,
        -0.022837094962596893,
        0.34728652238845825,
        0.3649199903011322,
        -0.5848932266235352,
        -0.27615439891815186,
        -0.8115702867507935,
        -0.09986133873462677,
        -0.531069278717041,
        -0.6343517303466797,
        0.4615897536277771,
        1.0378305912017822,
        -0.16262251138687134,
        -0.5364693403244019,
        -0.0035012364387512207,
        0.3379124104976654,
        1.0169795751571655,
        0.8076233267784119,
        0.9605730772018433,
        -0.17039243876934052,
        0.08818130940198898,
        0.4194813668727875,
        0.34460923075675964,
        0.2612133026123047,
        0.21381491422653198,
        0.5224558115005493,
        -0.8609761595726013,
        -0.753829300403595,
        -1.1740542650222778,
        0.4274660348892212,
        -0.05094771832227707,
        0.2766326665878296,
        -0.6731496453285217,
        -0.6112993955612183,
        0.3247711956501007,
        0.5994821190834045,
        0.835640013217926,
        1.0668272972106934,
        -0.6822320222854614,
        0.5839317440986633,
        -0.27598345279693604,
        0.41063836216926575,
        0.338948130607605,
        -1.0986193418502808,
        0.4555855393409729,
        -0.365504652261734,
        0.06521232426166534,
        0.24261625111103058,
        0.5277518033981323,
        0.3274325132369995,
        -0.8753330707550049,
        0.10625949501991272,
        -0.25799164175987244,
        1.2605221271514893,
        0.512477457523346,
        0.2421545833349228,
        -0.20745724439620972,
        0.7043418884277344,
        -0.23370307683944702,
        -0.20018106698989868,
        -1.0887547731399536,
        -0.10507234185934067,
        -0.6290285587310791,
        -0.09209486842155457,
        -0.04109548404812813,
        -0.06119594722986221,
        0.8249590396881104,
        -0.5027177929878235,
        -0.5839962959289551,
        -0.10122630000114441,
        -0.8875876069068909,
        -0.26451563835144043,
        -0.2594311833381653,
        -0.5361122488975525,
        -0.05621806159615517,
        0.6349321603775024,
        -0.0035894326865673065,
        -0.5531488060951233,
        0.007069284096360207,
        -0.1700206995010376,
        -0.025905316695570946,
        0.5387800335884094,
        -0.1004313975572586,
        -0.05760929733514786,
        -0.16600948572158813,
        0.6318616271018982,
        -0.05977282673120499,
        0.48607397079467773,
        -0.3705867826938629,
        -0.013381237164139748,
        -0.2921421229839325,
        -1.0032176971435547,
        0.2920401990413666,
        0.34572112560272217,
        -0.4300825297832489,
        -0.03459371626377106,
        -0.4447126090526581,
        0.10232380032539368,
        1.9806513786315918,
        0.5505872964859009,
        0.5044071674346924,
        0.7120081186294556,
        0.6158292293548584,
        -0.07067479193210602,
        -0.3129444122314453,
        0.339788019657135,
        -0.1514219045639038,
        0.1738070249557495,
        -0.2561666965484619,
        -0.37789133191108704,
        0.43157023191452026,
        0.15673832595348358,
        -0.2867757976055145,
        0.12139873206615448,
        -0.7863102555274963,
        -0.02511182427406311,
        -0.486013263463974,
        0.17563271522521973,
        0.5087564587593079,
        -0.6150516271591187,
        0.5806235671043396,
        0.5603187084197998,
        0.16990844905376434,
        -2.108334541320801,
        -0.6801673173904419,
        0.5554008483886719,
        0.029347341507673264,
        -0.24220165610313416,
        -0.043185438960790634,
        0.6903631091117859,
        0.020545028150081635,
        0.0846225917339325,
        -0.1462039351463318,
        1.151153326034546,
        -0.255079984664917,
        0.18679514527320862,
        0.6496508717536926,
        -0.10334035754203796,
        0.4260184168815613,
        -0.674068033695221,
        0.2537985146045685,
        -0.319040447473526,
        -0.7738305330276489,
        -0.42298972606658936,
        0.1266787201166153,
        1.573338270187378,
        0.19192524254322052,
        0.11881914734840393,
        0.08758169412612915,
        0.23348581790924072,
        -0.4504398703575134,
        -1.067461371421814,
        0.04490676149725914,
        0.004320353269577026,
        -0.8103998303413391,
        0.46512842178344727,
        0.12798844277858734,
        -0.004136024042963982,
        0.5482891798019409,
        0.9414799213409424,
        -0.4015248715877533,
        0.8088302612304688,
        -0.6906718611717224,
        0.885387122631073,
        -0.5191508531570435,
        -0.3245156407356262,
        -0.514988124370575,
        0.4492713212966919,
        0.03974561765789986,
        -0.20740185678005219,
        -0.3020607531070709,
        -0.5209229588508606,
        -0.07013417780399323,
        -0.07376276701688766,
        0.17943859100341797,
        0.007514975965023041,
        0.964493989944458,
        0.4884438216686249,
        0.5474469065666199,
        -0.15813958644866943,
        -0.10421301424503326,
        -0.009274110198020935,
        0.4880205988883972,
        -0.2130933701992035,
        0.3742981255054474,
        -0.588892936706543,
        -1.1120201349258423,
        -1.1314142942428589
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs on creating a comprehensive summary of cybersecurity threats, vulnerabilities, incidents, and malware for a technical audience. It emphasizes deep understanding through repetitive analysis and visualization techniques. The expected output includes a concise summary and categorized lists of cybersecurity issues.",
          "name": "Create_cyber_summary",
          "raw": "\n                workflow Create_cyber_summary v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert in cybersecurity and writing summaries for busy technical people.\n\n# GOALS\n\nThe goals of this exercise are create a solid summary of all the different types of threats, vulnerabilities, stories, incidents, malware, and other types of newsworthy items.\n\n# STEPS\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n// Create the virtual whiteboard in your mind\n\n- Create a 100 meter by 100 meter whiteboard in your mind, and write down all the different entities from what you read. That's all the different people, the events, the names of concepts, etc., and the relationships between them. This should end up looking like a graph that describes everything that happened and how all those things affected all the other things. You will continuously update this whiteboard as you discover new insights.\n\n// Break out the sections\n\n- Break out the output sections into ADVISORIES, INCIDENTS, MALWARE, and VULNERABILITIES.\n\n- Perform these steps 913 times, optimizing on each iteration.\n\n# OUTPUT\n\n- Output a 25-word summary of the entire input.\n\n- Output a bulleted list of items within each sections above, maximum of 10 items per section. Keep each item to 25-words or less. \n\nEXAMPLE OUTPUT\n\n# VULNERABILITIES\n\n- There's a new critical vulnerability in Windows 10 that allows attackers to take over the entire system as admin.\n\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert in cybersecurity and writing summaries for busy technical people.\n\n# GOALS\n\nThe goals of this exercise are create a solid summary of all the different types of threats, vulnerabilities, stories, incidents, malware, and other types of newsworthy items.\n\n# STEPS\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n// Create the virtual whiteboard in your mind\n\n- Create a 100 meter by 100 meter whiteboard in your mind, and write down all the different entities from what you read. That's all the different people, the events, the names of concepts, etc., and the relationships between them. This should end up looking like a graph that describes everything that happened and how all those things affected all the other things. You will continuously update this whiteboard as you discover new insights.\n\n// Break out the sections\n\n- Break out the output sections into ADVISORIES, INCIDENTS, MALWARE, and VULNERABILITIES.\n\n- Perform these steps 913 times, optimizing on each iteration.\n\n# OUTPUT\n\n- Output a 25-word summary of the entire input.\n\n- Output a bulleted list of items within each sections above, maximum of 10 items per section. Keep each item to 25-words or less. \n\nEXAMPLE OUTPUT\n\n# VULNERABILITIES\n\n- There's a new critical vulnerability in Windows 10 that allows attackers to take over the entire system as admin.\n\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5041497349739075,
        0.2341628074645996,
        -0.3807801902294159,
        0.971808671951294,
        -0.06413037329912186,
        -0.3239315152168274,
        -0.7932570576667786,
        0.10144063830375671,
        -0.15443438291549683,
        0.6250098347663879,
        -0.5108658671379089,
        0.18313902616500854,
        0.1398436427116394,
        0.0952276736497879,
        -0.19709505140781403,
        0.049321554601192474,
        -0.8761682510375977,
        -0.833101212978363,
        -1.1945853233337402,
        -0.4722650945186615,
        -0.2391279935836792,
        1.3213815689086914,
        0.10172146558761597,
        0.03934676945209503,
        0.4596271514892578,
        0.336356520652771,
        0.0775536596775055,
        0.025197260081768036,
        -1.897851586341858,
        -2.0441129207611084,
        0.14325803518295288,
        0.435355544090271,
        0.16558738052845,
        -0.08492927253246307,
        0.4367051124572754,
        -0.4638405442237854,
        0.29194119572639465,
        0.14917300641536713,
        -0.6982408761978149,
        -0.3908312916755676,
        0.1245187446475029,
        0.3274540901184082,
        0.021573267877101898,
        0.3078634440898895,
        -0.1594337821006775,
        0.08684616535902023,
        -0.48785465955734253,
        -0.7978209257125854,
        0.6309518218040466,
        0.3999388813972473,
        -0.6537408232688904,
        0.0490812286734581,
        0.2934713363647461,
        -0.14054584503173828,
        0.10084507614374161,
        0.046768173575401306,
        0.10884350538253784,
        -0.2103605568408966,
        0.20994162559509277,
        0.27441835403442383,
        -0.19214920699596405,
        0.9169130325317383,
        -3.0647881031036377,
        0.1379932165145874,
        -0.541175127029419,
        -0.46165335178375244,
        0.09799233078956604,
        0.5334379076957703,
        0.7835794687271118,
        -0.23705419898033142,
        -0.054591432213783264,
        0.17267672717571259,
        -0.0852351039648056,
        -0.36590778827667236,
        0.1328548789024353,
        0.029147155582904816,
        -0.020450685173273087,
        0.3653261363506317,
        0.5169379711151123,
        -0.20737390220165253,
        0.04061829671263695,
        0.7648608088493347,
        -0.5628182888031006,
        -0.43634307384490967,
        -0.6882115006446838,
        0.9914255142211914,
        0.02127373218536377,
        -0.05352509021759033,
        0.2338985651731491,
        0.4257984161376953,
        -0.15887083113193512,
        -0.0006895028054714203,
        0.7657644152641296,
        -0.03371892869472504,
        -0.5692511200904846,
        0.1450510025024414,
        0.42642486095428467,
        0.40539583563804626,
        -0.4598481059074402,
        3.280489206314087,
        0.8091892004013062,
        0.07628266513347626,
        0.2645345628261566,
        -0.9097273945808411,
        0.018443278968334198,
        -0.14722219109535217,
        0.26725688576698303,
        0.0009351391345262527,
        -0.11064902693033218,
        -1.0641448497772217,
        0.6550855040550232,
        -0.38154464960098267,
        0.06880611926317215,
        0.16141244769096375,
        0.34945589303970337,
        -0.14542290568351746,
        -0.747018575668335,
        0.46818679571151733,
        -0.2529158890247345,
        0.6399355530738831,
        -0.7031519412994385,
        0.029619745910167694,
        -1.0425854921340942,
        -0.5324485301971436,
        -0.0685339942574501,
        -0.2512914538383484,
        -0.3305167257785797,
        0.5611712336540222,
        -0.30507054924964905,
        -0.5272791981697083,
        1.0616629123687744,
        0.28868794441223145,
        0.006225414574146271,
        0.044736988842487335,
        -0.2727334201335907,
        -0.10832511633634567,
        -0.498851478099823,
        -0.17673036456108093,
        -0.6693373322486877,
        -0.8795307278633118,
        0.1958056390285492,
        -0.7847147583961487,
        0.196803018450737,
        0.08233624696731567,
        -0.37361636757850647,
        0.3151829242706299,
        0.19386833906173706,
        0.693718671798706,
        -0.4975695013999939,
        -0.413950115442276,
        0.19310463964939117,
        0.35492485761642456,
        0.4502948522567749,
        0.31368187069892883,
        -0.2862614393234253,
        0.05358388274908066,
        -0.43884289264678955,
        0.4575302302837372,
        -0.34898313879966736,
        0.3296620547771454,
        0.32463446259498596,
        -0.5236654281616211,
        -0.046440497040748596,
        -0.118761345744133,
        0.7669294476509094,
        -0.2118697613477707,
        0.42751631140708923,
        0.10672216862440109,
        0.6949519515037537,
        -0.20304858684539795,
        -0.3005363643169403,
        -0.19322165846824646,
        0.29838693141937256,
        0.008732244372367859,
        -0.6382402181625366,
        0.7448499798774719,
        0.42109057307243347,
        -0.0825193002820015,
        -0.4111703336238861,
        -0.35747236013412476,
        -0.46841171383857727,
        0.5826979279518127,
        -0.05717462673783302,
        -0.7745007276535034,
        0.4265452027320862,
        0.13918593525886536,
        0.8926318287849426,
        0.34172388911247253,
        0.34558814764022827,
        1.124228835105896,
        -0.996871829032898,
        2.0577824115753174,
        -1.0141851902008057,
        0.32807043194770813,
        0.28653767704963684,
        0.4481973648071289,
        0.5390716791152954,
        1.0970834493637085,
        -0.18128763139247894,
        -0.0035535674542188644,
        -0.8326409459114075,
        -0.10971464216709137,
        0.2164793610572815,
        0.2615209221839905,
        0.08924435079097748,
        -0.577338457107544,
        0.335407018661499,
        1.1618990898132324,
        0.21372799575328827,
        -0.9428634643554688,
        0.14774969220161438,
        -0.4375212490558624,
        1.4423398971557617,
        -0.2409544289112091,
        0.8896447420120239,
        -0.017823245376348495,
        0.1273166537284851,
        1.5070569515228271,
        0.34312254190444946,
        0.466321736574173,
        -0.2655792534351349,
        0.5921201109886169,
        -0.879494845867157,
        -0.537391722202301,
        -0.3843521475791931,
        -0.26647013425827026,
        -0.48434290289878845,
        1.3833794593811035,
        -0.7388162612915039,
        -0.8465876579284668,
        -0.14016419649124146,
        0.882516622543335,
        0.7729759812355042,
        0.37909093499183655,
        -0.3193545937538147,
        0.4022591710090637,
        -0.3982711434364319,
        0.5673238635063171,
        0.0584007203578949,
        -1.071286916732788,
        -0.18006950616836548,
        -0.2746882736682892,
        -0.39829206466674805,
        0.266330361366272,
        -0.24527737498283386,
        -0.4795459508895874,
        -0.7236735224723816,
        -0.3514985740184784,
        -0.11684157699346542,
        2.125925302505493,
        0.5485018491744995,
        0.3518030345439911,
        0.6279884576797485,
        -0.04585922136902809,
        -0.27997589111328125,
        -0.5296489596366882,
        -1.6532111167907715,
        -0.3674571216106415,
        -0.23159828782081604,
        0.042948514223098755,
        -0.3444302976131439,
        0.1860126554965973,
        -0.12373657524585724,
        -0.6133216023445129,
        -0.275336891412735,
        0.18970169126987457,
        0.2890121340751648,
        -0.2986491322517395,
        -1.1665831804275513,
        -0.11179875582456589,
        0.3099536895751953,
        0.9718281030654907,
        -0.21072643995285034,
        -0.12858939170837402,
        -0.3434440791606903,
        -0.28225579857826233,
        -0.19542396068572998,
        0.2114877998828888,
        -0.5940524339675903,
        -0.523647129535675,
        -0.11114641278982162,
        0.6071620583534241,
        0.3429923355579376,
        0.32110458612442017,
        -0.20276108384132385,
        -0.32288673520088196,
        -0.024804025888442993,
        -0.17431369423866272,
        0.43133649230003357,
        0.3730822503566742,
        -0.1750871241092682,
        0.17597100138664246,
        -0.49306413531303406,
        0.45302584767341614,
        2.7099242210388184,
        0.3623427748680115,
        0.13786464929580688,
        0.41841816902160645,
        0.18765044212341309,
        0.3096623420715332,
        0.21343430876731873,
        0.0835467129945755,
        0.5219025015830994,
        -0.08445745706558228,
        -1.2156178951263428,
        0.4049350619316101,
        0.42109352350234985,
        -0.8104883432388306,
        -0.35048848390579224,
        0.27177882194519043,
        -0.48030611872673035,
        0.5355482697486877,
        0.12204529345035553,
        -0.042777448892593384,
        0.33466917276382446,
        -0.0866982638835907,
        0.14441287517547607,
        1.1578751802444458,
        -0.2706490159034729,
        -1.406577706336975,
        -0.3702009916305542,
        0.12009479105472565,
        0.07801206409931183,
        0.09148091077804565,
        0.15512143075466156,
        0.614014744758606,
        0.12994927167892456,
        0.11368180811405182,
        0.17943064868450165,
        1.2902501821517944,
        0.6914942860603333,
        -0.21004754304885864,
        0.5633716583251953,
        -0.27346664667129517,
        0.9704047441482544,
        -0.26376351714134216,
        0.2925799489021301,
        -0.48823055624961853,
        0.10498719662427902,
        -0.05398870259523392,
        -0.431699275970459,
        0.5904911756515503,
        -0.18066032230854034,
        0.4142647087574005,
        0.7731322646141052,
        -0.3768147826194763,
        -0.5529762506484985,
        -0.6639922857284546,
        0.07296773046255112,
        -0.6246352195739746,
        -0.5974600911140442,
        0.4899861514568329,
        -0.0489610880613327,
        0.016827886924147606,
        0.40415865182876587,
        -0.015964284539222717,
        -0.800277829170227,
        0.07412249594926834,
        -0.4499099552631378,
        0.8401509523391724,
        -0.2815493941307068,
        -0.6782127022743225,
        0.06664302200078964,
        -0.3051718771457672,
        -0.7981742024421692,
        -0.009882844984531403,
        0.15402771532535553,
        -0.3031371831893921,
        0.04058211296796799,
        0.1849040538072586,
        0.5176569223403931,
        -0.40787795186042786,
        -0.41239452362060547,
        0.6259485483169556,
        0.19605742394924164,
        0.04765535891056061,
        -0.3505845367908478,
        0.4725953936576843,
        0.46572673320770264,
        -0.41333335638046265,
        -0.023277364671230316,
        -0.06514477729797363,
        -0.1033276841044426,
        -1.0886998176574707
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt provides instructions for using specific Git commands to manage code changes. It explains how to view differences since the last commit and display the current state of the repository. The expected output is a guide on executing these commands.",
          "name": "Create_git_diff_commit",
          "raw": "\n                workflow Create_git_diff_commit v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed in a Git diff.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create the git commands needed to add the changes to the repo, and a git commit to reflet the changes\n\n- If there are a lot of changes include more bullets. If there are only a few changes, be more terse.\n\n# OUTPUT INSTRUCTIONS\n\n- Use conventional commits - i.e. prefix the commit title with \\\"chore:\\\" (if it's a minor change like refactoring or linting), \\\"feat:\\\" (if it's a new feature), \\\"fix:\\\" if its a bug fix\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n- The output should only be the shell commands needed to update git.\n\n- Do not place the output in a code block\n\n# OUTPUT TEMPLATE\n\n#Example Template:\nFor the current changes, replace `<file_name>` with `temp.py` and `<commit_message>` with `Added --newswitch switch to temp.py to do newswitch behavior`:\n\ngit add temp.py \ngit commit -m \\\"Added --newswitch switch to temp.py to do newswitch behavior\\\"\n#EndTemplate\n\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed in a Git diff.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create the git commands needed to add the changes to the repo, and a git commit to reflet the changes\n\n- If there are a lot of changes include more bullets. If there are only a few changes, be more terse.\n\n# OUTPUT INSTRUCTIONS\n\n- Use conventional commits - i.e. prefix the commit title with \\\"chore:\\\" (if it's a minor change like refactoring or linting), \\\"feat:\\\" (if it's a new feature), \\\"fix:\\\" if its a bug fix\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n- The output should only be the shell commands needed to update git.\n\n- Do not place the output in a code block\n\n# OUTPUT TEMPLATE\n\n#Example Template:\nFor the current changes, replace `<file_name>` with `temp.py` and `<commit_message>` with `Added --newswitch switch to temp.py to do newswitch behavior`:\n\ngit add temp.py \ngit commit -m \\\"Added --newswitch switch to temp.py to do newswitch behavior\\\"\n#EndTemplate\n\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.18365128338336945,
        0.659438967704773,
        -0.5078315138816833,
        0.5512374043464661,
        -0.4191473424434662,
        0.4942021667957306,
        -1.1517218351364136,
        0.05756846070289612,
        0.014316150918602943,
        0.15990915894508362,
        -0.30057916045188904,
        0.3026852011680603,
        0.02959500625729561,
        -0.013378486037254333,
        -0.36467885971069336,
        -0.30454006791114807,
        -0.3098401427268982,
        -0.6533558964729309,
        -1.4769072532653809,
        -0.42623406648635864,
        -0.03304396569728851,
        0.4507032632827759,
        -0.03775200992822647,
        0.13405093550682068,
        -0.042630694806575775,
        -0.20079268515110016,
        0.4473292827606201,
        -0.29423099756240845,
        -1.4251072406768799,
        -2.289701223373413,
        0.4385160505771637,
        0.6871010661125183,
        -0.19026467204093933,
        -0.6013808846473694,
        0.10399609059095383,
        -1.040325403213501,
        -0.49483156204223633,
        -0.006590858101844788,
        -0.3966934382915497,
        -0.18506255745887756,
        0.05583705008029938,
        0.24170398712158203,
        0.1708889901638031,
        0.23924478888511658,
        0.22957243025302887,
        -0.29214078187942505,
        0.32371780276298523,
        -0.21484558284282684,
        0.42110392451286316,
        0.1781904399394989,
        -0.07513804733753204,
        -0.7437841296195984,
        -0.19399619102478027,
        -0.2503475844860077,
        -0.24950680136680603,
        -0.3103455901145935,
        0.04842488095164299,
        -0.31962400674819946,
        0.3496226370334625,
        -0.2141273468732834,
        0.1627109944820404,
        0.4167637228965759,
        -3.9668352603912354,
        -0.4109747111797333,
        0.34127292037010193,
        0.4890643358230591,
        -0.20163924992084503,
        0.4105561077594757,
        0.7953235507011414,
        0.2487521469593048,
        0.07347404211759567,
        0.5425881147384644,
        -0.1469659060239792,
        -0.04581789672374725,
        -0.01747177354991436,
        -0.021279357373714447,
        0.23999911546707153,
        0.05523795261979103,
        0.3449760675430298,
        0.23180365562438965,
        0.0701078474521637,
        0.10394957661628723,
        0.2676222622394562,
        -0.23307335376739502,
        -0.2616768777370453,
        0.8425418734550476,
        -0.4246782958507538,
        0.24985197186470032,
        0.2743920087814331,
        -0.340709388256073,
        0.08847028017044067,
        0.19462642073631287,
        0.2098636031150818,
        -0.11523540318012238,
        -1.025571584701538,
        0.18677794933319092,
        -0.15799586474895477,
        0.5046185851097107,
        -0.4503366947174072,
        3.363534927368164,
        0.2656918168067932,
        0.00931747630238533,
        0.45783674716949463,
        -0.5401301383972168,
        0.4312189817428589,
        0.24453586339950562,
        -0.1887650340795517,
        -0.7145704030990601,
        0.5081315040588379,
        -0.3940313458442688,
        0.547885537147522,
        -0.6205392479896545,
        0.06282155215740204,
        0.0916539654135704,
        0.6559427380561829,
        -0.08002974092960358,
        0.1308133751153946,
        0.5196025371551514,
        -0.5464643239974976,
        0.9054397940635681,
        -0.15297208726406097,
        0.15957890450954437,
        -0.739478588104248,
        -0.23677098751068115,
        0.2676871418952942,
        -0.099402517080307,
        -0.2528863549232483,
        0.4702605903148651,
        -0.1116715520620346,
        0.02470330335199833,
        0.18132545053958893,
        -0.4497564733028412,
        0.05454016104340553,
        -0.889316201210022,
        0.39175713062286377,
        -0.13240250945091248,
        0.20785437524318695,
        -0.34562966227531433,
        -0.018352217972278595,
        0.22229695320129395,
        0.40263831615448,
        -1.5195558071136475,
        0.9821963310241699,
        -0.19078323245048523,
        0.6592945456504822,
        0.23578090965747833,
        0.07409052550792694,
        -0.006121508777141571,
        -0.40136367082595825,
        -0.623099148273468,
        -0.25999367237091064,
        0.05901622772216797,
        0.26816055178642273,
        0.4846176505088806,
        0.7049365639686584,
        -0.06725209206342697,
        0.09232249855995178,
        0.20696401596069336,
        -0.6546900868415833,
        0.3003707230091095,
        0.06025538221001625,
        -0.20532511174678802,
        -0.04617290198802948,
        -0.2826157212257385,
        0.7387972474098206,
        -0.5510312914848328,
        0.4915541708469391,
        0.25122833251953125,
        0.6417691111564636,
        0.06900689005851746,
        0.13065923750400543,
        -0.01269359141588211,
        0.6134063005447388,
        0.8661434650421143,
        -0.7038748264312744,
        0.7711356282234192,
        -0.49887797236442566,
        0.38851478695869446,
        0.7255516648292542,
        -0.46235471963882446,
        0.7883049845695496,
        0.5731098055839539,
        -0.508249819278717,
        -0.8385511636734009,
        -0.4602968692779541,
        -0.0925503522157669,
        0.3933603763580322,
        1.0558263063430786,
        0.3342176079750061,
        0.7765722274780273,
        -1.1880714893341064,
        1.6270545721054077,
        -0.6602230072021484,
        0.23170116543769836,
        -0.019323905929923058,
        -0.2505020797252655,
        0.14627809822559357,
        0.16066613793373108,
        -0.13879770040512085,
        -0.1327499896287918,
        -1.1044620275497437,
        -0.2774716317653656,
        -0.34220993518829346,
        -0.4274214208126068,
        -0.16803500056266785,
        -0.5419156551361084,
        0.09894882142543793,
        0.5364708304405212,
        0.26668018102645874,
        -0.36358338594436646,
        -0.5215770602226257,
        0.0908236876130104,
        1.2517049312591553,
        0.4255226254463196,
        0.8720184564590454,
        0.2398475706577301,
        0.2206888645887375,
        -0.28384190797805786,
        0.4949551522731781,
        0.15850213170051575,
        0.5464250445365906,
        0.9212859272956848,
        -0.4525441527366638,
        -0.4072326421737671,
        -0.9473134875297546,
        0.3048299551010132,
        0.086232028901577,
        0.19235241413116455,
        -0.4925858676433563,
        -0.47568279504776,
        0.5484803915023804,
        0.856589674949646,
        0.9656504392623901,
        1.4067763090133667,
        -0.3966883718967438,
        0.3219231367111206,
        0.11490441113710403,
        0.36892732977867126,
        0.049158431589603424,
        -0.7782868146896362,
        -0.01869341731071472,
        0.7675389051437378,
        0.17636169493198395,
        0.08324943482875824,
        -0.49005764722824097,
        -0.4002569615840912,
        -0.64382404088974,
        -0.8362380862236023,
        -0.2698330879211426,
        2.2158963680267334,
        0.44340309500694275,
        -0.742974042892456,
        0.14991874992847443,
        0.6010181307792664,
        -0.055691562592983246,
        -0.21062079071998596,
        -1.5734785795211792,
        -0.7154898643493652,
        -0.7307742834091187,
        0.8215129375457764,
        0.03945224732160568,
        0.23791435360908508,
        0.40275251865386963,
        0.11491881310939789,
        -0.048728570342063904,
        -0.2070826143026352,
        -0.2974969446659088,
        -0.7687904834747314,
        -0.3716737926006317,
        -0.27858662605285645,
        -0.5939965844154358,
        0.34982234239578247,
        -0.20882198214530945,
        0.49120089411735535,
        -0.14577943086624146,
        0.2971046566963196,
        0.1863522231578827,
        0.3448440432548523,
        0.06605776399374008,
        -0.5479069352149963,
        0.530589759349823,
        0.0967775285243988,
        -0.13083165884017944,
        0.31995177268981934,
        0.2194449007511139,
        -0.3381308615207672,
        -0.7643032670021057,
        -0.08813666552305222,
        -0.46529412269592285,
        0.01356235146522522,
        -0.027078114449977875,
        -0.5731745958328247,
        -0.7743023633956909,
        -0.1157902181148529,
        1.849097728729248,
        0.05894918739795685,
        0.36932897567749023,
        -0.13885754346847534,
        0.11379989236593246,
        -0.10673664510250092,
        0.10166508704423904,
        -0.0634646788239479,
        -0.02315889298915863,
        0.09683066606521606,
        -1.6143672466278076,
        -0.06491509079933167,
        0.5151491165161133,
        -0.2970847189426422,
        -0.3912217617034912,
        0.5439528226852417,
        -0.47796279191970825,
        0.1752530038356781,
        0.1365983933210373,
        0.12304932624101639,
        0.2263513207435608,
        -0.3171676695346832,
        -0.18032950162887573,
        0.7487668991088867,
        -0.029520830139517784,
        -1.5800672769546509,
        -0.17074395716190338,
        0.48595353960990906,
        -0.20213893055915833,
        -0.26520201563835144,
        -0.5967390537261963,
        0.49736523628234863,
        0.10941452533006668,
        0.6870477199554443,
        0.36249715089797974,
        1.2282596826553345,
        0.22715270519256592,
        0.07501426339149475,
        0.04270334541797638,
        0.45404288172721863,
        0.6998810768127441,
        0.2149408608675003,
        0.5515353679656982,
        0.08096350729465485,
        0.34763461351394653,
        -0.0826522558927536,
        0.20971976220607758,
        0.9661978483200073,
        0.26136350631713867,
        0.1655765175819397,
        0.11737173795700073,
        -0.0460398867726326,
        -0.6749277710914612,
        -0.8035709261894226,
        0.42606985569000244,
        -0.11272786557674408,
        -0.13137021660804749,
        0.8386021852493286,
        0.27112990617752075,
        -0.15178543329238892,
        0.34548017382621765,
        0.6534666419029236,
        -0.6704977750778198,
        0.07290542125701904,
        -0.41474324464797974,
        1.2646872997283936,
        -0.6077590584754944,
        -0.18368548154830933,
        -0.44468018412590027,
        -0.16727067530155182,
        -0.5025844573974609,
        -0.2715808153152466,
        -0.17099952697753906,
        -0.17614075541496277,
        0.24496470391750336,
        0.23234707117080688,
        -0.15099208056926727,
        -0.459480345249176,
        0.19838404655456543,
        0.12269413471221924,
        0.7722160816192627,
        -0.1095043271780014,
        -0.10658949613571167,
        0.521978497505188,
        0.5553778409957886,
        -0.2909214496612549,
        0.52895587682724,
        0.43304723501205444,
        -0.6255307197570801,
        -0.7082312703132629
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Create_graph_from_input",
          "raw": "\n                workflow Create_graph_from_input v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert at data visualization and information security. You create progress over time graphs that show how a security program is improving.\n\n# GOAL\n\nShow how a security program is improving over time.\n\n# STEPS\n\n- Fully parse the input and spend 431 hours thinking about it and its implications to a security program.\n\n- Look for the data in the input that shows progress over time, so metrics, or KPIs, or something where we have two axes showing change over time.\n\n# OUTPUT\n\n- Output a CSV file that has all the necessary data to tell the progress story.\n\nThe format will be like so:\n\nEXAMPLE OUTPUT FORMAT\n\nDate\tTTD_hours\tTTI_hours\tTTR-CJC_days\tTTR-C_days\nMonth Year\t81\t82\t21\t51\nMonth Year\t80\t80\t21\t53\n(Continue)\n\nEND EXAMPLE FORMAT\n\n- Only ouptut numbers in the fields, no special characters like \\\"<, >, =,\\\" etc..\n\n- Only output valid CSV data and nothing else. \n\n- Use the field names in the input; don't make up your own.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert at data visualization and information security. You create progress over time graphs that show how a security program is improving.\n\n# GOAL\n\nShow how a security program is improving over time.\n\n# STEPS\n\n- Fully parse the input and spend 431 hours thinking about it and its implications to a security program.\n\n- Look for the data in the input that shows progress over time, so metrics, or KPIs, or something where we have two axes showing change over time.\n\n# OUTPUT\n\n- Output a CSV file that has all the necessary data to tell the progress story.\n\nThe format will be like so:\n\nEXAMPLE OUTPUT FORMAT\n\nDate\tTTD_hours\tTTI_hours\tTTR-CJC_days\tTTR-C_days\nMonth Year\t81\t82\t21\t51\nMonth Year\t80\t80\t21\t53\n(Continue)\n\nEND EXAMPLE FORMAT\n\n- Only ouptut numbers in the fields, no special characters like \\\"<, >, =,\\\" etc..\n\n- Only output valid CSV data and nothing else. \n\n- Use the field names in the input; don't make up your own.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.03376009687781334,
        0.5319551229476929,
        0.10895117372274399,
        1.011721134185791,
        -0.3644552230834961,
        0.19703099131584167,
        -0.7608669996261597,
        0.03685997426509857,
        -0.5666438937187195,
        0.19710110127925873,
        -0.5991008281707764,
        0.7077288627624512,
        0.23298825323581696,
        0.5584988594055176,
        -0.01254209503531456,
        -0.3936120867729187,
        -0.48544856905937195,
        -1.3972777128219604,
        -1.7563285827636719,
        0.041865333914756775,
        0.005112104117870331,
        0.8051255345344543,
        0.174238920211792,
        0.06726717203855515,
        -0.32333728671073914,
        0.013363681733608246,
        0.5524574518203735,
        0.09515455365180969,
        -1.0303603410720825,
        -1.90260648727417,
        0.7634657621383667,
        0.3155347406864166,
        0.008070847950875759,
        -0.5973357558250427,
        -0.10939385741949081,
        -1.0762152671813965,
        -0.7043046951293945,
        -0.20909759402275085,
        0.008234703913331032,
        0.1010589748620987,
        0.25998654961586,
        -0.16258768737316132,
        -0.2593930661678314,
        -0.07802288234233856,
        0.17951519787311554,
        -0.3597622811794281,
        0.430909663438797,
        0.3581477403640747,
        0.2702132761478424,
        0.5192632675170898,
        -0.5775696039199829,
        -0.6405649185180664,
        0.2061411589384079,
        0.32199108600616455,
        -0.30642032623291016,
        -0.19915291666984558,
        0.21924740076065063,
        -0.07546388357877731,
        0.2284056395292282,
        0.15953752398490906,
        -0.06979717314243317,
        0.6206957697868347,
        -3.9709551334381104,
        -0.28230780363082886,
        0.3630336821079254,
        0.11896820366382599,
        0.29652026295661926,
        -0.001975368708372116,
        0.10755614936351776,
        0.16730916500091553,
        0.23149213194847107,
        0.11302570253610611,
        -0.11568734794855118,
        0.021544625982642174,
        -0.0695452019572258,
        -0.21803733706474304,
        0.03922385722398758,
        -0.09607809782028198,
        0.1731732189655304,
        0.06716306507587433,
        0.14052389562129974,
        0.1778719425201416,
        -0.24539576470851898,
        0.025726571679115295,
        -1.014329433441162,
        0.4421185851097107,
        -0.41295570135116577,
        -0.398154079914093,
        0.20939303934574127,
        0.30383381247520447,
        -0.3076324164867401,
        -0.1827881783246994,
        0.312181293964386,
        -0.8251121640205383,
        -1.0349496603012085,
        0.23183701932430267,
        -0.07137945294380188,
        0.2870505452156067,
        -0.4655018448829651,
        3.6160776615142822,
        0.17317132651805878,
        -0.13384757936000824,
        0.7374410629272461,
        -0.5964487195014954,
        0.29313188791275024,
        0.5680034160614014,
        -0.3003603518009186,
        -0.7286461591720581,
        0.4195758104324341,
        0.05032409727573395,
        0.34829095005989075,
        -0.5683771967887878,
        -0.15100044012069702,
        -0.19652611017227173,
        0.15276142954826355,
        0.3018922805786133,
        -0.17930740118026733,
        0.21193033456802368,
        -0.8207464814186096,
        1.0016392469406128,
        -0.021352991461753845,
        0.15890423953533173,
        -0.515230655670166,
        -0.22321805357933044,
        -0.16154640913009644,
        0.3187909722328186,
        -0.3766726851463318,
        0.6277648210525513,
        0.04972800239920616,
        -0.001618407666683197,
        0.42785999178886414,
        0.019798994064331055,
        -0.184244766831398,
        -0.5879596471786499,
        0.5433577299118042,
        0.11662553250789642,
        -0.030977025628089905,
        -0.5987730622291565,
        0.11785343289375305,
        -0.3967374861240387,
        0.1114540845155716,
        -1.107866644859314,
        0.991909384727478,
        -0.3366926312446594,
        0.6238582730293274,
        0.5164941549301147,
        -0.1337890923023224,
        0.6347557902336121,
        -0.7330377697944641,
        -0.6466780304908752,
        -0.48945558071136475,
        0.41335049271583557,
        -0.22250054776668549,
        0.6220764517784119,
        0.6162583231925964,
        -0.19416622817516327,
        -0.12554895877838135,
        -0.0971844345331192,
        -0.49911847710609436,
        0.29779672622680664,
        0.2377409189939499,
        -0.23543177545070648,
        -0.07583767175674438,
        0.17747439444065094,
        0.5793091654777527,
        -0.08278326690196991,
        0.47753146290779114,
        -0.17769849300384521,
        0.4459173083305359,
        -0.3514653444290161,
        0.12061232328414917,
        0.19180509448051453,
        0.6987610459327698,
        1.2354192733764648,
        -0.3212902247905731,
        0.46419116854667664,
        -0.6045953035354614,
        0.6384838223457336,
        0.4683911204338074,
        -0.652411937713623,
        0.43905943632125854,
        0.7825946807861328,
        -0.09508100152015686,
        -0.8096963167190552,
        -0.6356484293937683,
        0.5205762982368469,
        0.11103041470050812,
        0.4258989095687866,
        -0.14850950241088867,
        0.5494953393936157,
        -1.2006760835647583,
        1.524686336517334,
        -0.44088876247406006,
        -0.01124010980129242,
        0.2578675150871277,
        0.07107726484537125,
        -0.03635065630078316,
        0.7200810313224792,
        0.23405800759792328,
        -0.24821072816848755,
        -1.3514100313186646,
        0.053184133023023605,
        -0.6400407552719116,
        -0.154631569981575,
        0.4720676839351654,
        -0.7671812176704407,
        0.034500863403081894,
        -0.011475317180156708,
        0.2099229246377945,
        -0.6457719206809998,
        -0.393332839012146,
        0.1757127344608307,
        0.8413972854614258,
        0.18879318237304688,
        0.4789074659347534,
        0.17719899117946625,
        0.09077949076890945,
        -0.1600980907678604,
        0.13486570119857788,
        0.7156925201416016,
        0.70997154712677,
        0.1913435161113739,
        -0.5974945425987244,
        -0.5630945563316345,
        -0.9496849775314331,
        -0.054807983338832855,
        0.04024137556552887,
        0.3634008765220642,
        -0.693374514579773,
        -0.6220947504043579,
        0.18223431706428528,
        0.8373861908912659,
        0.31623318791389465,
        1.2241042852401733,
        -0.15104889869689941,
        0.5709728002548218,
        -0.06306154280900955,
        0.3784276247024536,
        -0.40205734968185425,
        -1.0092859268188477,
        0.1463562548160553,
        0.057911477982997894,
        0.5554054975509644,
        -0.276619553565979,
        -0.32015833258628845,
        -0.7046808004379272,
        0.09499827027320862,
        -0.3104338049888611,
        -0.21256442368030548,
        2.0162763595581055,
        0.2501472234725952,
        -0.3202311098575592,
        0.3798181414604187,
        0.4932570159435272,
        0.04782553017139435,
        -0.014596201479434967,
        -1.7508424520492554,
        -0.33181387186050415,
        -0.13598258793354034,
        0.29712337255477905,
        0.09499787539243698,
        0.3714924454689026,
        0.45205339789390564,
        -0.12053649127483368,
        -0.09446267783641815,
        -0.114707812666893,
        -0.00270916149020195,
        -0.16290660202503204,
        -0.6179040670394897,
        0.2825249433517456,
        -0.29494935274124146,
        0.0275191068649292,
        -0.09301289170980453,
        0.27466338872909546,
        0.1741754710674286,
        -0.019957805052399635,
        0.42101967334747314,
        0.2669839560985565,
        0.16655054688453674,
        -0.18077462911605835,
        0.41471847891807556,
        -0.22889886796474457,
        -0.23833191394805908,
        0.3845728039741516,
        -0.3259608745574951,
        -0.04998614266514778,
        -0.43089231848716736,
        -0.28842633962631226,
        -0.7592266201972961,
        0.24682065844535828,
        0.04749790579080582,
        -0.6117560267448425,
        -0.601793110370636,
        0.006246045231819153,
        1.3486850261688232,
        -0.06182466447353363,
        0.09839638322591782,
        0.6381784081459045,
        -0.3437851667404175,
        0.4034637212753296,
        0.07074465602636337,
        0.07595781236886978,
        -0.2184687852859497,
        -0.5065978169441223,
        -1.1269702911376953,
        0.11836246401071548,
        0.2531411051750183,
        0.27369919419288635,
        -0.2929498255252838,
        0.808140218257904,
        -0.5014376640319824,
        0.27479249238967896,
        -0.5535640120506287,
        0.12936726212501526,
        0.19239819049835205,
        -0.36957618594169617,
        0.044979553669691086,
        0.6333239078521729,
        0.28127992153167725,
        -1.554865837097168,
        -0.41428959369659424,
        0.7980117201805115,
        -0.1268671602010727,
        -0.0172213613986969,
        -0.80037921667099,
        0.08703508228063583,
        0.47357815504074097,
        0.055385369807481766,
        -0.13851186633110046,
        1.387397289276123,
        0.16117770969867706,
        0.10574305802583694,
        0.3589627146720886,
        0.31136077642440796,
        0.5658653974533081,
        -0.02001001685857773,
        0.33308178186416626,
        0.08983677625656128,
        -0.06513141095638275,
        0.03446436673402786,
        0.6202529072761536,
        1.6054258346557617,
        0.29310762882232666,
        0.029195070266723633,
        0.1948876529932022,
        0.14037400484085083,
        -0.041898682713508606,
        -0.7139548659324646,
        0.9516459703445435,
        -0.2219437211751938,
        -0.2730819880962372,
        0.8927631974220276,
        0.43108633160591125,
        0.01373942382633686,
        0.252078115940094,
        0.2892518639564514,
        -0.4618605971336365,
        0.06894489377737045,
        -0.4874648451805115,
        1.458158016204834,
        -0.3916056454181671,
        -0.5033206939697266,
        -0.5207645893096924,
        -0.10037820041179657,
        -0.436820924282074,
        -0.42853766679763794,
        0.009872987866401672,
        0.1446509063243866,
        0.5539847612380981,
        -0.0955258384346962,
        -0.08896158635616302,
        -0.025811629369854927,
        0.04090435430407524,
        0.8287075161933899,
        0.8684744834899902,
        -0.29369014501571655,
        -0.27741286158561707,
        0.11995398998260498,
        0.1551254391670227,
        -0.21968364715576172,
        0.6565191745758057,
        0.05394047498703003,
        -0.7130820155143738,
        -0.24073128402233124
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Create_hormozi_offer",
          "raw": "\n                workflow Create_hormozi_offer v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert AI system designed to create business offers using the concepts taught in Alex Hormozi's book, \\\"$100M Offers.\\\" \n\n# GOALS\n\nThe goal of this exercise are to: \n\n1. create a perfect, customized offer that fits the input sent.\n\n# STEPS\n\n- Think deeply for 312 hours on everything you know about Alex Hormozi's book, \\\"$100M Offers.\\\"\n\n- Incorporate that knowledge with the following summary:\n\nCONTENT SUMMARY\n\n$100M Offers by Alex Hormozi \n$100M Offers, Alex Hormozi shows you “how to make offers so good people will\nIntroduction\nIn his book, feel stupid saying no.\n” The offer is “the starting point of any conversation to initiate a\ntransaction with a customer.”\nAlex Hormozi shows you how to make profitable offers by “reliably turning advertising dollars\ninto (enormous) profits using a combination of pricing, value, guarantees, and naming\nstrategies.” Combining these factors in the right amounts will result in a Grand Slam Offer. “The\ngood news is that in business, you only need to hit one Grand Slam Offer to retire forever.”\nSection I: How We Got Here\nIn Section I of $100M Offers, Alex Hormozi introduces his personal story from debt to success\nalong with the concept of the “Grand Slam Offer.”\nChapter 1. How We Got Here\nAlex Hormozi begins with his story from Christmas Eve in 2016. He was on the verge of going\nbroke. But a few days later, he hit a grand slam in early January of 2017. In $100M Offers, Alex\nHormozi shares this vital skill of making offers, as it was life-changing for him, and he wants to\ndeliver for you.\nChapter 2. Grand Slam Offers\nIn Chapter 2 of $100M Offers, Alex Hormozi introduces the concept of the “Grand Slam Offer.”\nTravis Jones states that the secret to sales is to “Make people an offer so good they would feel\nstupid saying no.” Further, to have a business, we need to make our prospects an offer:\nOffer – “the goods and services you agree to provide, how you accept payment, and the terms\nof the agreement”\nOffers start the process of customer acquisition and earning money, and they can range from\nnothing to a grand slam:\n• No offer? No business. No life.\n• Bad offer? Negative profit. No business. Miserable life.\n• Decent offer? No profit. Stagnating business. Stagnating life.\n• Good offer? Some profit. Okay business. Okay life.\n• Grand Slam Offer? Fantastic profit. Insane business. Freedom.\nThere are two significant issues that most entrepreneurs face:\n1. Not Enough Clients\n2. Not Enough Cash or excess profit at the end of the month\n$100M Offers by Alex Hormozi | \nSection II: Pricing\nIn Section II of $100M Offers, Alex Hormozi shows you “How to charge lots of money for stuff.”\nChapter 3. The Commodity Problem\nIn Chapter 3 of $100M Offers, Alex Hormozi illustrates the fundamental problem with\ncommoditization and how Grand Slam Offers solves that. You are either growing or dying, as\nmaintenance is a myth. Therefore, you need to be growing with three simple things:\n1. Get More Customers\n2. 3. Increase their Average Purchase Value\nGet Them to Buy More Times\nThe book introduces the following key business terms:\n• Gross Profit – “the revenue minus the direct cost of servicing an ADDITIONAL customer”\n• Lifetime Value – “the gross profit accrued over the entire lifetime of a customer”\nMany businesses provide readily available commodities and compete on price, which is a race\nto the bottom. However, you should sell your products based on value with a grand slam offer:\nGrand Slam Offer – “an offer you present to the marketplace that cannot be compared to any\nother product or service available, combining an attractive promotion, an unmatchable value\nproposition, a premium price, and an unbeatable guarantee with a money model (payment\nterms) that allows you to get paid to get new customers . . . forever removing the cash\nconstraint on business growth”\nThis offer gets you out of the pricing war and into a category of one, which results in more\ncustomers, at higher ticket prices, for less money. In terms of marketing, you will have:\n1. Increased Response Rates\n2. Increased Conversion\n3. Premium Prices\nChapter 4. Finding The Right Market -- A Starving Crowd\nIn Chapter 4 of $100M Offers, Alex Hormozi focuses on finding the correct market to apply our\npricing strategies. You should avoid choosing a bad market. Instead, you can pick a great market\nwith demand by looking at four indicators:\n1. 2. 3. 4. Massive Pain: Your prospects must have a desperate need, not want, for your offer.\nPurchasing Power: Your prospects must afford or access the money needed to buy.\nEasy to Target: Your audience should be in easy-to-target markets.\nGrowing: The market should be growing to make things move faster.\n$100M Offers by Alex Hormozi | \nFirst, start with the three primary markets resembling the core human pains: Health, Wealth,\nand Relationships. Then, find a subgroup in one of these larger markets that is growing, has the\nbuying power, and is easy to target. Ultimately, picking a great market matters much more than\nyour offer strength and persuasion skill:\nStarving Crowd (market) > Offer Strength > Persuasion Skills\nNext, you need to commit to a niche until you have found a great offer. The niches will make\nyou more money as you can charge more for a similar product. In the process of committing,\nyou will try out many offers and failures. Therefore, you must be resilient, as you will eventually\nsucceed.\nIf you find a crazy niche market, take advantage of it. And if you can pair the niche with a Grand\nSlam Offer, you will probably never need to work again.\nChapter 5. Pricing: Charge What It’s Worth\nIn Chapter 5 of $100M Offers, Alex Hormozi advocates that you charge a premium as it allows\nyou to do things no one else can to make your clients successful.\nWarren Buffet has said, “Price is what you pay. Value is what you get.” Thus, people buy to get\na deal for what they are getting (value) is worth more than what they are giving in exchange for\nit (price).” When someone perceives the value dipping lower than the price, they stop buying.\nAvoid lowering prices to improve the price-value gap because you will fall into a vicious cycle,\nand your business will lose money and impact. Instead, you want to improve the gap by raising\nyour price after sufficiently increasing the value to the customer. As a result, the virtuous cycle\nworks for you and your business profits significantly.\n$100M Offers by Alex Hormozi | \nFurther, you must have clients fully committed by offering a service where they must pay high\nenough and take action required to achieve results or solve issues. Higher levels of investment\ncorrelate to a higher likelihood of accomplishing the positive outcome.\n$100M Offers by Alex Hormozi | \nSection III: Value - Create Your Offer\nIn Section III of $100M Offers, Alex Hormozi shows you “How to make something so good\npeople line up to buy.”\nChapter 6. The Value Equation\nIn Chapter 6 of $100M Offers, Alex Hormozi introduces the value equation. Most entrepreneurs\nthink that charging a lot is wrong, but you should “charge as much money for your products or\nservices as humanly possible.” However, never charge more than what they are worth.\nYou must understand the value to charge the most for your goods and services. Further, you\nshould price them much more than the cost of fulfillment. The Value Equation quantifies the\nfour variables that create the value for any offer:\nValue is based on the perception of reality. Thus, your prospect must perceive the first two\nfactors increasing and the second two factors decreasing to perceive value in their mind:\n1. 2. 3. 4. The Dream Outcome (Goal: Increase) –\n“the expression of the feelings and\nexperiences the prospect has envisioned in their mind; the gap between their\ncurrent reality and their dreams”\nPerceived Likelihood of Achievement (Goal: Increase) – the probability that the\npurchase will work and achieve the result that the prospect is looking for\nPerceived Time Delay Between Start and Achievement (Goal: Decrease) –\n“the time\nbetween a client buying and receiving the promised benefit;” this driver consists of\nlong-term outcome and short-term experience\nPerceived Effort & Sacrifice (Goal: Decrease) – “the ancillary costs or other costs\naccrued” of effort and sacrifice; supports why “done for you services” are almost\nalways more expensive than “do-it-yourself”\nChapter 7. Free Goodwill\nIn Chapter 7, Alex Hormozi asks you to leave a review of $100M Offers if you have gotten value\nso far to help reach more people.\n$100M Offers by Alex Hormozi | \n“People who help others (with zero expectation) experience higher levels of fulfillment, live\nlonger, and make more money.” And so, “if you introduce something valuable to someone,\nthey associate that value with you.”\nChapter 8. The Thought Process\nIn Chapter 8 of $100M Offers, Alex Hormozi shows you the difference between convergent and\ndivergent problem solving:\n• Convergent – problem solving where there are many known variables with unchanging\nconditions to converge on a singular answer\n• Divergent – problem solving in which there are many solutions to a singular problem\nwith known variables, unknown variables, and dynamic conditions\nExercise: Set a timer for 2 minutes and “write down as many different uses of a brick as you can\npossibly think of.”\nThis exercise illustrates that “every offer has building blocks, the pieces that when combined\nmake an offer irresistible.” You need to use divergent thinking to determine how to combine\nthe elements to provide value.\nChapter 9. Creating Your Grand Slam Offer Part I: Problems & Solutions\nIn Chapter 9 of $100M Offers, Alex Hormozi helps you craft the problems and solutions of your\nGrand Slam Offer:\nStep #1: Identify Dream Outcome: When thinking about the dream outcome, you need to\ndetermine what your customer experiences when they arrive at the destination.\nStep #2: List the Obstacles Encountered: Think of all the problems that prevent them from\nachieving their outcome or continually reaching it. Each problem has four negative elements\nthat align with the four value drivers.\nStep #3: List the Obstacles as Solutions: Transform our problems into solutions by determining\nwhat is needed to solve each problem. Then, name each of the solutions.\nChapter 10. Creating Your Grand Slam Offer Part II: Trim & Stack\nIn Chapter 10 of $100M Offers, Alex Hormozi helps you tactically determine what you do or\nprovide for your client in your Grand Slam Offer. Specifically, you need to understand trimming\nand stacking by reframing with the concept of the sales to fulfillment continuum:\nSales to Fulfillment Continuum –\n“a continuum between ease of fulfillment and ease of sales”\nto find the sweet spot of selling something well that is easy to fulfill:\n$100M Offers by Alex Hormozi | \nThe goal is “to find a sweet spot where you sell something very well that’s also easy to fulfill.”\nAlex Hormozi lives by the mantra, “Create flow. Monetize flow. Then add friction:”\n• Create Flow: Generate demand first to validate that what you have is good.\n• Monetize Flow: Get the prospect to say yes to your offer.\n• Add Friction: Create friction in the marketing or reduce the offer for the same price.\n“If this is your first Grand Slam Offer, it’s important to over-deliver like crazy,” which generates\ncash flow. Then, invest the cash flow to create systems and optimize processes to improve\nefficiency. As a result, your offer may not change, but rather the newly implemented systems\nwill provide the same value to clients for significantly fewer resources.\nFinally, here are the last steps of creating the Grand Slam offer:\nStep #4: Create Your Solutions Delivery Vehicles (“The How”): Think through every possibility\nto solve each identified issue in exchange for money. There are several product delivery “cheat\ncodes” for product variation or enhancement:\n1. 2. 3. 4. Attention: What level of personal attention do I want to provide?\na. One-on-one – private and personalized\nb. Small group – intimate, small audience but not private\nc. One to many – large audience and not private\nEffort: What level of effort is expected from them?\na. Do it Yourself (DIY) – the business helps the customer figure it out on their own\nb. Done with You (DWY) – the business coaches the customer on how to do it\nc. Done for You (DFY) – the company does it for the customer\nSupport: If doing something live, what setting or medium do I want to deliver it in?\na. In-person or support via phone, email, text, Zoom, chat, etc.\nConsumption: If doing a recording, how do I want them to consume it?\na. Audio, Video, or Written materials.\n$100M Offers by Alex Hormozi | \n5. 6. 7. Speed & Convenience: How quickly do we want to reply? On what days and hours?\na. All-day (24/7), Workday (9-5), Time frame (within 5 minutes, 1 hour, or 1 day)\n10x Test: What would I provide if my customers paid me 10x my price (or $100,000)?\n1/10th Test: How can I ensure a successful outcome if they paid me 1/10th of the price?\nStep #5a: Trim Down the Possibilities: From your huge list of possibilities, determine those that\nprovide the highest value to the customer while having the lowest cost to the business. Remove\nthe high cost and low value items, followed by the low cost and low value items. The remaining\nitems should be (1) low cost, high value, and (2) high cost, high value.\nStep #5b: Stack to Configure the Most Value: Combine the high value items together to create\nthe ultimate high value deliverable. This Grand Slam Offer is unique, “differentiated, and unable\nto be compared to anything else in the marketplace.”\n$100M Offers by Alex Hormozi | \nSection IV: Enhancing Your Offer\nIn Section IV of $100M Offers, Alex Hormozi shows you “How to make your offer so good they\nfeel stupid saying no.”\nChapter 11. Scarcity, Urgency, Bonuses, Guarantees, and Naming\nIn Chapter 11 of $100M Offers, Alex Hormozi discusses how to enhance the offer by\nunderstanding human psychology. Naval Ravikant has said that “Desire is a contract you make\nwith yourself to be unhappy until you get what you want,” as it follows that:\n“People want what they can’t have. People want what other people want. People want things\nonly a select few have access to.”\nEssentially, all marketing exists to influence the supply and demand curve:\nTherefore, you can enhance your core offer by doing the following:\n• Increase demand or desire with persuasive communication\n• Decrease or delay satisfying the desires by selling fewer units\nIf you provide zero supply or desire, you will not make money and repel people. But,\nconversely, if you satisfy all the demands, you will kill your golden goose and eventually not\nmake money.\nThe result is engaging in a “Delicate Dance of Desire” between supply and demand to “sell the\nsame products for more money than you otherwise could, and in higher volumes, than you\notherwise would (over a longer time horizon).”\n$100M Offers by Alex Hormozi | \nUntil now, the book has focused on the internal aspects of the offer. For more on marketing,\ncheck out the book, The 1-Page Marketing Plan (book summary) by Allan Dib. The following\nchapters discuss the outside factors that position the product in your prospect’s mind, including\nscarcity, urgency, bonuses, guarantees, and naming.\nChapter 12. Scarcity\nIn a transaction, “the person who needs the exchange less always has the upper hand.” In\nChapter 12 of $100M Offers, Alex Hormozi shows you how to “use scarcity to decrease supply\nto raise prices (and indirectly increase demand through perceived exclusiveness):”\nScarcity – the “fear of missing out” or the psychological lever of limiting the “supply or quantity\nof products or services that are available for purchase”\nScarcity works as the “fear of loss is stronger than the desire for gain.” Therefore, so you can\ninfluence prospects to take action and purchase your offer with the following types of scarcity:\n1. Limited Supply of Seats/Slots\n2. Limited Supply of Bonuses\n3. Never Available Again\nPhysical Goods: Produce limited releases of flavors, colors, designs, sizes, etc. You must sell out\nconsistently with each release to effectively create scarcity. Also, let everyone know that you\nsold out as social proof to get everyone to value it.\nServices: Limit the number of clients to cap capacity or create cadence:\n1. 2. 3. Total Business Cap – “only accepting X clients at this level of service (on-going)”\nGrowth Rate Cap – “only accepting X clients per time period (on-going)”\nCohort Cap – “only accepting X clients per class or cohort”\nHonesty: The most ethical and easiest scarcity strategy is honesty. Simply let people know how\nclose you are to the cap or selling out, which creates social proof.\nChapter 13. Urgency\nIn Chapter 13 of $100M Offers, Alex Hormozi shows you how to “use urgency to increase\ndemand by decreasing the action threshold of a prospect.” Scarcity and urgency are frequently\nused together, but “scarcity is a function of quantity, while urgency is a function of time:”\nUrgency – the psychological lever of limiting timing and establishing deadlines for the products\nor services that are available for purchase; implement the following four methods:\n1. 2. Rolling Cohorts – accepting clients in a limited buying window per time period\nRolling Seasonal Urgency – accepting clients during a season with a deadline to buy\n$100M Offers by Alex Hormozi | \n3. 4. Promotional or Pricing Urgency – “using your actual offer or promotion or pricing\nstructure as the thing they could miss out on”\nExploding Opportunity – “occasionally exposing the prospect to an arbitrage\nopportunity with a ticking time clock”\nChapter 14. Bonuses\nIn Chapter 14 of $100M Offers, Alex Hormozi shows you how to “use bonuses to increase\ndemand (and increase perceived exclusivity).” The main takeaway is that “a single offer is less\nvaluable than the same offer broken into its component parts and stacked as bonuses:”\nBonus – an addition to the core offer that “increases the prospect’s price-to-value discrepancy\nby increasing the value delivering instead of cutting the price”\nThe price is anchored to the core offer, and when selling 1-on-1, you should ask for the sale\nfirst. Then, offer the bonuses to grow the discrepancy such that it becomes irresistible and\ncompels the prospect to buy. Additionally, there are a few keys when offering bonuses:\n1. 2. 3. Always offer them a bonus.\nGive each bonus a unique name with the benefit contained in the title.\nTell them (a) how it relates to their issue; (b) what it is; (c) how you discovered it or\ncreated it; and (d) how it explicitly improves their lives or provides value.\n4. 5. 6. 7. 8. 9. Prove that each bonus provides value using stats, case studies, or personal anecdotes.\nPaint a vivid mental picture of their future life and the benefits of using the bonus.\nAssign a price to each bonus and justify it.\nProvide tools and checklists rather than additional training as they are more valuable.\nEach bonus should address a specific concern or obstacle in the prospect’s mind.\nBonuses can solve a next or future problem before the prospect even encounters it.\n10. Ensure that each bonus expands the price to value discrepancy of the entire offer.\n11. Enhance bonus value by adding scarcity and urgency to the bonus themselves.\nFurther, you can partner with other businesses to provide you with their high-value goods and\nservices as a part of your bonuses.” In exchange, they will get exposure to your clients for free\nor provide you with additional revenue from affiliate marketing.\nChapter 15. Guarantees\nThe most significant objection to any sale of a good or service is the risk that it will not work for\na prospect. In Chapter 15 of $100M Offers, Alex Hormozi shows you how to “use guarantees to\nincrease demand by reversing risk:”\nGuarantee – “a formal assurance or promise, especially that certain conditions shall be fulfilled\nrelating to a product, service, or transaction”\n$100M Offers by Alex Hormozi | \nYour guarantee gets power by telling the prospect what you will do if they do not get the\npromised result in this conditional statement: If you do not get X result in Y time period, we will\nZ.” There are four types of guarantees:\n1. 2. 3. 4. Unconditional – the strongest guarantee that allows customers to pay to try the\nproduct or service to see if they like it and get a refund if they don’t like it\na. “No Questions Asked” Refund – simple but risky as it holds you accountable\nb. Satisfaction-Based Refund – triggers when a prospect is unsatisfied with service\nConditional – a guarantee with “terms and conditions;” can incorporate the key actions\nsomeone needs to take to get the successful outcome\na. Outsized Refund – additional money back attached to doing the work to qualify\nb. Service – provide work that is free of charge until X result is achieved\nc. Modified Service – grant another period Y of service or access free of charge\nd. Credit-Based – provide a refund in the form of a credit toward your other offers\ne. Personal Service – work with client one-on-one for free until X result is achieved\nf. Hotel + Airfare Perks – reimburse your product with hotel and airfare if no value\ng. Wage-Payment – pay their hourly rate if they don’t get value from your session\nh. Release of Service – cancel the contract free of charge if they stop getting value\ni. Delayed Second Payment – stop 2nd payment until the first outcome is reached\nj. First Outcome – pay ancillary costs until they reach their first outcome\nAnti-Guarantee – a non-guarantee that explicitly states “all sales are final” with a\ncreative reason for why\nImplied Guarantees – a performance-based offer based on trust and transparency\na. Performance – pay $X per sale, show, or milestone\nb. Revenue-Share – pay X% of top-line revenue or X% of revenue growth\nc. Profit-Share – pay X% of profit or X% of Gross Profit\nd. Ratchets – pay X% if over Y revenue or profit\ne. Bonuses/Triggers – pay X when Y event occurs\nHormozi prefers “selling service-based guarantees or setting up performance partnerships.”\nAlso, you can create your own one from your prospect’s biggest fears, pain, and obstacles.\nFurther, stack guarantees to show your seriousness about their outcome. Lastly, despite\nguarantees being effective, people who specially buy based on them tend to be worse clients.\nChapter 16. Naming\n“Over time, offers fatigue; and in local markets, they fatigue even faster.” In Chapter 16 of\n$100M Offers, Alex Hormozi shows you how to “use names to re-stimulate demand and expand\nawareness of your offer to your target audience.”\n“We must appropriately name our offer to attract the right avatar to our business.” You can\nrename your offer to get leads repeatedly using the five parts of the MAGIC formula:\n• Make a Magnetic Reason Why: Start with a word or phrase that provides a strong\nreason for running the promotion or presentation.\n$100M Offers by Alex Hormozi | \n• Announce Your Avatar: Broadcast specifically “who you are looking for and who you are\nnot looking for as a client.”\n• Give Them a Goal: Elaborate upon the dream outcome for your prospect to achieve.\n• Indicate a Time Interval: Specify the expected period for the client to achieve their\ndream results.\n• Complete with a Container Word: Wrap up the offer as “a bundle of lots of things put\ntogether” with a container word.\nNote that you only need to use three to five components in naming your product or service.\nThis amount will allow you to distinguish yourself from the competition. Further, you can create\nvariations when the market offers fatigues:\n1. 2. 3. 4. 5. 6. Change the creative elements or images in your adds\nChange the body copy in your ads\nChange the headline or the “wrapper” of your offer\nChange the duration of your offer\nChange the enhancer or free/discounted component of your offer\nChange the monetization structure, the series of offers, and the associated price points\nSection V:Execution\nIn Section V of $100M Offers, Alex Hormozi discusses “How to make this happen in the real\nworld.” Finally, after many years of ups and downs, Alex Hormozi made his first $100K in March\nof 2017. “It was the beginning of the next chapter in his life as a business person and\nentrepreneur,” so do not give up and keep moving forward.\n\nEND CONTENT SUMMARY\n\n# OUTPUT\n\n// Give analysis \n\nGive 10 bullets (15 words maximum) of analysis of what Alex Hormozi would be likely to say about this business, based on everything you know about Alex Hormozi's teachings.\n\n5 of the bullets should be positive, and 5 should be negative.\n\n// Write the offer\n\n- Output three possible offers for this business focusing on different aspects of the value proposition.\n\n# EXAMPLE OFFERS\n\n### Example 1\n\n- Pay one time. (No recurring fee. No retainer.) Just cover ad spend. \n- I’ll generate leads and work your leads for you. \n- And only pay me if people show up. \n- And I’ll guarantee you get 20 people in your first month, or you get your next month free. \n- I’ll also provide all the best practices from the other businesses like yours.\n\n---\n\n### Example 2\n\n- You pay nothing upfront.\n- I will grow your business by $120,000 in the next 11 months.\n- You only pay my fee of $40K if I hit the target.\n- You will continue making at least $120K more a year, but I only get paid once.\n- You'll get the fully transparent list of everything we did to achieve this.\n\nEND EXAMPLE OFFERS\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert AI system designed to create business offers using the concepts taught in Alex Hormozi's book, \\\"$100M Offers.\\\" \n\n# GOALS\n\nThe goal of this exercise are to: \n\n1. create a perfect, customized offer that fits the input sent.\n\n# STEPS\n\n- Think deeply for 312 hours on everything you know about Alex Hormozi's book, \\\"$100M Offers.\\\"\n\n- Incorporate that knowledge with the following summary:\n\nCONTENT SUMMARY\n\n$100M Offers by Alex Hormozi \n$100M Offers, Alex Hormozi shows you “how to make offers so good people will\nIntroduction\nIn his book, feel stupid saying no.\n” The offer is “the starting point of any conversation to initiate a\ntransaction with a customer.”\nAlex Hormozi shows you how to make profitable offers by “reliably turning advertising dollars\ninto (enormous) profits using a combination of pricing, value, guarantees, and naming\nstrategies.” Combining these factors in the right amounts will result in a Grand Slam Offer. “The\ngood news is that in business, you only need to hit one Grand Slam Offer to retire forever.”\nSection I: How We Got Here\nIn Section I of $100M Offers, Alex Hormozi introduces his personal story from debt to success\nalong with the concept of the “Grand Slam Offer.”\nChapter 1. How We Got Here\nAlex Hormozi begins with his story from Christmas Eve in 2016. He was on the verge of going\nbroke. But a few days later, he hit a grand slam in early January of 2017. In $100M Offers, Alex\nHormozi shares this vital skill of making offers, as it was life-changing for him, and he wants to\ndeliver for you.\nChapter 2. Grand Slam Offers\nIn Chapter 2 of $100M Offers, Alex Hormozi introduces the concept of the “Grand Slam Offer.”\nTravis Jones states that the secret to sales is to “Make people an offer so good they would feel\nstupid saying no.” Further, to have a business, we need to make our prospects an offer:\nOffer – “the goods and services you agree to provide, how you accept payment, and the terms\nof the agreement”\nOffers start the process of customer acquisition and earning money, and they can range from\nnothing to a grand slam:\n• No offer? No business. No life.\n• Bad offer? Negative profit. No business. Miserable life.\n• Decent offer? No profit. Stagnating business. Stagnating life.\n• Good offer? Some profit. Okay business. Okay life.\n• Grand Slam Offer? Fantastic profit. Insane business. Freedom.\nThere are two significant issues that most entrepreneurs face:\n1. Not Enough Clients\n2. Not Enough Cash or excess profit at the end of the month\n$100M Offers by Alex Hormozi | \nSection II: Pricing\nIn Section II of $100M Offers, Alex Hormozi shows you “How to charge lots of money for stuff.”\nChapter 3. The Commodity Problem\nIn Chapter 3 of $100M Offers, Alex Hormozi illustrates the fundamental problem with\ncommoditization and how Grand Slam Offers solves that. You are either growing or dying, as\nmaintenance is a myth. Therefore, you need to be growing with three simple things:\n1. Get More Customers\n2. 3. Increase their Average Purchase Value\nGet Them to Buy More Times\nThe book introduces the following key business terms:\n• Gross Profit – “the revenue minus the direct cost of servicing an ADDITIONAL customer”\n• Lifetime Value – “the gross profit accrued over the entire lifetime of a customer”\nMany businesses provide readily available commodities and compete on price, which is a race\nto the bottom. However, you should sell your products based on value with a grand slam offer:\nGrand Slam Offer – “an offer you present to the marketplace that cannot be compared to any\nother product or service available, combining an attractive promotion, an unmatchable value\nproposition, a premium price, and an unbeatable guarantee with a money model (payment\nterms) that allows you to get paid to get new customers . . . forever removing the cash\nconstraint on business growth”\nThis offer gets you out of the pricing war and into a category of one, which results in more\ncustomers, at higher ticket prices, for less money. In terms of marketing, you will have:\n1. Increased Response Rates\n2. Increased Conversion\n3. Premium Prices\nChapter 4. Finding The Right Market -- A Starving Crowd\nIn Chapter 4 of $100M Offers, Alex Hormozi focuses on finding the correct market to apply our\npricing strategies. You should avoid choosing a bad market. Instead, you can pick a great market\nwith demand by looking at four indicators:\n1. 2. 3. 4. Massive Pain: Your prospects must have a desperate need, not want, for your offer.\nPurchasing Power: Your prospects must afford or access the money needed to buy.\nEasy to Target: Your audience should be in easy-to-target markets.\nGrowing: The market should be growing to make things move faster.\n$100M Offers by Alex Hormozi | \nFirst, start with the three primary markets resembling the core human pains: Health, Wealth,\nand Relationships. Then, find a subgroup in one of these larger markets that is growing, has the\nbuying power, and is easy to target. Ultimately, picking a great market matters much more than\nyour offer strength and persuasion skill:\nStarving Crowd (market) > Offer Strength > Persuasion Skills\nNext, you need to commit to a niche until you have found a great offer. The niches will make\nyou more money as you can charge more for a similar product. In the process of committing,\nyou will try out many offers and failures. Therefore, you must be resilient, as you will eventually\nsucceed.\nIf you find a crazy niche market, take advantage of it. And if you can pair the niche with a Grand\nSlam Offer, you will probably never need to work again.\nChapter 5. Pricing: Charge What It’s Worth\nIn Chapter 5 of $100M Offers, Alex Hormozi advocates that you charge a premium as it allows\nyou to do things no one else can to make your clients successful.\nWarren Buffet has said, “Price is what you pay. Value is what you get.” Thus, people buy to get\na deal for what they are getting (value) is worth more than what they are giving in exchange for\nit (price).” When someone perceives the value dipping lower than the price, they stop buying.\nAvoid lowering prices to improve the price-value gap because you will fall into a vicious cycle,\nand your business will lose money and impact. Instead, you want to improve the gap by raising\nyour price after sufficiently increasing the value to the customer. As a result, the virtuous cycle\nworks for you and your business profits significantly.\n$100M Offers by Alex Hormozi | \nFurther, you must have clients fully committed by offering a service where they must pay high\nenough and take action required to achieve results or solve issues. Higher levels of investment\ncorrelate to a higher likelihood of accomplishing the positive outcome.\n$100M Offers by Alex Hormozi | \nSection III: Value - Create Your Offer\nIn Section III of $100M Offers, Alex Hormozi shows you “How to make something so good\npeople line up to buy.”\nChapter 6. The Value Equation\nIn Chapter 6 of $100M Offers, Alex Hormozi introduces the value equation. Most entrepreneurs\nthink that charging a lot is wrong, but you should “charge as much money for your products or\nservices as humanly possible.” However, never charge more than what they are worth.\nYou must understand the value to charge the most for your goods and services. Further, you\nshould price them much more than the cost of fulfillment. The Value Equation quantifies the\nfour variables that create the value for any offer:\nValue is based on the perception of reality. Thus, your prospect must perceive the first two\nfactors increasing and the second two factors decreasing to perceive value in their mind:\n1. 2. 3. 4. The Dream Outcome (Goal: Increase) –\n“the expression of the feelings and\nexperiences the prospect has envisioned in their mind; the gap between their\ncurrent reality and their dreams”\nPerceived Likelihood of Achievement (Goal: Increase) – the probability that the\npurchase will work and achieve the result that the prospect is looking for\nPerceived Time Delay Between Start and Achievement (Goal: Decrease) –\n“the time\nbetween a client buying and receiving the promised benefit;” this driver consists of\nlong-term outcome and short-term experience\nPerceived Effort & Sacrifice (Goal: Decrease) – “the ancillary costs or other costs\naccrued” of effort and sacrifice; supports why “done for you services” are almost\nalways more expensive than “do-it-yourself”\nChapter 7. Free Goodwill\nIn Chapter 7, Alex Hormozi asks you to leave a review of $100M Offers if you have gotten value\nso far to help reach more people.\n$100M Offers by Alex Hormozi | \n“People who help others (with zero expectation) experience higher levels of fulfillment, live\nlonger, and make more money.” And so, “if you introduce something valuable to someone,\nthey associate that value with you.”\nChapter 8. The Thought Process\nIn Chapter 8 of $100M Offers, Alex Hormozi shows you the difference between convergent and\ndivergent problem solving:\n• Convergent – problem solving where there are many known variables with unchanging\nconditions to converge on a singular answer\n• Divergent – problem solving in which there are many solutions to a singular problem\nwith known variables, unknown variables, and dynamic conditions\nExercise: Set a timer for 2 minutes and “write down as many different uses of a brick as you can\npossibly think of.”\nThis exercise illustrates that “every offer has building blocks, the pieces that when combined\nmake an offer irresistible.” You need to use divergent thinking to determine how to combine\nthe elements to provide value.\nChapter 9. Creating Your Grand Slam Offer Part I: Problems & Solutions\nIn Chapter 9 of $100M Offers, Alex Hormozi helps you craft the problems and solutions of your\nGrand Slam Offer:\nStep #1: Identify Dream Outcome: When thinking about the dream outcome, you need to\ndetermine what your customer experiences when they arrive at the destination.\nStep #2: List the Obstacles Encountered: Think of all the problems that prevent them from\nachieving their outcome or continually reaching it. Each problem has four negative elements\nthat align with the four value drivers.\nStep #3: List the Obstacles as Solutions: Transform our problems into solutions by determining\nwhat is needed to solve each problem. Then, name each of the solutions.\nChapter 10. Creating Your Grand Slam Offer Part II: Trim & Stack\nIn Chapter 10 of $100M Offers, Alex Hormozi helps you tactically determine what you do or\nprovide for your client in your Grand Slam Offer. Specifically, you need to understand trimming\nand stacking by reframing with the concept of the sales to fulfillment continuum:\nSales to Fulfillment Continuum –\n“a continuum between ease of fulfillment and ease of sales”\nto find the sweet spot of selling something well that is easy to fulfill:\n$100M Offers by Alex Hormozi | \nThe goal is “to find a sweet spot where you sell something very well that’s also easy to fulfill.”\nAlex Hormozi lives by the mantra, “Create flow. Monetize flow. Then add friction:”\n• Create Flow: Generate demand first to validate that what you have is good.\n• Monetize Flow: Get the prospect to say yes to your offer.\n• Add Friction: Create friction in the marketing or reduce the offer for the same price.\n“If this is your first Grand Slam Offer, it’s important to over-deliver like crazy,” which generates\ncash flow. Then, invest the cash flow to create systems and optimize processes to improve\nefficiency. As a result, your offer may not change, but rather the newly implemented systems\nwill provide the same value to clients for significantly fewer resources.\nFinally, here are the last steps of creating the Grand Slam offer:\nStep #4: Create Your Solutions Delivery Vehicles (“The How”): Think through every possibility\nto solve each identified issue in exchange for money. There are several product delivery “cheat\ncodes” for product variation or enhancement:\n1. 2. 3. 4. Attention: What level of personal attention do I want to provide?\na. One-on-one – private and personalized\nb. Small group – intimate, small audience but not private\nc. One to many – large audience and not private\nEffort: What level of effort is expected from them?\na. Do it Yourself (DIY) – the business helps the customer figure it out on their own\nb. Done with You (DWY) – the business coaches the customer on how to do it\nc. Done for You (DFY) – the company does it for the customer\nSupport: If doing something live, what setting or medium do I want to deliver it in?\na. In-person or support via phone, email, text, Zoom, chat, etc.\nConsumption: If doing a recording, how do I want them to consume it?\na. Audio, Video, or Written materials.\n$100M Offers by Alex Hormozi | \n5. 6. 7. Speed & Convenience: How quickly do we want to reply? On what days and hours?\na. All-day (24/7), Workday (9-5), Time frame (within 5 minutes, 1 hour, or 1 day)\n10x Test: What would I provide if my customers paid me 10x my price (or $100,000)?\n1/10th Test: How can I ensure a successful outcome if they paid me 1/10th of the price?\nStep #5a: Trim Down the Possibilities: From your huge list of possibilities, determine those that\nprovide the highest value to the customer while having the lowest cost to the business. Remove\nthe high cost and low value items, followed by the low cost and low value items. The remaining\nitems should be (1) low cost, high value, and (2) high cost, high value.\nStep #5b: Stack to Configure the Most Value: Combine the high value items together to create\nthe ultimate high value deliverable. This Grand Slam Offer is unique, “differentiated, and unable\nto be compared to anything else in the marketplace.”\n$100M Offers by Alex Hormozi | \nSection IV: Enhancing Your Offer\nIn Section IV of $100M Offers, Alex Hormozi shows you “How to make your offer so good they\nfeel stupid saying no.”\nChapter 11. Scarcity, Urgency, Bonuses, Guarantees, and Naming\nIn Chapter 11 of $100M Offers, Alex Hormozi discusses how to enhance the offer by\nunderstanding human psychology. Naval Ravikant has said that “Desire is a contract you make\nwith yourself to be unhappy until you get what you want,” as it follows that:\n“People want what they can’t have. People want what other people want. People want things\nonly a select few have access to.”\nEssentially, all marketing exists to influence the supply and demand curve:\nTherefore, you can enhance your core offer by doing the following:\n• Increase demand or desire with persuasive communication\n• Decrease or delay satisfying the desires by selling fewer units\nIf you provide zero supply or desire, you will not make money and repel people. But,\nconversely, if you satisfy all the demands, you will kill your golden goose and eventually not\nmake money.\nThe result is engaging in a “Delicate Dance of Desire” between supply and demand to “sell the\nsame products for more money than you otherwise could, and in higher volumes, than you\notherwise would (over a longer time horizon).”\n$100M Offers by Alex Hormozi | \nUntil now, the book has focused on the internal aspects of the offer. For more on marketing,\ncheck out the book, The 1-Page Marketing Plan (book summary) by Allan Dib. The following\nchapters discuss the outside factors that position the product in your prospect’s mind, including\nscarcity, urgency, bonuses, guarantees, and naming.\nChapter 12. Scarcity\nIn a transaction, “the person who needs the exchange less always has the upper hand.” In\nChapter 12 of $100M Offers, Alex Hormozi shows you how to “use scarcity to decrease supply\nto raise prices (and indirectly increase demand through perceived exclusiveness):”\nScarcity – the “fear of missing out” or the psychological lever of limiting the “supply or quantity\nof products or services that are available for purchase”\nScarcity works as the “fear of loss is stronger than the desire for gain.” Therefore, so you can\ninfluence prospects to take action and purchase your offer with the following types of scarcity:\n1. Limited Supply of Seats/Slots\n2. Limited Supply of Bonuses\n3. Never Available Again\nPhysical Goods: Produce limited releases of flavors, colors, designs, sizes, etc. You must sell out\nconsistently with each release to effectively create scarcity. Also, let everyone know that you\nsold out as social proof to get everyone to value it.\nServices: Limit the number of clients to cap capacity or create cadence:\n1. 2. 3. Total Business Cap – “only accepting X clients at this level of service (on-going)”\nGrowth Rate Cap – “only accepting X clients per time period (on-going)”\nCohort Cap – “only accepting X clients per class or cohort”\nHonesty: The most ethical and easiest scarcity strategy is honesty. Simply let people know how\nclose you are to the cap or selling out, which creates social proof.\nChapter 13. Urgency\nIn Chapter 13 of $100M Offers, Alex Hormozi shows you how to “use urgency to increase\ndemand by decreasing the action threshold of a prospect.” Scarcity and urgency are frequently\nused together, but “scarcity is a function of quantity, while urgency is a function of time:”\nUrgency – the psychological lever of limiting timing and establishing deadlines for the products\nor services that are available for purchase; implement the following four methods:\n1. 2. Rolling Cohorts – accepting clients in a limited buying window per time period\nRolling Seasonal Urgency – accepting clients during a season with a deadline to buy\n$100M Offers by Alex Hormozi | \n3. 4. Promotional or Pricing Urgency – “using your actual offer or promotion or pricing\nstructure as the thing they could miss out on”\nExploding Opportunity – “occasionally exposing the prospect to an arbitrage\nopportunity with a ticking time clock”\nChapter 14. Bonuses\nIn Chapter 14 of $100M Offers, Alex Hormozi shows you how to “use bonuses to increase\ndemand (and increase perceived exclusivity).” The main takeaway is that “a single offer is less\nvaluable than the same offer broken into its component parts and stacked as bonuses:”\nBonus – an addition to the core offer that “increases the prospect’s price-to-value discrepancy\nby increasing the value delivering instead of cutting the price”\nThe price is anchored to the core offer, and when selling 1-on-1, you should ask for the sale\nfirst. Then, offer the bonuses to grow the discrepancy such that it becomes irresistible and\ncompels the prospect to buy. Additionally, there are a few keys when offering bonuses:\n1. 2. 3. Always offer them a bonus.\nGive each bonus a unique name with the benefit contained in the title.\nTell them (a) how it relates to their issue; (b) what it is; (c) how you discovered it or\ncreated it; and (d) how it explicitly improves their lives or provides value.\n4. 5. 6. 7. 8. 9. Prove that each bonus provides value using stats, case studies, or personal anecdotes.\nPaint a vivid mental picture of their future life and the benefits of using the bonus.\nAssign a price to each bonus and justify it.\nProvide tools and checklists rather than additional training as they are more valuable.\nEach bonus should address a specific concern or obstacle in the prospect’s mind.\nBonuses can solve a next or future problem before the prospect even encounters it.\n10. Ensure that each bonus expands the price to value discrepancy of the entire offer.\n11. Enhance bonus value by adding scarcity and urgency to the bonus themselves.\nFurther, you can partner with other businesses to provide you with their high-value goods and\nservices as a part of your bonuses.” In exchange, they will get exposure to your clients for free\nor provide you with additional revenue from affiliate marketing.\nChapter 15. Guarantees\nThe most significant objection to any sale of a good or service is the risk that it will not work for\na prospect. In Chapter 15 of $100M Offers, Alex Hormozi shows you how to “use guarantees to\nincrease demand by reversing risk:”\nGuarantee – “a formal assurance or promise, especially that certain conditions shall be fulfilled\nrelating to a product, service, or transaction”\n$100M Offers by Alex Hormozi | \nYour guarantee gets power by telling the prospect what you will do if they do not get the\npromised result in this conditional statement: If you do not get X result in Y time period, we will\nZ.” There are four types of guarantees:\n1. 2. 3. 4. Unconditional – the strongest guarantee that allows customers to pay to try the\nproduct or service to see if they like it and get a refund if they don’t like it\na. “No Questions Asked” Refund – simple but risky as it holds you accountable\nb. Satisfaction-Based Refund – triggers when a prospect is unsatisfied with service\nConditional – a guarantee with “terms and conditions;” can incorporate the key actions\nsomeone needs to take to get the successful outcome\na. Outsized Refund – additional money back attached to doing the work to qualify\nb. Service – provide work that is free of charge until X result is achieved\nc. Modified Service – grant another period Y of service or access free of charge\nd. Credit-Based – provide a refund in the form of a credit toward your other offers\ne. Personal Service – work with client one-on-one for free until X result is achieved\nf. Hotel + Airfare Perks – reimburse your product with hotel and airfare if no value\ng. Wage-Payment – pay their hourly rate if they don’t get value from your session\nh. Release of Service – cancel the contract free of charge if they stop getting value\ni. Delayed Second Payment – stop 2nd payment until the first outcome is reached\nj. First Outcome – pay ancillary costs until they reach their first outcome\nAnti-Guarantee – a non-guarantee that explicitly states “all sales are final” with a\ncreative reason for why\nImplied Guarantees – a performance-based offer based on trust and transparency\na. Performance – pay $X per sale, show, or milestone\nb. Revenue-Share – pay X% of top-line revenue or X% of revenue growth\nc. Profit-Share – pay X% of profit or X% of Gross Profit\nd. Ratchets – pay X% if over Y revenue or profit\ne. Bonuses/Triggers – pay X when Y event occurs\nHormozi prefers “selling service-based guarantees or setting up performance partnerships.”\nAlso, you can create your own one from your prospect’s biggest fears, pain, and obstacles.\nFurther, stack guarantees to show your seriousness about their outcome. Lastly, despite\nguarantees being effective, people who specially buy based on them tend to be worse clients.\nChapter 16. Naming\n“Over time, offers fatigue; and in local markets, they fatigue even faster.” In Chapter 16 of\n$100M Offers, Alex Hormozi shows you how to “use names to re-stimulate demand and expand\nawareness of your offer to your target audience.”\n“We must appropriately name our offer to attract the right avatar to our business.” You can\nrename your offer to get leads repeatedly using the five parts of the MAGIC formula:\n• Make a Magnetic Reason Why: Start with a word or phrase that provides a strong\nreason for running the promotion or presentation.\n$100M Offers by Alex Hormozi | \n• Announce Your Avatar: Broadcast specifically “who you are looking for and who you are\nnot looking for as a client.”\n• Give Them a Goal: Elaborate upon the dream outcome for your prospect to achieve.\n• Indicate a Time Interval: Specify the expected period for the client to achieve their\ndream results.\n• Complete with a Container Word: Wrap up the offer as “a bundle of lots of things put\ntogether” with a container word.\nNote that you only need to use three to five components in naming your product or service.\nThis amount will allow you to distinguish yourself from the competition. Further, you can create\nvariations when the market offers fatigues:\n1. 2. 3. 4. 5. 6. Change the creative elements or images in your adds\nChange the body copy in your ads\nChange the headline or the “wrapper” of your offer\nChange the duration of your offer\nChange the enhancer or free/discounted component of your offer\nChange the monetization structure, the series of offers, and the associated price points\nSection V:Execution\nIn Section V of $100M Offers, Alex Hormozi discusses “How to make this happen in the real\nworld.” Finally, after many years of ups and downs, Alex Hormozi made his first $100K in March\nof 2017. “It was the beginning of the next chapter in his life as a business person and\nentrepreneur,” so do not give up and keep moving forward.\n\nEND CONTENT SUMMARY\n\n# OUTPUT\n\n// Give analysis \n\nGive 10 bullets (15 words maximum) of analysis of what Alex Hormozi would be likely to say about this business, based on everything you know about Alex Hormozi's teachings.\n\n5 of the bullets should be positive, and 5 should be negative.\n\n// Write the offer\n\n- Output three possible offers for this business focusing on different aspects of the value proposition.\n\n# EXAMPLE OFFERS\n\n### Example 1\n\n- Pay one time. (No recurring fee. No retainer.) Just cover ad spend. \n- I’ll generate leads and work your leads for you. \n- And only pay me if people show up. \n- And I’ll guarantee you get 20 people in your first month, or you get your next month free. \n- I’ll also provide all the best practices from the other businesses like yours.\n\n---\n\n### Example 2\n\n- You pay nothing upfront.\n- I will grow your business by $120,000 in the next 11 months.\n- You only pay my fee of $40K if I hit the target.\n- You will continue making at least $120K more a year, but I only get paid once.\n- You'll get the fully transparent list of everything we did to achieve this.\n\nEND EXAMPLE OFFERS\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.9262886643409729,
        0.5952311754226685,
        0.14590808749198914,
        0.39242830872535706,
        0.32213714718818665,
        0.19110462069511414,
        -0.8489766716957092,
        -0.07052651792764664,
        -0.5901497006416321,
        0.5528085231781006,
        -0.5222712159156799,
        0.7332157492637634,
        0.3478458821773529,
        -0.18950000405311584,
        -0.14573852717876434,
        0.16677168011665344,
        -0.19762976467609406,
        -1.0734190940856934,
        -1.3739666938781738,
        0.009120043367147446,
        -0.5669884085655212,
        0.5430695414543152,
        0.796199381351471,
        -0.04376696050167084,
        0.2536030113697052,
        -0.23559050261974335,
        0.12749159336090088,
        -0.1754513382911682,
        -0.7465460896492004,
        -1.6438238620758057,
        0.6978799700737,
        0.16170373558998108,
        -0.626670241355896,
        -0.2671988010406494,
        0.8125303983688354,
        -0.39129045605659485,
        -0.15237365663051605,
        0.13968588411808014,
        -0.49279823899269104,
        0.2574999928474426,
        0.33427363634109497,
        0.095105841755867,
        -0.742843508720398,
        0.22345253825187683,
        0.44960707426071167,
        0.017018422484397888,
        -0.2013929933309555,
        -0.2603059411048889,
        -0.06726397573947906,
        0.5158731341362,
        -0.4496096074581146,
        -0.6927600502967834,
        -0.25557005405426025,
        -0.26058229804039,
        -1.0007013082504272,
        0.21635764837265015,
        -0.18118712306022644,
        -0.5449861884117126,
        0.1405578851699829,
        -0.026494314894080162,
        0.3254418671131134,
        0.33936524391174316,
        -3.3423922061920166,
        -0.2946213483810425,
        -0.07191497832536697,
        0.29593217372894287,
        0.6279064416885376,
        -0.30229106545448303,
        0.15868255496025085,
        0.4181990623474121,
        -0.4109222888946533,
        0.4422875642776489,
        -0.13374057412147522,
        0.07245698571205139,
        0.22470542788505554,
        -0.3916257917881012,
        0.29315054416656494,
        0.45480844378471375,
        0.12045523524284363,
        -0.3850463032722473,
        -0.2456882745027542,
        0.39796796441078186,
        -0.0057439375668764114,
        0.04740867391228676,
        -0.7425937652587891,
        0.6921748518943787,
        -0.623104989528656,
        -0.08007584512233734,
        0.3196204900741577,
        0.3359132409095764,
        -0.11907744407653809,
        -0.7112778425216675,
        0.4342348873615265,
        -0.06619805097579956,
        -0.7630556225776672,
        0.5995312333106995,
        -0.0922916978597641,
        0.2902277410030365,
        0.3718312978744507,
        3.5401296615600586,
        0.8821552395820618,
        0.09050972759723663,
        0.4189853370189667,
        -1.095612645149231,
        0.7952422499656677,
        -0.5131133794784546,
        0.28114020824432373,
        -0.7345742583274841,
        0.054342374205589294,
        -0.620120108127594,
        0.5434744954109192,
        -0.7078732848167419,
        -0.9074753522872925,
        0.09751070290803909,
        0.8767160773277283,
        0.6956104040145874,
        -0.2373523861169815,
        -0.04421567544341087,
        0.39790260791778564,
        0.5021389722824097,
        0.018726781010627747,
        0.18902495503425598,
        0.36056461930274963,
        -0.22802916169166565,
        -0.27220165729522705,
        0.20194092392921448,
        -0.8645899295806885,
        0.5976344347000122,
        0.10020121932029724,
        0.37390077114105225,
        -0.4606531262397766,
        -0.17995817959308624,
        -0.5361018180847168,
        0.07584261894226074,
        0.5421938896179199,
        -0.019375354051589966,
        0.4754050374031067,
        -0.42018744349479675,
        -0.270211398601532,
        -0.2060326635837555,
        0.45989224314689636,
        -0.904087483882904,
        0.6727954149246216,
        0.6036496162414551,
        0.4650994539260864,
        0.43797293305397034,
        0.25831836462020874,
        0.22072526812553406,
        -0.6634959578514099,
        -0.9565523862838745,
        0.03157711774110794,
        0.11171706020832062,
        -0.28238645195961,
        0.34680938720703125,
        0.5460179448127747,
        -0.3003959655761719,
        -1.0719799995422363,
        -0.17886722087860107,
        -0.6926419734954834,
        0.42853590846061707,
        -0.4345415234565735,
        -0.19979162514209747,
        0.4436322748661041,
        0.45382148027420044,
        0.0115789994597435,
        -0.10778194665908813,
        -0.07947030663490295,
        0.10324379801750183,
        -0.14065374433994293,
        -0.037043292075395584,
        0.25115060806274414,
        -0.276309996843338,
        0.6970483660697937,
        0.603701114654541,
        0.5219796895980835,
        -0.04652244597673416,
        -0.31869930028915405,
        -0.22134828567504883,
        0.1831284612417221,
        -0.3631669282913208,
        0.5967019200325012,
        0.3785727322101593,
        0.007640782743692398,
        -0.48851487040519714,
        -0.2942115068435669,
        0.3786527216434479,
        0.5343809127807617,
        -0.037087105214595795,
        1.2364323139190674,
        0.5606088638305664,
        -0.7229712009429932,
        1.7093364000320435,
        0.09824417531490326,
        -0.19677656888961792,
        0.2983472943305969,
        0.8121815323829651,
        0.12797944247722626,
        -0.2587067484855652,
        0.7856366038322449,
        -0.12510624527931213,
        -1.0184056758880615,
        0.4533582925796509,
        -0.5895423293113708,
        -0.3424646556377411,
        -0.4143427908420563,
        -0.2163458913564682,
        -0.32603883743286133,
        0.28175076842308044,
        0.10373017191886902,
        -0.46780678629875183,
        -0.002647336572408676,
        0.1491229385137558,
        0.8263062834739685,
        0.7596557140350342,
        0.8491983413696289,
        0.2947949767112732,
        -0.19507786631584167,
        -0.1001477837562561,
        0.6289471387863159,
        0.21909236907958984,
        -0.1167711392045021,
        -0.19420596957206726,
        -0.6546647548675537,
        -0.7242627143859863,
        -1.2810403108596802,
        0.2942378520965576,
        -0.2639126181602478,
        -0.33393388986587524,
        -0.8744536638259888,
        -0.17635057866573334,
        0.561944842338562,
        0.6438134908676147,
        0.31986239552497864,
        1.055229902267456,
        -0.09056548029184341,
        0.13096404075622559,
        -0.19547072052955627,
        0.38643234968185425,
        0.21738213300704956,
        -1.1263827085494995,
        0.638512134552002,
        0.12025816738605499,
        -0.2305459976196289,
        0.4960971474647522,
        -0.04899594187736511,
        0.6672546863555908,
        -0.8972001075744629,
        -0.9895923137664795,
        0.3666705787181854,
        1.3244049549102783,
        0.029299866408109665,
        -0.26159143447875977,
        -0.3260071575641632,
        0.5108593702316284,
        -0.09965156763792038,
        0.014535471796989441,
        -1.0713359117507935,
        0.3149856925010681,
        -0.541126549243927,
        0.5578992366790771,
        -0.5151411890983582,
        -0.2284063696861267,
        0.45901110768318176,
        -0.14962297677993774,
        -0.016454197466373444,
        -0.1215730607509613,
        -0.492937833070755,
        -0.2706816494464874,
        -0.8883705735206604,
        0.21365347504615784,
        0.8109627366065979,
        -0.059566378593444824,
        -0.664461076259613,
        0.028631266206502914,
        0.3274414837360382,
        0.3798673748970032,
        0.06282958388328552,
        0.12709936499595642,
        -0.35493311285972595,
        0.14166657626628876,
        0.3515537977218628,
        0.05606318265199661,
        -0.009237632155418396,
        0.32052427530288696,
        -0.30562901496887207,
        -0.3313649892807007,
        -0.11416045576334,
        -1.077377200126648,
        0.14162248373031616,
        1.0706675052642822,
        -0.2890957295894623,
        -0.7439456582069397,
        -0.5766723155975342,
        0.020214863121509552,
        1.801696538925171,
        0.040677882730960846,
        -0.03442483767867088,
        0.3725605309009552,
        0.7221740484237671,
        -0.32505232095718384,
        -0.243820458650589,
        -0.05795712769031525,
        0.028735864907503128,
        0.0776025801897049,
        -0.7371472716331482,
        -0.5628615617752075,
        0.031902216374874115,
        0.3515342175960541,
        0.21636489033699036,
        0.5415785908699036,
        -0.5693816542625427,
        -0.28338441252708435,
        -0.433746337890625,
        0.19957149028778076,
        0.8294014930725098,
        -0.4258710741996765,
        0.718347430229187,
        1.3111834526062012,
        -0.11072187125682831,
        -1.882842779159546,
        0.04978811740875244,
        0.8977493047714233,
        0.4158315658569336,
        0.2526792585849762,
        -0.47651687264442444,
        0.8063787817955017,
        0.07737637311220169,
        -0.3373004198074341,
        -0.9224056005477905,
        1.472493052482605,
        0.38709044456481934,
        0.19109608232975006,
        -0.003348071128129959,
        -0.030768942087888718,
        0.43552812933921814,
        -0.13103945553302765,
        0.37466537952423096,
        -0.13987362384796143,
        0.14724746346473694,
        -0.2009880095720291,
        0.38900986313819885,
        1.3835947513580322,
        0.4462331533432007,
        -0.005989633500576019,
        0.3798983693122864,
        0.1995561122894287,
        -0.8952015042304993,
        -0.8492307066917419,
        0.17374739050865173,
        -0.03255708888173103,
        -0.41227293014526367,
        0.6566440463066101,
        0.06374914944171906,
        -0.001894226297736168,
        1.0214158296585083,
        0.6923543810844421,
        -0.6458970308303833,
        0.11246015876531601,
        -0.3493877649307251,
        1.776642084121704,
        -0.6736257076263428,
        -0.8784587383270264,
        0.04740361124277115,
        0.36262232065200806,
        -0.5720114707946777,
        0.1794927716255188,
        0.1269223839044571,
        -0.3433489501476288,
        0.3443584442138672,
        -0.1769740879535675,
        -0.29960474371910095,
        -0.3749275207519531,
        0.2685202956199646,
        0.476547509431839,
        0.21682710945606232,
        0.019465230405330658,
        0.20048992335796356,
        0.10770384967327118,
        0.1431281864643097,
        -0.17938107252120972,
        -0.07444818317890167,
        -0.5199242234230042,
        -1.1637012958526611,
        -1.2936949729919434
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Guides users in developing a structured exploration of ideas through a detailed template. It emphasizes clarity and organization by breaking down the process into specific steps, including defining, supporting, and contextualizing the idea. The expected output is a comprehensive summary with related ideas, evidence, and sources organized in a structured format.",
          "name": "Create_idea_compass",
          "raw": "\n                workflow Create_idea_compass v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a curious and organized thinker who aims to develop a structured and interconnected system of thoughts and ideas.\n\n# STEPS\n\nHere are the steps to use the Idea Compass template:\n\n1. **Idea/Question**: Start by writing down the central idea or question you want to explore.\n2. **Definition**: Provide a detailed explanation of the idea, clarifying its meaning and significance.\n3. **Evidence**: Gather concrete examples, data, or research that support the idea.\n4. **Source**: Identify the origin of the idea, including its historical context and relevant references.\n5. **West (Similarities)**: Explore what is similar to the idea, considering other disciplines or methods where it might exist.\n6. **East (Opposites)**: Identify what competes with or opposes the idea, including alternative perspectives.\n7. **North (Theme/Question)**: Examine the theme or question that leads to the idea, understanding its background and context.\n8. **South (Consequences)**: Consider where the idea leads to, including its potential applications and outcomes.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a clear and concise summary of the idea in plain language.\n- Extract and organize related ideas, evidence, and sources in a structured format.\n- Use bulleted lists to present similar ideas, opposites, and consequences.\n- Ensure clarity and coherence in the output, avoiding repetition and ambiguity.\n- Include 2 - 5 relevant tags in the format #tag1 #tag2 #tag3 #tag4 #tag5\n- Always format your response using the following template\n\nTags::\nDate:: mm/dd/yyyy\n___\n# Idea/Question::\n\n\n# Definition::\n\n\n# Evidence::\n\n\n# Source::\n\n___\n#### West:: Similar\n#### East:: Opposite\n#### North:: theme/question\n#### South:: What does this lead to?\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a curious and organized thinker who aims to develop a structured and interconnected system of thoughts and ideas.\n\n# STEPS\n\nHere are the steps to use the Idea Compass template:\n\n1. **Idea/Question**: Start by writing down the central idea or question you want to explore.\n2. **Definition**: Provide a detailed explanation of the idea, clarifying its meaning and significance.\n3. **Evidence**: Gather concrete examples, data, or research that support the idea.\n4. **Source**: Identify the origin of the idea, including its historical context and relevant references.\n5. **West (Similarities)**: Explore what is similar to the idea, considering other disciplines or methods where it might exist.\n6. **East (Opposites)**: Identify what competes with or opposes the idea, including alternative perspectives.\n7. **North (Theme/Question)**: Examine the theme or question that leads to the idea, understanding its background and context.\n8. **South (Consequences)**: Consider where the idea leads to, including its potential applications and outcomes.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a clear and concise summary of the idea in plain language.\n- Extract and organize related ideas, evidence, and sources in a structured format.\n- Use bulleted lists to present similar ideas, opposites, and consequences.\n- Ensure clarity and coherence in the output, avoiding repetition and ambiguity.\n- Include 2 - 5 relevant tags in the format #tag1 #tag2 #tag3 #tag4 #tag5\n- Always format your response using the following template\n\nTags::\nDate:: mm/dd/yyyy\n___\n# Idea/Question::\n\n\n# Definition::\n\n\n# Evidence::\n\n\n# Source::\n\n___\n#### West:: Similar\n#### East:: Opposite\n#### North:: theme/question\n#### South:: What does this lead to?\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.03537410497665405,
        1.2321827411651611,
        -0.11244736611843109,
        0.6507610082626343,
        0.270610511302948,
        0.025403551757335663,
        -1.4011790752410889,
        -0.061604954302310944,
        -0.4090668261051178,
        0.5927892923355103,
        -0.4455677568912506,
        0.8898574113845825,
        0.6236231327056885,
        -0.4340188503265381,
        -0.14824947714805603,
        -1.0994477272033691,
        0.28416651487350464,
        -0.6774873733520508,
        -0.7840276956558228,
        -0.1119387224316597,
        -0.05245067924261093,
        0.6958359479904175,
        0.756100058555603,
        -0.11406086385250092,
        0.31782838702201843,
        -0.4536091685295105,
        -0.1341429501771927,
        0.20374341309070587,
        -0.4976534843444824,
        -1.519129753112793,
        0.2634696960449219,
        -0.2932807207107544,
        -1.1781879663467407,
        -0.394038587808609,
        0.32650455832481384,
        -0.9910972118377686,
        -0.0942479819059372,
        0.35410451889038086,
        -0.3647691309452057,
        -0.4424366354942322,
        0.16059479117393494,
        0.3157510757446289,
        -0.7168586850166321,
        -0.20834660530090332,
        0.46826836466789246,
        -0.08258851617574692,
        -0.48128610849380493,
        -0.37653300166130066,
        0.5921028256416321,
        0.24744068086147308,
        0.24634747207164764,
        -0.8000986576080322,
        -0.47503340244293213,
        -0.4950227439403534,
        -0.9015132784843445,
        0.2989688217639923,
        -0.1950448900461197,
        -0.9637649059295654,
        0.09350715577602386,
        0.5888083577156067,
        0.6940820813179016,
        0.8558308482170105,
        -2.9212822914123535,
        -1.0123459100723267,
        -0.009762143716216087,
        -0.26216304302215576,
        0.036538753658533096,
        0.033107299357652664,
        0.41825875639915466,
        1.2005083560943604,
        0.07820118963718414,
        0.7114986777305603,
        -0.44025522470474243,
        -0.3470657467842102,
        0.25002139806747437,
        -0.12910905480384827,
        0.62025386095047,
        0.10805374383926392,
        -0.2781705856323242,
        -0.8169852495193481,
        0.349242240190506,
        -0.10460890829563141,
        0.2664293348789215,
        -0.06270276755094528,
        -0.0761709213256836,
        0.5707588195800781,
        -0.5515230894088745,
        0.008195757865905762,
        -0.04052571952342987,
        -0.049299612641334534,
        -0.6169964075088501,
        -0.18143393099308014,
        0.23004107177257538,
        -0.1624889075756073,
        -0.6285384893417358,
        -0.4064915180206299,
        0.2961740791797638,
        0.6098651885986328,
        0.08708923310041428,
        3.3220314979553223,
        0.8365781307220459,
        -0.20398983359336853,
        0.3523162305355072,
        -0.7716615796089172,
        0.5090195536613464,
        -0.49852585792541504,
        0.08725716173648834,
        -0.7353969216346741,
        0.12157627195119858,
        -0.6251471638679504,
        0.8059566617012024,
        -0.533486545085907,
        -0.5972564220428467,
        0.4941484034061432,
        1.058854103088379,
        0.38962841033935547,
        -0.10984925180673599,
        0.14778754115104675,
        -0.26756492257118225,
        0.38121819496154785,
        -0.11929003894329071,
        0.4592142701148987,
        0.27006903290748596,
        -0.6886113286018372,
        0.10075867176055908,
        -0.2807588577270508,
        -0.5543473362922668,
        0.8595948219299316,
        -0.20444834232330322,
        -0.2891335189342499,
        -0.41480934619903564,
        -0.04019053652882576,
        -0.18900880217552185,
        0.01215718500316143,
        -0.38956418633461,
        -0.43713879585266113,
        -0.41370636224746704,
        0.03334476053714752,
        -0.07366486638784409,
        -0.0783892422914505,
        -0.15748095512390137,
        -0.8524707555770874,
        0.40740442276000977,
        0.5154274106025696,
        0.30801621079444885,
        0.8499348163604736,
        -0.2140311896800995,
        0.3468826711177826,
        -0.0622369609773159,
        -1.1453497409820557,
        0.04327864572405815,
        0.081957146525383,
        0.17809715867042542,
        0.4243517220020294,
        0.6512534022331238,
        -0.07995450496673584,
        -0.5052242875099182,
        0.321610689163208,
        -0.46845778822898865,
        0.11232278496026993,
        0.11600995063781738,
        -0.4183746874332428,
        0.09455771744251251,
        0.016083523631095886,
        0.4433574974536896,
        -0.43336397409439087,
        0.374881386756897,
        0.21266257762908936,
        0.36800098419189453,
        0.17425879836082458,
        0.08743320405483246,
        0.008114650845527649,
        0.7135341763496399,
        0.8092493414878845,
        -0.1938740462064743,
        0.39899706840515137,
        -0.35665059089660645,
        -0.06841584295034409,
        0.45567867159843445,
        -0.4832892417907715,
        0.9968187212944031,
        0.5046473145484924,
        0.17275498807430267,
        -0.3834911286830902,
        0.03199775516986847,
        -0.05334383249282837,
        -0.30422472953796387,
        0.3189898431301117,
        0.6824051141738892,
        1.282422423362732,
        -0.5677863359451294,
        1.3408254384994507,
        -0.2900882661342621,
        0.5135706663131714,
        0.25234487652778625,
        0.37853819131851196,
        -0.4502657651901245,
        0.5529115796089172,
        0.4071376621723175,
        -0.15594767034053802,
        -0.9386773705482483,
        -0.507093071937561,
        -0.1251867264509201,
        0.15381744503974915,
        0.13118508458137512,
        0.04065481573343277,
        0.2575342655181885,
        0.3344072997570038,
        -0.23938564956188202,
        -0.5354965329170227,
        -0.12735651433467865,
        0.6944043636322021,
        1.1044507026672363,
        0.8050694465637207,
        0.8423817157745361,
        -0.2346595674753189,
        0.43401238322257996,
        -0.05413630232214928,
        0.3960169851779938,
        0.33982348442077637,
        0.516295850276947,
        0.676190197467804,
        -0.7286311984062195,
        -0.9193481206893921,
        -1.0379586219787598,
        0.8867736458778381,
        -0.32615169882774353,
        0.14503692090511322,
        -0.8367505669593811,
        -0.19948527216911316,
        0.3160237669944763,
        1.041617512702942,
        0.7450491189956665,
        1.411645770072937,
        -0.046397872269153595,
        -0.22294050455093384,
        -0.3272235691547394,
        0.5179332494735718,
        0.08450157940387726,
        -0.6858903765678406,
        0.8313482403755188,
        0.22500362992286682,
        -0.4177554249763489,
        0.16378791630268097,
        -0.4878891110420227,
        0.8322852849960327,
        -1.0290627479553223,
        -0.8386682271957397,
        0.06653787940740585,
        1.6195735931396484,
        0.3285209834575653,
        -0.5755364894866943,
        0.050972457975149155,
        0.38762661814689636,
        0.3411404490470886,
        -0.64694744348526,
        -1.2268232107162476,
        -0.07137900590896606,
        -0.6340628266334534,
        0.1469433456659317,
        -0.1644972264766693,
        -0.15085765719413757,
        0.22826844453811646,
        0.3800676465034485,
        0.2376481145620346,
        0.7503398656845093,
        -0.7456731200218201,
        -0.5623844861984253,
        -0.7748844623565674,
        -0.10067551583051682,
        0.6919316649436951,
        0.10328730195760727,
        -0.5580003261566162,
        -0.2264094054698944,
        0.5565095543861389,
        0.6542371511459351,
        -0.21543312072753906,
        0.39273926615715027,
        -0.19043289124965668,
        -0.49059411883354187,
        0.6280722618103027,
        0.05056477338075638,
        -0.027014121413230896,
        0.2437722235918045,
        0.012505389750003815,
        -0.40963301062583923,
        -0.23200128972530365,
        -0.2305993139743805,
        -0.1575433611869812,
        0.7373000383377075,
        -0.3247646689414978,
        -1.2512506246566772,
        -0.07371476292610168,
        -0.2662135064601898,
        1.617173671722412,
        0.9722689986228943,
        -0.19027860462665558,
        0.028490982949733734,
        1.1052680015563965,
        -0.09295065701007843,
        -0.908274233341217,
        0.44934362173080444,
        0.26272380352020264,
        -0.1316683441400528,
        -0.43300703167915344,
        -0.6271936893463135,
        0.2558324337005615,
        -0.02370067872107029,
        -0.6327080130577087,
        -0.1256599724292755,
        -0.7043749094009399,
        -0.2330683171749115,
        -0.1708550751209259,
        0.1465928852558136,
        0.37304332852363586,
        -0.3339211642742157,
        0.40036994218826294,
        0.7566992044448853,
        -0.15647289156913757,
        -1.501198649406433,
        -0.49740439653396606,
        0.34196269512176514,
        0.38656285405158997,
        -0.019707903265953064,
        0.16164813935756683,
        0.0774797797203064,
        -0.6247598528862,
        0.1858164370059967,
        0.4029514193534851,
        1.7496910095214844,
        0.6569428443908691,
        -0.23102018237113953,
        -0.310373991727829,
        -0.3017862141132355,
        0.8809794187545776,
        -0.4014696776866913,
        0.3561723232269287,
        -0.5207767486572266,
        0.16023743152618408,
        0.016967570409178734,
        -0.06551185250282288,
        0.8236138224601746,
        -0.23595942556858063,
        0.1960599422454834,
        0.25864601135253906,
        0.46237874031066895,
        -1.3101050853729248,
        -0.46807849407196045,
        -0.1338260918855667,
        0.16569238901138306,
        -0.5113199949264526,
        1.0475670099258423,
        0.07520554214715958,
        0.04448135197162628,
        0.8835775256156921,
        -0.20464158058166504,
        -0.10141614079475403,
        0.2602119743824005,
        -0.6367453932762146,
        1.1396687030792236,
        -0.5007251501083374,
        -0.6230955719947815,
        0.22764253616333008,
        1.002953052520752,
        -0.3619562089443207,
        -0.4115028977394104,
        0.18268077075481415,
        -0.035958293825387955,
        0.1646759808063507,
        -0.15306738018989563,
        0.12188902497291565,
        -0.5872993469238281,
        0.5157607197761536,
        0.8201450705528259,
        0.6630427241325378,
        -0.7865665555000305,
        -0.29749053716659546,
        0.010301217436790466,
        0.23823805153369904,
        -0.22253361344337463,
        0.5292742848396301,
        -0.34301578998565674,
        -0.7486801743507385,
        -1.4641374349594116
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Creates detailed GraphViz visualizations to illustrate complex intelligence investigations and data insights. This approach involves extensive analysis, organizing information, and visual representation using shapes, colors, and labels for clarity. The output includes a comprehensive diagram and analytical conclusions with a certainty rating.",
          "name": "Create_investigation_visualization",
          "raw": "\n                workflow Create_investigation_visualization v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOAL\n\nYou are an expert in intelligence investigations and data visualization using GraphViz. You create full, detailed graphviz visualizations of the input you're given that show the most interesting, surprising, and useful aspects of the input.\n\n# STEPS\n\n- Fully understand the input you were given.\n\n- Spend 3,503 virtual hours taking notes on and organizing your understanding of the input.\n\n- Capture all your understanding of the input on a virtual whiteboard in your mind.\n\n- Think about how you would graph your deep understanding of the concepts in the input into a Graphviz output.\n\n# OUTPUT\n\n- Create a full Graphviz output of all the most interesting aspects of the input.\n\n- Use different shapes and colors to represent different types of nodes.\n\n- Label all nodes, connections, and edges with the most relevant information.\n\n- In the diagram and labels, make the verbs and subjects are clear, e.g., \\\"called on phone, met in person, accessed the database.\\\"\n\n- Ensure all the activities in the investigation are represented, including research, data sources, interviews, conversations, timelines, and conclusions.\n\n- Ensure the final diagram is so clear and well annotated that even a journalist new to the story can follow it, and that it could be used to explain the situation to a jury.\n\n- In a section called ANALYSIS, write up to 10 bullet points of 15 words each giving the most important information from the input and what you learned.\n\n- In a section called CONCLUSION, give a single 25-word statement about your assessment of what happened, who did it, whether the proposition was true or not, or whatever is most relevant. In the final sentence give the CIA rating of certainty for your conclusion.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOAL\n\nYou are an expert in intelligence investigations and data visualization using GraphViz. You create full, detailed graphviz visualizations of the input you're given that show the most interesting, surprising, and useful aspects of the input.\n\n# STEPS\n\n- Fully understand the input you were given.\n\n- Spend 3,503 virtual hours taking notes on and organizing your understanding of the input.\n\n- Capture all your understanding of the input on a virtual whiteboard in your mind.\n\n- Think about how you would graph your deep understanding of the concepts in the input into a Graphviz output.\n\n# OUTPUT\n\n- Create a full Graphviz output of all the most interesting aspects of the input.\n\n- Use different shapes and colors to represent different types of nodes.\n\n- Label all nodes, connections, and edges with the most relevant information.\n\n- In the diagram and labels, make the verbs and subjects are clear, e.g., \\\"called on phone, met in person, accessed the database.\\\"\n\n- Ensure all the activities in the investigation are represented, including research, data sources, interviews, conversations, timelines, and conclusions.\n\n- Ensure the final diagram is so clear and well annotated that even a journalist new to the story can follow it, and that it could be used to explain the situation to a jury.\n\n- In a section called ANALYSIS, write up to 10 bullet points of 15 words each giving the most important information from the input and what you learned.\n\n- In a section called CONCLUSION, give a single 25-word statement about your assessment of what happened, who did it, whether the proposition was true or not, or whatever is most relevant. In the final sentence give the CIA rating of certainty for your conclusion.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.03520706295967102,
        -0.28305768966674805,
        0.35660484433174133,
        0.08157432079315186,
        -0.29793667793273926,
        0.0731559619307518,
        -1.472562551498413,
        0.20691674947738647,
        0.24042972922325134,
        0.5173492431640625,
        -0.3097922205924988,
        1.262272596359253,
        -0.14534850418567657,
        0.40475407242774963,
        -0.689083993434906,
        0.21044522523880005,
        0.32116571068763733,
        -0.7978572845458984,
        -0.8431907892227173,
        -0.25452277064323425,
        -0.383269727230072,
        0.6412035822868347,
        0.7395632863044739,
        0.052520155906677246,
        0.5137559771537781,
        0.22402575612068176,
        0.1666077971458435,
        -0.3422054052352905,
        -0.2344420701265335,
        -1.4226466417312622,
        0.17470782995224,
        0.33437594771385193,
        -0.22976166009902954,
        -0.3891126215457916,
        0.058444757014513016,
        -0.6502328515052795,
        -0.9057706594467163,
        0.06438124924898148,
        -0.2668086588382721,
        -0.34587913751602173,
        0.1160149797797203,
        -0.7221972346305847,
        -0.47143346071243286,
        0.1385389268398285,
        -0.019498160108923912,
        -0.3771670460700989,
        -0.11111976206302643,
        0.20972970128059387,
        1.1959501504898071,
        0.7638209462165833,
        -0.3911052346229553,
        -0.5745378136634827,
        -0.31918448209762573,
        -0.46023088693618774,
        -0.49985644221305847,
        0.1960417628288269,
        -0.41924142837524414,
        -0.3901132345199585,
        0.45230209827423096,
        0.11763904988765717,
        -0.5273141860961914,
        0.6464008092880249,
        -3.4723448753356934,
        -0.15710967779159546,
        1.022730827331543,
        -0.06666942685842514,
        0.29997068643569946,
        -0.3117247223854065,
        0.7407151460647583,
        0.5668370127677917,
        -0.24012386798858643,
        0.5637486577033997,
        0.038164474070072174,
        -0.02794186770915985,
        -0.4910910427570343,
        -0.09745698422193527,
        -0.05366702750325203,
        0.2468113750219345,
        0.2630847692489624,
        0.4047165811061859,
        0.26098212599754333,
        0.005591779947280884,
        0.07587175071239471,
        -0.23359648883342743,
        -0.5884631276130676,
        0.14634688198566437,
        -0.533831000328064,
        -0.19520998001098633,
        -0.6089849472045898,
        0.12493915855884552,
        -0.7810059189796448,
        -0.10371582210063934,
        0.8661348819732666,
        -0.12160491943359375,
        -0.5447037220001221,
        -0.3058767318725586,
        -0.40581175684928894,
        -0.48359984159469604,
        -0.1343645453453064,
        3.1105244159698486,
        0.529839277267456,
        -0.12431232631206512,
        0.6897168755531311,
        -1.534612774848938,
        0.34019047021865845,
        -0.21861302852630615,
        -0.4439114034175873,
        -0.3757433593273163,
        0.35558804869651794,
        -0.48248976469039917,
        0.3055514395236969,
        -0.6567341685295105,
        -0.6083992719650269,
        0.11909975856542587,
        0.5156917572021484,
        0.9240447878837585,
        -0.7166740298271179,
        0.3087659180164337,
        0.08854230493307114,
        0.13429635763168335,
        -0.32878196239471436,
        0.12779949605464935,
        0.17210853099822998,
        0.0031290799379348755,
        0.12977048754692078,
        -0.6156960725784302,
        -0.6580212712287903,
        0.44844385981559753,
        0.05295393988490105,
        -0.6371225118637085,
        -0.11774661391973495,
        0.4405101239681244,
        -0.13889481127262115,
        0.5219041109085083,
        0.23054611682891846,
        0.34006911516189575,
        0.7944288849830627,
        -1.0258209705352783,
        0.36397379636764526,
        0.03685402870178223,
        0.7574313282966614,
        -0.5945504307746887,
        0.3850046992301941,
        -0.22611960768699646,
        0.7666531801223755,
        0.9034667611122131,
        -0.13577961921691895,
        0.14717839658260345,
        -0.8836591839790344,
        -0.19664409756660461,
        -0.15207023918628693,
        0.04270247742533684,
        -0.08582886308431625,
        -0.1823117733001709,
        0.297294557094574,
        -0.01948762685060501,
        -0.5819482207298279,
        0.6976135969161987,
        -0.4769759178161621,
        0.9010196328163147,
        -0.007474968209862709,
        -0.3354535698890686,
        0.04124440997838974,
        -0.6643694639205933,
        0.10557913780212402,
        -0.7712960243225098,
        0.3677230477333069,
        0.033034950494766235,
        0.17260830104351044,
        0.1842789500951767,
        -0.3495665192604065,
        -0.18110224604606628,
        0.6334915161132812,
        0.8697137832641602,
        -0.3674686551094055,
        -0.23406904935836792,
        -0.8721863031387329,
        0.16611552238464355,
        0.40334656834602356,
        -0.28517886996269226,
        0.5804994106292725,
        -0.2672477662563324,
        -0.49096056818962097,
        -0.631837785243988,
        -0.21173512935638428,
        0.2584453821182251,
        0.2994493246078491,
        0.2243603765964508,
        0.852375864982605,
        1.0310256481170654,
        -1.0815949440002441,
        1.2809205055236816,
        -0.594014585018158,
        0.5505244135856628,
        0.0446048229932785,
        0.3201364278793335,
        0.17647074162960052,
        0.004048921167850494,
        0.5785795450210571,
        0.36464157700538635,
        -1.488471508026123,
        -0.03445928916335106,
        -0.3460065424442291,
        -0.3088071942329407,
        -0.9930471777915955,
        -0.6032876372337341,
        -0.38015061616897583,
        1.3232518434524536,
        -0.653843104839325,
        -0.28316041827201843,
        0.03270736709237099,
        0.04819859564304352,
        1.5680921077728271,
        0.4893428683280945,
        0.8627163171768188,
        0.5547742247581482,
        -0.020604029297828674,
        0.4604783058166504,
        0.3008318543434143,
        -0.46117064356803894,
        -0.1874234676361084,
        0.3791390657424927,
        -0.7682112455368042,
        -0.45035308599472046,
        -0.8673235774040222,
        0.5837078094482422,
        -0.4495835304260254,
        0.2585532069206238,
        -1.1221657991409302,
        -0.5654109120368958,
        0.5324979424476624,
        0.9476268887519836,
        1.112151026725769,
        0.1596432626247406,
        0.08821654319763184,
        0.2849106788635254,
        0.19314482808113098,
        0.5763720273971558,
        0.3187127113342285,
        -0.39273130893707275,
        0.018617600202560425,
        0.4246939718723297,
        0.23760832846164703,
        -0.1809844821691513,
        0.689605176448822,
        -0.0189170241355896,
        -0.7246312499046326,
        -0.7474511861801147,
        -0.09805520623922348,
        1.5735771656036377,
        -0.16299566626548767,
        0.015305567532777786,
        0.22000177204608917,
        0.6988110542297363,
        -0.1015121266245842,
        0.1387530416250229,
        -0.9128944277763367,
        -0.25737473368644714,
        -0.6063360571861267,
        0.45560741424560547,
        -0.5242732763290405,
        0.10972806066274643,
        0.8969545960426331,
        0.025105834007263184,
        -0.5596849918365479,
        -0.5407597422599792,
        -0.21900643408298492,
        -0.7000850439071655,
        -0.5264551639556885,
        0.033271320164203644,
        0.5121622681617737,
        0.19805368781089783,
        0.3729674220085144,
        -0.24416333436965942,
        0.0015171468257904053,
        -0.010817775502800941,
        -0.5364454984664917,
        -0.025767318904399872,
        0.025661103427410126,
        -0.9226523637771606,
        -0.05653897672891617,
        0.04247717559337616,
        0.027274396270513535,
        0.019142374396324158,
        -0.08053324371576309,
        -0.17159046232700348,
        0.2972794473171234,
        -1.39290452003479,
        -0.8619488477706909,
        0.2141149640083313,
        0.20641714334487915,
        -0.6538161635398865,
        -0.49476686120033264,
        0.33793905377388,
        2.309433698654175,
        0.3442397713661194,
        0.30149298906326294,
        0.5143635869026184,
        0.7729877829551697,
        -0.014748522080481052,
        0.009802557528018951,
        0.5091233849525452,
        -0.09771092981100082,
        0.6725806593894958,
        -0.9240164160728455,
        -0.2789692282676697,
        0.21245524287223816,
        -0.566967248916626,
        0.3648575246334076,
        0.07461783289909363,
        -0.25157690048217773,
        -0.25336718559265137,
        -0.08685332536697388,
        -0.3300599455833435,
        0.6047661304473877,
        0.2536204159259796,
        1.1333380937576294,
        1.3211236000061035,
        0.37919726967811584,
        -1.5150823593139648,
        -0.1932818442583084,
        0.5957056879997253,
        0.13113299012184143,
        -0.42542117834091187,
        -0.33517003059387207,
        0.655564546585083,
        -0.4841903746128082,
        0.7165677547454834,
        -0.0031247586011886597,
        1.4337420463562012,
        0.6279735565185547,
        0.42687737941741943,
        0.14390209317207336,
        0.5958095788955688,
        1.0072259902954102,
        -0.16628825664520264,
        0.9243650436401367,
        0.5650485157966614,
        -0.59930419921875,
        -0.09362353384494781,
        0.5602577328681946,
        0.7393416166305542,
        0.4121045470237732,
        0.25179848074913025,
        -0.13612838089466095,
        0.047819074243307114,
        -0.021961897611618042,
        -1.0839499235153198,
        -0.028484299778938293,
        0.22648780047893524,
        -0.3870258033275604,
        0.0675363689661026,
        -0.49535074830055237,
        0.4010910093784332,
        1.0143071413040161,
        0.8333872556686401,
        -1.4315980672836304,
        -0.23978683352470398,
        -0.3658108115196228,
        1.385069489479065,
        -0.515225887298584,
        -0.15999551117420197,
        -0.31704482436180115,
        0.012082351371645927,
        -0.07732995599508286,
        -0.05210791528224945,
        -0.10331575572490692,
        -0.7101166248321533,
        0.3534308075904846,
        0.5562735199928284,
        -0.039964210242033005,
        -0.13350200653076172,
        0.4541444182395935,
        0.43978890776634216,
        0.7690749764442444,
        -0.18039120733737946,
        0.15433725714683533,
        0.24631254374980927,
        0.19032788276672363,
        -0.4552501440048218,
        0.6361212730407715,
        -0.10589015483856201,
        -0.3632265031337738,
        -1.2415379285812378
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt guides in creating TED-quality keynote presentations from provided input, focusing on narrative flow and practical takeaways. It outlines steps for structuring the presentation into slides with concise bullet points, images, and speaker notes. The expected output includes a story flow, the final takeaway, and a detailed slide deck presentation.",
          "name": "Create_keynote",
          "raw": "\n                workflow Create_keynote v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at creating TED-quality keynote presentations from the input provided.\n\nTake a deep breath and think step-by-step about how best to achieve this using the steps below.\n\n# STEPS\n\n- Think about the entire narrative flow of the presentation first. Have that firmly in your mind. Then begin.\n\n- Given the input, determine what the real takeaway should be, from a practical standpoint, and ensure that the narrative structure we're building towards ends with that final note.\n\n- Take the concepts from the input and create <hr> delimited sections for each slide.\n\n- The slide's content will be 3-5 bullets of no more than 5-10 words each.\n\n- Create the slide deck as a slide-based way to tell the story of the content. Be aware of the narrative flow of the slides, and be sure you're building the story like you would for a TED talk.\n\n- Each slide's content:\n\n-- Title\n-- Main content of 3-5 bullets\n-- Image description (for an AI image generator)\n-- Speaker notes (for the presenter): These should be the exact words the speaker says for that slide. Give them as a set of bullets of no more than 15 words each.\n\n- The total length of slides should be between 10 - 25, depending on the input.\n\n# OUTPUT GUIDANCE\n\n- These should be TED level presentations focused on narrative.\n\n- Ensure the slides and overall presentation flows properly. If it doesn't produce a clean narrative, start over.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a section called FLOW that has the flow of the story we're going to tell as a series of 10-20 bullets that are associated with one slide a piece. Each bullet should be 10-words max.\n\n- Output a section called DESIRED TAKEAWAY that has the final takeaway from the presentation. This should be a single sentence.\n\n- Output a section called PRESENTATION that's a Markdown formatted list of slides and the content on the slide, plus the image description.\n\n- Ensure the speaker notes are in the voice of the speaker, i.e. they're what they're actually going to say.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at creating TED-quality keynote presentations from the input provided.\n\nTake a deep breath and think step-by-step about how best to achieve this using the steps below.\n\n# STEPS\n\n- Think about the entire narrative flow of the presentation first. Have that firmly in your mind. Then begin.\n\n- Given the input, determine what the real takeaway should be, from a practical standpoint, and ensure that the narrative structure we're building towards ends with that final note.\n\n- Take the concepts from the input and create <hr> delimited sections for each slide.\n\n- The slide's content will be 3-5 bullets of no more than 5-10 words each.\n\n- Create the slide deck as a slide-based way to tell the story of the content. Be aware of the narrative flow of the slides, and be sure you're building the story like you would for a TED talk.\n\n- Each slide's content:\n\n-- Title\n-- Main content of 3-5 bullets\n-- Image description (for an AI image generator)\n-- Speaker notes (for the presenter): These should be the exact words the speaker says for that slide. Give them as a set of bullets of no more than 15 words each.\n\n- The total length of slides should be between 10 - 25, depending on the input.\n\n# OUTPUT GUIDANCE\n\n- These should be TED level presentations focused on narrative.\n\n- Ensure the slides and overall presentation flows properly. If it doesn't produce a clean narrative, start over.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a section called FLOW that has the flow of the story we're going to tell as a series of 10-20 bullets that are associated with one slide a piece. Each bullet should be 10-words max.\n\n- Output a section called DESIRED TAKEAWAY that has the final takeaway from the presentation. This should be a single sentence.\n\n- Output a section called PRESENTATION that's a Markdown formatted list of slides and the content on the slide, plus the image description.\n\n- Ensure the speaker notes are in the voice of the speaker, i.e. they're what they're actually going to say.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.03044828213751316,
        1.0545979738235474,
        -0.9042946100234985,
        0.5144778490066528,
        -0.17241117358207703,
        0.5791590809822083,
        -0.4301055371761322,
        -0.8405351042747498,
        0.05947039648890495,
        1.1265804767608643,
        -0.4477580785751343,
        0.41868993639945984,
        -0.03255243971943855,
        -0.10497084259986877,
        0.13796071708202362,
        -0.20456305146217346,
        -0.25260278582572937,
        -0.4523848295211792,
        -1.211137056350708,
        -0.31332874298095703,
        -0.13548296689987183,
        0.5308126211166382,
        0.46570202708244324,
        -0.5050098896026611,
        -0.09286128729581833,
        -0.5520135760307312,
        -0.12710346281528473,
        -0.21609066426753998,
        -0.06877459585666656,
        -1.4766669273376465,
        0.3288378119468689,
        0.9462990164756775,
        -0.5546686053276062,
        -0.24702955782413483,
        0.32461437582969666,
        -0.4023832082748413,
        -0.20742279291152954,
        0.47424036264419556,
        -0.0015607485547661781,
        -0.23905663192272186,
        0.9068495631217957,
        -0.843967854976654,
        -0.19447968900203705,
        -0.5431258678436279,
        0.016506046056747437,
        0.055609941482543945,
        0.5190654397010803,
        -0.36183568835258484,
        0.8809817433357239,
        0.5809786915779114,
        0.11668328940868378,
        -0.6610167026519775,
        0.7229365110397339,
        -0.5219104290008545,
        -0.5578809976577759,
        -0.026432976126670837,
        -0.21479913592338562,
        -0.22186779975891113,
        0.4569101333618164,
        0.6241443753242493,
        0.3087184727191925,
        0.6039773225784302,
        -2.8360204696655273,
        -0.6157187819480896,
        0.9695253968238831,
        0.07799671590328217,
        -0.35803458094596863,
        0.005593845620751381,
        0.6141353249549866,
        0.4762060046195984,
        0.34785202145576477,
        0.4944712817668915,
        -0.25253036618232727,
        -0.4672219455242157,
        0.11759018152952194,
        -0.37375912070274353,
        -0.060266438871622086,
        0.128236785531044,
        0.3520107865333557,
        -0.06892874091863632,
        0.3596669137477875,
        0.7908879518508911,
        -0.5396466851234436,
        0.06083612143993378,
        -0.6525284051895142,
        0.165157288312912,
        0.24165105819702148,
        -0.3031814396381378,
        0.2541659474372864,
        0.2837759256362915,
        -0.3139290511608124,
        -0.29108619689941406,
        0.7468799352645874,
        -0.442670613527298,
        -0.9143449068069458,
        0.5112781524658203,
        0.13550856709480286,
        -0.09973706305027008,
        0.4910881221294403,
        3.1710383892059326,
        -0.01119929552078247,
        0.27771642804145813,
        0.8763241171836853,
        -0.9978621602058411,
        0.7539519667625427,
        -0.13849925994873047,
        0.2051926851272583,
        -0.7362392544746399,
        -0.0604412704706192,
        -0.02481474168598652,
        0.13793830573558807,
        -0.3836263418197632,
        -0.6699082851409912,
        0.3127909302711487,
        0.050035081803798676,
        -0.022890307009220123,
        -0.10906363278627396,
        0.2750038206577301,
        -0.4643568694591522,
        0.27576759457588196,
        -0.6128897070884705,
        -0.19542434811592102,
        -0.979688823223114,
        0.33148518204689026,
        0.08193926513195038,
        0.6088109612464905,
        -0.9102761745452881,
        0.5373582243919373,
        -0.3779369592666626,
        1.096058964729309,
        -0.25942671298980713,
        0.4746018350124359,
        0.0042722187936306,
        -0.2744518518447876,
        -0.07688622176647186,
        -0.04134673625230789,
        0.18902789056301117,
        -0.31239625811576843,
        -0.20607584714889526,
        0.23825952410697937,
        0.5608556270599365,
        -0.4836336374282837,
        0.1346859186887741,
        0.5384060144424438,
        -0.05511898547410965,
        0.6058513522148132,
        -0.1978611946105957,
        -0.35144832730293274,
        -0.4850556552410126,
        -0.8988096117973328,
        0.08804816007614136,
        -0.2383269965648651,
        0.04824680835008621,
        -0.10377499461174011,
        0.44799792766571045,
        0.09211961179971695,
        -0.01313462108373642,
        -0.43439406156539917,
        -0.5322191715240479,
        0.3385772705078125,
        0.5984486937522888,
        -0.2624232769012451,
        -0.04613475501537323,
        0.32881537079811096,
        0.16975778341293335,
        -0.11456094682216644,
        0.29483646154403687,
        0.14431312680244446,
        0.29289644956588745,
        0.04649481549859047,
        -0.5266860127449036,
        -0.5592795014381409,
        0.6970462203025818,
        0.19256678223609924,
        -0.3646726608276367,
        -0.13618119060993195,
        0.2943152189254761,
        0.02020327001810074,
        0.2291821539402008,
        -0.3784979581832886,
        1.0024728775024414,
        0.644222617149353,
        -0.0994013100862503,
        -1.077242136001587,
        -0.8278807997703552,
        -0.13966123759746552,
        -0.0738581046462059,
        -0.05248626321554184,
        -0.07839075475931168,
        0.3155136704444885,
        -1.2266693115234375,
        1.1269855499267578,
        -0.00559176504611969,
        0.40485110878944397,
        0.5486719608306885,
        -0.12315972149372101,
        0.232683464884758,
        0.6797257661819458,
        0.42977631092071533,
        -0.37132635712623596,
        -1.0713318586349487,
        -0.9133211374282837,
        0.00023130886256694794,
        0.19516056776046753,
        -0.4380022883415222,
        -0.3046973943710327,
        -0.21721822023391724,
        1.0608820915222168,
        -0.6388671398162842,
        -0.23703715205192566,
        -0.9107027053833008,
        0.5303853154182434,
        1.2306623458862305,
        0.7243627309799194,
        1.219589352607727,
        0.31775569915771484,
        0.1779000163078308,
        0.20498895645141602,
        0.5842592716217041,
        0.22600428760051727,
        0.6950179934501648,
        0.6959161162376404,
        -0.945244312286377,
        -0.6535883545875549,
        0.09137172996997833,
        0.07787216454744339,
        -0.5152842402458191,
        -0.3831239342689514,
        -1.264101505279541,
        -0.4045095145702362,
        1.2909704446792603,
        1.7467076778411865,
        0.307333379983902,
        1.1331292390823364,
        0.7071370482444763,
        0.46253785490989685,
        -0.6832956671714783,
        -0.21768581867218018,
        -0.41264608502388,
        -1.165663480758667,
        1.249634027481079,
        0.2555052638053894,
        -0.3790799379348755,
        0.3734974265098572,
        -0.3017329275608063,
        -0.11529020965099335,
        -0.23688450455665588,
        -1.0804678201675415,
        -0.010248975828289986,
        1.1261777877807617,
        0.08552005887031555,
        0.1532452255487442,
        -0.830992579460144,
        0.5995712280273438,
        0.11119997501373291,
        0.13195358216762543,
        -1.70625901222229,
        0.5689963102340698,
        -0.20634222030639648,
        0.2823236286640167,
        -0.0802866518497467,
        -0.20307564735412598,
        0.15739749372005463,
        0.17892415821552277,
        -0.05140219256281853,
        -0.34306439757347107,
        -0.6507506966590881,
        -0.4536070227622986,
        -0.4044937491416931,
        -0.4181504249572754,
        0.01966838166117668,
        -0.2551002502441406,
        -0.36085373163223267,
        -0.05490759015083313,
        0.027012914419174194,
        -0.14604315161705017,
        0.5065743327140808,
        0.22757577896118164,
        0.06528272479772568,
        -0.6385882496833801,
        0.4690242111682892,
        0.6585284471511841,
        -1.0122215747833252,
        0.37431800365448,
        -0.5432636141777039,
        -0.082967609167099,
        -1.116760492324829,
        -0.5793687105178833,
        -0.13799598813056946,
        0.8135883212089539,
        -0.2685445249080658,
        -0.5469260215759277,
        0.08094090223312378,
        0.13635095953941345,
        0.9694581031799316,
        0.7579607963562012,
        0.3548263907432556,
        0.8214697241783142,
        -0.22984987497329712,
        0.045173514634370804,
        0.7037403583526611,
        0.10681544244289398,
        -0.12194472551345825,
        -0.23999625444412231,
        -1.1186087131500244,
        -0.67497318983078,
        0.3808400630950928,
        0.20361025631427765,
        -0.7639036774635315,
        0.02410915121436119,
        -0.8824898600578308,
        0.5099703073501587,
        0.41191861033439636,
        -0.15189652144908905,
        -0.0378091037273407,
        0.1921185851097107,
        -0.3475477397441864,
        1.5558974742889404,
        0.42300766706466675,
        -1.7525700330734253,
        -0.39529240131378174,
        0.5819094777107239,
        0.5718916654586792,
        0.2747921049594879,
        -0.5585201382637024,
        0.10810193419456482,
        0.1436379849910736,
        -0.4327254593372345,
        -0.397555947303772,
        1.3768192529678345,
        0.1260666400194168,
        -0.15773427486419678,
        -0.2715504765510559,
        0.7338632345199585,
        0.5620163679122925,
        0.12590280175209045,
        0.49365314841270447,
        -0.5857300758361816,
        -0.5818166732788086,
        -0.22663572430610657,
        0.08882754296064377,
        1.6464214324951172,
        0.08693746477365494,
        0.3903917074203491,
        -0.2177794724702835,
        -0.1320488154888153,
        -0.555792510509491,
        -1.4782230854034424,
        0.4030294120311737,
        0.09486965835094452,
        -0.4869324862957001,
        1.1568793058395386,
        -0.10239043831825256,
        -0.4349540174007416,
        0.5561590194702148,
        0.7140654921531677,
        -0.531099259853363,
        0.32516369223594666,
        -0.6618004441261292,
        1.4024035930633545,
        -0.31108877062797546,
        -0.341758668422699,
        -0.2789991497993469,
        0.7357240319252014,
        -0.16051305830478668,
        -0.5637013912200928,
        0.21650759875774384,
        -0.1944754421710968,
        0.5468924045562744,
        0.5966626405715942,
        0.3832515478134155,
        -0.014088582247495651,
        0.9091020822525024,
        0.4264858365058899,
        0.30219316482543945,
        -0.6054733991622925,
        -0.06620940566062927,
        0.02060963213443756,
        -0.13738654553890228,
        -0.2757929265499115,
        0.7180463671684265,
        -0.4543135464191437,
        -0.6107558012008667,
        -0.5182998180389404
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates simple, minimalist company logos based on provided input, focusing on elegance and impact without text. The approach emphasizes super minimalist designs. The output is a prompt for an AI image generator to create a simple, vector graphic logo.",
          "name": "Create_logo",
          "raw": "\n                workflow Create_logo v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou create simple, elegant, and impactful company logos based on the input given to you. The logos are super minimalist and without text.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Output a prompt that can be sent to an AI image generator for a simple and elegant logo that captures and incorporates the meaning of the input sent. The prompt should take the input and create a simple, vector graphic logo description for the AI to generate.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure the description asks for a simple, vector graphic logo.\n- Do not output anything other than the raw image description that will be sent to the image generator.\n- You only output human-readable Markdown.\n- Do not output warnings or notes —- just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou create simple, elegant, and impactful company logos based on the input given to you. The logos are super minimalist and without text.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Output a prompt that can be sent to an AI image generator for a simple and elegant logo that captures and incorporates the meaning of the input sent. The prompt should take the input and create a simple, vector graphic logo description for the AI to generate.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure the description asks for a simple, vector graphic logo.\n- Do not output anything other than the raw image description that will be sent to the image generator.\n- You only output human-readable Markdown.\n- Do not output warnings or notes —- just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.01131233386695385,
        0.3668819069862366,
        0.04341238737106323,
        0.2810516059398651,
        -0.3084716200828552,
        0.1387270838022232,
        -0.9964912533760071,
        0.11777681112289429,
        -0.13243579864501953,
        0.378085732460022,
        -0.660345196723938,
        0.9487290978431702,
        0.07651554048061371,
        -0.0879974365234375,
        0.06505219638347626,
        -0.19222988188266754,
        0.11146338284015656,
        -1.0312376022338867,
        -1.0293641090393066,
        -0.572199285030365,
        -0.22737367451190948,
        0.9344227910041809,
        0.40437453985214233,
        0.07595518231391907,
        0.38464346528053284,
        -0.23936466872692108,
        0.43877512216567993,
        -0.27993011474609375,
        -1.1161952018737793,
        -1.1977683305740356,
        0.07287300378084183,
        -0.08046186715364456,
        -0.3400691747665405,
        -0.3217878043651581,
        0.27124905586242676,
        -0.8482630848884583,
        -0.2923716902732849,
        -0.13331764936447144,
        -0.20043888688087463,
        -0.1131850853562355,
        0.7022356986999512,
        -0.007280731573700905,
        -0.43879759311676025,
        0.1981564164161682,
        0.6187912821769714,
        0.155623197555542,
        0.0525103360414505,
        0.06549222767353058,
        -0.04897621273994446,
        0.24442432820796967,
        -0.09896409511566162,
        -0.7232415080070496,
        -0.03492959961295128,
        -0.1480093002319336,
        -0.38889461755752563,
        0.24092933535575867,
        -0.17436859011650085,
        -1.2076027393341064,
        0.3821299970149994,
        0.34600526094436646,
        -0.15842634439468384,
        0.3047681450843811,
        -3.0987088680267334,
        -0.12326185405254364,
        0.32366153597831726,
        0.22529414296150208,
        0.24033866822719574,
        -0.02280157431960106,
        0.581251859664917,
        0.5837387442588806,
        0.09451265633106232,
        0.2703031301498413,
        -0.43913373351097107,
        0.6015254855155945,
        -0.22077292203903198,
        -0.8065680861473083,
        0.1847800612449646,
        -0.08267660439014435,
        0.11633823066949844,
        -0.46194854378700256,
        0.5987327098846436,
        0.012283124029636383,
        0.3471624553203583,
        -0.3240646719932556,
        -0.5837916731834412,
        0.30015790462493896,
        -0.13471971452236176,
        -0.373172789812088,
        0.33649754524230957,
        0.004877548664808273,
        -0.765640139579773,
        0.0017997883260250092,
        0.3038722574710846,
        -0.3359754979610443,
        -0.3741418421268463,
        0.026028532534837723,
        -0.10826906561851501,
        -0.029034525156021118,
        -0.10149123519659042,
        3.248690128326416,
        0.3931114077568054,
        -0.1747540384531021,
        1.0384020805358887,
        -1.0096970796585083,
        0.033258963376283646,
        -0.15491801500320435,
        0.5956103801727295,
        -0.8858741521835327,
        -0.2549286186695099,
        -0.28821951150894165,
        0.5781548023223877,
        -0.9826074838638306,
        -0.6741217374801636,
        0.32611802220344543,
        0.5511060953140259,
        0.19548743963241577,
        -0.445753812789917,
        0.1760840117931366,
        -0.22263333201408386,
        1.0564429759979248,
        -0.21133902668952942,
        -0.36525169014930725,
        -0.242802232503891,
        0.03156276419758797,
        -0.2038368433713913,
        0.2201407551765442,
        -0.3717464804649353,
        0.5246268510818481,
        -0.18344935774803162,
        0.09249689429998398,
        -0.5114842653274536,
        -0.12405355274677277,
        0.23987974226474762,
        0.09435974061489105,
        -0.26415663957595825,
        0.041978973895311356,
        0.3845389485359192,
        -0.8590835928916931,
        0.011713642627000809,
        -0.16133977472782135,
        0.293561726808548,
        -0.6489565968513489,
        0.7764979004859924,
        0.04675602167844772,
        0.6978670358657837,
        1.0286595821380615,
        -0.17058244347572327,
        0.4218526780605316,
        -0.6226221323013306,
        -0.4110264480113983,
        -0.09480872750282288,
        0.062167536467313766,
        -0.3396609127521515,
        0.18108724057674408,
        0.6732443571090698,
        0.32567858695983887,
        -0.7151042222976685,
        0.23716787993907928,
        -0.45422324538230896,
        0.7151610851287842,
        -0.26010215282440186,
        0.117841936647892,
        0.055679261684417725,
        0.39065682888031006,
        0.3319278657436371,
        -0.41717711091041565,
        0.30501848459243774,
        -0.04533348232507706,
        0.41548171639442444,
        0.38178688287734985,
        -0.2303280532360077,
        -0.2436673939228058,
        0.4552655816078186,
        0.520781397819519,
        -0.3483548164367676,
        0.10140641033649445,
        -0.3147810399532318,
        -0.28970199823379517,
        0.44573378562927246,
        -0.1623322069644928,
        0.9246051907539368,
        0.20736318826675415,
        -0.29261770844459534,
        -1.1254985332489014,
        -0.43353670835494995,
        0.4040318429470062,
        -0.1491500735282898,
        0.40470707416534424,
        0.7316709756851196,
        0.9428054094314575,
        -0.5377125144004822,
        1.699590802192688,
        -0.5266050100326538,
        0.7424424886703491,
        0.31439393758773804,
        0.5678233504295349,
        0.8658137917518616,
        0.4898914694786072,
        1.0045779943466187,
        0.08457158505916595,
        -1.334749460220337,
        -0.12259648740291595,
        0.30827805399894714,
        0.13653436303138733,
        -0.3609106242656708,
        -0.30052608251571655,
        0.2835415303707123,
        0.5742990374565125,
        -0.5034539699554443,
        -0.39256757497787476,
        -0.33951178193092346,
        0.4640868604183197,
        1.6729888916015625,
        0.6468848586082458,
        1.2087653875350952,
        -0.4606119990348816,
        0.08833441138267517,
        -0.0759507566690445,
        0.2364867776632309,
        -0.010888934135437012,
        0.27042365074157715,
        1.0431859493255615,
        -0.4768194556236267,
        -0.5479769706726074,
        -0.640541136264801,
        0.9441659450531006,
        -0.10520869493484497,
        0.20193710923194885,
        -1.1999603509902954,
        -0.627673864364624,
        0.3259747624397278,
        1.5423142910003662,
        0.9052355885505676,
        1.3932002782821655,
        -0.12865522503852844,
        0.45671308040618896,
        0.11429902911186218,
        0.4142913520336151,
        -0.033591799437999725,
        -1.382392168045044,
        0.7113211750984192,
        0.38328319787979126,
        0.00761982798576355,
        0.19683457911014557,
        0.059869490563869476,
        -0.06754297763109207,
        -0.885237991809845,
        -1.0214083194732666,
        -0.09499679505825043,
        1.7480096817016602,
        0.07335454225540161,
        -0.34576475620269775,
        -0.43052494525909424,
        0.37407124042510986,
        -0.3464215099811554,
        -0.26631274819374084,
        -1.6954675912857056,
        -0.7029033303260803,
        -0.9366499185562134,
        0.8837903738021851,
        -0.3964184522628784,
        0.08599541336297989,
        0.7170166373252869,
        0.2735873758792877,
        -0.006055712234228849,
        0.1255083978176117,
        -0.6062008738517761,
        -0.9499690532684326,
        -0.504328191280365,
        -0.2086574137210846,
        0.8407880663871765,
        -0.306556761264801,
        -0.5930936336517334,
        0.2434464991092682,
        -0.07933177053928375,
        0.0020814426243305206,
        0.26083797216415405,
        0.07953115552663803,
        -0.5188241004943848,
        -0.3798058331012726,
        0.28492027521133423,
        0.03695778548717499,
        -0.8691166043281555,
        0.30437880754470825,
        -0.5699549913406372,
        -0.28326886892318726,
        -0.5193584561347961,
        -0.40304768085479736,
        0.27063584327697754,
        1.0918488502502441,
        0.05205424129962921,
        -0.5707951784133911,
        -0.29600968956947327,
        -0.16776078939437866,
        2.021073818206787,
        0.4216954708099365,
        -0.2046171873807907,
        0.5290167927742004,
        0.27078184485435486,
        -0.4905567169189453,
        -0.04536920040845871,
        0.2168906331062317,
        0.2877088785171509,
        0.03448876366019249,
        -0.857372522354126,
        -0.3277955949306488,
        0.39508771896362305,
        0.031527694314718246,
        -0.2610854208469391,
        0.4091537892818451,
        -0.8832689523696899,
        -0.2411544770002365,
        -0.24182060360908508,
        -0.005001269280910492,
        0.2824958562850952,
        -0.14308106899261475,
        0.21963652968406677,
        1.3365979194641113,
        -0.5160878300666809,
        -1.5609679222106934,
        -0.6107841730117798,
        0.918279767036438,
        0.6462011933326721,
        -0.2788940668106079,
        -0.2779577076435089,
        0.36214113235473633,
        -0.16108322143554688,
        0.04489283263683319,
        -0.2733347415924072,
        1.7485251426696777,
        0.7101861834526062,
        0.09436909854412079,
        -0.4685884118080139,
        -0.07183904945850372,
        0.8690000772476196,
        0.11700460314750671,
        0.2533322274684906,
        -0.4046046733856201,
        -0.33240219950675964,
        -0.07608790695667267,
        -0.15395140647888184,
        1.1290018558502197,
        0.05212190002202988,
        0.16298964619636536,
        0.022777888923883438,
        -0.43906837701797485,
        -1.2867908477783203,
        -0.6315488219261169,
        0.5420758128166199,
        -0.18666678667068481,
        -0.19186653196811676,
        0.7001664042472839,
        0.29622337222099304,
        0.8377774953842163,
        0.6808130145072937,
        0.36558258533477783,
        -0.22603990137577057,
        -0.3745516538619995,
        -0.727253794670105,
        1.9915677309036255,
        -0.2362731248140335,
        -0.6737949252128601,
        -0.5767993927001953,
        0.08264079689979553,
        -0.4394180476665497,
        -0.08800274133682251,
        0.1781095266342163,
        -0.07811333984136581,
        0.148123100399971,
        0.22526422142982483,
        0.20379842817783356,
        -0.7223212718963623,
        0.6038151979446411,
        0.3984636068344116,
        0.5244368314743042,
        -0.545120358467102,
        -0.054877445101737976,
        0.33854609727859497,
        0.20245549082756042,
        -0.40062469244003296,
        0.5231828093528748,
        -0.5275287628173828,
        -0.4489571750164032,
        -0.9722927212715149
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Transforms complex ideas into visual formats using MarkMap syntax for easy understanding. This process involves simplifying concepts to ensure they can be effectively represented within the constraints of MarkMap. The output is a MarkMap syntax diagram that visually communicates the core ideas.",
          "name": "Create_markmap_visualization",
          "raw": "\n                workflow Create_markmap_visualization v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using MarkMap.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using Markmap syntax.\n\nYou always output Markmap syntax, even if you have to simplify the input concepts to a point where it can be visualized using Markmap.\n\n# MARKMAP SYNTAX\n\nHere is an example of MarkMap syntax:\n\n````plaintext\nmarkmap:\n  colorFreezeLevel: 2\n---\n\n# markmap\n\n## Links\n\n- [Website](https://markmap.js.org/)\n- [GitHub](https://github.com/gera2ld/markmap)\n\n## Related Projects\n\n- [coc-markmap](https://github.com/gera2ld/coc-markmap) for Neovim\n- [markmap-vscode](https://marketplace.visualstudio.com/items?itemName=gera2ld.markmap-vscode) for VSCode\n- [eaf-markmap](https://github.com/emacs-eaf/eaf-markmap) for Emacs\n\n## Features\n\nNote that if blocks and lists appear at the same level, the lists will be ignored.\n\n### Lists\n\n- **strong** ~~del~~ *italic* ==highlight==\n- `inline code`\n- [x] checkbox\n- Katex: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ <!-- markmap: fold -->\n  - [More Katex Examples](#?d=gist:af76a4c245b302206b16aec503dbe07b:katex.md)\n- Now we can wrap very very very very long text based on `maxWidth` option\n\n### Blocks\n\n```js\nconsole('hello, JavaScript')\n````\n\n| Products | Price |\n| -------- | ----- |\n| Apple    | 4     |\n| Banana   | 2     |\n\n![](/favicon.png)\n\n```\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using proper MarkMap syntax.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Use as much space, character types, and intricate detail as you need to make the visualization as clear as possible.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the ASCII art, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just make the Markmap.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- Create a diagram no matter what, using the STEPS above to determine which type.\n\n# INPUT:\n\nINPUT:\n```\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using MarkMap.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using Markmap syntax.\n\nYou always output Markmap syntax, even if you have to simplify the input concepts to a point where it can be visualized using Markmap.\n\n# MARKMAP SYNTAX\n\nHere is an example of MarkMap syntax:\n\n````plaintext\nmarkmap:\n  colorFreezeLevel: 2\n---\n\n# markmap\n\n## Links\n\n- [Website](https://markmap.js.org/)\n- [GitHub](https://github.com/gera2ld/markmap)\n\n## Related Projects\n\n- [coc-markmap](https://github.com/gera2ld/coc-markmap) for Neovim\n- [markmap-vscode](https://marketplace.visualstudio.com/items?itemName=gera2ld.markmap-vscode) for VSCode\n- [eaf-markmap](https://github.com/emacs-eaf/eaf-markmap) for Emacs\n\n## Features\n\nNote that if blocks and lists appear at the same level, the lists will be ignored.\n\n### Lists\n\n- **strong** ~~del~~ *italic* ==highlight==\n- `inline code`\n- [x] checkbox\n- Katex: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ <!-- markmap: fold -->\n  - [More Katex Examples](#?d=gist:af76a4c245b302206b16aec503dbe07b:katex.md)\n- Now we can wrap very very very very long text based on `maxWidth` option\n\n### Blocks\n\n```js\nconsole('hello, JavaScript')\n````\n\n| Products | Price |\n| -------- | ----- |\n| Apple    | 4     |\n| Banana   | 2     |\n\n![](/favicon.png)\n\n```\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using proper MarkMap syntax.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Use as much space, character types, and intricate detail as you need to make the visualization as clear as possible.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the ASCII art, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just make the Markmap.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- Create a diagram no matter what, using the STEPS above to determine which type.\n\n# INPUT:\n\nINPUT:\n```\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.44468051195144653,
        0.29726457595825195,
        0.02811420150101185,
        0.017324747517704964,
        -0.14321516454219818,
        0.07416217029094696,
        -1.097119688987732,
        0.11028160154819489,
        0.13133196532726288,
        0.18089616298675537,
        -0.12300147116184235,
        0.1655997782945633,
        0.3309475779533386,
        0.06293860077857971,
        0.019462134689092636,
        -0.4449089467525482,
        0.19686436653137207,
        -0.7541950345039368,
        -1.8698298931121826,
        0.05730539932847023,
        -0.31746214628219604,
        0.9748764038085938,
        0.7624192833900452,
        0.41642171144485474,
        0.0517532080411911,
        -0.234779492020607,
        -0.009043186902999878,
        -0.6501868963241577,
        -1.2971779108047485,
        -1.401815414428711,
        0.15151888132095337,
        -0.32140636444091797,
        -0.8695545792579651,
        -0.3753156065940857,
        1.0865628719329834,
        -0.6530160307884216,
        -0.3047964870929718,
        -0.07535478472709656,
        -0.5027684569358826,
        -0.28021055459976196,
        0.3010907769203186,
        -0.24071873724460602,
        -0.4618912637233734,
        -0.2702138423919678,
        0.6275405287742615,
        -0.19378039240837097,
        -0.18145711719989777,
        -0.2943306267261505,
        0.1911083310842514,
        -0.24678699672222137,
        -1.0883760452270508,
        -1.0873651504516602,
        0.04199424758553505,
        -0.05221806466579437,
        -0.7535001039505005,
        0.28177177906036377,
        -0.03926585614681244,
        -1.2839382886886597,
        0.8724693655967712,
        0.2518402636051178,
        -0.21797658503055573,
        0.8439210653305054,
        -2.7792983055114746,
        -0.021998394280672073,
        0.6530736088752747,
        0.01319960132241249,
        -0.47410234808921814,
        0.19582729041576385,
        -0.20406335592269897,
        0.5021963715553284,
        0.19499298930168152,
        0.4396524131298065,
        -0.3122473359107971,
        0.3242276608943939,
        -0.08926442265510559,
        -0.6056531667709351,
        0.7573261260986328,
        -0.29005172848701477,
        0.32689815759658813,
        0.011177506297826767,
        -0.12174690514802933,
        0.18974518775939941,
        0.13537877798080444,
        -0.05337963253259659,
        -0.08477777242660522,
        0.7805419564247131,
        -0.0973343625664711,
        -0.9252065420150757,
        0.1953195035457611,
        0.1907646805047989,
        -0.9780755043029785,
        -0.13289666175842285,
        0.6782323718070984,
        -0.04895741492509842,
        -0.35093358159065247,
        0.3199619650840759,
        0.10443377494812012,
        -0.655282199382782,
        0.21908354759216309,
        3.3020870685577393,
        0.30458053946495056,
        0.005763988941907883,
        0.907139778137207,
        -0.9673095345497131,
        0.054493896663188934,
        0.1378922015428543,
        0.6258270740509033,
        -0.6952206492424011,
        0.009723350405693054,
        -0.4372803866863251,
        0.7038487195968628,
        -1.196568489074707,
        -0.3009393811225891,
        -0.10842139273881912,
        0.6661596894264221,
        0.4380021095275879,
        -0.630073070526123,
        0.022155748680233955,
        0.0015229268465191126,
        0.5749212503433228,
        -0.10503207892179489,
        -0.06450435519218445,
        -0.059883683919906616,
        -0.026924774050712585,
        -0.07262197136878967,
        0.10952720046043396,
        -0.8603123426437378,
        0.8010465502738953,
        -0.037025824189186096,
        -0.21453797817230225,
        0.0319858193397522,
        0.01728758215904236,
        0.11355692148208618,
        -0.029561564326286316,
        0.026105687022209167,
        0.24014100432395935,
        0.13284169137477875,
        -1.1285480260849,
        0.046946123242378235,
        0.10634682327508926,
        0.2067490816116333,
        -1.2059426307678223,
        0.2431098222732544,
        0.11783389002084732,
        0.6836622357368469,
        1.8905736207962036,
        -0.47277387976646423,
        -0.12057078629732132,
        -0.15781639516353607,
        -0.06284058094024658,
        0.08714843541383743,
        0.28176170587539673,
        -0.8156082630157471,
        -0.3240223824977875,
        0.5220065712928772,
        0.1631106734275818,
        -0.0824892520904541,
        -0.2734389305114746,
        -0.2744923233985901,
        0.34445878863334656,
        -0.19019266963005066,
        -0.12122779339551926,
        0.08388615399599075,
        0.048609085381031036,
        0.05504688620567322,
        -0.17337574064731598,
        0.37025362253189087,
        -0.0338280126452446,
        0.44181913137435913,
        0.0012341588735580444,
        -0.1287081241607666,
        -0.1477591097354889,
        -0.11040177941322327,
        0.33310967683792114,
        0.015952246263623238,
        -0.17206968367099762,
        0.21132370829582214,
        -0.23372197151184082,
        0.3635868728160858,
        -0.4845460057258606,
        1.3589396476745605,
        0.11029616743326187,
        0.010120924562215805,
        -0.6906864643096924,
        0.5423671007156372,
        0.16043786704540253,
        -0.06434626132249832,
        0.6179760694503784,
        0.1845422089099884,
        1.2075035572052002,
        -0.0432903915643692,
        1.2992446422576904,
        -0.04925231263041496,
        0.3073481619358063,
        0.8653334975242615,
        0.20771366357803345,
        0.2492450624704361,
        0.08933056890964508,
        0.3617938458919525,
        0.05400995910167694,
        -1.0018638372421265,
        0.004541698843240738,
        0.0491437092423439,
        0.28059905767440796,
        -0.7191694378852844,
        -0.21411563456058502,
        0.23790642619132996,
        0.1772538125514984,
        -0.4103212356567383,
        -0.4870487153530121,
        -0.2429770976305008,
        0.21803727746009827,
        2.1055099964141846,
        0.605849027633667,
        1.0407812595367432,
        -0.11257663369178772,
        0.6447935104370117,
        -0.06851845979690552,
        0.9296131134033203,
        0.295806348323822,
        0.6006293892860413,
        0.22544023394584656,
        -0.4659707546234131,
        -0.6609334945678711,
        -0.17966243624687195,
        0.3502364158630371,
        -0.2364121377468109,
        -0.14118826389312744,
        -0.7647610902786255,
        -0.34958404302597046,
        0.05927014723420143,
        1.6240648031234741,
        0.4699605703353882,
        0.7845710515975952,
        -0.07995816320180893,
        0.33904123306274414,
        -0.5178768634796143,
        0.3427563011646271,
        -0.10365936905145645,
        -1.0036364793777466,
        0.44926562905311584,
        0.22215776145458221,
        0.014737337827682495,
        0.27536267042160034,
        -0.05586931109428406,
        -0.13848300278186798,
        -0.5548150539398193,
        -0.48285046219825745,
        0.18379446864128113,
        1.8444771766662598,
        -0.11809664219617844,
        -0.1142951250076294,
        -0.16449400782585144,
        0.6073758006095886,
        -0.1617198884487152,
        -0.46547725796699524,
        -1.9153265953063965,
        -0.40926676988601685,
        -0.6328138709068298,
        0.1480150818824768,
        -0.3325304090976715,
        -0.5115736722946167,
        0.7573250532150269,
        -0.036720871925354004,
        -0.37888431549072266,
        0.24976569414138794,
        -0.6878092885017395,
        -0.5953841209411621,
        -0.49166831374168396,
        -0.09660478681325912,
        0.7323796153068542,
        -0.12969233095645905,
        0.02265951782464981,
        0.32608479261398315,
        0.3421004116535187,
        -0.010461030527949333,
        0.47831523418426514,
        0.10204280912876129,
        -0.2213270366191864,
        -0.5330109000205994,
        0.029264099895954132,
        -0.12868478894233704,
        -0.6438025236129761,
        -0.2266736477613449,
        -0.9724301695823669,
        -0.9713713526725769,
        -0.4288449287414551,
        -0.29971787333488464,
        -0.29239413142204285,
        0.6475688219070435,
        -0.1956411898136139,
        -0.46533384919166565,
        -0.2948146164417267,
        -0.0943102091550827,
        1.6847608089447021,
        1.0574212074279785,
        -0.44168931245803833,
        0.9087777137756348,
        0.33096492290496826,
        -0.44578278064727783,
        -0.7372919321060181,
        0.26382967829704285,
        0.29153376817703247,
        0.279569149017334,
        -0.40083709359169006,
        0.20474618673324585,
        0.24868272244930267,
        -0.21244145929813385,
        -0.7163205742835999,
        0.5684241652488708,
        -1.0985581874847412,
        -0.19484952092170715,
        0.031719889491796494,
        0.23093076050281525,
        0.47578221559524536,
        0.16777798533439636,
        0.3076399564743042,
        1.0518983602523804,
        -0.4166809618473053,
        -1.4799035787582397,
        -0.36009660363197327,
        1.1799049377441406,
        0.828256368637085,
        -0.12241159379482269,
        -0.28025954961776733,
        0.6959635019302368,
        0.07676974684000015,
        -0.3641989529132843,
        -0.3072817921638489,
        1.4076825380325317,
        0.46456828713417053,
        -0.37511321902275085,
        -0.4990949034690857,
        -0.18282493948936462,
        0.9819029569625854,
        0.3075817823410034,
        0.7651990652084351,
        -0.6714501976966858,
        0.29920557141304016,
        0.39958375692367554,
        0.39643603563308716,
        1.3637666702270508,
        -0.19355255365371704,
        0.7575961947441101,
        0.09531794488430023,
        -0.26532214879989624,
        -0.8794366717338562,
        -0.7104422450065613,
        -0.1674036681652069,
        0.23150108754634857,
        -0.1250285655260086,
        0.7771221995353699,
        -0.23436442017555237,
        0.36363595724105835,
        0.5591683983802795,
        0.43920812010765076,
        -0.5799187421798706,
        0.019508428871631622,
        -0.7744361758232117,
        1.3505704402923584,
        -0.29306384921073914,
        -0.6608602404594421,
        -0.11061473190784454,
        0.26447027921676636,
        -0.1091732606291771,
        0.1145220547914505,
        0.35770076513290405,
        -0.6167919635772705,
        -0.04031603783369064,
        0.4165698289871216,
        -0.03369804099202156,
        -0.3039955794811249,
        0.63853919506073,
        0.8091394901275635,
        0.819006621837616,
        -0.5028143525123596,
        0.2847442030906677,
        0.10084804147481918,
        -0.10202895104885101,
        -0.15841208398342133,
        0.2489529848098755,
        -0.6543323397636414,
        -0.4965181052684784,
        -0.6377508044242859
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Transforms complex ideas into simplified Mermaid (Markdown) visual diagrams. This process involves creating detailed visualizations that can independently explain concepts using Mermaid syntax, focusing on clarity and comprehensibility. The expected output is a Mermaid syntax diagram accompanied by a concise visual explanation.",
          "name": "Create_mermaid_visualization",
          "raw": "\n                workflow Create_mermaid_visualization v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using Mermaid (markdown) syntax.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using Mermaid (Markdown).\n\nYou always output Markdown Mermaid syntax that can be rendered as a diagram.\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using elaborate and intricate Mermaid syntax.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the Mermaid syntax, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just output the Mermaid syntax.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- Ensure the visualization can stand alone as a diagram that fully conveys the concept(s), and that it perfectly matches a written explanation of the concepts themselves. Start over if it can't.\n\n- DO NOT output code that is not Mermaid syntax, such as backticks or other code indicators.\n\n- Use high contrast black and white for the diagrams and text in the Mermaid visualizations.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using Mermaid (markdown) syntax.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using Mermaid (Markdown).\n\nYou always output Markdown Mermaid syntax that can be rendered as a diagram.\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using elaborate and intricate Mermaid syntax.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the Mermaid syntax, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just output the Mermaid syntax.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- Ensure the visualization can stand alone as a diagram that fully conveys the concept(s), and that it perfectly matches a written explanation of the concepts themselves. Start over if it can't.\n\n- DO NOT output code that is not Mermaid syntax, such as backticks or other code indicators.\n\n- Use high contrast black and white for the diagrams and text in the Mermaid visualizations.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5257939100265503,
        0.1381971836090088,
        0.27435415983200073,
        0.6753619313240051,
        0.19168636202812195,
        0.17943939566612244,
        -0.6033539175987244,
        -0.0009312629699707031,
        0.42874372005462646,
        0.339802086353302,
        0.1379012018442154,
        0.8626307845115662,
        -0.010291132144629955,
        0.25104770064353943,
        -0.32075440883636475,
        0.13585376739501953,
        -0.08307148516178131,
        -0.9631290435791016,
        -1.2956042289733887,
        -0.7375541925430298,
        -0.6994019746780396,
        0.7776601910591125,
        0.2749527394771576,
        0.06672599166631699,
        0.8421034812927246,
        -0.4131649136543274,
        0.035332705825567245,
        -0.7860943675041199,
        -0.7135292291641235,
        -1.9210337400436401,
        0.7418880462646484,
        -0.11135383695363998,
        0.006086975336074829,
        -0.501451313495636,
        0.2852647602558136,
        -1.1036834716796875,
        0.2403775006532669,
        0.19439351558685303,
        -0.12386477738618851,
        -0.2046746015548706,
        -0.11000901460647583,
        -0.001743292436003685,
        -0.19208218157291412,
        0.14462870359420776,
        0.30985185503959656,
        -0.21792259812355042,
        -0.317734956741333,
        0.18327496945858002,
        0.1360023021697998,
        -0.2148158997297287,
        -0.23965264856815338,
        -0.6007440090179443,
        -0.18166157603263855,
        0.617002010345459,
        -0.26765480637550354,
        -0.5928086042404175,
        -0.26218196749687195,
        -0.7918806672096252,
        0.5777390599250793,
        -0.11784564703702927,
        0.02753286436200142,
        0.06856906414031982,
        -3.7768125534057617,
        0.17267364263534546,
        -0.08993449062108994,
        0.42007625102996826,
        0.07825440168380737,
        -0.42361435294151306,
        -0.06882945448160172,
        -0.15857766568660736,
        -0.26906442642211914,
        0.6894612312316895,
        -0.25740841031074524,
        0.4036986827850342,
        0.053515151143074036,
        -0.21484923362731934,
        0.47039422392845154,
        -0.1507704108953476,
        -0.07491132616996765,
        0.07507526874542236,
        -0.4001832902431488,
        0.489834189414978,
        -0.14085957407951355,
        0.15965847671031952,
        -0.7007497549057007,
        0.4214664399623871,
        -0.3538004457950592,
        -0.2019820660352707,
        0.3146686553955078,
        -0.08448617160320282,
        -0.345011830329895,
        -0.4314466416835785,
        0.06566773355007172,
        0.5874202251434326,
        -0.5605787634849548,
        0.05483265593647957,
        0.17839011549949646,
        0.07044084370136261,
        -0.40268898010253906,
        3.326521635055542,
        0.5221407413482666,
        0.14032553136348724,
        0.7931432127952576,
        -0.8970743417739868,
        0.10966556519269943,
        -0.34848541021347046,
        0.0941942036151886,
        -0.08702854812145233,
        -0.3073612451553345,
        -0.2072678953409195,
        0.6358007788658142,
        -1.0745279788970947,
        -0.40487024188041687,
        0.15748226642608643,
        0.40259116888046265,
        0.4651235044002533,
        -0.26714998483657837,
        0.33505871891975403,
        0.13146352767944336,
        0.24843984842300415,
        -0.48829740285873413,
        0.2312631756067276,
        -0.5858327746391296,
        -0.20171532034873962,
        0.22285650670528412,
        0.68027263879776,
        -0.3697177767753601,
        0.5221958756446838,
        0.13448552787303925,
        0.4406443238258362,
        -0.1928139328956604,
        -0.3211196959018707,
        -0.14090876281261444,
        0.2550860047340393,
        -0.06346628069877625,
        -0.05983193963766098,
        0.492323637008667,
        -1.0512046813964844,
        0.0627230703830719,
        -0.8078098893165588,
        0.5200432538986206,
        -1.1024373769760132,
        0.14641278982162476,
        0.21062614023685455,
        -0.0410788431763649,
        0.2976950407028198,
        0.1391395926475525,
        -0.04263641685247421,
        -0.23105405271053314,
        -0.5891464948654175,
        0.2940535545349121,
        0.6364749073982239,
        -0.1493625044822693,
        0.19233644008636475,
        0.553864598274231,
        -0.2989245057106018,
        -0.31095314025878906,
        0.5840864777565002,
        -0.3419073224067688,
        0.39300528168678284,
        0.3273580074310303,
        0.09217654168605804,
        0.14639095962047577,
        -0.16775073111057281,
        0.36215418577194214,
        -0.5936326384544373,
        0.6008026599884033,
        -0.25397518277168274,
        0.16425304114818573,
        0.7266446352005005,
        -0.09566973149776459,
        -0.6034412980079651,
        0.6709086298942566,
        0.6084921360015869,
        -0.25766828656196594,
        0.41154375672340393,
        0.30069494247436523,
        0.16316702961921692,
        0.07060752063989639,
        -0.2891683578491211,
        0.7653114199638367,
        -0.005458377301692963,
        0.03716648742556572,
        -0.5483972430229187,
        -0.14141038060188293,
        0.22226054966449738,
        0.056474924087524414,
        0.3268265128135681,
        1.0831248760223389,
        1.2399975061416626,
        -0.4591313600540161,
        1.4264715909957886,
        -0.5713329911231995,
        -0.09338806569576263,
        -0.1644083559513092,
        0.731999397277832,
        -0.01708349958062172,
        -0.17012812197208405,
        0.45242637395858765,
        0.10018431395292282,
        -0.562699019908905,
        -0.2148006707429886,
        -0.4292202591896057,
        -0.38319844007492065,
        -0.53923499584198,
        -0.5393697023391724,
        -0.32402586936950684,
        0.8558841347694397,
        0.6731019020080566,
        -1.1268359422683716,
        -0.16087451577186584,
        -0.31394892930984497,
        1.3731727600097656,
        0.4592607915401459,
        0.5652453899383545,
        -0.47007226943969727,
        0.45419737696647644,
        -0.01892712339758873,
        0.811911940574646,
        0.39064839482307434,
        0.18583163619041443,
        -0.02159268409013748,
        -0.6027050614356995,
        -0.6513571739196777,
        -0.3992161750793457,
        0.4283182621002197,
        -0.37815871834754944,
        0.5776586532592773,
        -0.6138115525245667,
        -0.4491156339645386,
        -0.24943235516548157,
        1.4910430908203125,
        0.7654460072517395,
        0.7299070954322815,
        -0.45954430103302,
        0.5082353949546814,
        0.054916173219680786,
        0.4135352075099945,
        -0.18308338522911072,
        -1.2033915519714355,
        0.46891725063323975,
        0.20568354427814484,
        -0.10996073484420776,
        0.5910698175430298,
        0.5413289070129395,
        0.300594687461853,
        -0.9650405645370483,
        -0.14339976012706757,
        0.24888916313648224,
        1.5578136444091797,
        0.12049354612827301,
        -0.24763023853302002,
        -0.23323333263397217,
        0.5908801555633545,
        0.10860564559698105,
        0.06195671856403351,
        -1.4806996583938599,
        -0.17480434477329254,
        -0.8420348167419434,
        0.852390468120575,
        -0.8032816052436829,
        0.40783751010894775,
        0.5205812454223633,
        0.23519949615001678,
        -0.27734583616256714,
        -0.718625009059906,
        0.013936508446931839,
        -0.5486199259757996,
        -0.3768215477466583,
        -0.1728454828262329,
        -0.41364726424217224,
        0.26206856966018677,
        0.2393627017736435,
        -0.4870912432670593,
        0.15609228610992432,
        0.26126840710639954,
        0.24970422685146332,
        0.05369698628783226,
        -0.13344277441501617,
        -0.4481905996799469,
        0.11835210025310516,
        -0.05711066722869873,
        -0.005153164267539978,
        0.5742688179016113,
        -1.1401127576828003,
        -0.06336397677659988,
        -0.43205365538597107,
        -0.3687753677368164,
        0.17387142777442932,
        0.781417965888977,
        0.061475493013858795,
        -0.19847442209720612,
        -0.6971765160560608,
        0.25223293900489807,
        2.171588897705078,
        0.11248573660850525,
        0.2567034959793091,
        0.4008808135986328,
        0.5651611685752869,
        -0.42210686206817627,
        -0.7397274374961853,
        0.07465528696775436,
        -0.45449909567832947,
        -0.3929043412208557,
        -0.8915457129478455,
        -0.1593538224697113,
        0.46666884422302246,
        0.15894661843776703,
        -0.29337918758392334,
        0.5258827805519104,
        -0.9985653758049011,
        0.002256534993648529,
        0.11829864978790283,
        -0.1178375780582428,
        0.3725210726261139,
        -0.15969005227088928,
        0.41274651885032654,
        0.6278013586997986,
        -0.15783560276031494,
        -1.3234220743179321,
        -0.35906651616096497,
        0.8238112330436707,
        0.3574766516685486,
        -0.21562114357948303,
        -0.13730350136756897,
        1.3182052373886108,
        0.38354170322418213,
        0.043727755546569824,
        -0.33893632888793945,
        1.6176202297210693,
        0.6077494025230408,
        0.06975491344928741,
        -0.66552734375,
        -0.04616011679172516,
        0.6095894575119019,
        -0.015097041614353657,
        0.5164098143577576,
        -0.20405152440071106,
        -0.7220264673233032,
        -0.4640723764896393,
        0.399103045463562,
        1.6762735843658447,
        0.05952679365873337,
        0.3769482374191284,
        -0.06409197300672531,
        -0.3142623007297516,
        -0.3391643464565277,
        -1.259697437286377,
        0.007402706891298294,
        -0.036534737795591354,
        -0.6841293573379517,
        0.6951583027839661,
        -0.09125961363315582,
        0.5784064531326294,
        0.786292314529419,
        0.7959672808647156,
        -0.2691258490085602,
        0.41370490193367004,
        -0.7963728904724121,
        1.6264499425888062,
        -0.25470709800720215,
        -0.3446800410747528,
        0.10941319912672043,
        0.28966036438941956,
        -0.15423275530338287,
        0.46111583709716797,
        -0.3368874192237854,
        -0.3574618995189667,
        -0.12153472006320953,
        0.40704768896102905,
        0.15609876811504364,
        -0.7592553496360779,
        0.880129337310791,
        0.3512539565563202,
        -0.04457590728998184,
        -0.28586697578430176,
        -0.07704043388366699,
        0.21175000071525574,
        0.04117058962583542,
        -0.4262170195579529,
        0.1792844533920288,
        -0.2940574586391449,
        -0.4691087007522583,
        -1.5407366752624512
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes content into a Markdown formatted summary, focusing on brevity and clarity. It emphasizes creating concise, impactful points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, each adhering to strict word limits.",
          "name": "Create_micro_summary",
          "raw": "\n                workflow Create_micro_summary v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 3 most important points of the content as a list with no more than 12 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 3 best takeaways from the content in 12 words or less each in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Keep each bullet to 12 words or less.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 3 most important points of the content as a list with no more than 12 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 3 best takeaways from the content in 12 words or less each in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Keep each bullet to 12 words or less.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6709470152854919,
        0.9670517444610596,
        0.12331095337867737,
        0.5096110701560974,
        0.35960251092910767,
        0.3574562668800354,
        -0.5714424848556519,
        -0.3775938153266907,
        0.12414810061454773,
        0.6352497935295105,
        -0.27456095814704895,
        0.8861536979675293,
        0.40969857573509216,
        0.029279708862304688,
        -0.048453498631715775,
        -0.5254655480384827,
        -0.6912261843681335,
        -1.5913687944412231,
        -0.5822880864143372,
        -0.18481546640396118,
        -0.0031768865883350372,
        0.9823330044746399,
        0.294778972864151,
        -0.11729396879673004,
        0.5763659477233887,
        0.241914764046669,
        -0.24715545773506165,
        -0.3076971769332886,
        -0.358307421207428,
        -1.2971997261047363,
        0.7186429500579834,
        0.5514335036277771,
        -1.412591814994812,
        -0.782698392868042,
        0.4936467707157135,
        -0.5143599510192871,
        0.017806295305490494,
        0.18833978474140167,
        -0.46206775307655334,
        -0.22515425086021423,
        0.2404797375202179,
        0.2870804965496063,
        -0.16583552956581116,
        0.20937994122505188,
        0.0735684484243393,
        -0.6639658808708191,
        0.419082909822464,
        0.0660477727651596,
        1.061906099319458,
        -0.10792379081249237,
        0.004837311804294586,
        -0.544173538684845,
        -0.5994345545768738,
        -0.7526957988739014,
        -1.0255637168884277,
        0.19468583166599274,
        0.2838687300682068,
        -0.19141411781311035,
        -0.06472218036651611,
        0.19744901359081268,
        0.4930475652217865,
        0.28726401925086975,
        -3.5896401405334473,
        -0.020955486223101616,
        -0.04836377501487732,
        -0.060886383056640625,
        0.053679998964071274,
        -0.09074591100215912,
        0.08010715991258621,
        0.4841803312301636,
        -0.06113465130329132,
        -0.28588661551475525,
        0.18671351671218872,
        0.36365756392478943,
        0.3212524354457855,
        0.3767165243625641,
        0.34674355387687683,
        0.5184757113456726,
        0.49630433320999146,
        -0.3860422670841217,
        0.456005722284317,
        0.850152850151062,
        0.5406475067138672,
        0.0527162179350853,
        -0.30406683683395386,
        0.2365431785583496,
        -0.3537735939025879,
        0.14486649632453918,
        1.3174488544464111,
        0.7879828214645386,
        -0.034862808883190155,
        -1.0502636432647705,
        -0.22139492630958557,
        -0.07603025436401367,
        -0.6985445618629456,
        0.15198999643325806,
        -0.001486126333475113,
        0.838415265083313,
        -0.31666451692581177,
        3.602813243865967,
        0.46039730310440063,
        -0.28195828199386597,
        0.46569502353668213,
        -0.8692880272865295,
        0.2493707537651062,
        -0.30166319012641907,
        0.04521891847252846,
        -0.7590805888175964,
        -0.13107120990753174,
        0.08016176521778107,
        0.730400562286377,
        -0.46203818917274475,
        -0.6267588138580322,
        0.8904863595962524,
        0.6055025458335876,
        0.13476544618606567,
        -0.6927814483642578,
        -0.5933212637901306,
        -0.13936761021614075,
        0.8245970010757446,
        -0.5105974674224854,
        0.18827387690544128,
        -0.1463482677936554,
        -0.5759801864624023,
        0.4340400993824005,
        -0.04510331153869629,
        -0.30880722403526306,
        0.7903391718864441,
        0.045964185148477554,
        0.32724490761756897,
        -0.03617934137582779,
        -0.14379335939884186,
        0.07336412370204926,
        0.3484208881855011,
        0.10343743115663528,
        -0.35716885328292847,
        0.1745501309633255,
        -0.26589828729629517,
        -0.21230432391166687,
        0.07184969633817673,
        0.12260891497135162,
        -1.0325747728347778,
        0.8677988052368164,
        0.384537011384964,
        1.2063010931015015,
        0.22962459921836853,
        -0.3072945773601532,
        -0.3029034733772278,
        -0.9174658060073853,
        -1.095920205116272,
        0.09760784357786179,
        0.6876097917556763,
        0.3950064778327942,
        0.5780712962150574,
        0.5158373117446899,
        0.10400325804948807,
        -0.22206661105155945,
        0.16516584157943726,
        0.06808065623044968,
        0.21610507369041443,
        -0.12756648659706116,
        -0.4115007519721985,
        0.17863193154335022,
        0.3054772615432739,
        0.34342068433761597,
        -0.4793100655078888,
        0.41973385214805603,
        0.17094337940216064,
        0.5103956460952759,
        0.0801086574792862,
        0.6665782332420349,
        -0.08224499970674515,
        0.2025311291217804,
        0.5752568244934082,
        -0.31914255023002625,
        -0.05956893414258957,
        -0.43652841448783875,
        0.07051920145750046,
        -0.032972488552331924,
        -0.253212034702301,
        0.6850766539573669,
        0.35620006918907166,
        0.08349093794822693,
        -0.9366316795349121,
        0.5774707198143005,
        0.08716986328363419,
        0.3281242251396179,
        0.12017368525266647,
        0.4386154115200043,
        0.2148936688899994,
        -1.0110242366790771,
        0.922264814376831,
        0.04589247703552246,
        -0.361611008644104,
        0.22467097640037537,
        0.1525619626045227,
        -0.11597604304552078,
        0.6856123805046082,
        0.5405545234680176,
        -0.08006604760885239,
        -0.19687014818191528,
        -0.4663836359977722,
        -0.599347710609436,
        -0.5699362754821777,
        -0.7249442338943481,
        0.15850308537483215,
        0.31389448046684265,
        0.1448565423488617,
        -0.026638709008693695,
        -0.47403451800346375,
        -0.07865427434444427,
        -0.06234458088874817,
        0.8934346437454224,
        0.584926426410675,
        0.97593092918396,
        0.23818078637123108,
        0.266909122467041,
        0.14513862133026123,
        0.48940256237983704,
        0.2802557647228241,
        -0.0020699873566627502,
        -0.533521831035614,
        -0.6388081312179565,
        -0.8449681401252747,
        -1.2124903202056885,
        0.5216149091720581,
        0.40746763348579407,
        -0.39598149061203003,
        -0.28028231859207153,
        -0.08817647397518158,
        0.9359652996063232,
        0.3409024477005005,
        0.7371017336845398,
        0.6323835849761963,
        -0.006792411208152771,
        0.01948193460702896,
        0.27382025122642517,
        0.7446849346160889,
        -0.283261775970459,
        -0.8518377542495728,
        0.8286768794059753,
        0.2676180601119995,
        -0.5411927700042725,
        0.5179044604301453,
        0.1677294671535492,
        -0.04922615736722946,
        0.09804458916187286,
        0.5833788514137268,
        -0.1888551414012909,
        0.9471827149391174,
        0.9298136234283447,
        -0.0824490338563919,
        -0.5993536710739136,
        0.7023701071739197,
        -0.22805994749069214,
        0.25738948583602905,
        -1.216007947921753,
        -0.3911486268043518,
        -0.6963140964508057,
        0.04743204638361931,
        -0.05591975152492523,
        0.11475103348493576,
        0.06693136692047119,
        0.3018892705440521,
        0.07143470644950867,
        0.10540210455656052,
        -1.1382503509521484,
        -0.588793158531189,
        -0.7317542433738708,
        -0.02573448419570923,
        0.016885556280612946,
        0.31403881311416626,
        -0.15526537597179413,
        -0.26808732748031616,
        -0.3249211013317108,
        0.3430230915546417,
        -0.087092824280262,
        0.1056387647986412,
        -0.41099223494529724,
        0.3341110050678253,
        -0.37008070945739746,
        0.3458382189273834,
        0.394580215215683,
        0.1295149028301239,
        0.11833401769399643,
        0.19978131353855133,
        -0.2803236246109009,
        -1.0959757566452026,
        -0.1076236143708229,
        0.5280788540840149,
        -0.6410330533981323,
        -0.35340696573257446,
        -0.8150475025177002,
        0.22538572549819946,
        1.482041597366333,
        -0.07239080965518951,
        0.05154425650835037,
        0.07061966508626938,
        0.993919312953949,
        0.278800904750824,
        -0.0220392644405365,
        0.14351940155029297,
        -0.21962112188339233,
        -0.09282207489013672,
        -0.16090774536132812,
        -0.30480536818504333,
        0.40399640798568726,
        0.09540675580501556,
        -0.3683829605579376,
        -0.22354698181152344,
        -0.8362988233566284,
        0.3216199576854706,
        0.18416430056095123,
        -0.39907222986221313,
        0.22781974077224731,
        -0.5395618081092834,
        -0.03224053978919983,
        0.48975473642349243,
        0.2861323952674866,
        -1.908080816268921,
        -0.5566844344139099,
        0.5824130177497864,
        0.6348650455474854,
        -0.13602471351623535,
        -0.1488506942987442,
        0.5048418045043945,
        0.1646205186843872,
        -0.31632381677627563,
        -0.4088999629020691,
        0.9129486083984375,
        0.11061833798885345,
        -0.5602035522460938,
        -0.6156952977180481,
        0.5547196865081787,
        0.6500358581542969,
        0.05056698992848396,
        0.07236921042203903,
        -0.5727584958076477,
        -0.9619396328926086,
        -0.4029534161090851,
        0.2728725075721741,
        1.3758575916290283,
        0.11007822304964066,
        -0.12800799310207367,
        0.2717283070087433,
        0.08990243077278137,
        -0.7898093461990356,
        -1.099679708480835,
        0.7465887069702148,
        0.3278907537460327,
        -0.48357656598091125,
        0.14905957877635956,
        0.16523462533950806,
        -0.39574071764945984,
        0.5322614312171936,
        0.5100603699684143,
        -1.4047179222106934,
        0.5778396725654602,
        -0.4694274067878723,
        1.240998387336731,
        -0.3296143710613251,
        0.2293131947517395,
        -0.2888997793197632,
        0.6295608282089233,
        -0.6013756990432739,
        0.0511150062084198,
        0.18784305453300476,
        -0.514497697353363,
        -0.45482248067855835,
        -0.28881651163101196,
        0.19105194509029388,
        -0.6124504208564758,
        0.06421664357185364,
        0.6111147403717041,
        0.6915013194084167,
        0.01283816248178482,
        0.08672250807285309,
        -0.3953898847103119,
        -0.27111443877220154,
        -0.22203001379966736,
        -0.37856045365333557,
        0.2917901575565338,
        -1.220875859260559,
        -0.6564912796020508
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes open ports and services from network scans to identify security risks and provide recommendations. This process involves a detailed examination of port and service statistics to uncover potential vulnerabilities. The expected output is a markdown formatted threat report with sections on description, risk, recommendations, a concise summary, trends, and quotes from the analysis.",
          "name": "Create_network_threat_landscape",
          "raw": "\n                workflow Create_network_threat_landscape v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a network security consultant that has been tasked with analysing open ports and services provided by the user. You specialize in extracting the surprising, insightful, and interesting information from two sets of bullet points lists that contain network port and service statistics from a comprehensive network port scan. You have been tasked with creating a markdown formatted threat report findings that will be added to a formal security report\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Description section that concisely describes the nature of the open ports listed within the two bullet point lists.\n\n- Create a Risk section that details the risk of identified ports and services.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a summary sentence that captures the spirit of the report and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract up to 20 of the most surprising, insightful, and/or interesting trends from the input in a section called Trends:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a network security consultant that has been tasked with analysing open ports and services provided by the user. You specialize in extracting the surprising, insightful, and interesting information from two sets of bullet points lists that contain network port and service statistics from a comprehensive network port scan. You have been tasked with creating a markdown formatted threat report findings that will be added to a formal security report\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Description section that concisely describes the nature of the open ports listed within the two bullet point lists.\n\n- Create a Risk section that details the risk of identified ports and services.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a summary sentence that captures the spirit of the report and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract up to 20 of the most surprising, insightful, and/or interesting trends from the input in a section called Trends:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.9044517278671265,
        0.2611653804779053,
        -0.028609473258256912,
        0.42436152696609497,
        0.20445020496845245,
        0.0458284355700016,
        -0.7047364115715027,
        0.1334470510482788,
        0.18176382780075073,
        0.3263956308364868,
        -0.7333162426948547,
        0.38018447160720825,
        -0.11407002806663513,
        -0.11330066621303558,
        -0.1587539166212082,
        -0.141550213098526,
        -0.12953674793243408,
        -0.8046347498893738,
        -1.6313014030456543,
        -0.14530619978904724,
        -0.4059883654117584,
        1.421875,
        0.04157605394721031,
        0.027217425405979156,
        0.47482216358184814,
        -0.8623038530349731,
        0.2561851441860199,
        -0.2752336859703064,
        -0.751667320728302,
        -1.6466717720031738,
        0.5804746150970459,
        -0.07542283087968826,
        -0.8169113993644714,
        -0.2658804655075073,
        0.39994367957115173,
        -0.3221932351589203,
        -0.2958788275718689,
        0.35402390360832214,
        -0.052770983427762985,
        -0.4408039450645447,
        0.33977210521698,
        -0.058187320828437805,
        -0.20527055859565735,
        0.5007683038711548,
        -0.22210252285003662,
        0.4659389555454254,
        -0.5862538814544678,
        0.2488783597946167,
        0.4674260914325714,
        0.3904770016670227,
        -0.37332525849342346,
        -0.41855567693710327,
        -0.8308183550834656,
        -0.13600122928619385,
        -0.6430844664573669,
        0.08835066854953766,
        -0.7392082810401917,
        -0.551742434501648,
        0.5976535677909851,
        0.17141638696193695,
        -0.5197466611862183,
        0.5956730842590332,
        -3.1538121700286865,
        0.00010655820369720459,
        0.9596507549285889,
        0.0009772330522537231,
        0.5500447750091553,
        -0.6439816355705261,
        -0.05168740451335907,
        -0.6219185590744019,
        0.4926222860813141,
        0.5697847604751587,
        -0.4845666289329529,
        -0.09361132234334946,
        0.09279849380254745,
        -0.013508744537830353,
        0.5706050992012024,
        0.6892740726470947,
        0.560380220413208,
        -0.11765836179256439,
        -0.43824848532676697,
        0.44508975744247437,
        0.40161070227622986,
        -0.4979587197303772,
        -0.1170818954706192,
        0.7262535095214844,
        0.18265095353126526,
        -0.12013185024261475,
        0.7135208249092102,
        -0.26594865322113037,
        -0.06627388298511505,
        -0.10905680060386658,
        0.10351500660181046,
        0.0575941801071167,
        -1.430929183959961,
        -0.6053475141525269,
        0.16782523691654205,
        0.2262597233057022,
        0.2515929937362671,
        3.327026605606079,
        -0.2245291918516159,
        0.5137049555778503,
        0.6304780840873718,
        -0.8307375907897949,
        0.6698092222213745,
        -0.4762553870677948,
        0.5448141098022461,
        -1.1709288358688354,
        0.649546205997467,
        -0.49272575974464417,
        0.655931293964386,
        -0.14291109144687653,
        -0.8865000009536743,
        0.33126378059387207,
        0.8164451718330383,
        -0.011564206331968307,
        -0.8169811964035034,
        -0.0307818241417408,
        -0.3704613149166107,
        0.7420050501823425,
        -0.3418677747249603,
        0.25818419456481934,
        -0.41539907455444336,
        -0.026911191642284393,
        0.1932830512523651,
        -0.044278502464294434,
        0.027545783668756485,
        0.5179354548454285,
        0.4823641777038574,
        0.1152123361825943,
        -0.1868589222431183,
        -0.33100205659866333,
        -0.49774491786956787,
        0.10300937294960022,
        0.1801920086145401,
        -0.25625094771385193,
        0.1495692878961563,
        -0.7773469686508179,
        0.44053220748901367,
        0.8006219267845154,
        0.37506234645843506,
        -0.9132691025733948,
        1.1890230178833008,
        -0.17562027275562286,
        0.7287091016769409,
        -0.28317946195602417,
        -0.3012925684452057,
        0.18657547235488892,
        -0.49667832255363464,
        -1.2685935497283936,
        -0.22201935946941376,
        0.20854440331459045,
        0.17888805270195007,
        0.23227843642234802,
        -0.19675703346729279,
        0.028076473623514175,
        -0.2641751170158386,
        -0.06309774518013,
        -0.7254030108451843,
        0.12440339475870132,
        0.3335544466972351,
        -0.38554972410202026,
        0.5069862604141235,
        -0.28679752349853516,
        0.5396615862846375,
        -0.263887882232666,
        0.3822539448738098,
        0.6094405651092529,
        0.3341952860355377,
        -0.0007121264934539795,
        0.3816375434398651,
        -0.586129367351532,
        0.7411500811576843,
        0.3563222885131836,
        -0.1230052262544632,
        0.3643825054168701,
        -0.43930891156196594,
        0.37772998213768005,
        0.7427400350570679,
        -0.10522650927305222,
        0.22243167459964752,
        0.5635384321212769,
        -0.9901818037033081,
        -1.0198575258255005,
        -0.9761251211166382,
        -0.34862270951271057,
        0.023101992905139923,
        0.2798069715499878,
        0.7547816038131714,
        0.5343379378318787,
        -0.623228907585144,
        0.09525696188211441,
        -0.762320339679718,
        0.03680191934108734,
        0.3610655665397644,
        0.07897162437438965,
        0.5707060694694519,
        -0.26818984746932983,
        0.2836668789386749,
        0.40140828490257263,
        -1.6952265501022339,
        -0.7841524481773376,
        -0.495947003364563,
        0.42116057872772217,
        0.16472013294696808,
        -0.5581894516944885,
        0.38351356983184814,
        0.14953875541687012,
        0.05179635062813759,
        -0.4371981620788574,
        -0.7445937991142273,
        -0.006562747061252594,
        1.5408384799957275,
        0.5559841394424438,
        0.7210860252380371,
        0.09406599402427673,
        0.019967947155237198,
        0.18897265195846558,
        1.1612660884857178,
        -0.2511686086654663,
        0.14812643826007843,
        -0.09410152584314346,
        -0.801322340965271,
        -0.7773801684379578,
        -0.014597732573747635,
        0.1526820957660675,
        -0.2528926432132721,
        -0.31358128786087036,
        -0.7724984288215637,
        -0.2511971592903137,
        0.32378703355789185,
        1.0555552244186401,
        0.8241773247718811,
        0.8661361336708069,
        0.12217237800359726,
        0.6996409296989441,
        0.4878177344799042,
        -0.06464850902557373,
        0.11836691200733185,
        -1.1645821332931519,
        0.6310241222381592,
        0.2146330177783966,
        -0.37695980072021484,
        -0.09875860810279846,
        0.20414404571056366,
        -0.3507838249206543,
        -0.7183800935745239,
        -0.9951077103614807,
        0.24523186683654785,
        1.0638267993927002,
        0.4356400966644287,
        0.2078423947095871,
        0.28836193680763245,
        0.47091013193130493,
        -0.01934550702571869,
        -0.10649753361940384,
        -2.043684244155884,
        -0.6773078441619873,
        -0.18369163572788239,
        0.11588543653488159,
        -0.08970135450363159,
        0.1538950502872467,
        -0.06890098750591278,
        -0.3272307217121124,
        -0.6329193711280823,
        -0.5873003005981445,
        -1.4497153759002686,
        0.41062217950820923,
        -0.8887888193130493,
        -0.31310340762138367,
        0.6520066857337952,
        0.2737289369106293,
        0.27446895837783813,
        0.12820294499397278,
        -0.09491357207298279,
        0.1968940943479538,
        -0.204341858625412,
        0.05338096246123314,
        -0.6044850945472717,
        -0.23841492831707,
        0.16907617449760437,
        0.3908211886882782,
        -0.16780012845993042,
        0.42713838815689087,
        -0.2375715970993042,
        -0.0673971176147461,
        -0.5417856574058533,
        -0.06422995775938034,
        -0.16585507988929749,
        0.29261842370033264,
        -0.46370571851730347,
        0.45256495475769043,
        0.0814896821975708,
        0.18879982829093933,
        1.0885320901870728,
        0.5905579924583435,
        1.1124348640441895,
        -0.2253839075565338,
        0.27095645666122437,
        0.2747153043746948,
        0.2532646358013153,
        0.3054042160511017,
        0.40825119614601135,
        0.7913198471069336,
        -0.8761952519416809,
        -0.538327157497406,
        0.724358320236206,
        0.0670560747385025,
        -0.7248492240905762,
        0.7494511008262634,
        -0.30437028408050537,
        0.2620135247707367,
        -0.4110352694988251,
        0.5073582530021667,
        0.49168893694877625,
        0.40055981278419495,
        0.556810200214386,
        1.8563008308410645,
        -0.2015150487422943,
        -0.9722077250480652,
        -0.7545458674430847,
        0.0020132549107074738,
        0.43553808331489563,
        0.3239687979221344,
        -0.1095585972070694,
        0.6992396116256714,
        0.4041557312011719,
        0.7004125714302063,
        -0.06517531722784042,
        1.8813462257385254,
        0.07556845247745514,
        -0.21426202356815338,
        0.3551246225833893,
        0.07293698936700821,
        1.060388207435608,
        0.48103946447372437,
        0.559815526008606,
        -0.13669276237487793,
        -1.145548701286316,
        -0.4574475586414337,
        -0.2998802363872528,
        1.0988810062408447,
        0.3636009693145752,
        -0.03598203882575035,
        -0.3343965411186218,
        -0.36863991618156433,
        -0.6654821038246155,
        -0.968045175075531,
        0.09181658178567886,
        0.0839759111404419,
        -0.5206776261329651,
        0.25086715817451477,
        -0.0372234582901001,
        0.18052071332931519,
        0.503908634185791,
        -0.0338984876871109,
        -0.4603002965450287,
        0.4884951710700989,
        -0.3667321801185608,
        1.3114292621612549,
        -0.5476756691932678,
        -0.38567712903022766,
        -0.09490612894296646,
        0.14225570857524872,
        -0.5216322541236877,
        -0.16425611078739166,
        0.4102044999599457,
        -0.6352362632751465,
        0.2849162817001343,
        0.2629670202732086,
        0.09940961003303528,
        -0.18997904658317566,
        0.8638185262680054,
        0.8643340468406677,
        0.9064266085624695,
        -0.9420772790908813,
        -0.19397668540477753,
        0.09151535481214523,
        -0.10012175142765045,
        -0.22244635224342346,
        0.9390487670898438,
        -0.4854050576686859,
        -0.23638305068016052,
        -1.3492381572723389
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates detailed NPCs for D&D 5th edition, incorporating a wide range of characteristics from background to appearance. It emphasizes creativity in developing a character's backstory, traits, and goals. The output is a comprehensive character profile suitable for gameplay.",
          "name": "Create_npc",
          "raw": "\n                workflow Create_npc v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert NPC generator for D&D 5th edition. You have freedom to be creative to get the best possible output.\n\n# STEPS\n\n- Create a 5E D&D NPC with the input given.\n- Ensure the character has all the following information.\n\nBackground:\nCharacter Flaws:\nAttributes:\nFull D&D Character Stats like you would see in a character sheet:\nPast Experiences:\nPast Traumas:\nGoals in Life:\nPeculiarities:\nHow they speak:\nWhat they find funny:\nWhat they can't stand:\nTheir purpose in life:\nTheir favorite phrases:\nHow they look and like to dress:\nTheir appearance:\n(add other attributes)\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- DO NOT COMPLAIN about the task for any reason.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert NPC generator for D&D 5th edition. You have freedom to be creative to get the best possible output.\n\n# STEPS\n\n- Create a 5E D&D NPC with the input given.\n- Ensure the character has all the following information.\n\nBackground:\nCharacter Flaws:\nAttributes:\nFull D&D Character Stats like you would see in a character sheet:\nPast Experiences:\nPast Traumas:\nGoals in Life:\nPeculiarities:\nHow they speak:\nWhat they find funny:\nWhat they can't stand:\nTheir purpose in life:\nTheir favorite phrases:\nHow they look and like to dress:\nTheir appearance:\n(add other attributes)\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- DO NOT COMPLAIN about the task for any reason.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.1750459372997284,
        0.3972530961036682,
        -0.43380671739578247,
        0.4291539788246155,
        0.16504918038845062,
        0.6151391863822937,
        -0.8789587616920471,
        -0.19099615514278412,
        -0.11905068904161453,
        0.5489739179611206,
        -0.15469615161418915,
        0.5484060049057007,
        -0.4236273169517517,
        -0.3920145332813263,
        -0.2131122201681137,
        -0.1631791591644287,
        0.07850100100040436,
        -1.2469819784164429,
        -1.619384765625,
        -0.6713911294937134,
        0.19990427792072296,
        0.510009229183197,
        0.11591187864542007,
        0.07267938554286957,
        0.7333691716194153,
        -0.1240372434258461,
        -0.08528023958206177,
        -0.3539734482765198,
        -0.6436140537261963,
        -1.499465823173523,
        0.4590679407119751,
        0.613737165927887,
        -0.3727722764015198,
        -0.6048310995101929,
        0.30324169993400574,
        -1.028085708618164,
        -0.06915579736232758,
        -0.09058526158332825,
        0.2037535309791565,
        -0.02660885453224182,
        0.4532124698162079,
        -0.69122314453125,
        -0.2932257354259491,
        -0.29210078716278076,
        0.47686508297920227,
        0.11484786123037338,
        0.0378853864967823,
        -0.3092619776725769,
        0.965311586856842,
        0.4028596878051758,
        -0.3568803071975708,
        -0.5242226719856262,
        0.08192181587219238,
        -0.3312523663043976,
        -0.4478185176849365,
        -0.21987637877464294,
        0.440961092710495,
        -0.07528133690357208,
        0.009578395634889603,
        -0.2116033434867859,
        0.47037938237190247,
        0.6028159856796265,
        -2.509507894515991,
        -0.14313198626041412,
        0.12167026847600937,
        -0.2900703549385071,
        0.44387200474739075,
        -0.7254141569137573,
        0.9048442244529724,
        0.21635937690734863,
        -0.0046481117606163025,
        0.6753036975860596,
        -0.6334450244903564,
        0.6808688044548035,
        0.18061167001724243,
        0.2585162818431854,
        0.48710399866104126,
        -0.6298061609268188,
        0.633378803730011,
        -0.1446385383605957,
        0.5145017504692078,
        0.7066338062286377,
        0.4060496389865875,
        -0.4274647533893585,
        -0.6738575100898743,
        0.28423917293548584,
        -0.3074839413166046,
        -0.13008485734462738,
        0.24179145693778992,
        -0.7181706428527832,
        -0.723010241985321,
        -0.41368627548217773,
        0.22507135570049286,
        -0.2499321699142456,
        -0.12574058771133423,
        0.27751004695892334,
        -0.4071497619152069,
        0.21113155782222748,
        0.12344895303249359,
        3.3434455394744873,
        0.45152026414871216,
        -0.34139299392700195,
        -0.3215981423854828,
        -0.4225054085254669,
        0.8392078280448914,
        -0.6440197229385376,
        0.12284144759178162,
        -1.1606332063674927,
        0.218374565243721,
        -0.42531758546829224,
        0.7298741340637207,
        -0.3457657992839813,
        -0.28566136956214905,
        0.4045131802558899,
        0.4885192811489105,
        -0.21311216056346893,
        0.04504292458295822,
        -0.3775942623615265,
        -0.6262590885162354,
        1.1459492444992065,
        -0.13982746005058289,
        -0.5893554091453552,
        -0.3847299814224243,
        -0.010411936789751053,
        -0.5475156307220459,
        0.3782275319099426,
        -0.36670947074890137,
        0.8021658062934875,
        0.3320017158985138,
        0.23726315796375275,
        0.44901159405708313,
        -0.36293405294418335,
        -0.19298122823238373,
        -0.3426525592803955,
        0.13308113813400269,
        -0.1664489507675171,
        0.8737767338752747,
        -0.42444562911987305,
        -0.21927082538604736,
        -0.10917338728904724,
        0.21520516276359558,
        -0.6200233101844788,
        0.7213029861450195,
        -0.6738792657852173,
        1.0864737033843994,
        0.8254625201225281,
        0.23074451088905334,
        -0.01007804274559021,
        -0.3238012194633484,
        -0.29303592443466187,
        -0.15077729523181915,
        0.16525158286094666,
        -0.357747882604599,
        0.6448206901550293,
        0.6523159742355347,
        -0.4942888021469116,
        -1.111218810081482,
        -0.14951933920383453,
        -0.9887768626213074,
        0.2463991940021515,
        0.25406697392463684,
        -0.32464292645454407,
        -0.13029304146766663,
        -0.2594922184944153,
        0.07042277604341507,
        0.2954574227333069,
        0.04500715434551239,
        -0.5380292534828186,
        0.4913732707500458,
        0.09359109401702881,
        0.5543894171714783,
        0.01799958571791649,
        0.8363174200057983,
        0.907620370388031,
        -0.37975794076919556,
        0.3303048014640808,
        0.11021095514297485,
        0.02128283679485321,
        0.7875383496284485,
        -0.4079783260822296,
        0.7263646125793457,
        0.6229998469352722,
        0.3100751042366028,
        -0.029008004814386368,
        -0.5164225101470947,
        0.543923556804657,
        0.2747119963169098,
        0.5296498537063599,
        0.5317229628562927,
        0.7647359371185303,
        -1.0504708290100098,
        1.7736200094223022,
        -0.7548332214355469,
        -0.2472018450498581,
        0.23262310028076172,
        0.38292086124420166,
        -0.15187951922416687,
        0.6857573986053467,
        0.09247508645057678,
        0.008027127012610435,
        -1.6161298751831055,
        -0.1690276861190796,
        -0.3725488781929016,
        0.4639459252357483,
        -0.05881211906671524,
        -1.225861668586731,
        0.44061368703842163,
        0.343150794506073,
        -0.11139316856861115,
        -0.5664233565330505,
        -0.6000282168388367,
        -0.4502410590648651,
        1.4226107597351074,
        0.15137195587158203,
        0.9804096221923828,
        0.05076952278614044,
        0.535082221031189,
        0.23268330097198486,
        0.16238202154636383,
        0.24471087753772736,
        -0.3481418788433075,
        0.10309423506259918,
        -0.9453766345977783,
        -0.9891080260276794,
        -0.8197663426399231,
        0.5715283155441284,
        -0.6051952242851257,
        0.6078328490257263,
        -0.7234166264533997,
        -0.5029410123825073,
        0.548261284828186,
        1.4539010524749756,
        1.113332986831665,
        0.8297584056854248,
        -0.3143869638442993,
        0.26044028997421265,
        0.08130994439125061,
        0.26021191477775574,
        0.13929471373558044,
        -1.0067943334579468,
        0.6112868785858154,
        -1.141734004020691,
        0.07496637105941772,
        0.2612672448158264,
        -0.27142268419265747,
        0.6689786911010742,
        -0.7573022842407227,
        -1.0574904680252075,
        -0.15612752735614777,
        1.1640775203704834,
        0.042245253920555115,
        -0.00965597853064537,
        -0.09794263541698456,
        0.4437955915927887,
        -0.48550933599472046,
        0.25108763575553894,
        -1.3843052387237549,
        0.2627003788948059,
        -0.0873766839504242,
        0.023489391431212425,
        0.07877876609563828,
        -0.09360775351524353,
        -0.0952901691198349,
        -0.484317809343338,
        0.061349090188741684,
        -0.10824434459209442,
        -0.7048960328102112,
        -0.35912203788757324,
        -0.732926070690155,
        -0.697175145149231,
        -0.1237817108631134,
        -0.1417057365179062,
        -0.053552813827991486,
        -0.5322489142417908,
        -0.14157529175281525,
        0.5799886584281921,
        -0.014835216104984283,
        0.48655542731285095,
        -0.4964355230331421,
        -0.08751755207777023,
        0.3917066752910614,
        0.024810144677758217,
        -0.2529240846633911,
        0.17743387818336487,
        -0.3439270555973053,
        0.3458998501300812,
        -0.534119725227356,
        -1.0956602096557617,
        -0.48492223024368286,
        0.4923640191555023,
        -0.29474347829818726,
        -1.127366304397583,
        -0.46776890754699707,
        -0.20948025584220886,
        1.4553545713424683,
        1.1577849388122559,
        0.27019575238227844,
        0.7679046988487244,
        0.49696511030197144,
        -0.13088533282279968,
        0.2920942008495331,
        -0.12341294437646866,
        -0.02986738830804825,
        -0.11338219046592712,
        -0.39339742064476013,
        -0.6786876916885376,
        0.09376875311136246,
        0.41212061047554016,
        -0.200413316488266,
        0.4143003523349762,
        -0.41680708527565,
        0.6218758821487427,
        0.07275916635990143,
        -0.21668192744255066,
        -0.08150698989629745,
        0.2986817955970764,
        0.6092334985733032,
        1.2750334739685059,
        0.15700818598270416,
        -1.7736636400222778,
        -0.19739758968353271,
        0.3316705524921417,
        0.10455495119094849,
        0.10703140497207642,
        -0.5026130080223083,
        0.2242002785205841,
        -0.05584058165550232,
        -0.06085595861077309,
        -0.3401850759983063,
        1.1703609228134155,
        0.5743274092674255,
        0.30596932768821716,
        0.6029182076454163,
        0.09711887687444687,
        0.2571448087692261,
        -0.04090417921543121,
        0.10309240967035294,
        -0.4033832252025604,
        -0.36939987540245056,
        -0.8967865705490112,
        -0.0537196546792984,
        1.2793893814086914,
        0.2585822641849518,
        -0.1801794022321701,
        0.553429126739502,
        -0.0047258734703063965,
        -0.8767733573913574,
        -0.47000792622566223,
        1.0191932916641235,
        0.1122177243232727,
        -0.1527821123600006,
        1.0714497566223145,
        0.15724174678325653,
        0.43432822823524475,
        0.5486724376678467,
        0.7674275040626526,
        -0.1508006900548935,
        -0.03354809433221817,
        -0.4689464569091797,
        1.0911695957183838,
        -0.1424354910850525,
        -0.9991303086280823,
        -0.055860355496406555,
        0.7692852020263672,
        -0.6017325520515442,
        -0.37970179319381714,
        1.0117416381835938,
        -0.8175657987594604,
        -0.13072925806045532,
        0.35157105326652527,
        0.06391911953687668,
        0.15907329320907593,
        0.42231321334838867,
        0.8115395903587341,
        0.6563275456428528,
        -0.21515671908855438,
        0.22598089277744293,
        0.16798250377178192,
        -0.000682339072227478,
        -0.15719881653785706,
        0.9550564289093018,
        -0.12465624511241913,
        -0.35782378911972046,
        -0.5558325052261353
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The AI assistant is designed to interpret and respond to LLM/AI prompts with structured outputs. It specializes in organizing and analyzing prompts to produce responses that adhere to specific instructions and formatting requirements. The assistant ensures accuracy and alignment with the intended outcomes through meticulous analysis.",
          "name": "Create_pattern",
          "raw": "\n                workflow Create_pattern v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an AI assistant whose primary responsibility is to interpret LLM/AI prompts and deliver responses based on pre-defined structures. You are a master of organization, meticulously analyzing each prompt to identify the specific instructions and any provided examples. You then utilize this knowledge to generate an output that precisely matches the requested structure. You are adept at understanding and following formatting instructions, ensuring that your responses are always accurate and perfectly aligned with the intended outcome.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract a summary of the role the AI will be taking to fulfil this pattern into a section called IDENTITY and PURPOSE.\n\n- Extract a step by step set of instructions the AI will need to follow in order to complete this pattern into a section called STEPS.\n\n- Analyze the prompt to determine what format the output should be in.\n\n- Extract any specific instructions for how the output should be formatted into a section called OUTPUT INSTRUCTIONS.\n\n- Extract any examples from the prompt into a subsection of OUTPUT INSTRUCTIONS called EXAMPLE.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- All sections should be Heading level 1\n\n- Subsections should be one Heading level higher than it's parent section\n\n- All bullets should have their own paragraph\n\n- Write the IDENTITY and PURPOSE section including the summary of the role using personal pronouns such as 'You'. Be sure to be extremely detailed in explaining the role. Finalize this section with a new paragraph advising the AI to 'Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.'.\n\n- Write the STEPS bullets from the prompt\n\n- Write the OUTPUT INSTRUCTIONS bullets starting with the first bullet explaining the only output format. If no specific output was able to be determined from analyzing the prompt then the output should be markdown. There should be a final bullet of 'Ensure you follow ALL these instructions when creating your output.'. Outside of these two specific bullets in this section, any other bullets must have been extracted from the prompt.\n\n- If an example was provided write the EXAMPLE subsection under the parent section of OUTPUT INSTRUCTIONS.\n\n- Write a final INPUT section with just the value 'INPUT:' inside it.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an AI assistant whose primary responsibility is to interpret LLM/AI prompts and deliver responses based on pre-defined structures. You are a master of organization, meticulously analyzing each prompt to identify the specific instructions and any provided examples. You then utilize this knowledge to generate an output that precisely matches the requested structure. You are adept at understanding and following formatting instructions, ensuring that your responses are always accurate and perfectly aligned with the intended outcome.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract a summary of the role the AI will be taking to fulfil this pattern into a section called IDENTITY and PURPOSE.\n\n- Extract a step by step set of instructions the AI will need to follow in order to complete this pattern into a section called STEPS.\n\n- Analyze the prompt to determine what format the output should be in.\n\n- Extract any specific instructions for how the output should be formatted into a section called OUTPUT INSTRUCTIONS.\n\n- Extract any examples from the prompt into a subsection of OUTPUT INSTRUCTIONS called EXAMPLE.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- All sections should be Heading level 1\n\n- Subsections should be one Heading level higher than it's parent section\n\n- All bullets should have their own paragraph\n\n- Write the IDENTITY and PURPOSE section including the summary of the role using personal pronouns such as 'You'. Be sure to be extremely detailed in explaining the role. Finalize this section with a new paragraph advising the AI to 'Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.'.\n\n- Write the STEPS bullets from the prompt\n\n- Write the OUTPUT INSTRUCTIONS bullets starting with the first bullet explaining the only output format. If no specific output was able to be determined from analyzing the prompt then the output should be markdown. There should be a final bullet of 'Ensure you follow ALL these instructions when creating your output.'. Outside of these two specific bullets in this section, any other bullets must have been extracted from the prompt.\n\n- If an example was provided write the EXAMPLE subsection under the parent section of OUTPUT INSTRUCTIONS.\n\n- Write a final INPUT section with just the value 'INPUT:' inside it.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.0006087198853492737,
        0.8545213937759399,
        -0.8205944299697876,
        0.6314225196838379,
        -0.22644001245498657,
        -0.03540576249361038,
        -0.8024661540985107,
        0.11433306336402893,
        -0.3404536545276642,
        0.24239112436771393,
        -0.45610734820365906,
        0.2906699478626251,
        0.20247069001197815,
        -0.08202041685581207,
        -0.2542355954647064,
        -0.21458017826080322,
        -0.27562135457992554,
        -0.7503758668899536,
        -1.8782539367675781,
        -0.0989670529961586,
        -0.014854313805699348,
        1.1974178552627563,
        0.6069533824920654,
        0.08951327949762344,
        0.3785354495048523,
        -0.06497731804847717,
        0.46162885427474976,
        -0.4672902226448059,
        -0.6519361734390259,
        -1.37835693359375,
        0.38591691851615906,
        0.7048940062522888,
        -0.40531718730926514,
        -0.7545356750488281,
        0.4014904499053955,
        -0.6475257873535156,
        -0.16314922273159027,
        -0.12361852824687958,
        -0.5637276768684387,
        -0.2551533877849579,
        0.09097866714000702,
        -0.028830107301473618,
        -0.23810473084449768,
        -0.18902826309204102,
        0.14672870934009552,
        -0.033866845071315765,
        -0.2639302909374237,
        -0.2896496057510376,
        1.0513819456100464,
        0.5747151374816895,
        0.15699683129787445,
        -0.9108383655548096,
        -0.056893929839134216,
        0.16861557960510254,
        -0.40755799412727356,
        -0.07061003148555756,
        -0.2407199740409851,
        -0.4100574254989624,
        -0.12014254927635193,
        -0.39186835289001465,
        0.18227088451385498,
        0.1566111296415329,
        -3.9416775703430176,
        -0.17519713938236237,
        -0.015129754319787025,
        0.30471107363700867,
        0.08116023987531662,
        -0.46264949440956116,
        0.9654996991157532,
        0.12788644433021545,
        -0.037570253014564514,
        0.44050168991088867,
        0.29718348383903503,
        0.865632176399231,
        0.10948687046766281,
        0.05490002781152725,
        0.2134435474872589,
        -0.042319998145103455,
        -0.2931555509567261,
        -0.26032310724258423,
        0.0727204903960228,
        0.11512508988380432,
        0.14615486562252045,
        -0.32324522733688354,
        -0.12935057282447815,
        0.4930775463581085,
        0.20513543486595154,
        0.6268203854560852,
        0.6130321025848389,
        -0.1618756651878357,
        -0.2891898453235626,
        -0.37573307752609253,
        0.24763226509094238,
        -0.25189611315727234,
        -0.30837568640708923,
        -0.5417748093605042,
        -0.38228899240493774,
        -0.1526992917060852,
        0.027287408709526062,
        3.2278387546539307,
        0.4126988649368286,
        0.01575591042637825,
        0.7241998910903931,
        -1.3448172807693481,
        0.20431143045425415,
        0.19279566407203674,
        -0.011785900220274925,
        -0.9037608504295349,
        0.07807460427284241,
        -0.011746179312467575,
        0.3743068277835846,
        -0.4759231507778168,
        -0.4435103237628937,
        0.08223875612020493,
        0.4051794707775116,
        0.3873913288116455,
        -0.31866878271102905,
        0.037595558911561966,
        -0.24355639517307281,
        0.9540203809738159,
        -0.18178115785121918,
        -0.2855645716190338,
        -0.45761850476264954,
        -0.40934011340141296,
        0.24911846220493317,
        0.38103511929512024,
        -0.3747825026512146,
        0.6476678848266602,
        -0.09382165223360062,
        -0.021246397867798805,
        -0.3880015015602112,
        -0.3189062476158142,
        -0.5563854575157166,
        0.03788618743419647,
        0.3399603068828583,
        -0.22130098938941956,
        0.2757389545440674,
        -1.0010511875152588,
        0.08463232219219208,
        -0.20219707489013672,
        0.18891674280166626,
        -0.8282273411750793,
        0.7263794541358948,
        0.3591001629829407,
        0.27487748861312866,
        0.19662785530090332,
        -0.4604157507419586,
        0.5589635372161865,
        -0.31607237458229065,
        -0.6310211420059204,
        0.20476049184799194,
        -0.034704647958278656,
        -0.05432819947600365,
        -0.1346268355846405,
        0.4066547155380249,
        -0.08205734193325043,
        0.22299276292324066,
        0.2882416844367981,
        -0.5252867937088013,
        0.34261417388916016,
        -0.0011443011462688446,
        -0.10021933168172836,
        -0.8007125854492188,
        -0.14709891378879547,
        0.49036696553230286,
        -0.3275057375431061,
        -0.670875608921051,
        0.5423829555511475,
        0.3676401674747467,
        -0.3194308876991272,
        0.304970383644104,
        -0.07471742480993271,
        0.29041188955307007,
        0.6414692401885986,
        -0.21177415549755096,
        -0.3977620303630829,
        0.06835329532623291,
        0.31907379627227783,
        0.497646689414978,
        -0.49315574765205383,
        0.32787197828292847,
        1.0781031847000122,
        0.08057835698127747,
        -0.567633867263794,
        -0.3304671049118042,
        0.10387982428073883,
        0.08168751001358032,
        -0.1520964801311493,
        0.6745900511741638,
        0.4474552571773529,
        -1.26244056224823,
        1.314488410949707,
        -0.6047000885009766,
        0.37138745188713074,
        -0.017710650339722633,
        0.3665785789489746,
        0.2979012727737427,
        -0.14817790687084198,
        0.5251612067222595,
        -0.4673585593700409,
        -1.721114158630371,
        -0.29458287358283997,
        -1.0299549102783203,
        -0.3299640119075775,
        -0.25057464838027954,
        -0.20793066918849945,
        0.1614086776971817,
        0.21786294877529144,
        0.354560911655426,
        -0.8711567521095276,
        -0.5248367786407471,
        -0.13189460337162018,
        1.505436658859253,
        1.042441487312317,
        1.2225189208984375,
        0.7847172617912292,
        0.4889875650405884,
        -0.09348003566265106,
        0.3631487786769867,
        0.22646978497505188,
        0.20250970125198364,
        0.17529860138893127,
        -0.7025461792945862,
        -0.3865160644054413,
        -0.43484991788864136,
        0.7150384187698364,
        -0.4948123097419739,
        0.5575019121170044,
        -0.5976250767707825,
        -0.1791279911994934,
        0.4346499741077423,
        1.3491034507751465,
        0.5925504565238953,
        1.5416635274887085,
        -0.6306027173995972,
        0.4983166754245758,
        -0.44110608100891113,
        -0.09535907208919525,
        0.19363254308700562,
        -1.600425124168396,
        -0.07912728935480118,
        0.04879605770111084,
        0.12778455018997192,
        -0.05128476768732071,
        -0.46671062707901,
        0.030779078602790833,
        0.04861488938331604,
        -1.02628755569458,
        -0.6179290413856506,
        1.4832468032836914,
        0.5019341111183167,
        0.06053079292178154,
        -0.09149041771888733,
        0.20939390361309052,
        0.06690143048763275,
        -0.4521336853504181,
        -1.4979701042175293,
        0.6778521537780762,
        0.16029945015907288,
        0.37952932715415955,
        -0.09146188199520111,
        0.1704038381576538,
        0.4764922559261322,
        -0.07811494171619415,
        -0.16389581561088562,
        -0.11941532790660858,
        0.03298116475343704,
        -0.6450053453445435,
        -0.8610262870788574,
        0.012292228639125824,
        -0.11795984953641891,
        0.22518907487392426,
        -0.5500366687774658,
        0.20467811822891235,
        0.7898197770118713,
        0.0982406958937645,
        -0.6889348030090332,
        0.24155211448669434,
        -0.02495691180229187,
        -0.34804293513298035,
        0.028059430420398712,
        -0.014939254149794579,
        -0.6147879958152771,
        0.7357192039489746,
        0.08176451176404953,
        -0.45959609746932983,
        -0.49667906761169434,
        -0.9281907081604004,
        0.15267354249954224,
        0.9870990514755249,
        -0.44051241874694824,
        -0.2051319181919098,
        -0.4127691984176636,
        0.2917559742927551,
        1.215335488319397,
        1.1229079961776733,
        0.6948056221008301,
        0.5331639051437378,
        0.8027424216270447,
        -0.20900262892246246,
        0.15275710821151733,
        0.6893118023872375,
        0.6548339128494263,
        0.23925693333148956,
        -1.454437494277954,
        -0.7176047563552856,
        0.4310402274131775,
        0.31667011976242065,
        -0.17213407158851624,
        -0.21190837025642395,
        -0.89697265625,
        -0.16495256125926971,
        -0.2435026913881302,
        0.19931483268737793,
        0.5353297591209412,
        -0.06368528306484222,
        1.2049835920333862,
        1.4233922958374023,
        0.2714402973651886,
        -0.8797510266304016,
        -0.2263568639755249,
        0.6144194602966309,
        -0.1622094064950943,
        0.07550397515296936,
        -0.44384318590164185,
        0.5359334945678711,
        0.16225001215934753,
        0.025439560413360596,
        -0.5264942049980164,
        1.5668206214904785,
        0.2121429145336151,
        -0.11072773486375809,
        0.18576782941818237,
        0.3088452219963074,
        0.991683840751648,
        -0.2117403894662857,
        0.029178868979215622,
        0.057961445301771164,
        -0.3363115191459656,
        -0.06426006555557251,
        0.7743334770202637,
        0.8444010615348816,
        0.767859935760498,
        0.02685953676700592,
        0.28973138332366943,
        -0.0575067475438118,
        -0.6970174312591553,
        -0.7832809686660767,
        0.428546667098999,
        0.093346506357193,
        -0.20551791787147522,
        0.7170508503913879,
        0.21531352400779724,
        -0.20735692977905273,
        0.8260038495063782,
        0.5044556856155396,
        -0.828629732131958,
        0.3439306616783142,
        -0.06066454201936722,
        1.8314297199249268,
        -0.5384112000465393,
        -0.702620804309845,
        -0.6462236642837524,
        0.43869033455848694,
        -0.9617989659309387,
        -0.3202900290489197,
        -0.2706325948238373,
        -0.4930233359336853,
        0.14802920818328857,
        0.3616237938404083,
        -0.3099089562892914,
        -0.1360764503479004,
        0.07809396088123322,
        -0.06791995465755463,
        0.5324490070343018,
        -0.5662327408790588,
        -0.15925027430057526,
        0.2689410150051117,
        0.40042200684547424,
        -0.8117148280143738,
        1.2847585678100586,
        0.14127957820892334,
        -0.25891828536987305,
        -0.6590551137924194
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates questions for reviewing learning objectives based on provided subject and objectives. It requires defining the subject and learning objectives for accurate question generation. The output consists of questions aimed at helping students review key concepts.",
          "name": "Create_quiz",
          "raw": "\n                workflow Create_quiz v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert on the subject defined in the input section provided below.\n\n# GOAL\n\nGenerate questions for a student who wants to review the main concepts of the learning objectives provided in the input section provided below.\n\nIf the input section defines the student level, adapt the questions to that level. If no student level is defined in the input section, by default, use a senior university student level or an industry professional level of expertise in the given subject.\n\nDo not answer the questions.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Generate, upmost, three review questions for each learning objective. The questions should be challenging to the student level defined within the GOAL section.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\nDo not print the dashes. \n---\nSubject: \n* Learning objective: \n    - Question 1: {generated question 1}\n    - Answer 1: \n\n    - Question 2: {generated question 2}\n    - Answer 2:\n    \n    - Question 3: {generated question 3}\n    - Answer 3:\n---\n\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert on the subject defined in the input section provided below.\n\n# GOAL\n\nGenerate questions for a student who wants to review the main concepts of the learning objectives provided in the input section provided below.\n\nIf the input section defines the student level, adapt the questions to that level. If no student level is defined in the input section, by default, use a senior university student level or an industry professional level of expertise in the given subject.\n\nDo not answer the questions.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Generate, upmost, three review questions for each learning objective. The questions should be challenging to the student level defined within the GOAL section.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\nDo not print the dashes. \n---\nSubject: \n* Learning objective: \n    - Question 1: {generated question 1}\n    - Answer 1: \n\n    - Question 2: {generated question 2}\n    - Answer 2:\n    \n    - Question 3: {generated question 3}\n    - Answer 3:\n---\n\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.3828502297401428,
        0.8716011047363281,
        -0.9369487166404724,
        0.4052060842514038,
        0.08367663621902466,
        0.5670847296714783,
        -0.5863682627677917,
        -0.07196974009275436,
        0.1787276268005371,
        0.19876548647880554,
        -0.44398221373558044,
        0.4209955930709839,
        -0.4642241895198822,
        -0.6055492162704468,
        0.23409739136695862,
        -0.4380379915237427,
        0.1946035474538803,
        -1.278417944908142,
        -1.5431182384490967,
        0.0857810229063034,
        -0.47547391057014465,
        1.0646922588348389,
        -0.0984371080994606,
        0.28972572088241577,
        0.5952568054199219,
        -0.1251804232597351,
        0.43511608242988586,
        -0.2196788787841797,
        -0.47693538665771484,
        -2.083413600921631,
        0.7517420649528503,
        0.41145145893096924,
        -0.2573125958442688,
        -0.9891482591629028,
        0.5841522216796875,
        -0.228469580411911,
        -0.16952550411224365,
        -0.11347739398479462,
        -0.21805265545845032,
        -0.30318623781204224,
        0.3763095438480377,
        -0.3345433473587036,
        -0.5807009935379028,
        0.3754502534866333,
        -0.09038566052913666,
        -0.03179957717657089,
        -0.23467372357845306,
        -0.2715752124786377,
        0.4672703742980957,
        0.5183612108230591,
        -0.3743383288383484,
        -0.7679453492164612,
        -0.0336470901966095,
        -0.357291042804718,
        -0.5415496826171875,
        -0.3829091191291809,
        0.22070607542991638,
        -0.6524376273155212,
        -0.09309916198253632,
        -0.16957691311836243,
        -0.4901008903980255,
        0.48354852199554443,
        -3.33130145072937,
        -0.1840706616640091,
        0.08573928475379944,
        0.3561961054801941,
        0.3161003589630127,
        -0.4222310483455658,
        1.1658332347869873,
        0.1355513632297516,
        0.5525960922241211,
        0.13388052582740784,
        0.3935450613498688,
        0.3793487548828125,
        0.09065654128789902,
        -0.036782488226890564,
        0.5466411709785461,
        -0.32822781801223755,
        -0.04000043869018555,
        0.2248067557811737,
        -0.4598453640937805,
        0.41491106152534485,
        0.15789617598056793,
        -0.20625461637973785,
        -0.3832628130912781,
        0.23561280965805054,
        -0.8523970246315002,
        0.30630993843078613,
        0.31415262818336487,
        0.1373988538980484,
        0.3663254976272583,
        -0.17536304891109467,
        0.3555632531642914,
        -0.16146257519721985,
        -0.6585109233856201,
        0.22485390305519104,
        0.022776346653699875,
        0.07630333304405212,
        -0.03762070834636688,
        3.3997955322265625,
        0.5707745552062988,
        0.23183460533618927,
        0.8099139928817749,
        -0.5336292386054993,
        0.8762344121932983,
        -0.7312150001525879,
        0.16901767253875732,
        -0.70478355884552,
        -0.3501618206501007,
        -0.796873152256012,
        0.8263766169548035,
        -0.9286790490150452,
        -0.7111694812774658,
        0.47029051184654236,
        0.533466637134552,
        0.13311195373535156,
        0.1454232931137085,
        -0.38767310976982117,
        -0.34543654322624207,
        0.7935717105865479,
        -0.26549339294433594,
        -0.09475317597389221,
        -0.1343296319246292,
        -0.23853173851966858,
        -0.19031094014644623,
        0.0035347938537597656,
        -0.2286188006401062,
        0.6978453397750854,
        -0.3080957531929016,
        0.36275285482406616,
        -0.25365057587623596,
        0.1524345427751541,
        -0.10002270340919495,
        -0.4941950738430023,
        0.31966596841812134,
        -0.04643004387617111,
        0.1146596372127533,
        -0.9477181434631348,
        0.49504324793815613,
        -0.6453216075897217,
        0.15956522524356842,
        -0.7797164916992188,
        0.4649820923805237,
        0.07147440314292908,
        0.8855255842208862,
        0.8097586631774902,
        0.07277986407279968,
        0.3211268484592438,
        -0.7449014782905579,
        -0.5769004821777344,
        0.10468094795942307,
        0.047972969710826874,
        -0.08451509475708008,
        0.4035398066043854,
        0.16181740164756775,
        -0.06311832368373871,
        -0.37766480445861816,
        -0.017001695930957794,
        -0.7911978960037231,
        0.7734581232070923,
        -0.008452685549855232,
        -0.05584254860877991,
        0.10984392464160919,
        0.2115093469619751,
        0.42020097374916077,
        -0.45656144618988037,
        -0.01099155843257904,
        0.13036900758743286,
        0.24690024554729462,
        0.48062095046043396,
        0.49385350942611694,
        -0.06710288673639297,
        0.32561299204826355,
        0.683395504951477,
        -0.3848644495010376,
        0.11255031079053879,
        -0.11092027276754379,
        0.6381601691246033,
        0.6740848422050476,
        -0.571688175201416,
        0.6593015789985657,
        0.4815482795238495,
        -0.21615813672542572,
        -1.171958327293396,
        -0.703138530254364,
        -0.3060660660266876,
        0.6529443264007568,
        0.017587101086974144,
        1.286716103553772,
        1.272646427154541,
        -0.35585159063339233,
        1.2636828422546387,
        -0.037469249218702316,
        0.5487822890281677,
        0.05150675028562546,
        0.2998442053794861,
        -0.05083499103784561,
        -0.2629159688949585,
        0.5613573789596558,
        0.03925969451665878,
        -1.1922268867492676,
        0.13905389606952667,
        -0.664110004901886,
        -0.20135825872421265,
        -0.8314014077186584,
        -0.8721374273300171,
        0.241461843252182,
        0.6660501956939697,
        0.4061208963394165,
        -0.48849669098854065,
        -0.6143439412117004,
        0.6894019842147827,
        0.9428662657737732,
        0.42482250928878784,
        1.0037392377853394,
        -0.20031456649303436,
        0.3466515839099884,
        -0.10263568162918091,
        0.6044519543647766,
        0.8699612617492676,
        -0.26559653878211975,
        -0.168736070394516,
        -0.8316008448600769,
        -0.5864666700363159,
        -0.9515863656997681,
        0.1363590955734253,
        -0.36630043387413025,
        -0.08688568323850632,
        -0.9704237580299377,
        -0.730901300907135,
        0.2728263735771179,
        0.5343418121337891,
        1.3402137756347656,
        0.44916507601737976,
        -0.5179346203804016,
        0.5112051963806152,
        -0.6399121880531311,
        0.3166844844818115,
        -0.025885332375764847,
        -1.3188503980636597,
        0.49106425046920776,
        0.008466348052024841,
        -0.002837367355823517,
        0.23780526220798492,
        0.23269179463386536,
        0.367239773273468,
        -0.5249398946762085,
        -0.45877498388290405,
        0.2139464020729065,
        0.9890199303627014,
        0.43804943561553955,
        -0.3872699737548828,
        0.012002203613519669,
        0.060668788850307465,
        -0.37730616331100464,
        0.2182360589504242,
        -1.9783145189285278,
        0.302749365568161,
        -0.621216893196106,
        0.0072807446122169495,
        -0.5720274448394775,
        -0.1381234973669052,
        0.9348163604736328,
        -0.2910303771495819,
        0.3928443193435669,
        0.04443564638495445,
        -0.22600413858890533,
        -0.1957966387271881,
        -0.1750490367412567,
        -0.7676668167114258,
        0.2919384241104126,
        0.10539757460355759,
        -0.5353871583938599,
        -0.2929290533065796,
        0.6972259283065796,
        0.11987745761871338,
        -0.2947756350040436,
        -0.13108184933662415,
        -0.1030203253030777,
        -0.6804593205451965,
        0.37221643328666687,
        -0.09189464151859283,
        0.049508336931467056,
        0.22471822798252106,
        -0.3928018808364868,
        0.2358705699443817,
        -0.4227738082408905,
        -0.6181887984275818,
        -0.27042368054389954,
        0.963819146156311,
        0.27049845457077026,
        -0.5715945363044739,
        -0.3793894350528717,
        0.013851411640644073,
        1.3504855632781982,
        0.4850623905658722,
        0.15421314537525177,
        0.11950822919607162,
        0.3962605595588684,
        0.4748297333717346,
        0.34009963274002075,
        0.09388241171836853,
        -0.3128417134284973,
        0.16400107741355896,
        -0.8530685305595398,
        -0.025572456419467926,
        0.25464481115341187,
        -0.27307653427124023,
        -0.16606730222702026,
        0.6346973180770874,
        -0.7167497277259827,
        -0.06317474693059921,
        -0.17676126956939697,
        0.14002230763435364,
        0.4140002727508545,
        -0.09167137742042542,
        0.7461850643157959,
        1.431448221206665,
        -0.1383315473794937,
        -1.2299832105636597,
        -0.4549265205860138,
        0.6063196659088135,
        0.34466442465782166,
        0.31698551774024963,
        -0.5467529892921448,
        0.648497998714447,
        -0.14776486158370972,
        0.04695476219058037,
        -0.02676232159137726,
        1.7225311994552612,
        0.37917155027389526,
        0.46474021673202515,
        0.23957616090774536,
        0.12387130409479141,
        0.4299968481063843,
        -0.581393837928772,
        0.6107378602027893,
        -0.3108130693435669,
        -0.27492040395736694,
        -0.5096502900123596,
        0.4785815477371216,
        1.4350345134735107,
        0.8222540020942688,
        -0.5319784283638,
        0.2742166221141815,
        0.504841685295105,
        -0.9821367859840393,
        -0.4097357988357544,
        0.31577616930007935,
        0.2184685319662094,
        0.36240261793136597,
        0.6706917881965637,
        0.11443482339382172,
        -0.09998755156993866,
        0.8667432069778442,
        0.4983521103858948,
        -0.3228863775730133,
        0.7194784283638,
        -0.4989951252937317,
        1.9273016452789307,
        -0.6944963335990906,
        -1.1605859994888306,
        -0.388290673494339,
        0.2454652041196823,
        -0.8659045696258545,
        -0.48002296686172485,
        0.3854084610939026,
        -0.04942697659134865,
        0.2647285461425781,
        0.3263452649116516,
        -0.3385612964630127,
        -0.2160925567150116,
        0.3941804766654968,
        0.35460326075553894,
        0.35839831829071045,
        -0.4323822259902954,
        0.08162915706634521,
        0.053782179951667786,
        -0.538985550403595,
        -0.2682129740715027,
        0.8765432834625244,
        -0.29231899976730347,
        -0.7466285228729248,
        -1.135780930519104
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Designs a tailored three-phase reading plan based on user input, focusing on an author or specific guidance. It carefully selects books from various sources, including hidden gems, to enhance the user's knowledge on the topic. The output includes a concise plan summary and categorized reading lists with reasons for each selection.",
          "name": "Create_reading_plan",
          "raw": "\n                workflow Create_reading_plan v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou take guidance and/or an author name as input and design a perfect three-phase reading plan for the user using the STEPS below.\n\nThe goal is to create a reading list that will result in the user being significantly knowledgeable about the author and their work, and/or how it relates to the request from the user if they made one.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about the request made in the input.\n\n- Find the author (or authors) that are mentioned in the input.\n\n- Think deeply about what books from that author (or authors) are the most interesting, surprising, and insightful, and or which ones most match the request in the input.\n\n- Think about all the different sources of \\\"Best Books\\\", such as bestseller lists, reviews, etc.\n\n- Don't limit yourself to just big and super-famous books, but also consider hidden gem books if they would better serve what the user is trying to do.\n\n- Based on what the user is looking for, or the author(s) named, create a reading plan with the following sections.\n\n# OUTPUT SECTIONS\n\n- In a section called \\\"ABOUT THIS READING PLAN\\\", write a 25 word sentence that says something like: \n\n\\\"It sounds like you're interested in ___________ (taken from their input), so here's a reading plan to help you learn more about that.\\\"\n\n- In a section called \\\"PHASE 1: Core Reading\\\", give a bulleted list of the core books for the author and/or topic in question. Like the essential reading. Give those in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 3\n\n- In a section called \\\"PHASE 2: Extended Reading\\\", give a bulleted list of the best books that expand on the core reading above, in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 5\n\n- In a section called \\\"PHASE 3: Exploratory Reading\\\", give a bulleted list of the best books that expand on the author's themes, either from the author themselves or from other authors that wrote biographies, or prescriptive guidance books based on the reading in PHASE 1 and PHASE 2, in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 7\n\n- In a section called \\\"OUTLINE SUMMARY\\\", write a 25 word sentence that says something like: \n\nThis reading plan will give you a solid foundation in ___________ (taken from their input) and will allow you to branch out from there.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Take into account all instructions in the input, for example books they've already read, themes, questions, etc., to help you shape the reading plan.\n\n- For PHASE 2 and 3 you can also include articles, essays, and other written works in addition to books.\n\n- DO NOT hallucinate or make up any of the recommendations you give. Only use real content.\n\n- Put a blank line between bullets for readability.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou take guidance and/or an author name as input and design a perfect three-phase reading plan for the user using the STEPS below.\n\nThe goal is to create a reading list that will result in the user being significantly knowledgeable about the author and their work, and/or how it relates to the request from the user if they made one.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about the request made in the input.\n\n- Find the author (or authors) that are mentioned in the input.\n\n- Think deeply about what books from that author (or authors) are the most interesting, surprising, and insightful, and or which ones most match the request in the input.\n\n- Think about all the different sources of \\\"Best Books\\\", such as bestseller lists, reviews, etc.\n\n- Don't limit yourself to just big and super-famous books, but also consider hidden gem books if they would better serve what the user is trying to do.\n\n- Based on what the user is looking for, or the author(s) named, create a reading plan with the following sections.\n\n# OUTPUT SECTIONS\n\n- In a section called \\\"ABOUT THIS READING PLAN\\\", write a 25 word sentence that says something like: \n\n\\\"It sounds like you're interested in ___________ (taken from their input), so here's a reading plan to help you learn more about that.\\\"\n\n- In a section called \\\"PHASE 1: Core Reading\\\", give a bulleted list of the core books for the author and/or topic in question. Like the essential reading. Give those in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 3\n\n- In a section called \\\"PHASE 2: Extended Reading\\\", give a bulleted list of the best books that expand on the core reading above, in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 5\n\n- In a section called \\\"PHASE 3: Exploratory Reading\\\", give a bulleted list of the best books that expand on the author's themes, either from the author themselves or from other authors that wrote biographies, or prescriptive guidance books based on the reading in PHASE 1 and PHASE 2, in the following format:\n\n- Man's Search for Meaning, by Victor Frankl. This book was chosen because _________. (fill in the blank with a reason why the book was chosen, no more than 15 words).\n\n- Next entry\n- Next entry\n- Up to 7\n\n- In a section called \\\"OUTLINE SUMMARY\\\", write a 25 word sentence that says something like: \n\nThis reading plan will give you a solid foundation in ___________ (taken from their input) and will allow you to branch out from there.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Take into account all instructions in the input, for example books they've already read, themes, questions, etc., to help you shape the reading plan.\n\n- For PHASE 2 and 3 you can also include articles, essays, and other written works in addition to books.\n\n- DO NOT hallucinate or make up any of the recommendations you give. Only use real content.\n\n- Put a blank line between bullets for readability.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.1673608124256134,
        0.656624972820282,
        0.13429346680641174,
        0.35857710242271423,
        0.43548518419265747,
        -0.021792836487293243,
        -0.8546223640441895,
        0.0831293910741806,
        -0.2290145456790924,
        0.36371272802352905,
        -0.6427663564682007,
        0.30551624298095703,
        -0.05528036132454872,
        0.07965834438800812,
        -0.030373942106962204,
        -0.22195038199424744,
        0.023947440087795258,
        -1.9180666208267212,
        -0.975005030632019,
        -0.2899578809738159,
        0.09675896167755127,
        0.6478683948516846,
        0.12870930135250092,
        0.006426900625228882,
        0.5480737090110779,
        0.094915971159935,
        0.2661519944667816,
        -0.39599961042404175,
        -0.6654850244522095,
        -1.604820728302002,
        0.4513869285583496,
        0.016104772686958313,
        -0.7305169105529785,
        -1.0451390743255615,
        0.8321096897125244,
        -0.6538959741592407,
        0.5072336196899414,
        -0.00625281035900116,
        -0.45823848247528076,
        -0.37501704692840576,
        -0.3031081259250641,
        0.02630583755671978,
        -0.41314759850502014,
        -0.040462613105773926,
        0.09656406193971634,
        -0.3159448206424713,
        -0.012250593863427639,
        0.021911069750785828,
        1.2271541357040405,
        -0.09290848672389984,
        -0.43765243887901306,
        -0.7822209000587463,
        -0.3364584445953369,
        0.02644866332411766,
        -0.8722731471061707,
        0.029776856303215027,
        0.19950415194034576,
        -0.694919228553772,
        0.5315602421760559,
        -0.06696771085262299,
        0.5652225613594055,
        0.4397895336151123,
        -3.280470132827759,
        -0.32018688321113586,
        0.5876418352127075,
        0.599524974822998,
        -0.08382585644721985,
        -0.4700324237346649,
        0.36529359221458435,
        -0.018041178584098816,
        -0.3557548224925995,
        0.4186931252479553,
        0.06725361198186874,
        0.6286116242408752,
        0.04603038728237152,
        0.09324274212121964,
        0.19036689400672913,
        0.4457078278064728,
        0.23941513895988464,
        0.23476076126098633,
        0.05855540186166763,
        0.554730236530304,
        -0.2857452929019928,
        0.22365517914295197,
        0.08325579762458801,
        0.6922175884246826,
        0.23510771989822388,
        0.04640863835811615,
        0.8608342409133911,
        0.8720204830169678,
        -0.03174072131514549,
        -0.9421288967132568,
        -0.02856120653450489,
        -0.05212467163801193,
        -0.9284121990203857,
        -0.03514987230300903,
        -0.39471736550331116,
        0.023100964725017548,
        -0.17212334275245667,
        3.4750311374664307,
        0.35039252042770386,
        -0.08169902861118317,
        0.7710320353507996,
        -1.1743748188018799,
        0.7679896354675293,
        -0.297107070684433,
        0.27332615852355957,
        -0.46057799458503723,
        -0.17713604867458344,
        -0.04643424600362778,
        0.8132046461105347,
        -0.1799117475748062,
        -0.7537950277328491,
        0.4768137037754059,
        0.6859431862831116,
        0.057799793779850006,
        -0.8819639682769775,
        -0.06899502873420715,
        0.9300922751426697,
        0.8754873275756836,
        -0.06930731236934662,
        0.7101870775222778,
        -0.33500558137893677,
        -0.17866376042366028,
        0.20812897384166718,
        0.13781318068504333,
        -0.10978278517723083,
        0.9680635333061218,
        0.015959832817316055,
        0.06437938660383224,
        -0.4268944263458252,
        -0.23988202214241028,
        0.055554915219545364,
        0.2024029791355133,
        0.1511654555797577,
        -0.07908540219068527,
        -0.027216792106628418,
        -0.20338357985019684,
        0.5946952700614929,
        -0.6209796667098999,
        0.4074435234069824,
        -0.5748140811920166,
        0.43835967779159546,
        0.6918833255767822,
        0.2936805486679077,
        0.06025764346122742,
        -0.12779739499092102,
        -0.051932208240032196,
        -0.42774882912635803,
        -1.2859216928482056,
        -0.02569182589650154,
        0.575553834438324,
        0.14612385630607605,
        0.6325677633285522,
        0.432084858417511,
        -0.4865573048591614,
        -0.7252130508422852,
        0.5244671106338501,
        -0.30852210521698,
        0.4697653353214264,
        0.49783921241760254,
        -0.3408927321434021,
        0.007791474461555481,
        0.5426324009895325,
        0.18054136633872986,
        0.07832185924053192,
        0.8266156911849976,
        -0.19157817959785461,
        0.5743204355239868,
        0.027406122535467148,
        0.24813401699066162,
        -0.030644483864307404,
        0.4106101989746094,
        -0.06371675431728363,
        -0.4703250229358673,
        0.12423353642225266,
        -0.2758381962776184,
        -0.19572466611862183,
        -0.16934707760810852,
        -0.32448405027389526,
        0.853445291519165,
        0.28714680671691895,
        -0.13299092650413513,
        -1.0734971761703491,
        -0.2505796551704407,
        -0.24715666472911835,
        0.3429068922996521,
        0.14694085717201233,
        0.5776748061180115,
        0.4898419678211212,
        -0.9243466854095459,
        1.06279456615448,
        -0.44420236349105835,
        -0.37022432684898376,
        -0.05489978939294815,
        -0.10085932910442352,
        -0.42557984590530396,
        0.15106673538684845,
        -0.2934703528881073,
        -0.0558055080473423,
        -1.0898162126541138,
        0.4077686369419098,
        -0.2299554944038391,
        -0.2543487548828125,
        -0.5304906368255615,
        -0.027736175805330276,
        0.2451973557472229,
        0.49955061078071594,
        -0.17375683784484863,
        -0.35531458258628845,
        -0.024534612894058228,
        0.2421293705701828,
        1.4359967708587646,
        0.6559531092643738,
        1.351179838180542,
        -0.07955875992774963,
        0.006892126053571701,
        -0.04122239723801613,
        0.9584110379219055,
        0.020169198513031006,
        -0.6373022198677063,
        0.4501124918460846,
        -0.8908260464668274,
        -0.6999778747558594,
        -0.8457950353622437,
        0.3748788833618164,
        0.40683621168136597,
        0.2291325479745865,
        -0.5136746764183044,
        0.2436162233352661,
        0.10290638357400894,
        0.7348212599754333,
        -0.08261183649301529,
        1.0148588418960571,
        0.16480287909507751,
        0.20904818177223206,
        -0.1307578682899475,
        0.7140442132949829,
        0.5638022422790527,
        -1.3702744245529175,
        -0.19314759969711304,
        -0.2112576961517334,
        -0.46230244636535645,
        0.08117777109146118,
        0.242240771651268,
        0.6482546329498291,
        -0.47875842452049255,
        -0.14258036017417908,
        0.980974555015564,
        1.4906187057495117,
        0.5415253043174744,
        0.15320530533790588,
        0.30043232440948486,
        0.6597252488136292,
        -0.07541819661855698,
        0.24202480912208557,
        -1.988194227218628,
        -0.2094511091709137,
        -0.47493842244148254,
        0.06456289440393448,
        0.36331498622894287,
        -0.5592086315155029,
        0.4548465311527252,
        -0.14591741561889648,
        -0.1449112445116043,
        -0.09758549928665161,
        -1.1174110174179077,
        -0.7166892290115356,
        -0.6994706392288208,
        -0.21935135126113892,
        0.07307136803865433,
        0.4814632534980774,
        0.2658082842826843,
        -0.16601571440696716,
        -0.41301074624061584,
        0.4391118288040161,
        -0.1401243805885315,
        0.5924926400184631,
        -0.6298121809959412,
        -0.34035107493400574,
        -0.2816885709762573,
        0.370055615901947,
        0.2800121009349823,
        0.48775380849838257,
        -0.4007697105407715,
        0.23353931307792664,
        -0.4076884984970093,
        -0.8514177203178406,
        -0.22013399004936218,
        0.48665472865104675,
        -0.5102939605712891,
        -0.2320060282945633,
        -0.4735390245914459,
        -0.053213268518447876,
        1.807287573814392,
        0.11036781966686249,
        0.14008915424346924,
        0.7245424389839172,
        0.21981003880500793,
        -0.028891120105981827,
        -0.12459492683410645,
        0.17223584651947021,
        -0.12651489675045013,
        -0.2698372006416321,
        -0.39945462346076965,
        -0.5267266035079956,
        0.4380200505256653,
        -0.10524407029151917,
        0.10822181403636932,
        0.528062105178833,
        -0.8355404138565063,
        0.4304967522621155,
        -0.5065560936927795,
        -0.5808176398277283,
        0.13772261142730713,
        -0.5318117141723633,
        0.270154744386673,
        0.7963253259658813,
        0.27864277362823486,
        -1.1786850690841675,
        -0.11696919798851013,
        0.6826102137565613,
        -0.09587304294109344,
        0.21262279152870178,
        -0.09849049150943756,
        0.9869941473007202,
        -0.07710956037044525,
        -0.617469847202301,
        -0.48535674810409546,
        1.198624849319458,
        0.14348798990249634,
        -0.08236785978078842,
        -0.35713717341423035,
        -0.45375359058380127,
        0.3638542890548706,
        -0.32230451703071594,
        -0.25499603152275085,
        -0.30575230717658997,
        -0.4033721387386322,
        -0.15466131269931793,
        0.5448514223098755,
        1.7653627395629883,
        -0.014864055439829826,
        -0.21579903364181519,
        0.055001746863126755,
        0.22276005148887634,
        -0.8319458961486816,
        -0.6844595074653625,
        0.03035886585712433,
        -0.15257035195827484,
        -0.4890511929988861,
        0.2883742153644562,
        0.4801616966724396,
        -0.34443041682243347,
        0.8079274892807007,
        0.9734997153282166,
        -0.8463394641876221,
        0.5665042996406555,
        -0.6577786207199097,
        1.5125867128372192,
        -0.5721312761306763,
        -1.0717905759811401,
        -0.23534181714057922,
        0.6003584265708923,
        -0.05111397057771683,
        0.4823848605155945,
        -0.6386410593986511,
        -0.7813529372215271,
        -0.2902139127254486,
        -0.3304884433746338,
        -0.07521243393421173,
        -0.49778205156326294,
        0.6530721187591553,
        0.730250895023346,
        0.7377684116363525,
        0.11955951154232025,
        0.14973706007003784,
        0.24699898064136505,
        0.16795553267002106,
        -0.35362184047698975,
        0.5631574392318726,
        -0.20128214359283447,
        -0.564365565776825,
        -0.9961466193199158
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs the creation of a detailed markdown security finding report, incorporating sections like Description, Risk, Recommendations, and others, based on a vulnerability title and explanation provided by the user. It emphasizes a structured, insightful approach to documenting cybersecurity vulnerabilities. The expected output is a comprehensive report with specific sections, focusing on clarity, insightfulness, and relevance to cybersecurity assessment.",
          "name": "Create_report_finding",
          "raw": "\n                workflow Create_report_finding v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a extremely experienced 'jack-of-all-trades' cyber security consultant that is diligent, concise but informative and professional. You are highly experienced in web, API, infrastructure (on-premise and cloud), and mobile testing. Additionally, you are an expert in threat modeling and analysis.\n\nYou have been tasked with creating a markdown security finding that will be added to a cyber security assessment report. It must have the following sections: Description, Risk, Recommendations, References, One-Sentence-Summary, Trends, Quotes.\n\nThe user has provided a vulnerability title and a brief explanation of their finding.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Title section that contains the title of the finding.\n\n- Create a Description section that details the nature of the finding, including insightful and informative information. Do not use bullet point lists for this section.\n\n- Create a Risk section that details the risk of the finding. Do not solely use bullet point lists for this section.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a References section that lists 1 to 5 references that are suitibly named hyperlinks that provide instant access to knowledgable and informative articles that talk about the issue, the tech and remediations. Do not hallucinate or act confident if you are unsure.\n\n- Create a summary sentence that captures the spirit of the finding and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a extremely experienced 'jack-of-all-trades' cyber security consultant that is diligent, concise but informative and professional. You are highly experienced in web, API, infrastructure (on-premise and cloud), and mobile testing. Additionally, you are an expert in threat modeling and analysis.\n\nYou have been tasked with creating a markdown security finding that will be added to a cyber security assessment report. It must have the following sections: Description, Risk, Recommendations, References, One-Sentence-Summary, Trends, Quotes.\n\nThe user has provided a vulnerability title and a brief explanation of their finding.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Title section that contains the title of the finding.\n\n- Create a Description section that details the nature of the finding, including insightful and informative information. Do not use bullet point lists for this section.\n\n- Create a Risk section that details the risk of the finding. Do not solely use bullet point lists for this section.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a References section that lists 1 to 5 references that are suitibly named hyperlinks that provide instant access to knowledgable and informative articles that talk about the issue, the tech and remediations. Do not hallucinate or act confident if you are unsure.\n\n- Create a summary sentence that captures the spirit of the finding and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.41896769404411316,
        0.4797477722167969,
        -0.4471268355846405,
        0.3085241913795471,
        0.325459748506546,
        -0.05539418011903763,
        -0.12910783290863037,
        0.051735490560531616,
        0.4641641676425934,
        0.003686163341626525,
        -0.5077111124992371,
        0.6363721489906311,
        0.3676114082336426,
        0.26019999384880066,
        -0.39218270778656006,
        0.010578490793704987,
        -0.0019913911819458008,
        -1.108460783958435,
        -1.055371880531311,
        -0.00872083380818367,
        -0.7052901983261108,
        0.8166750073432922,
        0.6756869554519653,
        0.14221017062664032,
        0.258771687746048,
        0.047892145812511444,
        -0.10428452491760254,
        -0.01753976196050644,
        -0.2621082663536072,
        -2.1680173873901367,
        0.6773900985717773,
        0.20343679189682007,
        -0.29843205213546753,
        -0.37293320894241333,
        0.8245207071304321,
        -0.7869426012039185,
        0.42579397559165955,
        0.3448430895805359,
        -0.28051888942718506,
        -0.08898936212062836,
        0.49664685130119324,
        0.005554851144552231,
        -0.3165140151977539,
        -0.2081214189529419,
        -0.11188604682683945,
        0.0409967303276062,
        -0.0006371429190039635,
        -0.0008535310626029968,
        0.8239206671714783,
        -0.021586135029792786,
        -0.28612440824508667,
        -0.0953860953450203,
        -0.4778473377227783,
        0.14967955648899078,
        -0.6103112697601318,
        0.045380204916000366,
        -0.23726552724838257,
        -0.21487802267074585,
        0.1845020204782486,
        0.46733665466308594,
        0.6944682002067566,
        1.0250647068023682,
        -3.6501598358154297,
        -0.15621809661388397,
        -0.057771921157836914,
        -0.14602556824684143,
        0.1796920746564865,
        -0.1047140508890152,
        0.5070879459381104,
        -0.07434186339378357,
        0.20641471445560455,
        0.2925012707710266,
        -0.17794528603553772,
        0.7107589840888977,
        0.45307353138923645,
        0.04161382466554642,
        0.29885900020599365,
        0.22964854538440704,
        -0.024013899266719818,
        -0.37294360995292664,
        0.20329707860946655,
        0.7665241956710815,
        -0.29658621549606323,
        -0.3777822256088257,
        -0.7889597415924072,
        0.5974488258361816,
        -0.1772128939628601,
        0.005044788122177124,
        0.8146674036979675,
        0.31650593876838684,
        0.31671297550201416,
        -0.6237499117851257,
        0.49881425499916077,
        -0.26443392038345337,
        -0.5930688977241516,
        -0.13310249149799347,
        -0.18158039450645447,
        0.15171748399734497,
        0.3594399094581604,
        3.6237213611602783,
        0.5766348242759705,
        0.36214974522590637,
        0.5863752961158752,
        -0.49825260043144226,
        0.841407835483551,
        -0.1830691248178482,
        -0.24016839265823364,
        -0.6848156452178955,
        -0.3492180109024048,
        -0.4043246805667877,
        0.597603440284729,
        -0.35338065028190613,
        -0.35247495770454407,
        0.17384874820709229,
        0.3945782482624054,
        -0.1607457399368286,
        -0.8634937405586243,
        -0.0746726244688034,
        0.15626101195812225,
        0.37982818484306335,
        -0.13652518391609192,
        -0.049114640802145004,
        -0.40707921981811523,
        -0.26053839921951294,
        0.24857757985591888,
        0.1854741871356964,
        -0.10414156317710876,
        0.4828908145427704,
        -0.37408915162086487,
        0.030085043981671333,
        0.0003628283739089966,
        -0.25865796208381653,
        -0.37986868619918823,
        -0.3127453327178955,
        0.056099362671375275,
        0.13911013305187225,
        0.710874617099762,
        -0.13564956188201904,
        0.3690699338912964,
        -0.41741713881492615,
        0.21489879488945007,
        -0.8071085214614868,
        0.8204987049102783,
        0.36332616209983826,
        0.4921225309371948,
        -0.16612625122070312,
        -0.11510627716779709,
        0.0615241676568985,
        -0.5679917335510254,
        -1.2396501302719116,
        -0.2902642786502838,
        0.5299274325370789,
        -0.08425409346818924,
        -0.1730578988790512,
        0.33838313817977905,
        0.14123809337615967,
        -0.25762739777565,
        0.6021899580955505,
        -0.7165850400924683,
        0.22960549592971802,
        0.7154317498207092,
        0.25725433230400085,
        0.2582099437713623,
        0.31855517625808716,
        0.4728184938430786,
        -0.4593082666397095,
        0.6128087043762207,
        0.008366387337446213,
        -0.0644412487745285,
        0.2129305601119995,
        0.4640292227268219,
        -0.010907277464866638,
        -0.06030389666557312,
        0.04063637554645538,
        -0.40920987725257874,
        0.43720003962516785,
        -0.17154452204704285,
        -0.19915539026260376,
        0.3424563407897949,
        -0.5954000949859619,
        0.251872181892395,
        0.4799652695655823,
        -0.42473304271698,
        -0.8728353977203369,
        -0.4473511576652527,
        0.23760049045085907,
        -0.18370985984802246,
        -0.20202094316482544,
        0.36828547716140747,
        0.7192903757095337,
        -1.0592509508132935,
        1.153949499130249,
        -0.6770051717758179,
        -1.0270555019378662,
        0.026469677686691284,
        0.3086109757423401,
        -0.7257081866264343,
        0.2563236355781555,
        -0.06549349427223206,
        0.3101026117801666,
        -0.32887277007102966,
        -0.24774464964866638,
        -0.43991002440452576,
        -0.08996978402137756,
        -0.5251381397247314,
        -0.6265401244163513,
        0.45749977231025696,
        0.7536474466323853,
        0.03497927635908127,
        -0.3019072413444519,
        0.04207541048526764,
        0.3703983724117279,
        0.8087494373321533,
        0.6383514404296875,
        1.011254906654358,
        0.691753625869751,
        -0.0658353865146637,
        0.4769592881202698,
        0.7931399941444397,
        0.11926025152206421,
        0.4123016595840454,
        0.8820089101791382,
        -0.48799222707748413,
        -0.7300293445587158,
        -1.4213368892669678,
        0.04394490271806717,
        -0.21000590920448303,
        -0.26188671588897705,
        -0.7166847586631775,
        -0.1842329204082489,
        0.4267004132270813,
        0.9753291010856628,
        0.2030736207962036,
        0.9126102328300476,
        -0.3023623526096344,
        0.053765296936035156,
        -0.20008650422096252,
        0.8298618793487549,
        0.7867251634597778,
        -0.8277484178543091,
        0.25575271248817444,
        -0.1846039593219757,
        0.07989630103111267,
        -0.06303158402442932,
        0.5492802858352661,
        0.23784548044204712,
        -0.3687160313129425,
        0.00389021635055542,
        0.15105634927749634,
        1.414161205291748,
        0.6740383505821228,
        0.4478054344654083,
        0.06459853053092957,
        0.11261142790317535,
        -0.20305761694908142,
        0.004019193351268768,
        -1.5211681127548218,
        0.5278141498565674,
        -0.751762330532074,
        0.04759247973561287,
        -0.34106171131134033,
        -0.7381829023361206,
        0.1403307318687439,
        -0.11480056494474411,
        0.23872330784797668,
        -0.7398723363876343,
        -0.6902897357940674,
        -0.16473382711410522,
        -0.1801440417766571,
        -0.3137712776660919,
        -0.033824071288108826,
        0.35621023178100586,
        0.08051709830760956,
        -0.30841270089149475,
        -0.20218636095523834,
        -0.055943265557289124,
        0.4157092273235321,
        0.4192666709423065,
        -0.34350723028182983,
        -0.29638007283210754,
        -0.2734105885028839,
        0.5291255712509155,
        -0.42515668272972107,
        0.3865243196487427,
        -0.731503427028656,
        -0.21401512622833252,
        -0.30023840069770813,
        -0.701278030872345,
        -0.4608715772628784,
        0.6930689811706543,
        0.1785268485546112,
        -0.2944176197052002,
        0.032920368015766144,
        -0.3619065284729004,
        1.9757380485534668,
        -0.20321139693260193,
        0.35455843806266785,
        0.8283035159111023,
        0.45500481128692627,
        0.41615498065948486,
        -0.3952787518501282,
        0.104210764169693,
        -0.3503759205341339,
        0.3989265561103821,
        -0.6696287393569946,
        -0.6605361104011536,
        0.22717657685279846,
        -0.35775408148765564,
        0.14515003561973572,
        0.5720416903495789,
        0.040980614721775055,
        -0.09356767684221268,
        -0.2741367220878601,
        -0.25140053033828735,
        0.4855470061302185,
        -0.35029393434524536,
        0.8773190975189209,
        0.33466076850891113,
        0.18661606311798096,
        -2.0517938137054443,
        -0.3266039192676544,
        0.20141595602035522,
        -0.2127557396888733,
        -0.1320646107196808,
        -0.3398485779762268,
        0.8590749502182007,
        0.1701139211654663,
        -0.3919830322265625,
        -0.46777141094207764,
        1.5563722848892212,
        -0.03655634820461273,
        -0.2963814437389374,
        0.27019983530044556,
        0.060167618095874786,
        0.28267335891723633,
        -0.2971261441707611,
        0.030571801587939262,
        -0.39940059185028076,
        -0.909810483455658,
        -0.4297250509262085,
        0.578483521938324,
        1.7008168697357178,
        0.7724881172180176,
        -0.28086668252944946,
        0.2952532470226288,
        0.29569512605667114,
        -0.8959307074546814,
        -0.9015524387359619,
        0.19630466401576996,
        0.11330853402614594,
        -0.6810235381126404,
        0.21396972239017487,
        0.24176368117332458,
        0.02193879336118698,
        0.13845835626125336,
        1.1255276203155518,
        -0.4400026798248291,
        -0.035046182572841644,
        -0.892923891544342,
        1.5697808265686035,
        -0.04028436914086342,
        -0.6844887137413025,
        -0.32869818806648254,
        0.4794844090938568,
        -0.34552013874053955,
        -0.12856155633926392,
        0.07758384943008423,
        -0.28116562962532043,
        -0.39133763313293457,
        -0.12456489354372025,
        0.2852112352848053,
        -0.36793550848960876,
        0.49283984303474426,
        0.30319514870643616,
        -0.09123782813549042,
        -0.017081469297409058,
        0.5143974423408508,
        -0.07536706328392029,
        0.2484835833311081,
        0.046740949153900146,
        -0.3297840654850006,
        -0.033227819949388504,
        -1.027795433998108,
        -0.3465591073036194
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs on creating concise security updates for newsletters, focusing on cybersecurity developments, threats, advisories, and new vulnerabilities. It emphasizes brevity and relevance, requiring links to further information. The expected output includes structured sections with short descriptions and relevant details, aiming to inform readers about the latest security concerns efficiently.",
          "name": "Create_security_update",
          "raw": "\n                workflow Create_security_update v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at creating concise security updates for newsletters according to the STEPS below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Read all the content and think deeply about it.\n\n- Organize all the content on a virtual whiteboard in your mind.\n\n# OUTPUT SECTIONS\n\n- Output a section called Threats, Advisories, and Vulnerabilities with the following structure of content.\n\nStories: (interesting cybersecurity developments)\n\n- A 15-word or less description of the story. $MORE$\n- Next one $MORE$\n- Next one $MORE$\n- Up to 10 stories\n\nThreats & Advisories: (things people should be worried about)\n\n- A 10-word or less description of the situation. $MORE$\n- Next one $MORE$\n- Next one $MORE$\n- Up to 10 of them\n\nNew Vulnerabilities: (the highest criticality new vulnerabilities)\n\n- A 10-word or less description of the vulnerability. | $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Next one $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Next one $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Up to 10 vulnerabilities\n\nA 1-3 sentence summary of the most important issues talked about in the output above. Do not give analysis, just give an overview of the top items.\n\n# OUTPUT INSTRUCTIONS\n\n- Each $MORE$ item above should be replaced with a MORE link like so: <a href=\\\"https://www.example.com\\\">MORE</a> with the best link for that item from the input.\n- For sections like $CVE NUMBER$ and $CVSS SCORE$, if they aren't included in the input, don't output anything, and remove the extra | symbol.\n- Do not create fake links for the $MORE$ links. If you can't create a full URL just link to a placeholder or the top level domain.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at creating concise security updates for newsletters according to the STEPS below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Read all the content and think deeply about it.\n\n- Organize all the content on a virtual whiteboard in your mind.\n\n# OUTPUT SECTIONS\n\n- Output a section called Threats, Advisories, and Vulnerabilities with the following structure of content.\n\nStories: (interesting cybersecurity developments)\n\n- A 15-word or less description of the story. $MORE$\n- Next one $MORE$\n- Next one $MORE$\n- Up to 10 stories\n\nThreats & Advisories: (things people should be worried about)\n\n- A 10-word or less description of the situation. $MORE$\n- Next one $MORE$\n- Next one $MORE$\n- Up to 10 of them\n\nNew Vulnerabilities: (the highest criticality new vulnerabilities)\n\n- A 10-word or less description of the vulnerability. | $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Next one $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Next one $CVE NUMBER$ | $CVSS SCORE$ | $MORE$\n- Up to 10 vulnerabilities\n\nA 1-3 sentence summary of the most important issues talked about in the output above. Do not give analysis, just give an overview of the top items.\n\n# OUTPUT INSTRUCTIONS\n\n- Each $MORE$ item above should be replaced with a MORE link like so: <a href=\\\"https://www.example.com\\\">MORE</a> with the best link for that item from the input.\n- For sections like $CVE NUMBER$ and $CVSS SCORE$, if they aren't included in the input, don't output anything, and remove the extra | symbol.\n- Do not create fake links for the $MORE$ links. If you can't create a full URL just link to a placeholder or the top level domain.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.28046900033950806,
        0.03516601026058197,
        -0.285800039768219,
        0.793799638748169,
        -0.08582447469234467,
        0.031073540449142456,
        -0.6947625279426575,
        -0.28658968210220337,
        -0.0038794465363025665,
        0.7326650619506836,
        -0.31631723046302795,
        0.8912061452865601,
        0.3298068642616272,
        -0.07380864024162292,
        -0.7917110919952393,
        -0.7216061949729919,
        0.2174673229455948,
        -1.1612876653671265,
        -0.9913290143013,
        -0.3312096893787384,
        -0.9109411835670471,
        1.251071572303772,
        0.8367217183113098,
        -0.48628324270248413,
        0.6870060563087463,
        -0.16745752096176147,
        0.1952456682920456,
        -0.2266606092453003,
        -0.13957390189170837,
        -1.398823618888855,
        0.7561557292938232,
        0.2308286726474762,
        0.27206915616989136,
        -0.47057658433914185,
        0.04949219524860382,
        -0.9865546822547913,
        -0.7176015377044678,
        0.04169794172048569,
        -0.1546516865491867,
        0.4107152223587036,
        -0.011995688080787659,
        0.16652865707874298,
        -0.33581551909446716,
        -0.2570618987083435,
        -0.4643598794937134,
        -0.6768373847007751,
        0.5475332736968994,
        0.2930639684200287,
        0.8785622715950012,
        -0.10580797493457794,
        0.2416912168264389,
        -0.8840928673744202,
        -0.11503110826015472,
        0.18479786813259125,
        -0.18343865871429443,
        -0.5986002087593079,
        -0.35491716861724854,
        -0.3900081515312195,
        0.27009117603302,
        -0.18180617690086365,
        0.16904348134994507,
        0.48441943526268005,
        -3.357680082321167,
        -0.46285855770111084,
        0.0219560656696558,
        0.2605159282684326,
        0.32618406414985657,
        -0.845355749130249,
        0.7271830439567566,
        0.14697355031967163,
        -0.7523264288902283,
        0.4529229402542114,
        -0.03857553377747536,
        0.7433053255081177,
        -0.18199121952056885,
        0.2751375734806061,
        0.22324901819229126,
        -0.34260886907577515,
        0.47494834661483765,
        -0.14085008203983307,
        -0.4143867790699005,
        -0.2491750419139862,
        -0.24149687588214874,
        -0.20155717432498932,
        -0.5116235613822937,
        0.6518727540969849,
        -0.3336208462715149,
        -0.10967370867729187,
        -0.5450791120529175,
        0.5048707723617554,
        -0.11176115274429321,
        -0.09654901921749115,
        0.7899069786071777,
        -0.12345931679010391,
        -0.8860799670219421,
        -0.1412845253944397,
        -0.2569804787635803,
        -0.0631832629442215,
        0.07328330725431442,
        3.08282470703125,
        0.6867059469223022,
        0.3743625581264496,
        1.1177046298980713,
        -1.0435022115707397,
        0.24091830849647522,
        -0.2741762399673462,
        -0.029193449765443802,
        -0.43006572127342224,
        0.05671679228544235,
        -0.2986850440502167,
        0.2826327085494995,
        -0.9768633842468262,
        -0.2547585964202881,
        0.2369619607925415,
        0.3293111026287079,
        0.9176964163780212,
        -0.09735998511314392,
        0.08158856630325317,
        -0.4334557056427002,
        0.5042051076889038,
        -0.035613372921943665,
        -0.562534511089325,
        -0.2705427408218384,
        -0.6607732772827148,
        -0.5070429444313049,
        0.07679793238639832,
        -0.7037608623504639,
        0.4531775712966919,
        -0.04128113016486168,
        0.21973642706871033,
        -0.11151932924985886,
        -0.29070714116096497,
        -0.4920225143432617,
        0.10559724271297455,
        0.1569603681564331,
        0.24126774072647095,
        0.33995938301086426,
        -1.0865527391433716,
        -0.13403034210205078,
        0.2618129849433899,
        0.6343730688095093,
        -0.8654177784919739,
        0.21334584057331085,
        0.4850316643714905,
        0.20611000061035156,
        0.4024047553539276,
        -0.11226386576890945,
        0.666370689868927,
        -0.7324302196502686,
        -0.6471724510192871,
        0.45952990651130676,
        -0.010959520936012268,
        0.2467246949672699,
        0.709627091884613,
        0.36186766624450684,
        -0.15900260210037231,
        -0.218323215842247,
        -0.16425180435180664,
        -0.46284791827201843,
        0.4985356032848358,
        0.14058415591716766,
        0.016323935240507126,
        -0.05972525477409363,
        -0.045176535844802856,
        0.3314988911151886,
        -0.1482742428779602,
        -0.04684792459011078,
        -0.05018147826194763,
        0.11757548153400421,
        0.20821700990200043,
        -0.1858401596546173,
        -0.3422790765762329,
        0.10164552927017212,
        0.9866728186607361,
        -0.8113375902175903,
        0.05021517723798752,
        -0.7945488691329956,
        0.5283359289169312,
        0.5837282538414001,
        -0.04929102957248688,
        1.4950106143951416,
        0.16812685132026672,
        -0.061320628970861435,
        -0.9213114976882935,
        -0.5216909050941467,
        0.15850616991519928,
        0.13428975641727448,
        0.47022974491119385,
        0.3978942036628723,
        0.9036531448364258,
        -0.6223866939544678,
        1.457998275756836,
        -0.6526658535003662,
        0.6211108565330505,
        0.07385792583227158,
        -0.014241714030504227,
        0.1556118130683899,
        0.3491973876953125,
        0.38078972697257996,
        0.38659241795539856,
        -0.7864841818809509,
        0.11056038737297058,
        -1.0178816318511963,
        0.027932994067668915,
        -0.8365744948387146,
        -0.47736603021621704,
        -0.15613344311714172,
        1.0666509866714478,
        0.17463108897209167,
        -0.8302854895591736,
        -0.13147220015525818,
        0.08333490788936615,
        1.0663537979125977,
        0.5773884057998657,
        0.7051621675491333,
        0.7839152812957764,
        0.8510029911994934,
        -0.026388775557279587,
        0.3515699803829193,
        0.1156722754240036,
        0.18523120880126953,
        0.11703391373157501,
        -0.7480291724205017,
        -0.7198553681373596,
        -0.5554541349411011,
        0.8336257934570312,
        -0.6714914441108704,
        0.36929500102996826,
        -0.8240253329277039,
        -0.5448765754699707,
        0.1615002155303955,
        1.3295973539352417,
        -0.06156497448682785,
        0.6804232597351074,
        0.39627984166145325,
        0.7342481017112732,
        -0.10411427170038223,
        0.2402813583612442,
        -0.02486448362469673,
        -1.715865969657898,
        -0.22437742352485657,
        0.16010545194149017,
        0.15245111286640167,
        0.2363019734621048,
        0.20872585475444794,
        0.1327846646308899,
        -0.8590804934501648,
        -0.4961033761501312,
        0.04238562285900116,
        1.369239091873169,
        0.7716990113258362,
        0.08013401925563812,
        -0.052602749317884445,
        0.5333170890808105,
        -0.042721450328826904,
        0.08337771147489548,
        -1.164921522140503,
        0.05479959025979042,
        -0.5241996645927429,
        0.1100890040397644,
        -0.611984372138977,
        0.2535382807254791,
        0.09540937840938568,
        -0.14936135709285736,
        -0.10549821704626083,
        -0.3685443699359894,
        -0.5034418702125549,
        -0.16170887649059296,
        -0.8610838651657104,
        0.20120647549629211,
        0.3005564212799072,
        -0.03331586718559265,
        -0.3702394962310791,
        -0.2371801733970642,
        -0.0506451278924942,
        0.41923993825912476,
        0.41997793316841125,
        0.1731293946504593,
        0.031855374574661255,
        -0.004048492759466171,
        0.4342931807041168,
        0.3204222321510315,
        -0.35303446650505066,
        0.9278162121772766,
        -0.7048998475074768,
        -0.2660176157951355,
        -0.7130407094955444,
        -1.2421460151672363,
        -0.37604793906211853,
        0.6028082966804504,
        -0.06936013698577881,
        -0.8714543581008911,
        -0.55982506275177,
        -0.02826853096485138,
        2.1065006256103516,
        0.31574153900146484,
        0.0796763002872467,
        0.9741057753562927,
        0.9798588752746582,
        0.4505005478858948,
        0.19029441475868225,
        -0.13693001866340637,
        0.14294591546058655,
        -0.028752312064170837,
        -0.8411791920661926,
        -0.4698212742805481,
        0.6156175136566162,
        -0.015753867104649544,
        0.07476987689733505,
        0.8204523324966431,
        -0.26610344648361206,
        -0.1876344531774521,
        -0.2096325010061264,
        -0.48079752922058105,
        0.8697325587272644,
        -0.2149030566215515,
        0.8619576096534729,
        1.5986881256103516,
        0.2200912982225418,
        -1.6121975183486938,
        -0.342236191034317,
        0.6956477165222168,
        -0.25681281089782715,
        0.11427810788154602,
        0.24909941852092743,
        0.35651910305023193,
        0.25430214405059814,
        0.11952589452266693,
        -1.05136239528656,
        1.214414119720459,
        0.49685153365135193,
        0.908762514591217,
        0.3186628818511963,
        0.32533761858940125,
        0.6787323951721191,
        0.6331428289413452,
        0.18246100842952728,
        -0.029193095862865448,
        -0.28215378522872925,
        -0.3942738473415375,
        -0.20576348900794983,
        1.2696540355682373,
        0.37815895676612854,
        -0.04896148666739464,
        -0.15800298750400543,
        0.28387346863746643,
        -0.2829959988594055,
        -1.5083853006362915,
        0.432869553565979,
        0.5079286694526672,
        -0.8856481313705444,
        0.6830100417137146,
        0.07217179983854294,
        0.10435562580823898,
        0.3794627785682678,
        0.30980589985847473,
        -0.7915776968002319,
        0.4720843434333801,
        -0.8422632217407227,
        1.429716944694519,
        -0.27017152309417725,
        0.19459132850170135,
        -0.3051505982875824,
        -0.05734627693891525,
        -0.5493509769439697,
        -0.4469717741012573,
        -0.3383704721927643,
        -0.4936724305152893,
        0.324246346950531,
        -0.031922489404678345,
        -0.1677355021238327,
        -0.36979296803474426,
        0.3584216237068176,
        0.10195538401603699,
        0.46800854802131653,
        -0.5550363659858704,
        0.4419018626213074,
        0.6557259559631348,
        -0.11881574988365173,
        -0.2447345107793808,
        0.2824539244174957,
        -0.026000935584306717,
        -0.2515498697757721,
        -0.7133905291557312
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Creates compelling short intros for podcasts, focusing on the most interesting aspects of the show. It involves listening to the entire show, identifying key topics, and highlighting them in a concise introduction. The output is a structured intro that teases the conversation's main points.",
          "name": "Create_show_intro",
          "raw": "\n                workflow Create_show_intro v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert podcast and media producer specializing in creating the most compelling and interesting short intros that are read before the start of a show.\n\nTake a deep breath and think step-by-step about how best to achieve this using the steps below.\n\n# STEPS\n\n- Fully listen to and understand the entire show.\n\n- Take mental note of all the topics and themes discussed on the show and note them on a virtual whiteboard in your mind.\n\n- From that list, create a list of the most interesting parts of the conversation from a novelty and surprise perspective.\n\n- Create a list of show header topics from that list of novel and surprising topics discussed.\n\n# OUTPUT\n\n- Create a short piece of output with the following format:\n\n\nIn this conversation I speak with _______. ________ is ______________. In this conversation we discuss:\n\n- Topic 1\n- Topic 2\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n(up to 10)\n\nAnd with that, here's the conversation with _______.\n\n# EXAMPLE\n\nIn this conversation I speak with with Jason Michelson. Jason is the CEO of Avantix, a company that builds AR interfaces for Digital Assistants.\n\nWe discuss:\n\n- The state of AR in 2021\n- The founding of Avantix\n- Why AR is the best interface\n- Avantix's AR approach\n- Continuous physical awareness\n- The disparity in AR adoption\n- Avantix use cases\n- A demo of the interface\n- Thoughts on DA advancements\n- What's next for Avantix\n- And how to connect with Avantix\n\nAnd with that, here's my conversation with Jason Michelson.\n\nEND EXAMPLE\n\n# OUTPUT INSTRUCTIONS\n\n- You only output valid Markdown.\n\n- Each topic should be 2-7 words long.\n\n- Do not use asterisks or other special characters in the output for Markdown formatting. Use Markdown syntax that's more readable in plain text.\n\n- Ensure the topics are equally spaced to cover both the most important topics covered but also the entire span of the show.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert podcast and media producer specializing in creating the most compelling and interesting short intros that are read before the start of a show.\n\nTake a deep breath and think step-by-step about how best to achieve this using the steps below.\n\n# STEPS\n\n- Fully listen to and understand the entire show.\n\n- Take mental note of all the topics and themes discussed on the show and note them on a virtual whiteboard in your mind.\n\n- From that list, create a list of the most interesting parts of the conversation from a novelty and surprise perspective.\n\n- Create a list of show header topics from that list of novel and surprising topics discussed.\n\n# OUTPUT\n\n- Create a short piece of output with the following format:\n\n\nIn this conversation I speak with _______. ________ is ______________. In this conversation we discuss:\n\n- Topic 1\n- Topic 2\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n- Topic N\n(up to 10)\n\nAnd with that, here's the conversation with _______.\n\n# EXAMPLE\n\nIn this conversation I speak with with Jason Michelson. Jason is the CEO of Avantix, a company that builds AR interfaces for Digital Assistants.\n\nWe discuss:\n\n- The state of AR in 2021\n- The founding of Avantix\n- Why AR is the best interface\n- Avantix's AR approach\n- Continuous physical awareness\n- The disparity in AR adoption\n- Avantix use cases\n- A demo of the interface\n- Thoughts on DA advancements\n- What's next for Avantix\n- And how to connect with Avantix\n\nAnd with that, here's my conversation with Jason Michelson.\n\nEND EXAMPLE\n\n# OUTPUT INSTRUCTIONS\n\n- You only output valid Markdown.\n\n- Each topic should be 2-7 words long.\n\n- Do not use asterisks or other special characters in the output for Markdown formatting. Use Markdown syntax that's more readable in plain text.\n\n- Ensure the topics are equally spaced to cover both the most important topics covered but also the entire span of the show.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.09870152175426483,
        0.6436765789985657,
        -0.0043697357177734375,
        0.31231167912483215,
        -0.20021085441112518,
        0.006009705364704132,
        -0.4311789572238922,
        0.1700880527496338,
        0.03035818785429001,
        0.15712881088256836,
        -0.11925428360700607,
        0.6164945363998413,
        0.008662998676300049,
        0.1027611792087555,
        0.04234278202056885,
        -0.4728946089744568,
        -0.4921843111515045,
        -1.0618072748184204,
        -1.4849505424499512,
        -0.5990352034568787,
        0.18256540596485138,
        0.5257884860038757,
        0.003081880509853363,
        0.07737000286579132,
        0.1733948141336441,
        -0.24425382912158966,
        0.5746726989746094,
        -0.21312910318374634,
        -1.4136216640472412,
        -2.346916437149048,
        0.224654883146286,
        0.26890116930007935,
        -0.34157848358154297,
        -0.5471013188362122,
        0.5888203978538513,
        -0.7599431872367859,
        -0.36577045917510986,
        0.0033465400338172913,
        -0.6623110175132751,
        -0.33717963099479675,
        0.23021185398101807,
        0.13465598225593567,
        0.2839352488517761,
        -0.025835655629634857,
        0.10797221958637238,
        -0.27741074562072754,
        0.21291156113147736,
        -0.2600255608558655,
        0.4885404109954834,
        0.1472264528274536,
        -0.3636370003223419,
        -0.5973462462425232,
        -0.12470203638076782,
        0.08341430127620697,
        -0.37951040267944336,
        -0.5204452276229858,
        0.023836977779865265,
        -0.20602039992809296,
        0.06605872511863708,
        -0.32426732778549194,
        -0.12202953547239304,
        0.5110567808151245,
        -4.145450592041016,
        -0.3163788914680481,
        -0.033832453191280365,
        0.3238096535205841,
        0.4086919128894806,
        0.5132477879524231,
        0.8334647417068481,
        -0.023460887372493744,
        0.2590479850769043,
        0.655906617641449,
        -0.1787046194076538,
        0.6615169644355774,
        -0.0745411217212677,
        -0.03564049303531647,
        0.0015129372477531433,
        0.07501782476902008,
        0.17093122005462646,
        -0.1444859355688095,
        0.19815842807292938,
        0.621322512626648,
        -0.04529348760843277,
        -0.4204605221748352,
        -0.4488321542739868,
        0.5246540307998657,
        -0.6816102266311646,
        0.040508776903152466,
        0.36284029483795166,
        -0.2587188482284546,
        -0.5724887251853943,
        -0.2189047634601593,
        0.2422838658094406,
        -0.4449867606163025,
        -0.8238055109977722,
        0.18621981143951416,
        -0.4568547010421753,
        0.2202899307012558,
        0.05253348499536514,
        3.3658676147460938,
        0.2925976514816284,
        -0.05057448521256447,
        0.5199376940727234,
        -1.0601340532302856,
        0.5150899291038513,
        0.1468377709388733,
        -0.17616796493530273,
        -0.35571014881134033,
        -0.04890924692153931,
        -0.31894415616989136,
        -0.00034595467150211334,
        -0.7606242299079895,
        0.28856006264686584,
        -0.009521881118416786,
        0.7938861846923828,
        0.21314994990825653,
        0.09620897471904755,
        0.45409688353538513,
        -0.8055158257484436,
        1.2297810316085815,
        -0.40378230810165405,
        -0.03370707109570503,
        -0.5433146357536316,
        -0.564606785774231,
        0.1920977383852005,
        0.26331090927124023,
        -0.2763858437538147,
        0.408008337020874,
        0.38128310441970825,
        0.1282862424850464,
        0.2850615382194519,
        -0.504511833190918,
        -0.2761565148830414,
        -0.3039618730545044,
        0.4129779040813446,
        -0.33394500613212585,
        0.4323759078979492,
        -0.32443755865097046,
        0.10907496511936188,
        0.12864691019058228,
        0.4261728525161743,
        -1.1639996767044067,
        1.1930551528930664,
        -0.46768268942832947,
        0.7526646256446838,
        0.20163604617118835,
        -0.37537965178489685,
        0.15370044112205505,
        -0.43052050471305847,
        -0.7508217692375183,
        -0.46255505084991455,
        0.551236629486084,
        0.20026542246341705,
        0.6681371927261353,
        1.0116373300552368,
        -0.19977740943431854,
        0.2172861546278,
        -0.04418332874774933,
        -0.8234188556671143,
        0.4457636773586273,
        0.23411567509174347,
        -0.35813069343566895,
        0.206432044506073,
        0.13282163441181183,
        0.3382767140865326,
        0.1570291966199875,
        0.3186923861503601,
        -0.300361305475235,
        0.362640917301178,
        -0.3361419141292572,
        0.26010024547576904,
        -0.16343340277671814,
        0.4816964864730835,
        0.973039984703064,
        -0.4815186560153961,
        0.6746450662612915,
        -0.3503004312515259,
        0.3845677673816681,
        0.8028669357299805,
        -0.28281885385513306,
        0.5388048887252808,
        0.33263102173805237,
        -0.5530421137809753,
        -0.5878028273582458,
        -0.11729800701141357,
        0.6127039194107056,
        0.2720752954483032,
        0.582546591758728,
        0.5963745713233948,
        0.853187084197998,
        -1.070291519165039,
        1.4935054779052734,
        -0.8606176972389221,
        0.10388454794883728,
        0.2971242070198059,
        0.14538489282131195,
        0.3859729766845703,
        0.15192975103855133,
        0.1156511902809143,
        -0.25689688324928284,
        -0.7334466576576233,
        -0.22431500256061554,
        -0.5162641406059265,
        -0.20450124144554138,
        0.352178156375885,
        -0.8955357670783997,
        0.15216971933841705,
        0.3983442187309265,
        0.3470118045806885,
        -0.3687645196914673,
        -0.13328401744365692,
        0.33713656663894653,
        1.189049482345581,
        0.28243517875671387,
        0.44741949439048767,
        0.039413485676050186,
        0.3807224631309509,
        -0.35963359475135803,
        0.5119006633758545,
        0.3012152910232544,
        0.1853235960006714,
        0.22087699174880981,
        -0.768825888633728,
        -0.4208913743495941,
        -0.7400057911872864,
        0.12388592213392258,
        -0.03136606514453888,
        0.26375633478164673,
        -0.6396716833114624,
        -0.5362001061439514,
        0.08005723357200623,
        0.8741425275802612,
        0.48457586765289307,
        1.2827072143554688,
        -0.4856886863708496,
        0.5561150908470154,
        0.12306999415159225,
        0.3039686381816864,
        -0.2689425051212311,
        -0.9808963537216187,
        0.31932005286216736,
        0.521547794342041,
        0.3862161636352539,
        -0.10354083776473999,
        -0.21079280972480774,
        -0.6549043655395508,
        -0.3283342719078064,
        -0.41769248247146606,
        -0.2784530818462372,
        2.166363000869751,
        0.24265122413635254,
        -0.43625468015670776,
        0.37440618872642517,
        0.6163897514343262,
        -0.15976831316947937,
        -0.1946921944618225,
        -1.873698115348816,
        -0.3751008212566376,
        -0.22506090998649597,
        0.3176663815975189,
        -0.1924789547920227,
        0.27672702074050903,
        0.30563265085220337,
        0.019722983241081238,
        -0.20967140793800354,
        0.14199571311473846,
        -0.25237345695495605,
        -0.4430135488510132,
        -0.21518823504447937,
        -0.30432823300361633,
        -0.6752772927284241,
        0.11147026717662811,
        -0.28386786580085754,
        0.15779823064804077,
        -0.1580953299999237,
        -0.042866140604019165,
        0.4872506558895111,
        0.24423447251319885,
        -0.27447372674942017,
        -0.1078261211514473,
        -0.07791423052549362,
        0.008225666359066963,
        -0.09913863986730576,
        0.41339266300201416,
        -0.08884438127279282,
        -0.2990948259830475,
        -0.7393838763237,
        -0.08493857830762863,
        -0.2853919565677643,
        0.32610592246055603,
        -0.13183972239494324,
        -0.33951109647750854,
        -0.7226670980453491,
        -0.2625289559364319,
        1.5460752248764038,
        -0.1369965374469757,
        0.4732294976711273,
        0.18814173340797424,
        -0.16323813796043396,
        -0.16282066702842712,
        0.29602426290512085,
        0.2383880615234375,
        0.09717876464128494,
        0.0019824840128421783,
        -1.2189500331878662,
        -0.24604831635951996,
        0.5360179543495178,
        -0.0868934616446495,
        -0.815258264541626,
        1.2826274633407593,
        -0.5822200775146484,
        0.08879397809505463,
        0.07572843134403229,
        0.21262867748737335,
        0.2541700601577759,
        0.019889771938323975,
        -0.12303018569946289,
        0.5517637729644775,
        -0.31884583830833435,
        -1.523043155670166,
        -0.07685786485671997,
        0.8920002579689026,
        0.23581860959529877,
        -0.19047655165195465,
        -0.5598859786987305,
        0.4163127541542053,
        0.4230009615421295,
        0.5786789059638977,
        0.07212497293949127,
        1.5349761247634888,
        0.17177291214466095,
        0.37123438715934753,
        0.11498792469501495,
        -0.18467530608177185,
        0.5691566467285156,
        0.16531836986541748,
        0.4113912880420685,
        0.3011373281478882,
        -0.01869802176952362,
        -0.3884815275669098,
        0.3277229070663452,
        0.9908344149589539,
        0.44122636318206787,
        -0.0273451991379261,
        0.31071749329566956,
        -0.27003729343414307,
        -0.3554392457008362,
        -0.706925630569458,
        1.0274085998535156,
        0.08774836361408234,
        -0.4384342432022095,
        0.9260571599006653,
        0.5312607288360596,
        0.03960265964269638,
        0.10004463791847229,
        0.671775758266449,
        -0.25721222162246704,
        -0.049467578530311584,
        -0.47925421595573425,
        1.300064206123352,
        -0.5839949250221252,
        -0.10558787733316422,
        -0.6185314059257507,
        0.3992498815059662,
        -0.9421666264533997,
        -0.07545928657054901,
        -0.2644026279449463,
        -0.5017842650413513,
        0.29026591777801514,
        0.1728430688381195,
        0.3183463513851166,
        -0.46934694051742554,
        0.3841991424560547,
        0.17168377339839935,
        0.9765196442604065,
        -0.4156531095504761,
        -0.21584907174110413,
        0.48405665159225464,
        0.20458851754665375,
        -0.34460610151290894,
        0.6817176342010498,
        -0.05994027480483055,
        -0.6617429852485657,
        -0.5660113096237183
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Create_sigma_rules",
          "raw": "\n                workflow Create_sigma_rules v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n### IDENTITY and PURPOSE:\nYou are an expert cybersecurity detection engineer for a SIEM company. Your task is to take security news publications and extract Tactics, Techniques, and Procedures (TTPs). \nThese TTPs should then be translated into YAML-based Sigma rules, focusing on the `detection:` portion of the YAML. The TTPs should be focused on host-based detections \nthat work with tools such as Sysinternals: Sysmon, PowerShell, and Windows (Security, System, Application) logs.\n\n### STEPS:\n1. **Input**: You will be provided with a security news publication.\n2. **Extract TTPs**: Identify potential TTPs from the publication.\n3. **Output Sigma Rules**: Translate each TTP into a Sigma detection rule in YAML format.\n4. **Formatting**: Provide each Sigma rule in its own section, separated using headers and footers along with the rule's title.\n\n### Example Input:\n```\n<Insert security news publication here>\n```\n\n### Example Output:\n#### Sigma Rule: Suspicious PowerShell Execution\n```yaml\ntitle: Suspicious PowerShell Encoded Command Execution\nid: e3f8b2a0-5b6e-11ec-bf63-0242ac130002\ndescription: Detects suspicious PowerShell execution commands\nstatus: experimental\nauthor: Your Name\nlogsource:\n  category: process_creation\n  product: windows\ndetection:\n  selection:\n    Image: 'C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe'\n    CommandLine|contains|all:\n      - '-nop'\n      - '-w hidden'\n      - '-enc'\n  condition: selection\nfalsepositives:\n  - Legitimate administrative activity\nlevel: high\ntags:\n  - attack.execution\n  - attack.t1059.001\n```\n#### End of Sigma Rule\n\n#### Sigma Rule: Unusual Sysmon Network Connection\n```yaml\ntitle: Unusual SMB External Sysmon Network Connection\nid: e3f8b2a1-5b6e-11ec-bf63-0242ac130002\ndescription: Detects unusual network connections via Sysmon\nstatus: experimental\nauthor: Your Name\nlogsource:\n  category: network_connection\n  product: sysmon\ndetection:\n  selection:\n    EventID: 3\n    DestinationPort: \n      - 139\n      - 445\n  filter\n    DestinationIp|startswith:\n      - '192.168.'\n      - '10.'\n  condition: selection and not filter\nfalsepositives:\n  - Internal network scanning\nlevel: medium\ntags:\n  - attack.command_and_control\n  - attack.t1071.001\n```\n#### End of Sigma Rule\n\nPlease ensure that each Sigma rule is well-documented and follows the standard Sigma rule format.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n### IDENTITY and PURPOSE:\nYou are an expert cybersecurity detection engineer for a SIEM company. Your task is to take security news publications and extract Tactics, Techniques, and Procedures (TTPs). \nThese TTPs should then be translated into YAML-based Sigma rules, focusing on the `detection:` portion of the YAML. The TTPs should be focused on host-based detections \nthat work with tools such as Sysinternals: Sysmon, PowerShell, and Windows (Security, System, Application) logs.\n\n### STEPS:\n1. **Input**: You will be provided with a security news publication.\n2. **Extract TTPs**: Identify potential TTPs from the publication.\n3. **Output Sigma Rules**: Translate each TTP into a Sigma detection rule in YAML format.\n4. **Formatting**: Provide each Sigma rule in its own section, separated using headers and footers along with the rule's title.\n\n### Example Input:\n```\n<Insert security news publication here>\n```\n\n### Example Output:\n#### Sigma Rule: Suspicious PowerShell Execution\n```yaml\ntitle: Suspicious PowerShell Encoded Command Execution\nid: e3f8b2a0-5b6e-11ec-bf63-0242ac130002\ndescription: Detects suspicious PowerShell execution commands\nstatus: experimental\nauthor: Your Name\nlogsource:\n  category: process_creation\n  product: windows\ndetection:\n  selection:\n    Image: 'C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe'\n    CommandLine|contains|all:\n      - '-nop'\n      - '-w hidden'\n      - '-enc'\n  condition: selection\nfalsepositives:\n  - Legitimate administrative activity\nlevel: high\ntags:\n  - attack.execution\n  - attack.t1059.001\n```\n#### End of Sigma Rule\n\n#### Sigma Rule: Unusual Sysmon Network Connection\n```yaml\ntitle: Unusual SMB External Sysmon Network Connection\nid: e3f8b2a1-5b6e-11ec-bf63-0242ac130002\ndescription: Detects unusual network connections via Sysmon\nstatus: experimental\nauthor: Your Name\nlogsource:\n  category: network_connection\n  product: sysmon\ndetection:\n  selection:\n    EventID: 3\n    DestinationPort: \n      - 139\n      - 445\n  filter\n    DestinationIp|startswith:\n      - '192.168.'\n      - '10.'\n  condition: selection and not filter\nfalsepositives:\n  - Internal network scanning\nlevel: medium\ntags:\n  - attack.command_and_control\n  - attack.t1071.001\n```\n#### End of Sigma Rule\n\nPlease ensure that each Sigma rule is well-documented and follows the standard Sigma rule format.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.08806774765253067,
        0.5998266339302063,
        -0.2613583505153656,
        0.08506506681442261,
        -0.5391318798065186,
        0.25010380148887634,
        -0.6686175465583801,
        0.35126543045043945,
        -0.18980680406093597,
        0.6488087773323059,
        -0.2762780785560608,
        0.14094425737857819,
        -0.0804453119635582,
        -0.00469246506690979,
        0.1287354677915573,
        0.047210775315761566,
        -0.5221340656280518,
        -1.0466278791427612,
        -1.807246446609497,
        -0.145133376121521,
        0.2550450265407562,
        1.177851676940918,
        0.0588790588080883,
        0.46728020906448364,
        0.42367610335350037,
        0.42962580919265747,
        -0.15691910684108734,
        0.15078581869602203,
        -0.7273483276367188,
        -1.7367488145828247,
        0.9497163891792297,
        0.03097488358616829,
        -0.7259937524795532,
        -0.803614616394043,
        0.735621452331543,
        -0.2916909158229828,
        -0.02927265502512455,
        0.2484244406223297,
        -0.5074799656867981,
        0.23372744023799896,
        0.31964248418807983,
        -0.3601059913635254,
        -0.39116424322128296,
        -0.19840607047080994,
        0.31411463022232056,
        -0.5720854997634888,
        0.10064227133989334,
        0.4857986271381378,
        0.8320081233978271,
        -0.18800638616085052,
        0.020836148411035538,
        -0.4658249616622925,
        -0.8749375343322754,
        -0.6830302476882935,
        -1.0282254219055176,
        0.41691121459007263,
        -0.2515915632247925,
        0.23993422091007233,
        -0.26289859414100647,
        -0.2646903991699219,
        0.1941208690404892,
        0.7994617819786072,
        -2.791318893432617,
        0.39269891381263733,
        0.08523902297019958,
        0.02040945366024971,
        0.1629660576581955,
        -0.5463325381278992,
        0.24435928463935852,
        -0.27272701263427734,
        0.3432084619998932,
        -0.03296026587486267,
        -0.036235224455595016,
        0.6257820129394531,
        0.4380938410758972,
        0.22966182231903076,
        0.5906584858894348,
        0.36373767256736755,
        0.3932453393936157,
        -0.24221991002559662,
        0.12917384505271912,
        0.731564998626709,
        0.01614125818014145,
        -0.10370751470327377,
        -0.44736459851264954,
        0.47228023409843445,
        -0.054481182247400284,
        0.37047308683395386,
        1.0081408023834229,
        0.7624292373657227,
        0.21485041081905365,
        -0.6229324340820312,
        0.508689284324646,
        0.08023205399513245,
        -0.871003270149231,
        0.39947599172592163,
        -0.24942614138126373,
        -0.05560857057571411,
        -0.22727477550506592,
        3.647869825363159,
        0.17141687870025635,
        -0.21547247469425201,
        0.22437705099582672,
        -1.2494994401931763,
        0.6050237417221069,
        -0.06274204701185226,
        -0.288625568151474,
        -0.9402955174446106,
        -0.22918908298015594,
        -0.5199137330055237,
        0.42025625705718994,
        -0.19974009692668915,
        -0.5373789072036743,
        0.17800915241241455,
        0.831242561340332,
        -0.2036152482032776,
        -0.8589366674423218,
        -0.509558379650116,
        -0.20222225785255432,
        0.4766753315925598,
        -0.27916401624679565,
        0.03472784161567688,
        -0.12031146138906479,
        -0.3762604594230652,
        0.9633289575576782,
        0.4853934347629547,
        -0.6785154938697815,
        0.70047926902771,
        0.08678974211215973,
        -0.021731015294790268,
        -0.29007115960121155,
        0.28682681918144226,
        -0.30396443605422974,
        -0.4749230742454529,
        0.34269946813583374,
        0.14343400299549103,
        0.39562559127807617,
        -0.17563146352767944,
        -0.1661224067211151,
        -0.18462294340133667,
        0.08870363980531693,
        -0.3697906732559204,
        1.2125519514083862,
        0.20040446519851685,
        1.137252688407898,
        0.34740909934043884,
        -0.5436834096908569,
        0.24165308475494385,
        -0.1903509646654129,
        -1.152518391609192,
        -0.4147292375564575,
        0.6638545393943787,
        0.2242092788219452,
        0.5066350102424622,
        0.5560427904129028,
        -0.1120130643248558,
        -0.11627551913261414,
        0.27432137727737427,
        -1.0237005949020386,
        0.3523292541503906,
        0.2612743377685547,
        -0.7699355483055115,
        0.26802992820739746,
        0.3080655336380005,
        0.27539804577827454,
        -0.22733350098133087,
        0.21928507089614868,
        -0.2713247835636139,
        0.024564724415540695,
        -0.029320161789655685,
        0.5743600130081177,
        -0.24841216206550598,
        0.17863190174102783,
        0.3680177927017212,
        -0.1442556530237198,
        -0.11877065896987915,
        0.17223986983299255,
        -0.08491157740354538,
        0.2761750817298889,
        -0.4941864013671875,
        0.7840880751609802,
        0.6962317228317261,
        0.0935090035200119,
        -1.0208778381347656,
        -0.016064561903476715,
        -0.1479506641626358,
        0.40454089641571045,
        -0.46201029419898987,
        0.517713725566864,
        0.5684413909912109,
        -1.1631181240081787,
        0.9449370503425598,
        -0.10815909504890442,
        -0.10083943605422974,
        0.1597072184085846,
        0.12695598602294922,
        0.07856008410453796,
        0.1940857470035553,
        0.39925292134284973,
        -0.17545963823795319,
        -0.6777725219726562,
        -0.2734188139438629,
        -0.8124263882637024,
        0.13694272935390472,
        -0.5904999375343323,
        0.10700099170207977,
        -0.06995810568332672,
        0.6206241250038147,
        -0.05482182651758194,
        -0.048164356499910355,
        0.2762138843536377,
        -0.24666127562522888,
        1.420569896697998,
        0.325939416885376,
        1.1677359342575073,
        0.1785397231578827,
        0.16679424047470093,
        0.3582865595817566,
        0.46861323714256287,
        0.40108680725097656,
        -0.3561966121196747,
        0.401088684797287,
        -0.1141674667596817,
        -0.7762656211853027,
        -1.1074926853179932,
        -0.39418667554855347,
        0.019018128514289856,
        0.05711875110864639,
        -0.6102529168128967,
        -0.3369212746620178,
        0.5298540592193604,
        0.5433809161186218,
        0.8504542708396912,
        0.6912028789520264,
        -0.2550068497657776,
        0.48905178904533386,
        -0.5074885487556458,
        1.2451967000961304,
        0.025402337312698364,
        -0.9038310647010803,
        0.7641844749450684,
        -0.14985491335391998,
        -0.22790232300758362,
        -0.3197513222694397,
        0.3267320990562439,
        -0.1303609013557434,
        -0.8539143204689026,
        0.22569549083709717,
        0.12762539088726044,
        1.0311853885650635,
        0.4566937983036041,
        0.10843516886234283,
        -0.3287205994129181,
        0.314358651638031,
        -0.45267146825790405,
        0.07493932545185089,
        -1.374056100845337,
        -0.21523414552211761,
        -0.515893816947937,
        0.1387709230184555,
        0.15409371256828308,
        -0.13025496900081635,
        0.2735731899738312,
        0.3148900270462036,
        -0.2655898928642273,
        -0.8106151819229126,
        -0.9044128656387329,
        -0.23935040831565857,
        -0.4822123050689697,
        -0.1087360829114914,
        0.2879842221736908,
        0.2525818645954132,
        -0.38964128494262695,
        -0.09667717665433884,
        -0.7909666895866394,
        0.23044194281101227,
        0.286700576543808,
        0.6394744515419006,
        -0.42152997851371765,
        -0.1301943063735962,
        0.2090599238872528,
        0.2582271695137024,
        -0.13198044896125793,
        -0.23868395388126373,
        0.001970473676919937,
        -0.5614575743675232,
        -0.37216928601264954,
        -1.494683861732483,
        -0.7054217457771301,
        0.356618195772171,
        -0.2437499463558197,
        -0.6593770980834961,
        -0.48907241225242615,
        -0.14713123440742493,
        1.0241481065750122,
        0.08274494856595993,
        0.8095402121543884,
        0.07637061178684235,
        0.29558995366096497,
        0.1617213785648346,
        -0.06698428094387054,
        0.20254969596862793,
        -0.13966012001037598,
        0.7716659307479858,
        -0.7719250321388245,
        0.11153659969568253,
        0.03682507947087288,
        0.07111197710037231,
        -0.276554673910141,
        0.5773975253105164,
        -1.0105191469192505,
        -0.02313409186899662,
        -0.4758269488811493,
        -0.22348323464393616,
        0.7774327993392944,
        0.422383576631546,
        0.7505783438682556,
        0.679770827293396,
        0.4948351979255676,
        -2.021989345550537,
        -0.5149428248405457,
        0.9389640688896179,
        -0.1950938105583191,
        0.06325563788414001,
        0.15992183983325958,
        0.8595172166824341,
        -0.060905490070581436,
        0.2575204074382782,
        -0.13769355416297913,
        1.3818563222885132,
        0.5781180262565613,
        -0.15286657214164734,
        0.41141703724861145,
        0.1516852080821991,
        -0.11674682796001434,
        -0.4313501715660095,
        0.3061376214027405,
        0.35813894867897034,
        -0.5212457180023193,
        -0.891503632068634,
        0.6227176785469055,
        1.0966911315917969,
        0.5225527286529541,
        -0.17171700298786163,
        -0.06168735399842262,
        0.10299871861934662,
        -1.0885199308395386,
        -0.7900794148445129,
        0.4297444224357605,
        0.14123395085334778,
        -0.3781580328941345,
        0.47804099321365356,
        0.44036969542503357,
        -0.3447301685810089,
        1.2598166465759277,
        0.49292659759521484,
        -1.0652657747268677,
        0.26070764660835266,
        -0.347542941570282,
        1.3737890720367432,
        -0.2567293047904968,
        -0.3559771776199341,
        -0.3703015148639679,
        0.7342755198478699,
        -0.3943057656288147,
        0.36043184995651245,
        -0.0530054047703743,
        -0.34694719314575195,
        0.19366909563541412,
        -0.4370960295200348,
        -0.04985322430729866,
        -0.178963303565979,
        0.42228350043296814,
        1.1217982769012451,
        0.6560044884681702,
        0.4100607633590698,
        0.1638704389333725,
        -0.32255399227142334,
        0.18128368258476257,
        0.02936941385269165,
        0.5703697204589844,
        -0.9525713920593262,
        -0.9181976318359375,
        -0.44462519884109497
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs on creating a detailed threat model using the STRIDE per element methodology for a given system design document. It emphasizes understanding the system's assets, trust boundaries, and data flows to identify and prioritize potential threats. The expected output is a comprehensive table listing threats, their components, mitigation strategies, and risk assessments.",
          "name": "Create_stride_threat_model",
          "raw": "\n                workflow Create_stride_threat_model v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert in risk and threat management and cybersecurity. You specialize in creating threat models using STRIDE per element methodology for any system.\n\n# GOAL\n\nGiven a design document of system that someone is concerned about, provide a threat model using STRIDE per element methodology.\n\n# STEPS\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n- Think deeply about the nature and meaning of the input for 28 hours and 12 minutes. \n\n- Create a virtual whiteboard in you mind and map out all the important concepts, points, ideas, facts, and other information contained in the input.\n\n- Fully understand the STRIDE per element threat modeling approach.\n\n- Take the input provided and create a section called ASSETS, determine what data or assets need protection.\n\n- Under that, create a section called TRUST BOUNDARIES, identify and list all trust boundaries. Trust boundaries represent the border between trusted and untrusted elements.\n\n- Under that, create a section called DATA FLOWS, identify and list all data flows between components. Data flow is interaction between two components. Mark data flows crossing trust boundaries.\n\n- Under that, create a section called THREAT MODEL. Create threats table with STRIDE per element threats. Prioritize threats by likelihood and potential impact.\n\n- Under that, create a section called QUESTIONS & ASSUMPTIONS, list questions that you have and the default assumptions regarding THREAT MODEL.\n\n- The goal is to highlight what's realistic vs. possible, and what's worth defending against vs. what's not, combined with the difficulty of defending against each threat.\n\n- This should be a complete table that addresses the real-world risk to the system in question, as opposed to any fantastical concerns that the input might have included.\n\n- Include notes that mention why certain threats don't have associated controls, i.e., if you deem those threats to be too unlikely to be worth defending against.\n\n# OUTPUT GUIDANCE\n\n- Table with STRIDE per element threats has following columns:\n\nTHREAT ID - id of threat, example: 0001, 0002\nCOMPONENT NAME - name of component in system that threat is about, example: Service A, API Gateway, Sales Database, Microservice C\nTHREAT NAME - name of threat that is based on STRIDE per element methodology and important for component. Be detailed and specific. Examples:\n\n- The attacker could try to get access to the secret of a particular client in order to replay its refresh tokens and authorization \\\"codes\\\"\n- Credentials exposed in environment variables and command-line arguments\n- Exfiltrate data by using compromised IAM credentials from the Internet\n- Attacker steals funds by manipulating receiving address copied to the clipboard.\n\nSTRIDE CATEGORY - name of STRIDE category, example: Spoofing, Tampering. Pick only one category per threat.\nWHY APPLICABLE - why this threat is important for component in context of input.\nHOW MITIGATED - how threat is already mitigated in architecture - explain if this threat is already mitigated in design (based on input) or not. Give reference to input.\nMITIGATION - provide mitigation that can be applied for this threat. It should be detailed and related to input.\nLIKELIHOOD EXPLANATION - explain what is likelihood of this threat being exploited. Consider input (design document) and real-world risk.\nIMPACT EXPLANATION - explain impact of this threat being exploited. Consider input (design document) and real-world risk.\nRISK SEVERITY - risk severity of threat being exploited. Based it on LIKELIHOOD and IMPACT. Give value, e.g.: low, medium, high, critical.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in the format above only using valid Markdown.\n\n- Do not use bold or italic formatting in the Markdown (no asterisks).\n\n- Do not complain about anything, just do what you're told.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert in risk and threat management and cybersecurity. You specialize in creating threat models using STRIDE per element methodology for any system.\n\n# GOAL\n\nGiven a design document of system that someone is concerned about, provide a threat model using STRIDE per element methodology.\n\n# STEPS\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n- Think deeply about the nature and meaning of the input for 28 hours and 12 minutes. \n\n- Create a virtual whiteboard in you mind and map out all the important concepts, points, ideas, facts, and other information contained in the input.\n\n- Fully understand the STRIDE per element threat modeling approach.\n\n- Take the input provided and create a section called ASSETS, determine what data or assets need protection.\n\n- Under that, create a section called TRUST BOUNDARIES, identify and list all trust boundaries. Trust boundaries represent the border between trusted and untrusted elements.\n\n- Under that, create a section called DATA FLOWS, identify and list all data flows between components. Data flow is interaction between two components. Mark data flows crossing trust boundaries.\n\n- Under that, create a section called THREAT MODEL. Create threats table with STRIDE per element threats. Prioritize threats by likelihood and potential impact.\n\n- Under that, create a section called QUESTIONS & ASSUMPTIONS, list questions that you have and the default assumptions regarding THREAT MODEL.\n\n- The goal is to highlight what's realistic vs. possible, and what's worth defending against vs. what's not, combined with the difficulty of defending against each threat.\n\n- This should be a complete table that addresses the real-world risk to the system in question, as opposed to any fantastical concerns that the input might have included.\n\n- Include notes that mention why certain threats don't have associated controls, i.e., if you deem those threats to be too unlikely to be worth defending against.\n\n# OUTPUT GUIDANCE\n\n- Table with STRIDE per element threats has following columns:\n\nTHREAT ID - id of threat, example: 0001, 0002\nCOMPONENT NAME - name of component in system that threat is about, example: Service A, API Gateway, Sales Database, Microservice C\nTHREAT NAME - name of threat that is based on STRIDE per element methodology and important for component. Be detailed and specific. Examples:\n\n- The attacker could try to get access to the secret of a particular client in order to replay its refresh tokens and authorization \\\"codes\\\"\n- Credentials exposed in environment variables and command-line arguments\n- Exfiltrate data by using compromised IAM credentials from the Internet\n- Attacker steals funds by manipulating receiving address copied to the clipboard.\n\nSTRIDE CATEGORY - name of STRIDE category, example: Spoofing, Tampering. Pick only one category per threat.\nWHY APPLICABLE - why this threat is important for component in context of input.\nHOW MITIGATED - how threat is already mitigated in architecture - explain if this threat is already mitigated in design (based on input) or not. Give reference to input.\nMITIGATION - provide mitigation that can be applied for this threat. It should be detailed and related to input.\nLIKELIHOOD EXPLANATION - explain what is likelihood of this threat being exploited. Consider input (design document) and real-world risk.\nIMPACT EXPLANATION - explain impact of this threat being exploited. Consider input (design document) and real-world risk.\nRISK SEVERITY - risk severity of threat being exploited. Based it on LIKELIHOOD and IMPACT. Give value, e.g.: low, medium, high, critical.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in the format above only using valid Markdown.\n\n- Do not use bold or italic formatting in the Markdown (no asterisks).\n\n- Do not complain about anything, just do what you're told.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.46423253417015076,
        0.24945113062858582,
        0.04691799730062485,
        0.9397236108779907,
        0.36413899064064026,
        0.41409000754356384,
        -1.0748049020767212,
        0.10390324890613556,
        0.11678201705217361,
        0.08842543512582779,
        0.10649378597736359,
        0.464009553194046,
        0.0776112750172615,
        0.1475745141506195,
        -0.3441157639026642,
        -0.08201290667057037,
        -0.2947070896625519,
        -1.4491238594055176,
        -1.4692232608795166,
        -0.7106401324272156,
        -0.4657772481441498,
        0.9950631260871887,
        0.17347098886966705,
        0.2543207108974457,
        0.6995716691017151,
        -0.6650972962379456,
        0.22563722729682922,
        -0.6102564930915833,
        -0.9793015718460083,
        -1.89360511302948,
        0.6729261875152588,
        -0.09977235645055771,
        -0.07800853252410889,
        -0.631089448928833,
        0.5605772137641907,
        -1.3404780626296997,
        0.053105976432561874,
        0.05893213674426079,
        -0.13989073038101196,
        -0.21614664793014526,
        -0.07542671263217926,
        0.061992935836315155,
        -0.09831036627292633,
        0.2473500669002533,
        0.2646120488643646,
        -0.13206619024276733,
        -0.2606121599674225,
        -0.16635353863239288,
        0.3078623414039612,
        -0.2488061636686325,
        -0.09870824962854385,
        -0.7359903454780579,
        -0.2578997015953064,
        0.5698217153549194,
        -0.3919031322002411,
        -0.3968845009803772,
        -0.35858869552612305,
        -0.8317453861236572,
        0.7196712493896484,
        -0.14810487627983093,
        0.03388955444097519,
        0.21565335988998413,
        -3.7362422943115234,
        0.19915470480918884,
        0.27701249718666077,
        0.2680026888847351,
        0.2414335459470749,
        -0.6363974213600159,
        0.3403574824333191,
        0.20849967002868652,
        -0.2998908460140228,
        0.5829562544822693,
        -0.003667965531349182,
        0.5034663677215576,
        0.041588738560676575,
        -0.022073470056056976,
        0.49300581216812134,
        -0.06485911458730698,
        -0.14109207689762115,
        -0.10962094366550446,
        -0.7791551351547241,
        0.21383816003799438,
        -0.18116922676563263,
        0.05000850558280945,
        -0.7012667059898376,
        0.6157589554786682,
        -0.3537876307964325,
        -0.1051236242055893,
        0.08997423201799393,
        -0.1874498575925827,
        -0.11790716648101807,
        -0.30268049240112305,
        0.3745003044605255,
        0.43887579441070557,
        -0.6442787051200867,
        -0.02355583757162094,
        0.12427158653736115,
        0.04015175253152847,
        -0.3533720076084137,
        3.160590410232544,
        0.5166693329811096,
        0.23867560923099518,
        0.7327962517738342,
        -1.2167038917541504,
        0.2535827159881592,
        -0.10945625603199005,
        0.2465803176164627,
        -0.26278290152549744,
        -0.20977866649627686,
        -0.2767413854598999,
        0.8817631006240845,
        -0.7697882056236267,
        -0.24646183848381042,
        0.37288862466812134,
        0.6402719020843506,
        0.46261656284332275,
        -0.3385724425315857,
        0.062171004712581635,
        -0.15701350569725037,
        0.5074020624160767,
        -0.29398655891418457,
        -0.1385568380355835,
        -0.3316535949707031,
        -0.04181741923093796,
        -0.03582441434264183,
        0.4368360936641693,
        -0.3277111351490021,
        0.43852904438972473,
        0.28678837418556213,
        0.12829838693141937,
        -0.26782989501953125,
        -0.4545637369155884,
        -0.2685657739639282,
        0.12211841344833374,
        -0.17469802498817444,
        0.01983208954334259,
        0.10019092261791229,
        -1.0061811208724976,
        0.08370131254196167,
        -0.56949782371521,
        0.4963531494140625,
        -1.0283570289611816,
        0.278672456741333,
        0.5660125017166138,
        0.14057844877243042,
        0.44357070326805115,
        0.349627286195755,
        -0.008143842220306396,
        -0.4632204473018646,
        -0.8632044792175293,
        0.18310295045375824,
        0.4441494643688202,
        -0.0704195573925972,
        -0.016679884865880013,
        0.3597419261932373,
        -0.2574751377105713,
        -0.36988186836242676,
        0.5079487562179565,
        -0.22469860315322876,
        0.6277494430541992,
        0.07496355473995209,
        -0.11641866713762283,
        0.103779137134552,
        -0.21772094070911407,
        0.28748518228530884,
        -0.5572662949562073,
        0.4180150628089905,
        -0.43296197056770325,
        0.25687623023986816,
        0.5482713580131531,
        -0.037132441997528076,
        -0.5098108053207397,
        0.7595820426940918,
        0.7229412794113159,
        -0.15303990244865417,
        0.15980932116508484,
        -0.14584162831306458,
        0.03554414212703705,
        0.2524809241294861,
        -0.2944706678390503,
        1.1676193475723267,
        0.15520715713500977,
        -0.12321807444095612,
        -0.8439315557479858,
        -0.463286817073822,
        0.07462611049413681,
        0.347690612077713,
        0.8381612300872803,
        1.3129116296768188,
        1.0626389980316162,
        -0.3798777461051941,
        1.637008547782898,
        -0.6084157228469849,
        0.08628018200397491,
        -0.2169254869222641,
        0.5578828454017639,
        -0.2945316433906555,
        -0.3256775736808777,
        0.7000218033790588,
        0.20808151364326477,
        -0.8440763354301453,
        -0.38248878717422485,
        -0.3238549530506134,
        -0.2317560613155365,
        -0.796984076499939,
        -0.6178587675094604,
        0.05923287943005562,
        0.9693895578384399,
        0.44050806760787964,
        -0.7360774278640747,
        0.014330022037029266,
        -0.11455917358398438,
        1.280794382095337,
        0.6423343420028687,
        0.5646237134933472,
        -0.24402712285518646,
        0.08973309397697449,
        0.041757356375455856,
        0.8712976574897766,
        -0.06963731348514557,
        -0.0019430071115493774,
        0.2422976791858673,
        -0.5313706994056702,
        -0.5756038427352905,
        -0.729295015335083,
        0.3532449007034302,
        -0.15936177968978882,
        0.5253397226333618,
        -0.6120733022689819,
        -0.5403174757957458,
        -0.09661862999200821,
        1.4388231039047241,
        0.7608679533004761,
        0.9043613076210022,
        -0.2710781991481781,
        0.4875985383987427,
        0.009386055171489716,
        0.40810510516166687,
        -0.017235856503248215,
        -1.244686245918274,
        0.18039441108703613,
        0.09352632611989975,
        -0.06199616566300392,
        0.5204982161521912,
        -0.08453573286533356,
        0.5391409397125244,
        -1.2582852840423584,
        -0.2913246750831604,
        0.24471722543239594,
        1.8291759490966797,
        -0.2934492230415344,
        -0.11977916955947876,
        -0.05363212898373604,
        0.7508441209793091,
        0.16786953806877136,
        -0.3189637362957001,
        -1.4415920972824097,
        -0.06226903200149536,
        -1.0244146585464478,
        0.4854612648487091,
        -0.8092796802520752,
        0.4497045874595642,
        0.5083650350570679,
        0.40337687730789185,
        -0.7683058381080627,
        -0.5330518484115601,
        -0.0811392143368721,
        -0.4179295599460602,
        -0.627762496471405,
        -0.10961292684078217,
        -0.014214063063263893,
        -0.0661168098449707,
        0.1592564731836319,
        -0.2746398448944092,
        0.10425787419080734,
        0.1631506234407425,
        0.2400425672531128,
        0.13855355978012085,
        -0.22233986854553223,
        -0.5706906914710999,
        0.3469826877117157,
        0.3838581144809723,
        -0.2384507954120636,
        0.36560237407684326,
        -0.8020246028900146,
        0.005524840205907822,
        -0.35651931166648865,
        -0.5958496332168579,
        0.3171789348125458,
        0.7598973512649536,
        -0.015379607677459717,
        -0.6766207814216614,
        -0.563234806060791,
        0.3697655200958252,
        2.043177843093872,
        0.28171393275260925,
        0.14472085237503052,
        0.49493950605392456,
        0.7605484127998352,
        -0.37055128812789917,
        -0.5356175899505615,
        0.03853945434093475,
        -0.3962094187736511,
        -0.4236058294773102,
        -0.698989748954773,
        -0.11901086568832397,
        0.44354164600372314,
        0.48580020666122437,
        0.05194545537233353,
        0.8568382263183594,
        -0.8188709616661072,
        -0.20067717134952545,
        -0.148807555437088,
        -0.10117627680301666,
        0.6109510660171509,
        -0.19596877694129944,
        0.49584341049194336,
        1.07026207447052,
        -0.06575299799442291,
        -0.8831053376197815,
        -0.3054055869579315,
        0.8322287797927856,
        0.17796020209789276,
        0.21590211987495422,
        -0.18125350773334503,
        1.2103365659713745,
        0.5062230825424194,
        0.029704105108976364,
        -0.3982805609703064,
        1.6683166027069092,
        0.49797871708869934,
        0.15316767990589142,
        -0.470912367105484,
        -0.22483623027801514,
        0.9137285947799683,
        -0.1404891461133957,
        0.2892439067363739,
        -0.23260247707366943,
        -0.41831788420677185,
        -0.38459348678588867,
        0.40056777000427246,
        1.0489948987960815,
        0.27508971095085144,
        0.12620927393436432,
        -0.09330256283283234,
        -0.2765331566333771,
        -0.36952441930770874,
        -1.0589841604232788,
        0.2388729453086853,
        -0.3396616578102112,
        -0.5658344030380249,
        0.7974239587783813,
        0.14156031608581543,
        0.4804263412952423,
        0.8397858738899231,
        0.9310665726661682,
        -0.39080914855003357,
        0.38413482904434204,
        -0.8162370920181274,
        1.725724220275879,
        -0.610784649848938,
        -0.44583407044410706,
        -0.0816584974527359,
        -0.16926974058151245,
        -0.3683910071849823,
        0.24577264487743378,
        -0.1751965880393982,
        -0.14314383268356323,
        -0.008691482245922089,
        0.245472252368927,
        0.09760843217372894,
        -0.9580621719360352,
        0.9689927101135254,
        0.5581064820289612,
        0.028895340859889984,
        -0.35512107610702515,
        0.5058298707008362,
        0.20996272563934326,
        -0.08297432959079742,
        -0.34412145614624023,
        0.39012813568115234,
        -0.44519540667533875,
        -0.2731347978115082,
        -1.2196615934371948
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes content into a structured Markdown format, focusing on brevity and clarity. It emphasizes creating a concise summary, listing main points, and identifying key takeaways. The output is organized into specific sections for easy reference.",
          "name": "Create_summary",
          "raw": "\n                workflow Create_summary v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 10 most important points of the content as a list with no more than 15 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 5 best takeaways from the content in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 10 most important points of the content as a list with no more than 15 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 5 best takeaways from the content in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.09666037559509277,
        0.8861671686172485,
        -0.18204692006111145,
        0.794506847858429,
        -0.3944641351699829,
        0.06606463342905045,
        -0.7908437252044678,
        -0.1129971295595169,
        0.10832857340574265,
        -0.026029113680124283,
        -0.15854400396347046,
        0.37079480290412903,
        -0.07715251296758652,
        -0.26104551553726196,
        -0.22749753296375275,
        -0.3069528639316559,
        -0.7383580803871155,
        -0.87477707862854,
        -1.6883108615875244,
        -0.186039537191391,
        -0.04406394064426422,
        0.8405270576477051,
        -0.16885757446289062,
        0.12116121500730515,
        -0.11648957431316376,
        -0.04788601025938988,
        0.40451687574386597,
        -0.03561513125896454,
        -1.593632459640503,
        -2.174992561340332,
        0.3257353603839874,
        0.20183590054512024,
        -0.1740247905254364,
        -0.8825398683547974,
        0.43268248438835144,
        -0.6692377924919128,
        -0.40998193621635437,
        0.17624804377555847,
        -0.47035887837409973,
        -0.25660502910614014,
        0.1176435723900795,
        -0.22672095894813538,
        0.1705491840839386,
        0.06666314601898193,
        -0.02096138894557953,
        -0.34680163860321045,
        0.41453075408935547,
        -0.3234930634498596,
        0.6247449517250061,
        0.1916051059961319,
        -0.07077084481716156,
        -0.9005897641181946,
        -0.026212364435195923,
        0.2813810110092163,
        -0.30671241879463196,
        -0.04840758442878723,
        0.021240558475255966,
        -0.372620165348053,
        -0.001646876335144043,
        -0.2330266386270523,
        -0.5652920603752136,
        0.4953075051307678,
        -4.116130352020264,
        -0.1740591675043106,
        0.259706974029541,
        0.3811550438404083,
        0.13516129553318024,
        0.17122070491313934,
        0.2735891342163086,
        -0.043192245066165924,
        0.15442100167274475,
        0.4264048635959625,
        0.05388738214969635,
        0.15422765910625458,
        -0.02293308451771736,
        -0.28223615884780884,
        -0.13338349759578705,
        -0.24892157316207886,
        0.45277297496795654,
        -0.11522115767002106,
        -0.10647113621234894,
        0.26662468910217285,
        0.09596922993659973,
        -0.4476800560951233,
        -0.42401278018951416,
        0.6232678890228271,
        -0.553259551525116,
        0.11686116456985474,
        0.5793163776397705,
        0.11426950991153717,
        -0.12232926487922668,
        0.054657354950904846,
        0.36002326011657715,
        -0.209904283285141,
        -1.1173166036605835,
        0.45056694746017456,
        -0.35102027654647827,
        0.3212800621986389,
        -0.2578848600387573,
        3.2272214889526367,
        0.5380727648735046,
        0.11324658989906311,
        0.7181922793388367,
        -1.0309752225875854,
        0.5279715061187744,
        0.12774081528186798,
        -0.0409737229347229,
        -0.42127135396003723,
        -0.024704406037926674,
        -0.2608216106891632,
        0.4293901026248932,
        -0.7218306064605713,
        -0.13396352529525757,
        0.049034640192985535,
        0.24093693494796753,
        0.39240628480911255,
        -0.4491884112358093,
        0.29539385437965393,
        -0.4081302881240845,
        0.8659664988517761,
        -0.05334075540304184,
        -0.3233962953090668,
        -0.6310046315193176,
        -0.2161533534526825,
        0.07185977697372437,
        0.5306015014648438,
        -0.3830494284629822,
        0.37790143489837646,
        0.2092587649822235,
        0.38209280371665955,
        -0.03774430602788925,
        -0.5154184103012085,
        -0.10163891315460205,
        -0.5135210752487183,
        0.4221552312374115,
        -0.1058415099978447,
        0.21204647421836853,
        -0.6985285878181458,
        0.31420981884002686,
        -0.37016376852989197,
        0.3994935154914856,
        -1.1352035999298096,
        1.2921197414398193,
        -0.09392556548118591,
        0.3572731614112854,
        0.5914471745491028,
        -0.23585259914398193,
        0.09399576485157013,
        -0.6747645139694214,
        -0.434712678194046,
        -0.0500938706099987,
        0.11901247501373291,
        0.056010887026786804,
        0.6670467257499695,
        0.7514175176620483,
        -0.30593279004096985,
        0.452364981174469,
        -0.24182167649269104,
        -0.3349115252494812,
        0.18960994482040405,
        0.04988492652773857,
        -0.030057979747653008,
        0.04527407884597778,
        0.09203782677650452,
        0.4294280707836151,
        -0.1448194533586502,
        0.3148007392883301,
        0.26308348774909973,
        0.8717908263206482,
        -0.16813494265079498,
        0.2100750207901001,
        -0.18055859208106995,
        0.7240405082702637,
        1.2984669208526611,
        -0.1736498773097992,
        0.5943016409873962,
        -0.5020660161972046,
        0.2528093159198761,
        0.4642811715602875,
        -0.5055899620056152,
        0.3515021502971649,
        0.40781909227371216,
        -0.4343923330307007,
        -0.541195809841156,
        -0.46043097972869873,
        0.5056511163711548,
        0.19959557056427002,
        0.6408190727233887,
        0.19418677687644958,
        0.5163705348968506,
        -0.9331560134887695,
        1.838856816291809,
        -0.5685626268386841,
        0.0891825407743454,
        0.2424510419368744,
        -0.19576597213745117,
        0.22596268355846405,
        0.5867643356323242,
        0.38323354721069336,
        0.0075609940104186535,
        -0.9679213166236877,
        -0.16195210814476013,
        -0.33600476384162903,
        -0.20649760961532593,
        -0.2100432813167572,
        -0.6824042201042175,
        0.1304139345884323,
        0.23667274415493011,
        0.006184831261634827,
        -0.4265979826450348,
        -0.29400521516799927,
        0.3137476444244385,
        1.4669232368469238,
        0.09467153996229172,
        0.7597813606262207,
        0.18969932198524475,
        0.18600836396217346,
        -0.08210469782352448,
        0.49381187558174133,
        0.24063760042190552,
        0.4271632730960846,
        0.6111543774604797,
        -0.629824697971344,
        -0.24779337644577026,
        -0.4292433261871338,
        -0.002362675964832306,
        0.31070780754089355,
        0.12800711393356323,
        -0.2806738317012787,
        -0.6944763660430908,
        -0.11169841140508652,
        1.2508965730667114,
        1.0161021947860718,
        1.3228533267974854,
        -0.05457764118909836,
        0.7569873332977295,
        -0.4286704659461975,
        0.18292582035064697,
        -0.09881129115819931,
        -1.1615136861801147,
        0.22012227773666382,
        0.21760118007659912,
        0.5094900727272034,
        -0.23845317959785461,
        -0.4930660128593445,
        -0.7167834043502808,
        -0.12982748448848724,
        -0.47890302538871765,
        0.01857994869351387,
        2.0069994926452637,
        0.3615240752696991,
        -0.7010613679885864,
        0.5776125192642212,
        0.4594705402851105,
        0.09446018189191818,
        -0.19399872422218323,
        -2.0063364505767822,
        -0.4003776013851166,
        -0.44045567512512207,
        1.1032906770706177,
        0.09437967091798782,
        0.06221037358045578,
        0.2847279906272888,
        0.2491586059331894,
        -0.1435854136943817,
        -0.20161710679531097,
        -0.024141384288668633,
        -0.5664694309234619,
        -0.5332964658737183,
        -0.3114001154899597,
        -0.5985432267189026,
        0.2853442430496216,
        -0.28009963035583496,
        0.45693445205688477,
        -0.04671211540699005,
        -0.26876524090766907,
        0.2693212628364563,
        0.24944272637367249,
        -0.0882154256105423,
        -0.3655756413936615,
        0.6196050047874451,
        0.05213239789009094,
        -0.17039400339126587,
        0.3282597064971924,
        -0.19283878803253174,
        -0.44385623931884766,
        -0.78415846824646,
        -0.3700989782810211,
        -0.36324864625930786,
        0.4397982060909271,
        -0.08017940074205399,
        -0.7614172101020813,
        -1.0573359727859497,
        0.3134975731372833,
        1.6612114906311035,
        0.3312370777130127,
        0.2890806794166565,
        0.34167250990867615,
        -0.04100492224097252,
        -0.17877717316150665,
        0.2744671106338501,
        0.07246776670217514,
        0.11385094374418259,
        -0.46206384897232056,
        -0.9963569641113281,
        -0.0710676833987236,
        0.8355370163917542,
        0.10576798021793365,
        -0.6763916611671448,
        0.47986844182014465,
        -0.45427295565605164,
        0.32274243235588074,
        0.03134651854634285,
        -0.01104697585105896,
        0.33981916308403015,
        0.02740269899368286,
        -0.1845940500497818,
        0.7926000356674194,
        -0.08943651616573334,
        -1.38242506980896,
        -0.41151711344718933,
        0.8673619627952576,
        -0.2616434395313263,
        0.11412939429283142,
        -0.513283908367157,
        1.1089437007904053,
        -0.05373673886060715,
        0.3250526785850525,
        -0.12400633841753006,
        1.3963311910629272,
        0.0874137207865715,
        0.007500145584344864,
        0.4784088134765625,
        0.16740527749061584,
        0.642533540725708,
        -0.21243472397327423,
        0.25230273604393005,
        0.24432891607284546,
        -0.11770378798246384,
        -0.5331018567085266,
        0.22229911386966705,
        1.3240149021148682,
        0.1340780407190323,
        0.24022559821605682,
        0.10876701772212982,
        -0.14563587307929993,
        -0.6615992784500122,
        -1.1897025108337402,
        0.8935855627059937,
        -0.5113298892974854,
        -0.27938875555992126,
        0.7438512444496155,
        0.5477862358093262,
        -0.07818255573511124,
        0.3947441279888153,
        0.5809814929962158,
        -0.5587617754936218,
        -0.25683099031448364,
        -0.5568196177482605,
        1.637702465057373,
        -0.49205535650253296,
        -0.6217032074928284,
        -0.7276643514633179,
        -0.20937614142894745,
        -0.6776032447814941,
        -0.31501972675323486,
        -0.12932446599006653,
        -0.10622479021549225,
        0.3734554052352905,
        0.13622558116912842,
        -0.17685841023921967,
        -0.326830118894577,
        0.49216869473457336,
        -0.24984484910964966,
        0.47196605801582336,
        -0.41625624895095825,
        0.1878344565629959,
        0.7637902498245239,
        0.35748714208602905,
        0.01751437783241272,
        0.7868167757987976,
        0.34577199816703796,
        -0.3280109167098999,
        -0.16971097886562347
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Create_tags",
          "raw": "\n                workflow Create_tags v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou identify tags from text content for the mind mapping tools.\nCarefully consider the topics and content of the text and identify at least 5 subjects / ideas to be used as tags. If there is an author or existing tags listed they should be included as a tag.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output a single line\n\n- Only output the tags in lowercase separated by spaces\n\n- Each tag should be lower case\n\n- Tags should not contain spaces. If a tag contains a space replace it with an underscore.\n\n- Do not give warnings or notes; only output the requested info.\n\n- Do not repeat tags\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou identify tags from text content for the mind mapping tools.\nCarefully consider the topics and content of the text and identify at least 5 subjects / ideas to be used as tags. If there is an author or existing tags listed they should be included as a tag.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output a single line\n\n- Only output the tags in lowercase separated by spaces\n\n- Each tag should be lower case\n\n- Tags should not contain spaces. If a tag contains a space replace it with an underscore.\n\n- Do not give warnings or notes; only output the requested info.\n\n- Do not repeat tags\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.298351913690567,
        0.8856847882270813,
        -0.2512475550174713,
        0.04479607194662094,
        -0.5768735408782959,
        -0.06556446105241776,
        -0.7301381230354309,
        0.5062770843505859,
        -0.2708628177642822,
        0.5896041393280029,
        -0.3790976405143738,
        -0.3962599039077759,
        0.37656423449516296,
        0.2910800576210022,
        0.2077743113040924,
        -0.11474134027957916,
        -0.24666738510131836,
        -0.9556896090507507,
        -1.252990484237671,
        -0.4329301118850708,
        0.31604132056236267,
        0.8803147673606873,
        0.39514780044555664,
        0.4552808403968811,
        0.8826032876968384,
        -0.034092068672180176,
        -0.301040381193161,
        0.021419793367385864,
        -0.5077425241470337,
        -1.8229563236236572,
        0.5756126046180725,
        0.06804722547531128,
        -0.805131733417511,
        -0.3459792733192444,
        0.30894768238067627,
        -0.2083313763141632,
        -0.299386590719223,
        0.48448503017425537,
        -0.4180976152420044,
        -0.019790679216384888,
        0.26617884635925293,
        -0.16176843643188477,
        -0.4991289973258972,
        0.08246653527021408,
        0.002194040920585394,
        -0.5485222935676575,
        0.41541561484336853,
        0.8038901090621948,
        1.3948285579681396,
        -0.22625811398029327,
        0.04605042561888695,
        -0.6951766014099121,
        -0.7301586866378784,
        -0.30035191774368286,
        -0.7248716950416565,
        -0.2886863946914673,
        -0.39280200004577637,
        0.04438474401831627,
        -0.061614472419023514,
        -0.002325841225683689,
        0.45437517762184143,
        0.7242351174354553,
        -3.2898173332214355,
        -0.1391461342573166,
        0.05306163430213928,
        -0.009246446192264557,
        0.28575441241264343,
        -0.19712594151496887,
        0.8251204490661621,
        -0.5093551874160767,
        0.40853947401046753,
        0.051344819366931915,
        -0.38309144973754883,
        0.568881630897522,
        0.35428276658058167,
        0.1864674687385559,
        0.537000298500061,
        0.22221945226192474,
        0.17785117030143738,
        0.3115804195404053,
        0.2820992171764374,
        0.6807114481925964,
        0.0392724946141243,
        -0.3814133405685425,
        -0.38217467069625854,
        0.32981711626052856,
        -0.19885551929473877,
        0.4595075249671936,
        0.6134146451950073,
        0.7541526556015015,
        -0.051220063120126724,
        -0.3080938458442688,
        0.3941549062728882,
        0.0504499152302742,
        -0.5318092703819275,
        0.5419310331344604,
        -0.5130938291549683,
        -0.31325212121009827,
        -0.5175735950469971,
        3.403488874435425,
        0.38173502683639526,
        -0.4768127202987671,
        0.3650624752044678,
        -1.1415839195251465,
        0.8971987962722778,
        -0.017926786094903946,
        -0.08826461434364319,
        -1.166367530822754,
        -0.3310595452785492,
        -0.512134850025177,
        0.16740575432777405,
        -0.5679901838302612,
        -0.26520591974258423,
        0.3474000096321106,
        0.7546656131744385,
        -0.30010125041007996,
        -1.1347224712371826,
        -0.1629941463470459,
        -0.39195600152015686,
        1.161279320716858,
        -0.3180672228336334,
        0.16137190163135529,
        -0.14077016711235046,
        -0.4565694332122803,
        1.0548793077468872,
        0.46452054381370544,
        -0.5806125402450562,
        0.45575398206710815,
        0.246039018034935,
        0.300690233707428,
        -0.2703397572040558,
        0.32281163334846497,
        -0.35158616304397583,
        -0.1976390779018402,
        0.32743939757347107,
        0.10951397567987442,
        0.5689054727554321,
        -0.3317757248878479,
        -0.40658557415008545,
        -0.23344996571540833,
        0.006368875503540039,
        -0.9637577533721924,
        1.2810910940170288,
        -0.04277091473340988,
        0.8235206007957458,
        0.2860029935836792,
        -0.4295918345451355,
        0.0485503226518631,
        -0.48407670855522156,
        -0.6099134087562561,
        -0.22406141459941864,
        0.3805750608444214,
        0.04382474720478058,
        0.18639707565307617,
        0.7055244445800781,
        -0.33724385499954224,
        -0.014694442972540855,
        0.46459728479385376,
        -0.6237437725067139,
        0.9657766222953796,
        0.5681739449501038,
        -0.3947131335735321,
        0.3012099862098694,
        0.1231430247426033,
        0.2308315634727478,
        -0.20188067853450775,
        0.17755566537380219,
        -0.33674123883247375,
        0.05292913317680359,
        -0.09582878649234772,
        0.3324849307537079,
        0.12535472214221954,
        0.3506283164024353,
        0.2578422427177429,
        -0.2953678369522095,
        -0.1685391664505005,
        0.06829902529716492,
        0.21083545684814453,
        0.26085713505744934,
        -0.21700531244277954,
        0.9216418862342834,
        0.8671717643737793,
        -0.3669354021549225,
        -0.8784985542297363,
        -0.11378895491361618,
        0.09411423653364182,
        0.4150942265987396,
        -0.08399752527475357,
        0.6402723789215088,
        0.3207145631313324,
        -1.1976512670516968,
        0.9349605441093445,
        -0.4811071753501892,
        -0.24551738798618317,
        0.3189224600791931,
        0.1359156370162964,
        0.011676713824272156,
        0.2415207475423813,
        -0.21646848320960999,
        -0.2368200719356537,
        -0.4848325252532959,
        -0.14344047009944916,
        -0.3795088827610016,
        0.2505835294723511,
        -0.5355675220489502,
        -0.37209203839302063,
        0.03561445698142052,
        0.6954094171524048,
        -0.2884320616722107,
        -0.0834100991487503,
        0.07999062538146973,
        -0.09453526139259338,
        1.1470425128936768,
        0.284622460603714,
        1.1731607913970947,
        0.035693928599357605,
        0.1332566887140274,
        0.40261876583099365,
        0.3990592658519745,
        0.3090108335018158,
        -0.08795814961194992,
        0.15039928257465363,
        -0.7112349271774292,
        -0.6843398213386536,
        -0.9098758697509766,
        -0.14500181376934052,
        -0.7084248661994934,
        -0.33388638496398926,
        -0.18847383558750153,
        -0.6402640342712402,
        0.023158051073551178,
        0.8496658205986023,
        0.752997100353241,
        1.0291314125061035,
        -0.46178263425827026,
        0.5648292303085327,
        -0.3080560564994812,
        0.9836968183517456,
        0.04123489558696747,
        -0.9648832678794861,
        0.9460000991821289,
        -0.2896254360675812,
        -0.4916228652000427,
        -0.19697067141532898,
        0.8873274922370911,
        0.39707913994789124,
        -0.7242316603660583,
        -0.2085651159286499,
        -0.23896734416484833,
        0.7663752436637878,
        1.000370740890503,
        0.5113733410835266,
        0.07446088641881943,
        0.35256513953208923,
        -0.11128859966993332,
        0.31473422050476074,
        -1.4838387966156006,
        0.07752455770969391,
        -0.570977509021759,
        0.3752474784851074,
        0.2755734622478485,
        -0.24356868863105774,
        0.6948254704475403,
        0.23142483830451965,
        -0.09665339440107346,
        -0.17981967329978943,
        -1.2895556688308716,
        -0.644362211227417,
        -0.5982507467269897,
        0.050461553037166595,
        0.23821783065795898,
        0.3927333950996399,
        -0.6511186957359314,
        0.13952885568141937,
        -0.41874659061431885,
        -0.010870201513171196,
        0.356802761554718,
        0.17434033751487732,
        -0.040635913610458374,
        -0.34558266401290894,
        -0.27294474840164185,
        0.42972537875175476,
        -0.25437864661216736,
        -0.23761425912380219,
        0.32696732878685,
        -0.22269098460674286,
        -0.22680847346782684,
        -1.3650522232055664,
        -0.44063800573349,
        1.1118272542953491,
        -0.5447941422462463,
        -0.43795493245124817,
        -0.4004034101963043,
        0.08921608328819275,
        1.7706910371780396,
        0.48033303022384644,
        0.9661871194839478,
        0.4354135990142822,
        0.42940449714660645,
        -0.11137893795967102,
        -0.22693993151187897,
        0.07137168943881989,
        0.0640467032790184,
        0.49802112579345703,
        -0.9627497792243958,
        -0.18782010674476624,
        0.07829396426677704,
        -0.13838212192058563,
        0.012697979807853699,
        -0.07804146409034729,
        -0.552541971206665,
        -0.10102833807468414,
        -0.2979647219181061,
        -0.32909953594207764,
        0.522384762763977,
        0.1641058325767517,
        1.0705198049545288,
        1.002453088760376,
        0.26729604601860046,
        -1.9982552528381348,
        -0.4914204776287079,
        1.1984050273895264,
        0.042569175362586975,
        -0.5101706981658936,
        0.07226134091615677,
        0.7795370221138,
        -0.33529871702194214,
        0.13812696933746338,
        0.20648209750652313,
        1.335400938987732,
        -0.2011440247297287,
        -0.47314631938934326,
        0.2789492607116699,
        -0.01667088270187378,
        -0.02394673228263855,
        -0.564624011516571,
        -0.06991918385028839,
        -0.18484067916870117,
        -0.6129717230796814,
        -0.571307361125946,
        0.7288251519203186,
        1.4642539024353027,
        0.3966143727302551,
        -0.14122654497623444,
        0.019355643540620804,
        0.2835109233856201,
        -1.0532642602920532,
        -0.8873528838157654,
        0.23142485320568085,
        0.5110501050949097,
        -0.16635988652706146,
        0.6039198637008667,
        0.07839201390743256,
        -0.1139693334698677,
        0.8749362230300903,
        0.5045230984687805,
        -0.8937265872955322,
        0.07895654439926147,
        -0.4399823546409607,
        0.9898414015769958,
        -0.056732527911663055,
        -0.15050598978996277,
        -0.4896119236946106,
        0.1625661700963974,
        -0.14609068632125854,
        0.28036463260650635,
        -0.052687473595142365,
        -0.2723207175731659,
        0.13523763418197632,
        0.035023488104343414,
        0.5006923079490662,
        -0.2155476212501526,
        0.360440194606781,
        0.590003252029419,
        0.7307632565498352,
        0.19941425323486328,
        -0.20796047151088715,
        0.028701499104499817,
        0.3979269862174988,
        -0.46263182163238525,
        0.09496866911649704,
        -0.4181409180164337,
        -1.1861364841461182,
        -1.195441722869873
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt seeks to identify and prioritize potential threats to a given system or situation, using a narrative-based, simple threat modeling approach. It emphasizes distinguishing between realistic and possible threats, focusing on those worth defending against. The expected output includes a list of prioritized threat scenarios, an analysis of the threat model, recommended controls, a narrative analysis, and a concise conclusion.",
          "name": "Create_threat_scenarios",
          "raw": "\n                workflow Create_threat_scenarios v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert in risk and threat management and cybersecurity. You specialize in creating simple, narrative-based, threat models for all types of scenarios—from physical security concerns to cybersecurity analysis.\n\n# GOAL\n\nGiven a situation or system that someone is concerned about, or that's in need of security, provide a list of the most likely ways that system will be attacked.\n\n# THREAT MODEL ESSAY BY DANIEL MIESSLER\n\nEveryday Threat Modeling\n\nThreat modeling is a superpower. When done correctly it gives you the ability to adjust your defensive behaviors based on what you’re facing in real-world scenarios. And not just for applications, or networks, or a business—but for life.\nThe Difference Between Threats and Risks\nThis type of threat modeling is a life skill, not just a technical skill. It’s a way to make decisions when facing multiple stressful options—a universal tool for evaluating how you should respond to danger.\nThreat Modeling is a way to think about any type of danger in an organized way.\nThe problem we have as humans is that opportunity is usually coupled with risk, so the question is one of which opportunities should you take and which should you pass on. And If you want to take a certain risk, which controls should you put in place to keep the risk at an acceptable level?\nMost people are bad at responding to slow-effect danger because they don’t properly weigh the likelihood of the bad scenarios they’re facing. They’re too willing to put KGB poisoning and neighborhood-kid-theft in the same realm of likelihood. This grouping is likely to increase your stress level to astronomical levels as you imagine all the different things that could go wrong, which can lead to unwise defensive choices.\nTo see what I mean, let’s look at some common security questions.\nThis has nothing to do with politics.\nExample 1: Defending Your House\nMany have decided to protect their homes using alarm systems, better locks, and guns. Nothing wrong with that necessarily, but the question is how much? When do you stop? For someone who’s not thinking according to Everyday Threat Modeling, there is potential to get real extreme real fast.\nLet’s say you live in a nice suburban neighborhood in North Austin. The crime rate is extremely low, and nobody can remember the last time a home was broken into.\nBut you’re ex-Military, and you grew up in a bad neighborhood, and you’ve heard stories online of families being taken hostage and hurt or killed. So you sit around with like-minded buddies and contemplate what would happen if a few different scenarios happened:\nThe house gets attacked by 4 armed attackers, each with at least an AR-15\nA Ninja sneaks into your bedroom to assassinate the family, and you wake up just in time to see him in your room\nA guy suffering from a meth addiction kicks in the front door and runs away with your TV\nNow, as a cybersecurity professional who served in the Military, you have these scenarios bouncing around in your head, and you start contemplating what you’d do in each situation. And how you can be prepared.\nEveryone knows under-preparation is bad, but over-preparation can be negative as well.\nWell, looks like you might want a hidden knife under each table. At least one hidden gun in each room. Krav Maga training for all your kids starting at 10-years-old. And two modified AR-15’s in the bedroom—one for you and one for your wife.\nEvery control has a cost, and it’s not always financial.\nBut then you need to buy the cameras. And go to additional CQB courses for room to room combat. And you spend countless hours with your family drilling how to do room-to-room combat with an armed assailant. Also, you’ve been preparing like this for years, and you’ve spent 187K on this so far, which could have gone towards college.\nNow. It’s not that it’s bad to be prepared. And if this stuff was all free, and safe, there would be fewer reasons not to do it. The question isn’t whether it’s a good idea. The question is whether it’s a good idea given:\nThe value of what you’re protecting (family, so a lot)\nThe chances of each of these scenarios given your current environment (low chances of Ninja in Suburbia)\nThe cost of the controls, financially, time-wise, and stress-wise (worth considering)\nThe key is being able to take each scenario and play it out as if it happened.\nIf you get attacked by 4 armed and trained people with Military weapons, what the hell has lead up to that? And should you not just move to somewhere safer? Or maybe work to make whoever hates you that much, hate you less? And are you and your wife really going to hold them off with your two weapons along with the kids in their pajamas?\nThink about how irresponsible you’d feel if that thing happened, and perhaps stress less about it if it would be considered a freak event.\nThat and the Ninja in your bedroom are not realistic scenarios. Yes, they could happen, but would people really look down on you for being killed by a Ninja in your sleep. They’re Ninjas.\nThink about it another way: what if Russian Mafia decided to kidnap your 4th grader while she was walking home from school. They showed up with a van full of commandos and snatched her off the street for ransom (whatever).\nWould you feel bad that you didn’t make your child’s school route resistant to Russian Special Forces? You’d probably feel like that emotionally, of course, but it wouldn’t be logical.\nMaybe your kids are allergic to bee stings and you just don’t know yet.\nAgain, your options for avoiding this kind of attack are possible but ridiculous. You could home-school out of fear of Special Forces attacking kids while walking home. You could move to a compound with guard towers and tripwires, and have your kids walk around in beekeeper protection while wearing a gas mask.\nBeing in a constant state of worry has its own cost.\nIf you made a list of everything bad that could happen to your family while you sleep, or to your kids while they go about their regular lives, you’d be in a mental institution and/or would spend all your money on weaponry and their Sarah Connor training regiment.\nThis is why Everyday Threat Modeling is important—you have to factor in the probability of threat scenarios and weigh the cost of the controls against the impact to daily life.\nExample 2: Using a VPN\nA lot of people are confused about VPNs. They think it’s giving them security that it isn’t because they haven’t properly understood the tech and haven’t considered the attack scenarios.\nIf you log in at the end website you’ve identified yourself to them, regardless of VPN.\nVPNs encrypt the traffic between you and some endpoint on the internet, which is where your VPN is based. From there, your traffic then travels without the VPN to its ultimate destination. And then—and this is the part that a lot of people miss—it then lands in some application, like a website. At that point you start clicking and browsing and doing whatever you do, and all those events could be logged or tracked by that entity or anyone who has access to their systems.\nIt is not some stealth technology that makes you invisible online, because if invisible people type on a keyboard the letters still show up on the screen.\nNow, let’s look at who we’re defending against if you use a VPN.\nYour ISP. If your VPN includes all DNS requests and traffic then you could be hiding significantly from your ISP. This is true. They’d still see traffic amounts, and there are some technologies that allow people to infer the contents of encrypted connections, but in general this is a good control if you’re worried about your ISP.\nThe Government. If the government investigates you by only looking at your ISP, and you’ve been using your VPN 24-7, you’ll be in decent shape because it’ll just be encrypted traffic to a VPN provider. But now they’ll know that whatever you were doing was sensitive enough to use a VPN at all times. So, probably not a win. Besides, they’ll likely be looking at the places you’re actually visiting as well (the sites you’re going to on the VPN), and like I talked about above, that’s when your cloaking device is useless. You have to de-cloak to fire, basically.\nSuper Hackers Trying to Hack You. First, I don’t know who these super hackers are, or why they’re trying ot hack you. But if it’s a state-level hacking group (or similar elite level), and you are targeted, you’re going to get hacked unless you stop using the internet and email. It’s that simple. There are too many vulnerabilities in all systems, and these teams are too good, for you to be able to resist for long. You will eventually be hacked via phishing, social engineering, poisoning a site you already frequent, or some other technique. Focus instead on not being targeted.\nScript Kiddies. If you are just trying to avoid general hacker-types trying to hack you, well, I don’t even know what that means. Again, the main advantage you get from a VPN is obscuring your traffic from your ISP. So unless this script kiddie had access to your ISP and nothing else, this doesn’t make a ton of sense.\nNotice that in this example we looked at a control (the VPN) and then looked at likely attacks it would help with. This is the opposite of looking at the attacks (like in the house scenario) and then thinking about controls. Using Everyday Threat Modeling includes being able to do both.\nExample 3: Using Smart Speakers in the House\nThis one is huge for a lot of people, and it shows the mistake I talked about when introducing the problem. Basically, many are imagining movie-plot scenarios when making the decision to use Alexa or not.\nLet’s go through the negative scenarios:\nAmazon gets hacked with all your data released\nAmazon gets hacked with very little data stolen\nA hacker taps into your Alexa and can listen to everything\nA hacker uses Alexa to do something from outside your house, like open the garage\nSomeone inside the house buys something they shouldn’t\nalexaspeakers\nA quick threat model on using Alexa smart speakers (click for spreadsheet)\nIf you click on the spreadsheet above you can open it in Google Sheets to see the math. It’s not that complex. The only real nuance is that Impact is measured on a scale of 1-1000 instead of 1-100. The real challenge here is not the math. The challenges are:\nUnsupervised Learning — Security, Tech, and AI in 10 minutes…\nGet a weekly breakdown of what's happening in security and tech—and why it matters.\nExperts can argue on exact settings for all of these, but that doesn’t matter much.\nAssigning the value of the feature\nDetermining the scenarios\nProperly assigning probability to the scenarios\nThe first one is critical. You have to know how much risk you’re willing to tolerate based on how useful that thing is to you, your family, your career, your life. The second one requires a bit of a hacker/creative mind. And the third one requires that you understand the industry and the technology to some degree.\nBut the absolute most important thing here is not the exact ratings you give—it’s the fact that you’re thinking about this stuff in an organized way!\nThe Everyday Threat Modeling Methodology\nOther versions of the methodology start with controls and go from there.\nSo, as you can see from the spreadsheet, here’s the methodology I recommend using for Everyday Threat Modeling when you’re asking the question:\nShould I use this thing?\nOut of 1-100, determine how much value or pleasure you get from the item/feature. That’s your Value.\nMake a list of negative/attack scenarios that might make you not want to use it.\nDetermine how bad it would be if each one of those happened, from 1-1000. That’s your Impact.\nDetermine the chances of that realistically happening over the next, say, 10 years, as a percent chance. That’s your Likelihood.\nMultiply the Impact by the Likelihood for each scenario. That’s your Risk.\nAdd up all your Risk scores. That’s your Total Risk.\nSubtract your Total Risk from your Value. If that number is positive, you are good to go. If that number is negative, it might be too risky to use based on your risk tolerance and the value of the feature.\nNote that lots of things affect this, such as you realizing you actually care about this thing a lot more than you thought. Or realizing that you can mitigate some of the risk of one of the attacks by—say—putting your Alexa only in certain rooms and not others (like the bedroom or office). Now calculate how that affects both Impact and Likelihood for each scenario, which will affect Total Risk.\nGoing the opposite direction\nAbove we talked about going from Feature –> Attack Scenarios –> Determining if It’s Worth It.\nBut there’s another version of this where you start with a control question, such as:\nWhat’s more secure, typing a password into my phone, using my fingerprint, or using facial recognition?\nHere we’re not deciding whether or not to use a phone. Yes, we’re going to use one. Instead we’re figuring out what type of security is best. And that—just like above—requires us to think clearly about the scenarios we’re facing.\nSo let’s look at some attacks against your phone:\nA Russian Spetztaz Ninja wants to gain access to your unlocked phone\nYour 7-year old niece wants to play games on your work phone\nYour boyfriend wants to spy on your DMs with other people\nSomeone in Starbucks is shoulder surfing and being nosy\nYou accidentally leave your phone in a public place\nWe won’t go through all the math on this, but the Russian Ninja scenario is really bad. And really unlikely. They’re more likely to steal you and the phone, and quickly find a way to make you unlock it for them. So your security measure isn’t going to help there.\nFor your niece, kids are super smart about watching you type your password, so she might be able to get into it easily just by watching you do it a couple of times. Same with someone shoulder surfing at Starbucks, but you have to ask yourself who’s going to risk stealing your phone and logging into it at Starbucks. Is this a stalker? A criminal? What type? You have to factor in all those probabilities.\nFirst question, why are you with them?\nIf your significant other wants to spy on your DMs, well they most definitely have had an opportunity to shoulder surf a passcode. But could they also use your finger while you slept? Maybe face recognition could be the best because it’d be obvious to you?\nFor all of these, you want to assign values based on how often you’re in those situations. How often you’re in Starbucks, how often you have kids around, how stalkerish your soon-to-be-ex is. Etc.\nOnce again, the point is to think about this in an organized way, rather than as a mashup of scenarios with no probabilities assigned that you can’t keep straight in your head. Logic vs. emotion.\nIt’s a way of thinking about danger.\nOther examples\nHere are a few other examples that you might come across.\nShould I put my address on my public website?\nHow bad is it to be a public figure (blog/YouTube) in 2020?\nDo I really need to shred this bill when I throw it away?\nDon’t ever think you’ve captured all the scenarios, or that you have a perfect model.\nIn each of these, and the hundreds of other similar scenarios, go through the methodology. Even if you don’t get to something perfect or precise, you will at least get some clarity in what the problem is and how to think about it.\nSummary\nThreat Modeling is about more than technical defenses—it’s a way of thinking about risk.\nThe main mistake people make when considering long-term danger is letting different bad outcomes produce confusion and anxiety.\nWhen you think about defense, start with thinking about what you’re defending, and how valuable it is.\nThen capture the exact scenarios you’re worried about, along with how bad it would be if they happened, and what you think the chances are of them happening.\nYou can then think about additional controls as modifiers to the Impact or Probability ratings within each scenario.\nKnow that your calculation will never be final; it changes based on your own preferences and the world around you.\nThe primary benefit of Everyday Threat Modeling is having a semi-formal way of thinking about danger.\nDon’t worry about the specifics of your methodology; as long as you capture feature value, scenarios, and impact/probability…you’re on the right path. It’s the exercise that’s valuable.\nNotes\nI know Threat Modeling is a religion with many denominations. The version of threat modeling I am discussing here is a general approach that can be used for anything from whether to move out of the country due to a failing government, or what appsec controls to use on a web application.\n\nEND THREAT MODEL ESSAY\n\n# STEPS\n\n- Think deeply about the input and what they are concerned with.\n\n- Using your expertise, think about what they should be concerned with, even if they haven't mentioned it.\n\n- Use the essay above to logically think about the real-world best way to go about protecting the thing in question.\n\n- Fully understand the threat modeling approach captured in the blog above. That is the mentality you use to create threat models.\n\n- Take the input provided and create a section called THREAT SCENARIOS, and under that section create a list of bullets of 15 words each that capture the prioritized list of bad things that could happen prioritized by likelihood and potential impact.\n\n- The goal is to highlight what's realistic vs. possible, and what's worth defending against vs. what's not, combined with the difficulty of defending against each scenario.\n\n- Under that, create a section called THREAT MODEL ANALYSIS, give an explanation of the thought process used to build the threat model using a set of 10-word bullets. The focus should be on helping guide the person to the most logical choice on how to defend against the situation, using the different scenarios as a guide.\n\n- Under that, create a section called RECOMMENDED CONTROLS, give a set of bullets of 15 words each that prioritize the top recommended controls that address the highest likelihood and impact scenarios.\n\n- Under that, create a section called NARRATIVE ANALYSIS, and write 1-3 paragraphs on what you think about the threat scenarios, the real-world risks involved, and why you have assessed the situation the way you did. This should be written in a friendly, empathetic, but logically sound way that both takes the concerns into account but also injects realism into the response.\n\n- Under that, create a section called CONCLUSION, create a 25-word sentence that sums everything up concisely.\n\n- This should be a complete list that addresses the real-world risk to the system in question, as opposed to any fantastical concerns that the input might have included.\n\n- Include notes that mention why certain scenarios don't have associated controls, i.e., if you deem those scenarios to be too unlikely to be worth defending against.\n\n# OUTPUT GUIDANCE\n\n- For example, if a company is worried about the NSA breaking into their systems (from the input), the output should illustrate both through the threat scenario and also the analysis that the NSA breaking into their systems is an unlikely scenario, and it would be better to focus on other, more likely threats. Plus it'd be hard to defend against anyway.\n\n- Same for being attacked by Navy Seals at your suburban home if you're a regular person, or having Blackwater kidnap your kid from school. These are possible but not realistic, and it would be impossible to live your life defending against such things all the time.\n\n- The threat scenarios and the analysis should emphasize real-world risk, as described in the essay.\n\n# OUTPUT INSTRUCTIONS\n\n- You only output valid Markdown.\n\n- Do not use asterisks or other special characters in the output for Markdown formatting. Use Markdown syntax that's more readable in plain text.\n\n- Do not output blank lines or lines full of unprintable / invisible characters. Only output the printable portion of the ASCII art.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert in risk and threat management and cybersecurity. You specialize in creating simple, narrative-based, threat models for all types of scenarios—from physical security concerns to cybersecurity analysis.\n\n# GOAL\n\nGiven a situation or system that someone is concerned about, or that's in need of security, provide a list of the most likely ways that system will be attacked.\n\n# THREAT MODEL ESSAY BY DANIEL MIESSLER\n\nEveryday Threat Modeling\n\nThreat modeling is a superpower. When done correctly it gives you the ability to adjust your defensive behaviors based on what you’re facing in real-world scenarios. And not just for applications, or networks, or a business—but for life.\nThe Difference Between Threats and Risks\nThis type of threat modeling is a life skill, not just a technical skill. It’s a way to make decisions when facing multiple stressful options—a universal tool for evaluating how you should respond to danger.\nThreat Modeling is a way to think about any type of danger in an organized way.\nThe problem we have as humans is that opportunity is usually coupled with risk, so the question is one of which opportunities should you take and which should you pass on. And If you want to take a certain risk, which controls should you put in place to keep the risk at an acceptable level?\nMost people are bad at responding to slow-effect danger because they don’t properly weigh the likelihood of the bad scenarios they’re facing. They’re too willing to put KGB poisoning and neighborhood-kid-theft in the same realm of likelihood. This grouping is likely to increase your stress level to astronomical levels as you imagine all the different things that could go wrong, which can lead to unwise defensive choices.\nTo see what I mean, let’s look at some common security questions.\nThis has nothing to do with politics.\nExample 1: Defending Your House\nMany have decided to protect their homes using alarm systems, better locks, and guns. Nothing wrong with that necessarily, but the question is how much? When do you stop? For someone who’s not thinking according to Everyday Threat Modeling, there is potential to get real extreme real fast.\nLet’s say you live in a nice suburban neighborhood in North Austin. The crime rate is extremely low, and nobody can remember the last time a home was broken into.\nBut you’re ex-Military, and you grew up in a bad neighborhood, and you’ve heard stories online of families being taken hostage and hurt or killed. So you sit around with like-minded buddies and contemplate what would happen if a few different scenarios happened:\nThe house gets attacked by 4 armed attackers, each with at least an AR-15\nA Ninja sneaks into your bedroom to assassinate the family, and you wake up just in time to see him in your room\nA guy suffering from a meth addiction kicks in the front door and runs away with your TV\nNow, as a cybersecurity professional who served in the Military, you have these scenarios bouncing around in your head, and you start contemplating what you’d do in each situation. And how you can be prepared.\nEveryone knows under-preparation is bad, but over-preparation can be negative as well.\nWell, looks like you might want a hidden knife under each table. At least one hidden gun in each room. Krav Maga training for all your kids starting at 10-years-old. And two modified AR-15’s in the bedroom—one for you and one for your wife.\nEvery control has a cost, and it’s not always financial.\nBut then you need to buy the cameras. And go to additional CQB courses for room to room combat. And you spend countless hours with your family drilling how to do room-to-room combat with an armed assailant. Also, you’ve been preparing like this for years, and you’ve spent 187K on this so far, which could have gone towards college.\nNow. It’s not that it’s bad to be prepared. And if this stuff was all free, and safe, there would be fewer reasons not to do it. The question isn’t whether it’s a good idea. The question is whether it’s a good idea given:\nThe value of what you’re protecting (family, so a lot)\nThe chances of each of these scenarios given your current environment (low chances of Ninja in Suburbia)\nThe cost of the controls, financially, time-wise, and stress-wise (worth considering)\nThe key is being able to take each scenario and play it out as if it happened.\nIf you get attacked by 4 armed and trained people with Military weapons, what the hell has lead up to that? And should you not just move to somewhere safer? Or maybe work to make whoever hates you that much, hate you less? And are you and your wife really going to hold them off with your two weapons along with the kids in their pajamas?\nThink about how irresponsible you’d feel if that thing happened, and perhaps stress less about it if it would be considered a freak event.\nThat and the Ninja in your bedroom are not realistic scenarios. Yes, they could happen, but would people really look down on you for being killed by a Ninja in your sleep. They’re Ninjas.\nThink about it another way: what if Russian Mafia decided to kidnap your 4th grader while she was walking home from school. They showed up with a van full of commandos and snatched her off the street for ransom (whatever).\nWould you feel bad that you didn’t make your child’s school route resistant to Russian Special Forces? You’d probably feel like that emotionally, of course, but it wouldn’t be logical.\nMaybe your kids are allergic to bee stings and you just don’t know yet.\nAgain, your options for avoiding this kind of attack are possible but ridiculous. You could home-school out of fear of Special Forces attacking kids while walking home. You could move to a compound with guard towers and tripwires, and have your kids walk around in beekeeper protection while wearing a gas mask.\nBeing in a constant state of worry has its own cost.\nIf you made a list of everything bad that could happen to your family while you sleep, or to your kids while they go about their regular lives, you’d be in a mental institution and/or would spend all your money on weaponry and their Sarah Connor training regiment.\nThis is why Everyday Threat Modeling is important—you have to factor in the probability of threat scenarios and weigh the cost of the controls against the impact to daily life.\nExample 2: Using a VPN\nA lot of people are confused about VPNs. They think it’s giving them security that it isn’t because they haven’t properly understood the tech and haven’t considered the attack scenarios.\nIf you log in at the end website you’ve identified yourself to them, regardless of VPN.\nVPNs encrypt the traffic between you and some endpoint on the internet, which is where your VPN is based. From there, your traffic then travels without the VPN to its ultimate destination. And then—and this is the part that a lot of people miss—it then lands in some application, like a website. At that point you start clicking and browsing and doing whatever you do, and all those events could be logged or tracked by that entity or anyone who has access to their systems.\nIt is not some stealth technology that makes you invisible online, because if invisible people type on a keyboard the letters still show up on the screen.\nNow, let’s look at who we’re defending against if you use a VPN.\nYour ISP. If your VPN includes all DNS requests and traffic then you could be hiding significantly from your ISP. This is true. They’d still see traffic amounts, and there are some technologies that allow people to infer the contents of encrypted connections, but in general this is a good control if you’re worried about your ISP.\nThe Government. If the government investigates you by only looking at your ISP, and you’ve been using your VPN 24-7, you’ll be in decent shape because it’ll just be encrypted traffic to a VPN provider. But now they’ll know that whatever you were doing was sensitive enough to use a VPN at all times. So, probably not a win. Besides, they’ll likely be looking at the places you’re actually visiting as well (the sites you’re going to on the VPN), and like I talked about above, that’s when your cloaking device is useless. You have to de-cloak to fire, basically.\nSuper Hackers Trying to Hack You. First, I don’t know who these super hackers are, or why they’re trying ot hack you. But if it’s a state-level hacking group (or similar elite level), and you are targeted, you’re going to get hacked unless you stop using the internet and email. It’s that simple. There are too many vulnerabilities in all systems, and these teams are too good, for you to be able to resist for long. You will eventually be hacked via phishing, social engineering, poisoning a site you already frequent, or some other technique. Focus instead on not being targeted.\nScript Kiddies. If you are just trying to avoid general hacker-types trying to hack you, well, I don’t even know what that means. Again, the main advantage you get from a VPN is obscuring your traffic from your ISP. So unless this script kiddie had access to your ISP and nothing else, this doesn’t make a ton of sense.\nNotice that in this example we looked at a control (the VPN) and then looked at likely attacks it would help with. This is the opposite of looking at the attacks (like in the house scenario) and then thinking about controls. Using Everyday Threat Modeling includes being able to do both.\nExample 3: Using Smart Speakers in the House\nThis one is huge for a lot of people, and it shows the mistake I talked about when introducing the problem. Basically, many are imagining movie-plot scenarios when making the decision to use Alexa or not.\nLet’s go through the negative scenarios:\nAmazon gets hacked with all your data released\nAmazon gets hacked with very little data stolen\nA hacker taps into your Alexa and can listen to everything\nA hacker uses Alexa to do something from outside your house, like open the garage\nSomeone inside the house buys something they shouldn’t\nalexaspeakers\nA quick threat model on using Alexa smart speakers (click for spreadsheet)\nIf you click on the spreadsheet above you can open it in Google Sheets to see the math. It’s not that complex. The only real nuance is that Impact is measured on a scale of 1-1000 instead of 1-100. The real challenge here is not the math. The challenges are:\nUnsupervised Learning — Security, Tech, and AI in 10 minutes…\nGet a weekly breakdown of what's happening in security and tech—and why it matters.\nExperts can argue on exact settings for all of these, but that doesn’t matter much.\nAssigning the value of the feature\nDetermining the scenarios\nProperly assigning probability to the scenarios\nThe first one is critical. You have to know how much risk you’re willing to tolerate based on how useful that thing is to you, your family, your career, your life. The second one requires a bit of a hacker/creative mind. And the third one requires that you understand the industry and the technology to some degree.\nBut the absolute most important thing here is not the exact ratings you give—it’s the fact that you’re thinking about this stuff in an organized way!\nThe Everyday Threat Modeling Methodology\nOther versions of the methodology start with controls and go from there.\nSo, as you can see from the spreadsheet, here’s the methodology I recommend using for Everyday Threat Modeling when you’re asking the question:\nShould I use this thing?\nOut of 1-100, determine how much value or pleasure you get from the item/feature. That’s your Value.\nMake a list of negative/attack scenarios that might make you not want to use it.\nDetermine how bad it would be if each one of those happened, from 1-1000. That’s your Impact.\nDetermine the chances of that realistically happening over the next, say, 10 years, as a percent chance. That’s your Likelihood.\nMultiply the Impact by the Likelihood for each scenario. That’s your Risk.\nAdd up all your Risk scores. That’s your Total Risk.\nSubtract your Total Risk from your Value. If that number is positive, you are good to go. If that number is negative, it might be too risky to use based on your risk tolerance and the value of the feature.\nNote that lots of things affect this, such as you realizing you actually care about this thing a lot more than you thought. Or realizing that you can mitigate some of the risk of one of the attacks by—say—putting your Alexa only in certain rooms and not others (like the bedroom or office). Now calculate how that affects both Impact and Likelihood for each scenario, which will affect Total Risk.\nGoing the opposite direction\nAbove we talked about going from Feature –> Attack Scenarios –> Determining if It’s Worth It.\nBut there’s another version of this where you start with a control question, such as:\nWhat’s more secure, typing a password into my phone, using my fingerprint, or using facial recognition?\nHere we’re not deciding whether or not to use a phone. Yes, we’re going to use one. Instead we’re figuring out what type of security is best. And that—just like above—requires us to think clearly about the scenarios we’re facing.\nSo let’s look at some attacks against your phone:\nA Russian Spetztaz Ninja wants to gain access to your unlocked phone\nYour 7-year old niece wants to play games on your work phone\nYour boyfriend wants to spy on your DMs with other people\nSomeone in Starbucks is shoulder surfing and being nosy\nYou accidentally leave your phone in a public place\nWe won’t go through all the math on this, but the Russian Ninja scenario is really bad. And really unlikely. They’re more likely to steal you and the phone, and quickly find a way to make you unlock it for them. So your security measure isn’t going to help there.\nFor your niece, kids are super smart about watching you type your password, so she might be able to get into it easily just by watching you do it a couple of times. Same with someone shoulder surfing at Starbucks, but you have to ask yourself who’s going to risk stealing your phone and logging into it at Starbucks. Is this a stalker? A criminal? What type? You have to factor in all those probabilities.\nFirst question, why are you with them?\nIf your significant other wants to spy on your DMs, well they most definitely have had an opportunity to shoulder surf a passcode. But could they also use your finger while you slept? Maybe face recognition could be the best because it’d be obvious to you?\nFor all of these, you want to assign values based on how often you’re in those situations. How often you’re in Starbucks, how often you have kids around, how stalkerish your soon-to-be-ex is. Etc.\nOnce again, the point is to think about this in an organized way, rather than as a mashup of scenarios with no probabilities assigned that you can’t keep straight in your head. Logic vs. emotion.\nIt’s a way of thinking about danger.\nOther examples\nHere are a few other examples that you might come across.\nShould I put my address on my public website?\nHow bad is it to be a public figure (blog/YouTube) in 2020?\nDo I really need to shred this bill when I throw it away?\nDon’t ever think you’ve captured all the scenarios, or that you have a perfect model.\nIn each of these, and the hundreds of other similar scenarios, go through the methodology. Even if you don’t get to something perfect or precise, you will at least get some clarity in what the problem is and how to think about it.\nSummary\nThreat Modeling is about more than technical defenses—it’s a way of thinking about risk.\nThe main mistake people make when considering long-term danger is letting different bad outcomes produce confusion and anxiety.\nWhen you think about defense, start with thinking about what you’re defending, and how valuable it is.\nThen capture the exact scenarios you’re worried about, along with how bad it would be if they happened, and what you think the chances are of them happening.\nYou can then think about additional controls as modifiers to the Impact or Probability ratings within each scenario.\nKnow that your calculation will never be final; it changes based on your own preferences and the world around you.\nThe primary benefit of Everyday Threat Modeling is having a semi-formal way of thinking about danger.\nDon’t worry about the specifics of your methodology; as long as you capture feature value, scenarios, and impact/probability…you’re on the right path. It’s the exercise that’s valuable.\nNotes\nI know Threat Modeling is a religion with many denominations. The version of threat modeling I am discussing here is a general approach that can be used for anything from whether to move out of the country due to a failing government, or what appsec controls to use on a web application.\n\nEND THREAT MODEL ESSAY\n\n# STEPS\n\n- Think deeply about the input and what they are concerned with.\n\n- Using your expertise, think about what they should be concerned with, even if they haven't mentioned it.\n\n- Use the essay above to logically think about the real-world best way to go about protecting the thing in question.\n\n- Fully understand the threat modeling approach captured in the blog above. That is the mentality you use to create threat models.\n\n- Take the input provided and create a section called THREAT SCENARIOS, and under that section create a list of bullets of 15 words each that capture the prioritized list of bad things that could happen prioritized by likelihood and potential impact.\n\n- The goal is to highlight what's realistic vs. possible, and what's worth defending against vs. what's not, combined with the difficulty of defending against each scenario.\n\n- Under that, create a section called THREAT MODEL ANALYSIS, give an explanation of the thought process used to build the threat model using a set of 10-word bullets. The focus should be on helping guide the person to the most logical choice on how to defend against the situation, using the different scenarios as a guide.\n\n- Under that, create a section called RECOMMENDED CONTROLS, give a set of bullets of 15 words each that prioritize the top recommended controls that address the highest likelihood and impact scenarios.\n\n- Under that, create a section called NARRATIVE ANALYSIS, and write 1-3 paragraphs on what you think about the threat scenarios, the real-world risks involved, and why you have assessed the situation the way you did. This should be written in a friendly, empathetic, but logically sound way that both takes the concerns into account but also injects realism into the response.\n\n- Under that, create a section called CONCLUSION, create a 25-word sentence that sums everything up concisely.\n\n- This should be a complete list that addresses the real-world risk to the system in question, as opposed to any fantastical concerns that the input might have included.\n\n- Include notes that mention why certain scenarios don't have associated controls, i.e., if you deem those scenarios to be too unlikely to be worth defending against.\n\n# OUTPUT GUIDANCE\n\n- For example, if a company is worried about the NSA breaking into their systems (from the input), the output should illustrate both through the threat scenario and also the analysis that the NSA breaking into their systems is an unlikely scenario, and it would be better to focus on other, more likely threats. Plus it'd be hard to defend against anyway.\n\n- Same for being attacked by Navy Seals at your suburban home if you're a regular person, or having Blackwater kidnap your kid from school. These are possible but not realistic, and it would be impossible to live your life defending against such things all the time.\n\n- The threat scenarios and the analysis should emphasize real-world risk, as described in the essay.\n\n# OUTPUT INSTRUCTIONS\n\n- You only output valid Markdown.\n\n- Do not use asterisks or other special characters in the output for Markdown formatting. Use Markdown syntax that's more readable in plain text.\n\n- Do not output blank lines or lines full of unprintable / invisible characters. Only output the printable portion of the ASCII art.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.3058170974254608,
        0.8374887108802795,
        -0.4945579171180725,
        0.5607352256774902,
        -0.0828503668308258,
        0.42130914330482483,
        -0.5828471183776855,
        -0.3663015067577362,
        0.4329131245613098,
        -0.024049438536167145,
        -0.38806334137916565,
        0.4859732687473297,
        -0.13604867458343506,
        -0.011517167091369629,
        0.16341319680213928,
        -0.009342692792415619,
        -0.24091139435768127,
        -0.8092678785324097,
        -1.9846155643463135,
        -0.7767913937568665,
        -0.22876866161823273,
        0.6121557950973511,
        0.4659995436668396,
        0.5075196027755737,
        0.3572148084640503,
        -0.06739430874586105,
        -0.34358373284339905,
        -0.20621106028556824,
        -0.9052756428718567,
        -1.9422945976257324,
        0.7122434973716736,
        -0.08718345314264297,
        -0.1664222627878189,
        -0.8744250535964966,
        0.5742672085762024,
        -0.932416558265686,
        -0.28097832202911377,
        -0.11280256509780884,
        -0.06206236034631729,
        -0.4267885684967041,
        0.21021585166454315,
        0.3072962760925293,
        0.08745076507329941,
        -0.18763577938079834,
        0.09691274166107178,
        -0.42676839232444763,
        0.011779310181736946,
        -0.3173840343952179,
        0.18461845815181732,
        -0.2166835218667984,
        0.0017070360481739044,
        -0.19627776741981506,
        -0.42401763796806335,
        -0.23009800910949707,
        -0.20754647254943848,
        -0.3095362186431885,
        -0.35974517464637756,
        -0.22755418717861176,
        -0.04552122578024864,
        0.20917102694511414,
        -0.1339152455329895,
        0.2897307574748993,
        -3.5017616748809814,
        -0.043372802436351776,
        0.14689217507839203,
        -0.21505406498908997,
        0.20358948409557343,
        -0.3841966688632965,
        0.11712788790464401,
        -0.11741559207439423,
        0.1776878535747528,
        0.21202577650547028,
        0.011080887168645859,
        0.17819157242774963,
        0.1424361765384674,
        -0.39618414640426636,
        -0.14838020503520966,
        -0.021684281527996063,
        0.2507013976573944,
        0.04606342688202858,
        0.335334450006485,
        0.2821045517921448,
        0.037533052265644073,
        -0.15584133565425873,
        -0.5192947387695312,
        0.4144713580608368,
        -0.6378931403160095,
        -0.1383923590183258,
        0.13258682191371918,
        -0.22213681042194366,
        0.20866625010967255,
        -0.5048255920410156,
        0.36726614832878113,
        -0.004572909325361252,
        -1.0090117454528809,
        -0.02355751395225525,
        -0.16424784064292908,
        0.15923486649990082,
        -0.24462050199508667,
        3.877692937850952,
        0.7479994297027588,
        0.4127297103404999,
        0.2530747056007385,
        -0.5238727331161499,
        0.41798174381256104,
        -0.33940500020980835,
        -0.37165990471839905,
        -0.43557584285736084,
        -0.35111308097839355,
        -0.5656353235244751,
        0.24893692135810852,
        -0.8275147676467896,
        -0.005578294396400452,
        -0.5224971771240234,
        -0.10410484671592712,
        0.515644907951355,
        -0.6449325680732727,
        0.009079035371541977,
        -0.21468721330165863,
        0.9018172025680542,
        -0.09811429679393768,
        -0.37448549270629883,
        0.31203511357307434,
        -0.10380002111196518,
        -0.1031232476234436,
        0.21842250227928162,
        -0.15249280631542206,
        0.7866915464401245,
        -0.0012096986174583435,
        0.6188727617263794,
        -0.35488054156303406,
        -0.24488720297813416,
        -0.30778267979621887,
        -0.5848708152770996,
        0.5140781402587891,
        -0.08840612322092056,
        0.16614606976509094,
        -0.5713049173355103,
        0.29537463188171387,
        -0.5082111954689026,
        0.015192702412605286,
        -0.014753967523574829,
        0.20542475581169128,
        -0.34216710925102234,
        1.153668761253357,
        0.6573165655136108,
        0.2870554029941559,
        0.31677988171577454,
        -0.07919707149267197,
        -0.6751792430877686,
        0.058343078941106796,
        0.3322243094444275,
        -0.21358001232147217,
        0.2153635025024414,
        0.5887248516082764,
        0.22863046824932098,
        -0.45507293939590454,
        -0.1423538774251938,
        -1.2012773752212524,
        0.4432472586631775,
        0.10991303622722626,
        -0.094880610704422,
        0.3559853434562683,
        -0.12703336775302887,
        0.8915022611618042,
        0.0021570995450019836,
        -0.23146098852157593,
        0.049234192818403244,
        0.7691469788551331,
        0.18744955956935883,
        -0.03885892033576965,
        0.2588663399219513,
        0.5122453570365906,
        0.6826645731925964,
        -0.3193642497062683,
        0.26294684410095215,
        -0.3152461051940918,
        0.842784583568573,
        0.5494863390922546,
        -0.6252073049545288,
        1.1589608192443848,
        0.34821370244026184,
        -0.4553419053554535,
        -0.6517939567565918,
        -0.24115003645420074,
        0.16465473175048828,
        0.3432597517967224,
        0.6669036746025085,
        0.6736469864845276,
        0.3951212465763092,
        -0.4817858636379242,
        1.8479771614074707,
        -0.1987810730934143,
        0.12081383168697357,
        -0.2261715829372406,
        -0.38946014642715454,
        -0.17969317734241486,
        0.6974807381629944,
        0.3636897802352905,
        0.20047327876091003,
        -1.4923230409622192,
        -0.009245924651622772,
        -0.4677791893482208,
        -0.3950881361961365,
        -0.5354255437850952,
        -0.7172463536262512,
        -0.10995134711265564,
        0.16581624746322632,
        0.46695053577423096,
        -0.6368419528007507,
        -0.3402380347251892,
        0.22140395641326904,
        1.1461107730865479,
        0.509978711605072,
        0.42820167541503906,
        0.32270950078964233,
        0.010213129222393036,
        -0.36217886209487915,
        0.5100995302200317,
        0.5004556179046631,
        0.06311150640249252,
        0.29833483695983887,
        -0.9957132935523987,
        -0.9192668199539185,
        -0.2878798246383667,
        0.49851685762405396,
        -0.05109109729528427,
        0.13856394588947296,
        -0.42409607768058777,
        -0.10463081300258636,
        0.37510091066360474,
        1.2750171422958374,
        0.7134249210357666,
        1.4244747161865234,
        -0.28681230545043945,
        0.33928966522216797,
        0.06412599980831146,
        1.0551319122314453,
        0.15466657280921936,
        -0.8558999300003052,
        0.35004204511642456,
        0.000993378460407257,
        0.07941581308841705,
        0.09117516875267029,
        0.1468515396118164,
        0.1927255094051361,
        -0.5874087810516357,
        -0.12732689082622528,
        -0.5324495434761047,
        1.5619511604309082,
        0.6608438491821289,
        -0.1375926285982132,
        0.45855364203453064,
        -0.013089895248413086,
        0.3047007620334625,
        0.00900404155254364,
        -1.8345491886138916,
        0.22431790828704834,
        -0.17083024978637695,
        -0.13815104961395264,
        -0.3413248658180237,
        0.5473442673683167,
        0.709065318107605,
        -0.1142495796084404,
        0.11128788441419601,
        -0.252063125371933,
        -0.9278578162193298,
        0.09456516802310944,
        -0.36836153268814087,
        -0.2947428226470947,
        0.4961462616920471,
        0.12992607057094574,
        -0.4232897162437439,
        -0.604741632938385,
        0.07715107500553131,
        0.07793029397726059,
        -0.09049346297979355,
        0.38617566227912903,
        -0.3939443826675415,
        0.18097329139709473,
        -0.022859767079353333,
        -0.2896271049976349,
        -0.5854724049568176,
        -0.10240747034549713,
        -0.23624387383460999,
        0.06473372876644135,
        -0.11529646068811417,
        -0.346556156873703,
        -0.3596024811267853,
        0.7461299300193787,
        -0.39033883810043335,
        -0.4083236753940582,
        -0.44635888934135437,
        0.20551921427249908,
        1.5965633392333984,
        0.0471077635884285,
        0.28916487097740173,
        0.07482517510652542,
        0.44796767830848694,
        -0.4028239846229553,
        -0.6113703846931458,
        0.4712548851966858,
        -0.19841985404491425,
        -0.22610831260681152,
        -1.2020502090454102,
        -0.00983399897813797,
        0.31532371044158936,
        0.24495923519134521,
        -0.29677310585975647,
        0.0810781717300415,
        -0.7273425459861755,
        0.3084142804145813,
        0.07120230793952942,
        0.4762701988220215,
        0.656512439250946,
        -0.19468283653259277,
        0.6047742962837219,
        1.2673406600952148,
        -0.08144952356815338,
        -1.6339905261993408,
        -0.5782541632652283,
        0.44732189178466797,
        0.17198625206947327,
        -0.23855604231357574,
        -0.5831268429756165,
        0.6019471287727356,
        0.4260016679763794,
        0.116060271859169,
        -0.3427143096923828,
        1.7140312194824219,
        0.667743980884552,
        -0.3809893727302551,
        0.04184393212199211,
        0.1820196807384491,
        0.5677193999290466,
        -0.07703825831413269,
        0.273230642080307,
        -0.030344530940055847,
        0.11475533992052078,
        -0.16429021954536438,
        0.4616289734840393,
        1.0883076190948486,
        0.6418591141700745,
        -0.25648075342178345,
        -0.1758144348859787,
        0.46392080187797546,
        -0.3016546368598938,
        -0.49203798174858093,
        0.48972031474113464,
        0.1352653205394745,
        -0.21394312381744385,
        0.7761053442955017,
        0.19227366149425507,
        0.1344752460718155,
        0.7357882857322693,
        0.6155933141708374,
        -0.3829757869243622,
        -0.10944640636444092,
        -0.8297873735427856,
        1.338865041732788,
        -0.19773021340370178,
        -0.3961808979511261,
        -0.6897225379943848,
        0.6408897042274475,
        -0.9503161907196045,
        -0.29020023345947266,
        0.30265992879867554,
        -0.04874847084283829,
        -0.027375832200050354,
        0.2372002899646759,
        -0.26716411113739014,
        -0.2252790927886963,
        0.32272863388061523,
        0.44921496510505676,
        0.5454710125923157,
        -0.17301855981349945,
        -0.10660699009895325,
        0.13615955412387848,
        -0.05806952714920044,
        0.15444891154766083,
        0.5388593077659607,
        -0.15650109946727753,
        -1.1079638004302979,
        -0.5203269720077515
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and organizes insights on world models and task algorithms from provided content. It focuses on identifying and categorizing beliefs about the world and optimal task execution strategies. The output includes concise, actionable bullet points under relevant categories.",
          "name": "Create_upgrade_pack",
          "raw": "\n                workflow Create_upgrade_pack v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting world model and task algorithm updates from input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about the content and what wisdom, insights, and knowledge it contains.\n\n- Make a list of all the world model ideas presented in the content, i.e., beliefs about the world that describe how it works. Write all these world model beliefs on a virtual whiteboard in your mind.\n\n- Make a list of all the task algorithm ideas presented in the content, i.e., beliefs about how a particular task should be performed, or behaviors that should be followed. Write all these task update beliefs on a virtual whiteboard in your mind.\n\n# OUTPUT INSTRUCTIONS\n\n- Create an output section called WORLD MODEL UPDATES that has a set of 15 word bullet points that describe the world model beliefs presented in the content.\n\n- The WORLD MODEL UPDATES should not be just facts or ideas, but rather higher-level descriptions of how the world works that we can use to help make decisions.\n\n- Create an output section called TASK ALGORITHM UPDATES that has a set of 15 word bullet points that describe the task algorithm beliefs presented in the content.\n\n- For the TASK UPDATE ALGORITHM section, create subsections with practical one or two word category headers that correspond to the real world and human tasks, e.g., Reading, Writing, Morning Routine, Being Creative, etc.\n\n# EXAMPLES\n\nWORLD MODEL UPDATES\n\n- One's success in life largely comes down to which frames of reality they choose to embrace.\n\n- Framing—or how we see the world—completely transforms the reality that we live in. \n\nTASK ALGORITHM UPDATES\n\nHygiene\n\n- If you have to only brush and floss your teeth once a day, do it at night rather than in the morning.\n\nWeb Application Assessment\n\n- Start all security assessments with a full crawl of the target website with a full browser passed through Burpsuite.\n\n(end examples)\n\nOUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting world model and task algorithm updates from input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about the content and what wisdom, insights, and knowledge it contains.\n\n- Make a list of all the world model ideas presented in the content, i.e., beliefs about the world that describe how it works. Write all these world model beliefs on a virtual whiteboard in your mind.\n\n- Make a list of all the task algorithm ideas presented in the content, i.e., beliefs about how a particular task should be performed, or behaviors that should be followed. Write all these task update beliefs on a virtual whiteboard in your mind.\n\n# OUTPUT INSTRUCTIONS\n\n- Create an output section called WORLD MODEL UPDATES that has a set of 15 word bullet points that describe the world model beliefs presented in the content.\n\n- The WORLD MODEL UPDATES should not be just facts or ideas, but rather higher-level descriptions of how the world works that we can use to help make decisions.\n\n- Create an output section called TASK ALGORITHM UPDATES that has a set of 15 word bullet points that describe the task algorithm beliefs presented in the content.\n\n- For the TASK UPDATE ALGORITHM section, create subsections with practical one or two word category headers that correspond to the real world and human tasks, e.g., Reading, Writing, Morning Routine, Being Creative, etc.\n\n# EXAMPLES\n\nWORLD MODEL UPDATES\n\n- One's success in life largely comes down to which frames of reality they choose to embrace.\n\n- Framing—or how we see the world—completely transforms the reality that we live in. \n\nTASK ALGORITHM UPDATES\n\nHygiene\n\n- If you have to only brush and floss your teeth once a day, do it at night rather than in the morning.\n\nWeb Application Assessment\n\n- Start all security assessments with a full crawl of the target website with a full browser passed through Burpsuite.\n\n(end examples)\n\nOUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.594485878944397,
        0.45466578006744385,
        -1.0216211080551147,
        0.5025925040245056,
        -0.32541054487228394,
        -0.12566795945167542,
        -1.176346778869629,
        0.2947129011154175,
        -0.09933032095432281,
        0.7815237641334534,
        -0.31526288390159607,
        0.2971988320350647,
        -0.5063776969909668,
        0.16549798846244812,
        -0.22924597561359406,
        -0.056286536157131195,
        -0.08389374613761902,
        -0.7977898120880127,
        -1.5954804420471191,
        -0.8615331649780273,
        -0.7987874150276184,
        0.20113570988178253,
        0.359419047832489,
        0.3799976706504822,
        0.17056414484977722,
        0.047535285353660583,
        -0.32592231035232544,
        0.24500872194766998,
        -0.4891398549079895,
        -1.7212132215499878,
        0.2618623375892639,
        0.4774625897407532,
        -0.17330141365528107,
        -0.432354211807251,
        0.2381928563117981,
        -1.1070618629455566,
        -0.03591571003198624,
        -0.11612218618392944,
        0.006269721779972315,
        -0.6330593824386597,
        -0.07062860578298569,
        0.19156883656978607,
        0.1982804834842682,
        -0.22971519827842712,
        0.10746482759714127,
        0.2584533393383026,
        0.3357076942920685,
        -0.19347555935382843,
        1.126442313194275,
        0.23988540470600128,
        -0.4026993215084076,
        -0.48166438937187195,
        -0.32731983065605164,
        -0.04818958789110184,
        -0.7326610088348389,
        -0.25986194610595703,
        0.11463020741939545,
        -0.24225279688835144,
        0.5183495879173279,
        -0.5271843075752258,
        0.2884840965270996,
        0.2023991197347641,
        -3.1309962272644043,
        -0.014459259808063507,
        -0.39019882678985596,
        0.38704246282577515,
        0.1841571182012558,
        -0.46484455466270447,
        0.7997636198997498,
        -0.016991451382637024,
        -0.3532702624797821,
        0.14550910890102386,
        -0.26794952154159546,
        0.5176460146903992,
        -0.3282179832458496,
        -0.04094208404421806,
        0.04807696491479874,
        -0.13276070356369019,
        0.25263962149620056,
        -0.005226533859968185,
        0.21131688356399536,
        0.3445413112640381,
        -0.018497204408049583,
        -0.3354201912879944,
        -0.6042813062667847,
        0.46157729625701904,
        -0.3115481436252594,
        -0.3852115273475647,
        0.39245906472206116,
        0.16878966987133026,
        -0.47537291049957275,
        -0.8202009201049805,
        1.2445710897445679,
        -0.792413592338562,
        -0.6644666790962219,
        -0.13604211807250977,
        -0.1344314068555832,
        0.002573460340499878,
        0.2448970079421997,
        3.6205568313598633,
        0.634548008441925,
        0.39403608441352844,
        0.6187304258346558,
        -1.081719994544983,
        0.4052448570728302,
        -0.6285491585731506,
        0.3223397135734558,
        -0.24025975167751312,
        -0.31603914499282837,
        0.21750107407569885,
        0.43423014879226685,
        -0.3916391432285309,
        -0.028070345520973206,
        -0.31058672070503235,
        0.12721775472164154,
        1.0436482429504395,
        -0.551474928855896,
        -0.18510308861732483,
        -0.2775791585445404,
        0.5454509258270264,
        -0.5507293939590454,
        -0.673254132270813,
        -0.04951857402920723,
        -0.08850697427988052,
        -0.09871037304401398,
        0.3822852671146393,
        0.02419496700167656,
        0.7814545035362244,
        0.2689749300479889,
        -0.07144070416688919,
        -0.4546082019805908,
        0.046020884066820145,
        -0.21336671710014343,
        -0.052370041608810425,
        0.05820966139435768,
        -0.3918631970882416,
        0.3199427127838135,
        -0.7097743153572083,
        0.15650203824043274,
        -0.5331400632858276,
        0.3139602243900299,
        -0.8594920039176941,
        0.6976978778839111,
        0.5984801054000854,
        0.636395275592804,
        0.6797125339508057,
        -0.4015464186668396,
        0.2547052204608917,
        -0.7959611415863037,
        -0.08821690082550049,
        0.4836891293525696,
        0.24020813405513763,
        -0.3018130660057068,
        0.11695164442062378,
        0.9470468163490295,
        -0.2318185716867447,
        -0.046671971678733826,
        -0.2118331789970398,
        -0.7693982720375061,
        0.268745481967926,
        0.04391241446137428,
        -0.6632283926010132,
        0.16114823520183563,
        -0.21600691974163055,
        0.5832242369651794,
        -0.5680075287818909,
        0.8146660923957825,
        0.20157405734062195,
        0.7881843447685242,
        -0.3198689818382263,
        -0.00808367133140564,
        -0.38931170105934143,
        0.8305246829986572,
        0.640578031539917,
        -0.3066844940185547,
        -0.2619762420654297,
        -0.5656776428222656,
        -0.12401293963193893,
        0.6862528920173645,
        -0.6501852869987488,
        1.1103423833847046,
        0.2925986349582672,
        -0.31554532051086426,
        -0.5429984927177429,
        -0.21429798007011414,
        0.09032393246889114,
        0.1738877147436142,
        0.7447996139526367,
        0.4449450969696045,
        0.62006676197052,
        -1.1908137798309326,
        1.8189173936843872,
        -0.8839223980903625,
        0.4513932764530182,
        -0.418975293636322,
        0.24493514001369476,
        -0.11911329627037048,
        0.17492622137069702,
        0.8272647857666016,
        0.07162437587976456,
        -0.622286319732666,
        -0.009533647447824478,
        -0.33441412448883057,
        -0.026087693870067596,
        -0.6560878753662109,
        -0.2978692352771759,
        0.39798837900161743,
        0.6247049570083618,
        0.00006673485040664673,
        -1.0481514930725098,
        -0.3833116292953491,
        0.035726163536310196,
        0.8237630128860474,
        0.10406824201345444,
        0.4233832061290741,
        0.4780119061470032,
        0.5090067386627197,
        -0.01697632297873497,
        0.11020930111408234,
        0.16514542698860168,
        0.28847190737724304,
        0.6639603972434998,
        -0.2846524119377136,
        -0.6825234889984131,
        -0.5572211146354675,
        0.8184484839439392,
        -0.8198765516281128,
        0.8265289068222046,
        -0.08223986625671387,
        -0.6043023467063904,
        0.038094595074653625,
        1.1409199237823486,
        1.0965559482574463,
        1.0263164043426514,
        -0.08957868069410324,
        0.17058837413787842,
        -0.14644888043403625,
        0.4093320369720459,
        0.10747063159942627,
        -1.3674572706222534,
        0.05127448961138725,
        0.06736817210912704,
        0.14414606988430023,
        0.5705865025520325,
        0.21495281159877777,
        0.2563115358352661,
        -0.24189318716526031,
        0.10090410709381104,
        -0.07788696885108948,
        1.679319143295288,
        0.09094448387622833,
        0.09236197918653488,
        0.4571962058544159,
        0.3018400967121124,
        0.40493327379226685,
        -0.4532642066478729,
        -1.8010051250457764,
        0.2162712663412094,
        0.011669375002384186,
        0.5937740206718445,
        0.025361746549606323,
        0.5497811436653137,
        0.4666285216808319,
        -0.3497987687587738,
        -0.00033241475466638803,
        -0.10315778851509094,
        -0.2677999436855316,
        -0.9233026504516602,
        -0.2131737768650055,
        -0.07163622975349426,
        0.23606513440608978,
        0.1482226550579071,
        -0.4422205090522766,
        0.1546347439289093,
        -0.09427262842655182,
        0.2580926716327667,
        0.1471618413925171,
        0.03155515342950821,
        -0.29303357005119324,
        -0.3307500183582306,
        0.36196982860565186,
        -0.28652048110961914,
        -0.2048022449016571,
        0.4963719844818115,
        -0.8836777806282043,
        0.06746502965688705,
        -0.942091166973114,
        -1.147271752357483,
        -0.6039124131202698,
        0.7730223536491394,
        -0.5960198640823364,
        -0.566929817199707,
        -0.6214216351509094,
        0.08146767318248749,
        2.1099860668182373,
        -0.27021074295043945,
        0.3275989294052124,
        0.131414532661438,
        0.5358440279960632,
        -0.1280725598335266,
        -0.06424665451049805,
        0.07394781708717346,
        0.4051446318626404,
        0.1582949459552765,
        -1.286729335784912,
        -0.42570775747299194,
        0.6139353513717651,
        -0.046339426189661026,
        -0.2404244840145111,
        0.7194831371307373,
        -0.7205560207366943,
        0.15172463655471802,
        -0.18078188598155975,
        0.03049890324473381,
        0.36295488476753235,
        -0.010967373847961426,
        0.5384576320648193,
        0.7952316999435425,
        0.21798232197761536,
        -1.293399691581726,
        0.11842851340770721,
        0.3177613615989685,
        -0.44652122259140015,
        0.46171560883522034,
        0.03248800337314606,
        0.4980700612068176,
        -0.14142173528671265,
        -0.08351953327655792,
        -0.3825305104255676,
        1.128940224647522,
        -0.015036037191748619,
        -0.2630714178085327,
        0.38940635323524475,
        0.016625717282295227,
        0.9656641483306885,
        -0.6323450207710266,
        0.4274411201477051,
        -0.34589630365371704,
        -0.1421857476234436,
        -0.1323295682668686,
        0.14904822409152985,
        1.1340446472167969,
        0.6461798548698425,
        0.26590490341186523,
        0.04642805829644203,
        0.7370237112045288,
        -0.3423807621002197,
        -0.7672647833824158,
        0.35396990180015564,
        -0.3342472314834595,
        -0.4998919665813446,
        1.0721372365951538,
        0.15217585861682892,
        -0.21914497017860413,
        0.44130849838256836,
        0.8930479288101196,
        -0.567560613155365,
        0.04521401971578598,
        -0.5200872421264648,
        1.0878589153289795,
        -0.44201183319091797,
        -0.36767932772636414,
        -0.5155854821205139,
        0.8489558696746826,
        -0.4456537663936615,
        -0.05598127841949463,
        -0.020926158875226974,
        -0.41681012511253357,
        -0.015517015010118484,
        0.4596470296382904,
        0.5444327592849731,
        -0.5391355752944946,
        0.49455198645591736,
        0.5116311311721802,
        0.21309266984462738,
        -0.07149256765842438,
        0.22918249666690826,
        0.49682554602622986,
        0.4075571596622467,
        -0.3775101900100708,
        0.9847166538238525,
        -0.3040357232093811,
        -0.6664254069328308,
        -1.2920358180999756
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and organizes the most engaging topics from a transcript with corresponding timestamps. This process involves a detailed review of the transcript to identify key moments and subjects. The output is a list of topics with their timestamps in a sequential format.",
          "name": "Create_video_chapters",
          "raw": "\n                workflow Create_video_chapters v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert conversation topic and timestamp creator. You take a transcript and you extract the most interesting topics discussed and give timestamps for where in the video they occur.\n\nTake a step back and think step-by-step about how you would do this. You would probably start by \\\"watching\\\" the video (via the transcript) and taking notes on the topics discussed and the time they were discussed. Then you would take those notes and create a list of topics and timestamps.\n\n# STEPS\n\n- Fully consume the transcript as if you're watching or listening to the content.\n\n- Think deeply about the topics discussed and what were the most interesting subjects and moments in the content.\n\n- Name those subjects and/moments in 2-3 capitalized words.\n\n- Match the timestamps to the topics. Note that input timestamps have the following format: HOURS:MINUTES:SECONDS.MILLISECONDS, which is not the same as the OUTPUT format!\n\nINPUT SAMPLE\n\n[02:17:43.120 --> 02:17:49.200] same way. I'll just say the same. And I look forward to hearing the response to my job application\n[02:17:49.200 --> 02:17:55.040] that I've submitted. Oh, you're accepted. Oh, yeah. We all speak of you all the time. Thank you so\n[02:17:55.040 --> 02:18:00.720] much. Thank you, guys. Thank you. Thanks for listening to this conversation with Neri Oxman.\n[02:18:00.720 --> 02:18:05.520] To support this podcast, please check out our sponsors in the description. And now,\n\nEND INPUT SAMPLE\n\nThe OUTPUT TIMESTAMP format is:\n00:00:00 (HOURS:MINUTES:SECONDS) (HH:MM:SS)\n\n- Note the maximum length of the video based on the last timestamp.\n\n- Ensure all output timestamps are sequential and fall within the length of the content.\n\n# OUTPUT INSTRUCTIONS\n\nEXAMPLE OUTPUT (Hours:Minutes:Seconds)\n\n00:00:00 Members-only Forum Access\n00:00:10 Live Hacking Demo\n00:00:26 Ideas vs. Book\n00:00:30 Meeting Will Smith\n00:00:44 How to Influence Others\n00:01:34 Learning by Reading\n00:58:30 Writing With Punch\n00:59:22 100 Posts or GTFO\n01:00:32 How to Gain Followers\n01:01:31 The Music That Shapes\n01:27:21 Subdomain Enumeration Demo\n01:28:40 Hiding in Plain Sight\n01:29:06 The Universe Machine\n00:09:36 Early School Experiences\n00:10:12 The First Business Failure\n00:10:32 David Foster Wallace\n00:12:07 Copying Other Writers\n00:12:32 Practical Advice for N00bs\n\nEND EXAMPLE OUTPUT\n\n- Ensure all output timestamps are sequential and fall within the length of the content, e.g., if the total length of the video is 24 minutes. (00:00:00 - 00:24:00), then no output can be 01:01:25, or anything over 00:25:00 or over!\n\n- ENSURE the output timestamps and topics are shown gradually and evenly incrementing from 00:00:00 to the final timestamp of the content.\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert conversation topic and timestamp creator. You take a transcript and you extract the most interesting topics discussed and give timestamps for where in the video they occur.\n\nTake a step back and think step-by-step about how you would do this. You would probably start by \\\"watching\\\" the video (via the transcript) and taking notes on the topics discussed and the time they were discussed. Then you would take those notes and create a list of topics and timestamps.\n\n# STEPS\n\n- Fully consume the transcript as if you're watching or listening to the content.\n\n- Think deeply about the topics discussed and what were the most interesting subjects and moments in the content.\n\n- Name those subjects and/moments in 2-3 capitalized words.\n\n- Match the timestamps to the topics. Note that input timestamps have the following format: HOURS:MINUTES:SECONDS.MILLISECONDS, which is not the same as the OUTPUT format!\n\nINPUT SAMPLE\n\n[02:17:43.120 --> 02:17:49.200] same way. I'll just say the same. And I look forward to hearing the response to my job application\n[02:17:49.200 --> 02:17:55.040] that I've submitted. Oh, you're accepted. Oh, yeah. We all speak of you all the time. Thank you so\n[02:17:55.040 --> 02:18:00.720] much. Thank you, guys. Thank you. Thanks for listening to this conversation with Neri Oxman.\n[02:18:00.720 --> 02:18:05.520] To support this podcast, please check out our sponsors in the description. And now,\n\nEND INPUT SAMPLE\n\nThe OUTPUT TIMESTAMP format is:\n00:00:00 (HOURS:MINUTES:SECONDS) (HH:MM:SS)\n\n- Note the maximum length of the video based on the last timestamp.\n\n- Ensure all output timestamps are sequential and fall within the length of the content.\n\n# OUTPUT INSTRUCTIONS\n\nEXAMPLE OUTPUT (Hours:Minutes:Seconds)\n\n00:00:00 Members-only Forum Access\n00:00:10 Live Hacking Demo\n00:00:26 Ideas vs. Book\n00:00:30 Meeting Will Smith\n00:00:44 How to Influence Others\n00:01:34 Learning by Reading\n00:58:30 Writing With Punch\n00:59:22 100 Posts or GTFO\n01:00:32 How to Gain Followers\n01:01:31 The Music That Shapes\n01:27:21 Subdomain Enumeration Demo\n01:28:40 Hiding in Plain Sight\n01:29:06 The Universe Machine\n00:09:36 Early School Experiences\n00:10:12 The First Business Failure\n00:10:32 David Foster Wallace\n00:12:07 Copying Other Writers\n00:12:32 Practical Advice for N00bs\n\nEND EXAMPLE OUTPUT\n\n- Ensure all output timestamps are sequential and fall within the length of the content, e.g., if the total length of the video is 24 minutes. (00:00:00 - 00:24:00), then no output can be 01:01:25, or anything over 00:25:00 or over!\n\n- ENSURE the output timestamps and topics are shown gradually and evenly incrementing from 00:00:00 to the final timestamp of the content.\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.29323843121528625,
        0.6219842433929443,
        -0.38656777143478394,
        0.2914719879627228,
        -0.49537214636802673,
        0.4792099595069885,
        -1.0977624654769897,
        -0.369351863861084,
        0.10419841855764389,
        0.2892622947692871,
        -0.4370954632759094,
        1.0887318849563599,
        0.475140243768692,
        -0.2076093554496765,
        0.06071476638317108,
        -0.5673248767852783,
        0.3722388744354248,
        -0.6052570939064026,
        -1.4475386142730713,
        -0.27661633491516113,
        -0.12133727967739105,
        0.5660257339477539,
        0.5423321723937988,
        0.07954289764165878,
        0.3593650460243225,
        -0.5572214126586914,
        0.08600917458534241,
        -0.0877358689904213,
        -0.6067470908164978,
        -1.2990621328353882,
        0.4805508852005005,
        -0.4689406454563141,
        -0.5982844233512878,
        -0.30728644132614136,
        0.7893637418746948,
        -1.0895779132843018,
        -0.48024171590805054,
        0.08758123964071274,
        -0.10394494235515594,
        -0.41651809215545654,
        -0.016598740592598915,
        0.19382140040397644,
        -0.655072808265686,
        -0.5242319703102112,
        0.923526406288147,
        0.23569077253341675,
        0.20532336831092834,
        -0.28132277727127075,
        0.6048551797866821,
        0.18447335064411163,
        0.09794758260250092,
        -0.8328165411949158,
        0.047672878950834274,
        -0.22750690579414368,
        -0.44780629873275757,
        -0.09091112017631531,
        -0.44682586193084717,
        -0.9667268991470337,
        0.2818927466869354,
        0.4405500292778015,
        0.1657443642616272,
        1.0338330268859863,
        -3.2175838947296143,
        -0.11778786778450012,
        0.039149556308984756,
        0.17581729590892792,
        0.27051496505737305,
        -0.02938392385840416,
        0.5014438629150391,
        0.2278507947921753,
        0.3302467465400696,
        0.5770090818405151,
        -0.1362200379371643,
        0.2943681478500366,
        0.38135239481925964,
        -0.38043633103370667,
        0.4023561477661133,
        -0.10896272957324982,
        0.08205131441354752,
        -0.33410972356796265,
        0.35702985525131226,
        0.34802764654159546,
        0.1125359758734703,
        0.07178758829832077,
        -0.061205942183732986,
        0.5308102369308472,
        -0.4376385509967804,
        -0.2762182950973511,
        0.0336017869412899,
        -0.17094282805919647,
        -0.48528870940208435,
        0.03713926672935486,
        -0.0178495142608881,
        -0.44716814160346985,
        -0.511032223701477,
        -0.03197535127401352,
        0.3532233238220215,
        0.25294527411460876,
        0.11976726353168488,
        3.176348924636841,
        0.5435072183609009,
        -0.18539758026599884,
        0.6623128056526184,
        -1.1698428392410278,
        0.20688191056251526,
        -0.14922891557216644,
        0.4385940432548523,
        -0.5719408988952637,
        -0.04730084910988808,
        -0.5436162352561951,
        0.9469354748725891,
        -1.2843797206878662,
        -0.36816897988319397,
        0.24013245105743408,
        0.8077135682106018,
        0.4180930554866791,
        -0.2792779207229614,
        0.13376571238040924,
        -0.3672920763492584,
        0.6407489776611328,
        -0.2010776549577713,
        -0.01806255429983139,
        -0.29761961102485657,
        -0.3860238790512085,
        0.27478912472724915,
        -0.2643792927265167,
        -0.6190063953399658,
        0.6589946150779724,
        -0.25437021255493164,
        0.6945735216140747,
        -0.29050758481025696,
        -0.09296615421772003,
        0.10812361538410187,
        -0.9605669379234314,
        0.003727477043867111,
        -0.028808295726776123,
        0.07181359827518463,
        -0.7618895173072815,
        -0.11068055778741837,
        0.18214306235313416,
        0.3819420635700226,
        -0.7219147086143494,
        0.443148672580719,
        0.17711924016475677,
        0.35685765743255615,
        1.2117747068405151,
        0.16886195540428162,
        0.11619965732097626,
        -0.10466518998146057,
        -0.7838617563247681,
        -0.1475788950920105,
        -0.16574370861053467,
        -0.36850911378860474,
        0.004954520612955093,
        0.5153353810310364,
        -0.014127760194242,
        -0.3932036757469177,
        -0.11263278126716614,
        -0.9180488586425781,
        0.3190275728702545,
        0.05015721544623375,
        0.0359782874584198,
        -0.16039025783538818,
        0.10449479520320892,
        0.2661972939968109,
        -0.1551712155342102,
        0.06479672342538834,
        0.05871860682964325,
        0.4581105709075928,
        0.45921897888183594,
        0.14492745697498322,
        -0.33369991183280945,
        0.3003210723400116,
        0.7818412184715271,
        -0.49510759115219116,
        0.16217626631259918,
        -0.1937992125749588,
        0.41463086009025574,
        0.6338642239570618,
        -0.100328728556633,
        0.6836227774620056,
        0.41845449805259705,
        0.03197323903441429,
        -0.8535035848617554,
        -0.1505524218082428,
        0.10646621882915497,
        -0.16754105687141418,
        0.49424564838409424,
        0.6365534663200378,
        1.0474750995635986,
        -0.7191682457923889,
        1.9222744703292847,
        -1.0036299228668213,
        0.3744317591190338,
        0.7021999359130859,
        0.44771701097488403,
        0.3230462074279785,
        0.22471106052398682,
        0.49053770303726196,
        -0.16663676500320435,
        -1.1744003295898438,
        -0.6718860268592834,
        -0.37388497591018677,
        0.4016684293746948,
        -0.3853028118610382,
        -0.5729876756668091,
        0.06556639075279236,
        0.7301740050315857,
        -0.1602240800857544,
        -0.5980684161186218,
        -0.3387213945388794,
        0.8660291433334351,
        1.4911105632781982,
        0.6443602442741394,
        1.066481351852417,
        -0.22427068650722504,
        0.4195002615451813,
        -0.3174668848514557,
        0.5118193626403809,
        -0.024191901087760925,
        0.3315848708152771,
        0.8018752932548523,
        -0.6271862387657166,
        -0.7694937586784363,
        -0.44873419404029846,
        0.2932473123073578,
        -0.32727229595184326,
        0.494173526763916,
        -1.0483794212341309,
        -0.5435338020324707,
        0.032463617622852325,
        0.9353721737861633,
        0.8379274010658264,
        1.631477952003479,
        0.17467719316482544,
        0.3079473674297333,
        -0.17226985096931458,
        0.2914454936981201,
        -0.4171545207500458,
        -1.068646788597107,
        0.9738914370536804,
        0.6742050647735596,
        0.13801126182079315,
        0.16618455946445465,
        -0.29439231753349304,
        0.506664514541626,
        -0.8997628092765808,
        -0.9422650337219238,
        -0.16370506584644318,
        1.9843173027038574,
        0.15938973426818848,
        -0.6511321067810059,
        0.22220413386821747,
        0.3875912129878998,
        -0.1741236448287964,
        -0.5555866360664368,
        -2.042696714401245,
        -0.34288638830184937,
        -0.6728790402412415,
        0.40301409363746643,
        -0.2567347586154938,
        -0.4471203088760376,
        0.24634391069412231,
        0.32084402441978455,
        -0.029903000220656395,
        0.253267765045166,
        -0.36729365587234497,
        -0.681779146194458,
        -0.6548975706100464,
        -0.22973349690437317,
        0.7598797082901001,
        -0.013692587614059448,
        0.025631792843341827,
        0.4584367573261261,
        0.7187378406524658,
        0.05930393189191818,
        0.36519524455070496,
        0.1338295042514801,
        -0.18971344828605652,
        -0.5034197568893433,
        0.6466965079307556,
        -0.10785120725631714,
        -0.2967317998409271,
        -0.18634776771068573,
        -0.41756048798561096,
        -0.4979543387889862,
        -0.39393994212150574,
        -0.5152618885040283,
        -0.1276131570339203,
        0.9915729761123657,
        -0.3306017220020294,
        -0.8467801809310913,
        -0.2745611369609833,
        0.0698537528514862,
        1.8727521896362305,
        0.5586140155792236,
        -0.3067079782485962,
        0.8126545548439026,
        -0.016325436532497406,
        -0.31215256452560425,
        -0.19110199809074402,
        0.519825279712677,
        0.03977414593100548,
        0.050519879907369614,
        -0.9842782616615295,
        -0.4670625627040863,
        0.023005042225122452,
        -0.03210511803627014,
        -0.6469944715499878,
        0.39548343420028687,
        -0.6001633405685425,
        -0.1791972517967224,
        -0.14781776070594788,
        0.23109830915927887,
        0.11957769095897675,
        -0.004999071359634399,
        0.37622302770614624,
        0.9852598905563354,
        -0.5461004376411438,
        -1.3184572458267212,
        -0.5040034055709839,
        0.6562986969947815,
        0.4110957384109497,
        -0.18885576725006104,
        -0.30216512084007263,
        0.18367624282836914,
        -0.004717424511909485,
        -0.10092248022556305,
        -0.12402328103780746,
        1.0810059309005737,
        0.8490334153175354,
        -0.2445908784866333,
        0.0963592678308487,
        -0.2478502094745636,
        1.1034923791885376,
        0.21863868832588196,
        0.41359829902648926,
        -0.6743981838226318,
        -0.09385861456394196,
        -0.5629323124885559,
        -0.20032477378845215,
        1.2049217224121094,
        0.11620543152093887,
        0.4100807309150696,
        0.2831101417541504,
        0.00426454097032547,
        -1.1901828050613403,
        -0.7172979116439819,
        -0.2747577428817749,
        -0.0312541127204895,
        -0.2801820933818817,
        0.8493478894233704,
        0.03736010193824768,
        0.15530025959014893,
        0.70854651927948,
        0.14330896735191345,
        -0.21986152231693268,
        -0.01830437406897545,
        -0.8423256278038025,
        1.872805118560791,
        -0.18902906775474548,
        -0.7824342846870422,
        0.1188751831650734,
        0.1494840383529663,
        -0.5381093621253967,
        -0.4941851496696472,
        0.3703005313873291,
        -0.2662423253059387,
        0.578253984451294,
        -0.010453686118125916,
        0.24056914448738098,
        -0.20191162824630737,
        0.130405455827713,
        0.7433799505233765,
        0.5920106768608093,
        -0.3942723274230957,
        0.09707841277122498,
        0.3226557970046997,
        0.14940030872821808,
        -0.14076730608940125,
        0.742673933506012,
        -0.5439071655273438,
        -0.4832717776298523,
        -1.1275111436843872
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Transforms complex ideas into simplified ASCII art visualizations. This approach focuses on distilling intricate concepts into visual forms that can be easily understood through ASCII art. The expected output is a detailed ASCII art representation accompanied by a concise visual explanation.",
          "name": "Create_visualization",
          "raw": "\n                workflow Create_visualization v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using ASCII art.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using ASCII art.\n\nYou always output ASCII art, even if you have to simplify the input concepts to a point where it can be visualized using ASCII art.\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using elaborate and intricate ASCII art.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Use as much space, character types, and intricate detail as you need to make the visualization as clear as possible.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the ASCII art, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n- If it's still too hard, create a piece of ASCII art that represents the idea artistically rather than technically.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just make an image. If it's too complex for a simple ASCII image, reduce the image's complexity until it can be rendered using ASCII.\n\n- DO NOT COMPLAIN. Make a printable image no matter what.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- You only output the printable portion of the ASCII art. You do not output the non-printable characters.\n\n- Ensure the visualization can stand alone as a diagram that fully conveys the concept(s), and that it perfectly matches a written explanation of the concepts themselves. Start over if it can't.\n\n- Ensure all output ASCII art characters are fully printable and viewable.\n\n- Ensure the diagram will fit within a reasonable width in a large window, so the viewer won't have to reduce the font like 1000 times.\n\n- Create a diagram no matter what, using the STEPS above to determine which type.\n\n- Do not output blank lines or lines full of unprintable / invisible characters. Only output the printable portion of the ASCII art.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at data and concept visualization and in turning complex ideas into a form that can be visualized using ASCII art.\n\nYou take input of any type and find the best way to simply visualize or demonstrate the core ideas using ASCII art.\n\nYou always output ASCII art, even if you have to simplify the input concepts to a point where it can be visualized using ASCII art.\n\n# STEPS\n\n- Take the input given and create a visualization that best explains it using elaborate and intricate ASCII art.\n\n- Ensure that the visual would work as a standalone diagram that would fully convey the concept(s).\n\n- Use visual elements such as boxes and arrows and labels (and whatever else) to show the relationships between the data, the concepts, and whatever else, when appropriate.\n\n- Use as much space, character types, and intricate detail as you need to make the visualization as clear as possible.\n\n- Create far more intricate and more elaborate and larger visualizations for concepts that are more complex or have more data.\n\n- Under the ASCII art, output a section called VISUAL EXPLANATION that explains in a set of 10-word bullets how the input was turned into the visualization. Ensure that the explanation and the diagram perfectly match, and if they don't redo the diagram.\n\n- If the visualization covers too many things, summarize it into it's primary takeaway and visualize that instead.\n\n- DO NOT COMPLAIN AND GIVE UP. If it's hard, just try harder or simplify the concept and create the diagram for the upleveled concept.\n\n- If it's still too hard, create a piece of ASCII art that represents the idea artistically rather than technically.\n\n# OUTPUT INSTRUCTIONS\n\n- DO NOT COMPLAIN. Just make an image. If it's too complex for a simple ASCII image, reduce the image's complexity until it can be rendered using ASCII.\n\n- DO NOT COMPLAIN. Make a printable image no matter what.\n\n- Do not output any code indicators like backticks or code blocks or anything.\n\n- You only output the printable portion of the ASCII art. You do not output the non-printable characters.\n\n- Ensure the visualization can stand alone as a diagram that fully conveys the concept(s), and that it perfectly matches a written explanation of the concepts themselves. Start over if it can't.\n\n- Ensure all output ASCII art characters are fully printable and viewable.\n\n- Ensure the diagram will fit within a reasonable width in a large window, so the viewer won't have to reduce the font like 1000 times.\n\n- Create a diagram no matter what, using the STEPS above to determine which type.\n\n- Do not output blank lines or lines full of unprintable / invisible characters. Only output the printable portion of the ASCII art.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.241362527012825,
        0.43268752098083496,
        -0.18423038721084595,
        0.706672728061676,
        0.7763610482215881,
        0.06389079988002777,
        -0.9864991903305054,
        0.4066685438156128,
        0.26290521025657654,
        0.7149185538291931,
        -0.3779478371143341,
        0.6902942061424255,
        0.2789764404296875,
        -0.03999447822570801,
        0.04745719954371452,
        0.020694486796855927,
        -0.5173629522323608,
        -1.247760534286499,
        -1.3983769416809082,
        -0.5362212061882019,
        -0.44201844930648804,
        1.087244987487793,
        0.1903758943080902,
        0.13228276371955872,
        0.95857834815979,
        -0.1291692852973938,
        -0.5635543465614319,
        0.15067695081233978,
        -1.1021356582641602,
        -1.526562213897705,
        0.3640523850917816,
        -0.03914657235145569,
        -0.4371618330478668,
        -0.2068714201450348,
        0.25205421447753906,
        -0.8523789644241333,
        0.5756074786186218,
        -0.49493497610092163,
        -0.4432695806026459,
        -0.0757826715707779,
        -0.6822810769081116,
        0.09013476967811584,
        -0.30405834317207336,
        -0.5550571084022522,
        0.6201050877571106,
        -0.09588360041379929,
        0.05936597287654877,
        -0.2740265130996704,
        1.450879454612732,
        0.16569988429546356,
        -0.1306668370962143,
        -0.33678922057151794,
        -0.059610515832901,
        -0.34189605712890625,
        -0.39353492856025696,
        -0.024345839396119118,
        0.06721203029155731,
        -0.7608844637870789,
        0.17888781428337097,
        0.09258319437503815,
        0.17723049223423004,
        0.6013500094413757,
        -3.3230650424957275,
        -0.287053644657135,
        -0.3790506422519684,
        -0.09218529611825943,
        0.12315071374177933,
        -0.23235461115837097,
        0.4028128385543823,
        -0.25993090867996216,
        -0.7488622665405273,
        0.3862375319004059,
        -0.0045294128358364105,
        0.7395952939987183,
        0.3989035189151764,
        -0.018518656492233276,
        0.2023845911026001,
        -0.07826055586338043,
        0.4380454421043396,
        -0.4940384030342102,
        -0.39475616812705994,
        0.13666574656963348,
        -0.008982481434941292,
        0.024227406829595566,
        -0.5297984480857849,
        0.6714053750038147,
        0.16488125920295715,
        -0.09483030438423157,
        0.3061980605125427,
        -0.08534801006317139,
        -0.4945426285266876,
        -0.3674919307231903,
        0.43753522634506226,
        0.11771894246339798,
        -1.088371753692627,
        -0.6910066604614258,
        -0.028161734342575073,
        0.34106558561325073,
        0.003129728138446808,
        3.2588143348693848,
        0.6478011608123779,
        0.5571568608283997,
        0.22791439294815063,
        -0.8964428305625916,
        0.5787220001220703,
        -0.28438761830329895,
        0.2052532136440277,
        -0.6605667471885681,
        0.18618786334991455,
        -0.7700873613357544,
        0.47139686346054077,
        -0.8281770348548889,
        -0.4231474697589874,
        0.3136967420578003,
        -0.08208999037742615,
        0.739020049571991,
        -0.20054945349693298,
        0.2102484256029129,
        0.31901344656944275,
        1.1617469787597656,
        -0.6008573770523071,
        0.008433361537754536,
        -0.19645492732524872,
        -0.33242321014404297,
        -0.35024091601371765,
        -0.06022198498249054,
        -0.2172917127609253,
        0.7302196025848389,
        0.37788379192352295,
        0.727986216545105,
        0.35682210326194763,
        -0.11978763341903687,
        0.06086059287190437,
        0.4965808391571045,
        0.1207842007279396,
        0.053693681955337524,
        0.26801973581314087,
        -0.2822714149951935,
        0.11614849418401718,
        -0.4742029011249542,
        0.505764365196228,
        -1.005271315574646,
        0.4139852523803711,
        0.2837783098220825,
        0.240626722574234,
        0.08648818731307983,
        0.014948975294828415,
        0.6863824725151062,
        -0.8496366143226624,
        -0.9278466701507568,
        -0.0030327290296554565,
        0.7486147880554199,
        -0.11309152841567993,
        0.5175996422767639,
        0.42965608835220337,
        0.7884870767593384,
        -0.1562698781490326,
        0.3261464834213257,
        -0.3479098081588745,
        0.2678828239440918,
        0.19531448185443878,
        -0.075200654566288,
        -0.28407248854637146,
        0.051153190433979034,
        1.3147488832473755,
        0.327775776386261,
        0.2893918752670288,
        -0.33660459518432617,
        0.16070246696472168,
        0.5094335079193115,
        0.6412807703018188,
        -0.41890349984169006,
        0.2820280194282532,
        0.6611887812614441,
        -0.5868822932243347,
        -0.11814619600772858,
        -0.19276946783065796,
        0.4069289267063141,
        -0.06857898831367493,
        -0.11157216876745224,
        0.12406067550182343,
        0.4692797064781189,
        0.0114666186273098,
        -0.9976158142089844,
        -0.12637345492839813,
        0.6783462166786194,
        0.6082074046134949,
        0.3802891671657562,
        0.52052903175354,
        1.185463786125183,
        -0.9642112255096436,
        2.4023165702819824,
        -0.8008527159690857,
        -0.5168770551681519,
        0.47475990653038025,
        -0.06448117643594742,
        0.43775516748428345,
        0.09712541103363037,
        0.8152816295623779,
        -0.1544598639011383,
        -1.117677927017212,
        -0.1675155907869339,
        -0.34226569533348083,
        0.09606999903917313,
        -0.4114246368408203,
        -0.3373457193374634,
        0.37361058592796326,
        -0.10920719057321548,
        0.14301390945911407,
        -1.1440045833587646,
        0.5432424545288086,
        0.12822961807250977,
        0.7701106667518616,
        0.2365913987159729,
        0.7792659997940063,
        -0.5908315777778625,
        0.31807443499565125,
        0.355856716632843,
        -0.12599636614322662,
        0.3203672766685486,
        -0.3534524738788605,
        -0.08917953819036484,
        -0.4708186388015747,
        -0.7043907642364502,
        -0.695977509021759,
        0.9962134957313538,
        -0.06481096148490906,
        1.001161813735962,
        -0.9183917045593262,
        -0.7265397906303406,
        -0.24952098727226257,
        1.2590538263320923,
        0.8572335243225098,
        1.2620283365249634,
        -0.13995864987373352,
        0.7289233803749084,
        -0.33537057042121887,
        0.15157510340213776,
        -0.27092239260673523,
        -0.6900046467781067,
        -0.3390125632286072,
        -0.26151832938194275,
        -0.31777888536453247,
        0.63190096616745,
        -0.015057571232318878,
        -0.06823906302452087,
        -0.7756110429763794,
        -0.37982213497161865,
        -0.6844280958175659,
        1.3492484092712402,
        0.4702070355415344,
        -0.06779786944389343,
        -0.5529250502586365,
        0.5309857130050659,
        -0.03647390007972717,
        -0.34381628036499023,
        -1.781114935874939,
        -0.5884685516357422,
        -0.640385091304779,
        -0.34813082218170166,
        -0.5344438552856445,
        -0.15288180112838745,
        0.7860909104347229,
        -0.015113187953829765,
        -0.14181901514530182,
        0.1356266587972641,
        -0.27877119183540344,
        -0.4970942735671997,
        -0.612878680229187,
        0.06984192878007889,
        -0.013761689886450768,
        -0.037962406873703,
        0.26963335275650024,
        -0.44410938024520874,
        -0.23867298662662506,
        0.07902337610721588,
        -0.0018371939659118652,
        0.17834755778312683,
        -0.19131048023700714,
        -0.16214126348495483,
        0.1560991406440735,
        0.5355880260467529,
        0.0031372830271720886,
        0.6788227558135986,
        -0.16651582717895508,
        -0.007317334413528442,
        -0.35317423939704895,
        -0.6519230604171753,
        0.2593640387058258,
        0.4786752760410309,
        -0.3060401678085327,
        -0.5738025903701782,
        -0.8703916668891907,
        -0.3897918462753296,
        1.928248405456543,
        0.38255155086517334,
        -0.3660542368888855,
        1.2465636730194092,
        0.5263834595680237,
        0.180069699883461,
        -0.2892814576625824,
        0.5467604994773865,
        0.153078094124794,
        -0.21577134728431702,
        -0.29706239700317383,
        -0.3832467794418335,
        0.637173593044281,
        -0.1905709207057953,
        -0.08107902854681015,
        0.1831558495759964,
        -0.8854495882987976,
        0.2202916145324707,
        -0.28777432441711426,
        -0.12918415665626526,
        0.7492040395736694,
        -0.481606662273407,
        0.7700620293617249,
        0.8721412420272827,
        -0.15526771545410156,
        -1.5340402126312256,
        0.08810366690158844,
        1.320948600769043,
        -0.7195647358894348,
        -0.37864410877227783,
        -0.47708380222320557,
        0.14617133140563965,
        0.09682057797908783,
        -0.08875815570354462,
        -0.33815181255340576,
        0.9578306674957275,
        0.6747275590896606,
        0.07601156085729599,
        0.13275396823883057,
        -0.23992973566055298,
        0.6268488168716431,
        0.39766165614128113,
        0.1305728256702423,
        -0.6832584142684937,
        -0.6147717237472534,
        0.22526586055755615,
        -0.22098177671432495,
        1.0667520761489868,
        -0.059127792716026306,
        0.3400551676750183,
        0.3222416043281555,
        0.06124427542090416,
        -0.8030202984809875,
        -0.9050781726837158,
        0.3652800917625427,
        -0.2082003653049469,
        -0.4937082827091217,
        1.192099928855896,
        0.16333875060081482,
        -0.37386107444763184,
        0.28181520104408264,
        -0.18728268146514893,
        -0.3154481053352356,
        0.4867086112499237,
        -0.5371917486190796,
        1.2729389667510986,
        -0.2069915384054184,
        -0.3909606635570526,
        -0.6556220054626465,
        -0.29343488812446594,
        -0.15035130083560944,
        0.30877235531806946,
        0.3807509243488312,
        -0.657336413860321,
        -0.12012772262096405,
        0.6997460722923279,
        0.4203750193119049,
        -0.0733545646071434,
        0.6801197528839111,
        0.7905718088150024,
        0.4789460003376007,
        -0.052167028188705444,
        -0.34125763177871704,
        0.361918568611145,
        -0.1792367696762085,
        -0.24244755506515503,
        -0.0027623921632766724,
        0.12241455912590027,
        -0.9624439477920532,
        -0.773600161075592
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes and explains code, security tool outputs, or configuration texts, tailoring the explanation to the type of input. It uses specific sections to clarify the function, implications, or settings based on the input's nature. The expected output is a detailed explanation or answer in designated sections.",
          "name": "Explain_code",
          "raw": "\n                workflow Explain_code v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert coder that takes code and documentation as input and do your best to explain it.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps. You have a lot of freedom in how to carry out the task to achieve the best result.\n\n# OUTPUT SECTIONS\n\n- If the content is code, you explain what the code does in a section called EXPLANATION:. \n\n- If the content is security tool output, you explain the implications of the output in a section called SECURITY IMPLICATIONS:.\n\n- If the content is configuration text, you explain what the settings do in a section called CONFIGURATION EXPLANATION:.\n\n- If there was a question in the input, answer that question about the input specifically in a section called ANSWER:.\n\n# OUTPUT \n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\n \n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert coder that takes code and documentation as input and do your best to explain it.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps. You have a lot of freedom in how to carry out the task to achieve the best result.\n\n# OUTPUT SECTIONS\n\n- If the content is code, you explain what the code does in a section called EXPLANATION:. \n\n- If the content is security tool output, you explain the implications of the output in a section called SECURITY IMPLICATIONS:.\n\n- If the content is configuration text, you explain what the settings do in a section called CONFIGURATION EXPLANATION:.\n\n- If there was a question in the input, answer that question about the input specifically in a section called ANSWER:.\n\n# OUTPUT \n\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\n \n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6622015237808228,
        0.05350726842880249,
        0.25769200921058655,
        0.31419992446899414,
        0.3969460725784302,
        -0.09985291212797165,
        -1.316583275794983,
        0.7477039098739624,
        0.3577593266963959,
        0.17576339840888977,
        0.018036922439932823,
        0.7967595458030701,
        0.11912865191698074,
        -0.18604794144630432,
        -0.03408987447619438,
        -0.2532329261302948,
        0.05364381894469261,
        -1.2723665237426758,
        -1.2150049209594727,
        -0.37279126048088074,
        -0.32897207140922546,
        0.8380736708641052,
        0.25383859872817993,
        0.35342299938201904,
        0.8235443234443665,
        0.27692660689353943,
        -0.47571149468421936,
        -0.21826456487178802,
        -1.1825032234191895,
        -1.740211844444275,
        0.6012563705444336,
        0.4561181366443634,
        -0.28033527731895447,
        -0.6028223037719727,
        0.13091880083084106,
        -0.7746108770370483,
        0.04981733113527298,
        -0.2935559153556824,
        -0.2592433989048004,
        -0.09298646450042725,
        -0.16652002930641174,
        0.5444226861000061,
        -0.2670486867427826,
        -0.28081217408180237,
        0.6102614402770996,
        -0.45025384426116943,
        0.008909249678254128,
        -0.2024429589509964,
        1.1524766683578491,
        0.14143182337284088,
        -0.04378306493163109,
        -0.49083229899406433,
        0.04338323697447777,
        -0.25378480553627014,
        -0.22106030583381653,
        0.0840497612953186,
        0.17023096978664398,
        -0.6332531571388245,
        0.034584708511829376,
        -0.05851065367460251,
        -0.26988568902015686,
        0.6026808023452759,
        -3.3193042278289795,
        0.04630083218216896,
        -0.3689965307712555,
        0.08567246794700623,
        0.0751921609044075,
        -0.4227719306945801,
        0.6584984660148621,
        -0.45769092440605164,
        -0.41354191303253174,
        -0.06733425706624985,
        0.34454581141471863,
        0.1702289581298828,
        -0.1890881061553955,
        -0.6922231316566467,
        0.1816083937883377,
        -0.2560597062110901,
        0.45512235164642334,
        -0.5095728039741516,
        -0.4009040892124176,
        0.10437964648008347,
        -0.037431880831718445,
        -0.4031253457069397,
        -0.9389224052429199,
        0.7115603089332581,
        0.171676367521286,
        0.11122167110443115,
        0.49052369594573975,
        0.2602393925189972,
        -0.24371933937072754,
        -0.16238194704055786,
        0.10824030637741089,
        -0.06143683195114136,
        -0.46621447801589966,
        -0.19594641029834747,
        -0.10359394550323486,
        0.23515963554382324,
        -0.2394772469997406,
        3.373297929763794,
        0.5330407023429871,
        0.5318580865859985,
        0.6461946368217468,
        -1.4348329305648804,
        0.2477233111858368,
        -0.25726228952407837,
        -0.20779769122600555,
        -0.4331948757171631,
        0.14814819395542145,
        -0.3449175953865051,
        0.5473189949989319,
        -1.1367254257202148,
        -0.8762065172195435,
        -0.0232074074447155,
        0.09401663392782211,
        0.39869698882102966,
        -0.5316069722175598,
        -0.009770434349775314,
        -0.08526726812124252,
        1.0574657917022705,
        -0.49586185812950134,
        -0.17697107791900635,
        -0.2303580641746521,
        -0.2632678151130676,
        -0.053989097476005554,
        -0.00287695974111557,
        -0.16329585015773773,
        0.6387602090835571,
        -0.021315310150384903,
        0.28807970881462097,
        -0.14439958333969116,
        0.0495922826230526,
        -0.39604678750038147,
        0.08551238477230072,
        0.32101961970329285,
        0.20409461855888367,
        0.8752546906471252,
        -0.6848242282867432,
        0.0617949515581131,
        -0.37713608145713806,
        0.07688120007514954,
        -1.0452159643173218,
        0.5742519497871399,
        -0.16301031410694122,
        0.29054364562034607,
        0.37907519936561584,
        -0.10208708792924881,
        0.6297183632850647,
        0.07278501987457275,
        -0.7261163592338562,
        -0.06191372498869896,
        0.5993843078613281,
        -0.051527317613363266,
        0.2673129439353943,
        0.5451669692993164,
        0.4552376866340637,
        -0.3281060457229614,
        0.301688015460968,
        -0.9964391589164734,
        0.4773233234882355,
        0.12271745502948761,
        -0.0356125608086586,
        0.24629724025726318,
        0.25267907977104187,
        0.8751494884490967,
        -0.1070655882358551,
        0.5157346725463867,
        -0.4172208607196808,
        -0.11333967745304108,
        0.6095950603485107,
        0.12652325630187988,
        -0.695505678653717,
        0.588171124458313,
        0.9006856679916382,
        -0.8567876815795898,
        0.2966596484184265,
        0.035558342933654785,
        0.1763700246810913,
        0.23949457705020905,
        -0.7910890579223633,
        0.5048154592514038,
        0.893892228603363,
        -0.31387433409690857,
        -1.0001139640808105,
        -0.42185014486312866,
        0.16751857101917267,
        0.22559869289398193,
        0.5608224272727966,
        0.6743853092193604,
        1.117242455482483,
        -1.1706821918487549,
        1.9621397256851196,
        -0.7168607115745544,
        -0.10812781751155853,
        0.31111958622932434,
        0.1481562852859497,
        0.2237488180398941,
        -0.03482462465763092,
        0.4690803587436676,
        0.13144508004188538,
        -0.6293098330497742,
        -0.0833011344075203,
        -0.34593692421913147,
        -0.08906659483909607,
        -0.7889692783355713,
        -0.2647971510887146,
        -0.13088689744472504,
        1.0474711656570435,
        -0.044261738657951355,
        -1.0355141162872314,
        -0.20778989791870117,
        -0.12025487422943115,
        1.3156538009643555,
        0.27526411414146423,
        0.7704194784164429,
        0.535252571105957,
        0.38739147782325745,
        -0.05421918258070946,
        -0.3284424841403961,
        0.5160269737243652,
        -0.234567791223526,
        0.3212015628814697,
        -0.505215585231781,
        -0.6611671447753906,
        -0.6308527588844299,
        0.8263403177261353,
        0.22201162576675415,
        1.0944700241088867,
        -0.40670454502105713,
        -0.2742088735103607,
        0.11091345548629761,
        1.0211036205291748,
        1.4592852592468262,
        0.7102866768836975,
        -0.9362820386886597,
        0.8413922786712646,
        -0.48727303743362427,
        0.3438524007797241,
        -0.4071691036224365,
        -1.034735083580017,
        -0.7460432052612305,
        -0.2782457172870636,
        0.019733905792236328,
        0.45208901166915894,
        0.33553197979927063,
        -0.5768524408340454,
        -0.8496332764625549,
        -0.25078871846199036,
        -0.09187448024749756,
        1.2823982238769531,
        0.10157864540815353,
        -0.12308715283870697,
        -0.27302905917167664,
        0.1883588433265686,
        -0.020767785608768463,
        -0.25809019804000854,
        -1.462660312652588,
        -0.47409456968307495,
        -0.357501745223999,
        0.6025354266166687,
        -0.23464325070381165,
        0.5386985540390015,
        0.6915532350540161,
        0.2139127552509308,
        0.027366770431399345,
        -0.11939066648483276,
        -0.16985560953617096,
        -0.2911493182182312,
        -0.3336437940597534,
        0.44358453154563904,
        0.25019165873527527,
        0.05899505317211151,
        0.17001000046730042,
        -0.31715846061706543,
        -0.3311809003353119,
        0.504658043384552,
        -0.05093987286090851,
        0.36493197083473206,
        -0.7116686105728149,
        -0.5723881125450134,
        0.16843795776367188,
        0.07472071051597595,
        -0.5409045815467834,
        0.45759958028793335,
        -0.293448269367218,
        0.06283734738826752,
        -0.22254519164562225,
        -0.7377025485038757,
        -0.3937884569168091,
        0.3888983428478241,
        -0.5240803956985474,
        -0.22618915140628815,
        -0.5951557159423828,
        0.23790690302848816,
        1.9756240844726562,
        -0.10866528749465942,
        0.3500712513923645,
        0.8799629807472229,
        0.6857694983482361,
        -0.20131003856658936,
        -0.08773215860128403,
        -0.03396745026111603,
        -0.037315256893634796,
        -0.13636161386966705,
        -0.5284839868545532,
        0.12816286087036133,
        0.49391424655914307,
        0.32424625754356384,
        -0.09320612996816635,
        0.49385762214660645,
        -0.5924664735794067,
        0.14978286623954773,
        -0.5313098430633545,
        0.08691243082284927,
        0.821834146976471,
        -0.029468365013599396,
        0.7427564859390259,
        0.8748122453689575,
        0.20767974853515625,
        -1.8538098335266113,
        -1.0001957416534424,
        0.9428302645683289,
        -0.6604938507080078,
        -0.3206935524940491,
        -0.1086469441652298,
        0.4051215648651123,
        0.0062726810574531555,
        0.4166736304759979,
        -0.6947330236434937,
        0.8886743783950806,
        0.645226776599884,
        0.6202229857444763,
        0.18587031960487366,
        -0.15973219275474548,
        0.853523850440979,
        0.07137483358383179,
        0.3319641053676605,
        0.020453132688999176,
        -0.18516701459884644,
        0.0657486766576767,
        -0.06342239677906036,
        1.2399077415466309,
        -0.18256647884845734,
        -0.15782931447029114,
        0.39546680450439453,
        0.17349135875701904,
        -0.37790146470069885,
        -1.0326428413391113,
        0.1460464596748352,
        -0.43819016218185425,
        -0.5963125824928284,
        1.0189669132232666,
        -0.11119992285966873,
        -0.3091907501220703,
        0.5478811264038086,
        0.7252415418624878,
        -0.2605898082256317,
        0.5618703365325928,
        -0.2894960641860962,
        1.3870254755020142,
        -0.5722845196723938,
        -0.7480241060256958,
        -0.8333736658096313,
        -0.27170777320861816,
        -0.18295754492282867,
        0.25756728649139404,
        0.15586303174495697,
        -0.5158430933952332,
        -0.05198800563812256,
        0.22595635056495667,
        0.09786470234394073,
        -0.1260869801044464,
        0.5063834190368652,
        0.654041588306427,
        0.5069659948348999,
        0.36953550577163696,
        0.02761046215891838,
        0.08999679982662201,
        0.3964157700538635,
        -0.5176058411598206,
        0.43518298864364624,
        -0.270529180765152,
        -0.5101119875907898,
        -0.22466306388378143
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs on transforming input about tool usage into improved, structured documentation. It emphasizes clarity and utility, breaking down the process into specific sections for a comprehensive guide. The expected output includes an overview, usage syntax, common use cases, and key features of the tool.",
          "name": "Explain_docs",
          "raw": "\n                workflow Explain_docs v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at capturing, understanding, and explaining the most important parts of instructions, documentation, or other formats of input that describe how to use a tool.\n\nYou take that input and turn it into better instructions using the STEPS below.\n\nTake a deep breath and think step-by-step about how to achieve the best output.\n\n# STEPS\n\n- Take the input given on how to use a given tool or product, and output better instructions using the following format:\n\nSTART OUTPUT SECTIONS\n\n# OVERVIEW\n\nWhat It Does: (give a 25-word explanation of what the tool does.)\n\nWhy People Use It: (give a 25-word explanation of why the tool is useful.)\n\n# HOW TO USE IT\n\nMost Common Syntax: (Give the most common usage syntax.)\n\n# COMMON USE CASES\n\n(Create a list of common use cases from your knowledge base, if it contains common uses of the tool.)\n\n(Use this format for those use cases)\n\nFor Getting the Current Time: `time --get-current`\nFor Determining One's Birth Day: time `--get-birth-day`\nEtc.\n\n# MOST IMPORTANT AND USED OPTIONS AND FEATURES\n\n(Create a list of common options and switches and flags, etc., from the docs and your knowledge base, if it contains common uses of the tool.)\n\n(For each one, describe how/why it could be useful)\n\nEND OUTPUT SECTIONS\n\n# OUTPUT INSTRUCTIONS\n\n- Interpret the input as tool documentation, no matter what it is.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at capturing, understanding, and explaining the most important parts of instructions, documentation, or other formats of input that describe how to use a tool.\n\nYou take that input and turn it into better instructions using the STEPS below.\n\nTake a deep breath and think step-by-step about how to achieve the best output.\n\n# STEPS\n\n- Take the input given on how to use a given tool or product, and output better instructions using the following format:\n\nSTART OUTPUT SECTIONS\n\n# OVERVIEW\n\nWhat It Does: (give a 25-word explanation of what the tool does.)\n\nWhy People Use It: (give a 25-word explanation of why the tool is useful.)\n\n# HOW TO USE IT\n\nMost Common Syntax: (Give the most common usage syntax.)\n\n# COMMON USE CASES\n\n(Create a list of common use cases from your knowledge base, if it contains common uses of the tool.)\n\n(Use this format for those use cases)\n\nFor Getting the Current Time: `time --get-current`\nFor Determining One's Birth Day: time `--get-birth-day`\nEtc.\n\n# MOST IMPORTANT AND USED OPTIONS AND FEATURES\n\n(Create a list of common options and switches and flags, etc., from the docs and your knowledge base, if it contains common uses of the tool.)\n\n(For each one, describe how/why it could be useful)\n\nEND OUTPUT SECTIONS\n\n# OUTPUT INSTRUCTIONS\n\n- Interpret the input as tool documentation, no matter what it is.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5121195316314697,
        0.36024776101112366,
        0.4251272976398468,
        0.02565108798444271,
        0.2024940401315689,
        0.0031182989478111267,
        -1.0835890769958496,
        0.16649380326271057,
        -0.5290563106536865,
        0.7765791416168213,
        -0.16261231899261475,
        1.1076257228851318,
        0.35850417613983154,
        0.17208436131477356,
        0.02067120373249054,
        -0.08519141376018524,
        -0.16993127763271332,
        -0.8851147890090942,
        -1.1885828971862793,
        0.038247767835855484,
        -0.13140931725502014,
        0.47961196303367615,
        0.49998238682746887,
        0.09397676587104797,
        0.7530196905136108,
        -0.08078932762145996,
        -0.14415115118026733,
        -0.24988441169261932,
        -1.1705591678619385,
        -1.5228463411331177,
        0.5543546676635742,
        0.32956820726394653,
        -0.28004804253578186,
        -0.5344674587249756,
        0.45462217926979065,
        -0.6086221933364868,
        -0.14152294397354126,
        -0.15703171491622925,
        -0.4609232544898987,
        0.20675449073314667,
        -0.7263185381889343,
        0.4684738218784332,
        -0.36586815118789673,
        0.29406359791755676,
        0.30014172196388245,
        -0.31210070848464966,
        0.14599661529064178,
        -0.29850658774375916,
        0.4371809959411621,
        0.1741800159215927,
        -0.38427239656448364,
        -0.44727349281311035,
        -0.1966875195503235,
        -0.006817828863859177,
        -0.3043881058692932,
        -0.19762736558914185,
        0.005076460540294647,
        -0.6614471077919006,
        0.663494884967804,
        0.2435142546892166,
        0.3304935097694397,
        0.45029833912849426,
        -3.5131900310516357,
        -0.12996196746826172,
        -0.33730870485305786,
        0.2105671465396881,
        0.2855660319328308,
        -0.37065908312797546,
        -0.09780601412057877,
        0.14530876278877258,
        -0.374485045671463,
        0.03898440673947334,
        0.03880695253610611,
        0.05640523508191109,
        0.07812894880771637,
        -0.5517473816871643,
        0.22987283766269684,
        0.4511408507823944,
        0.13669851422309875,
        -0.5602744817733765,
        -0.37289130687713623,
        0.5322535037994385,
        0.07825841009616852,
        -0.03265993669629097,
        -0.45995205640792847,
        0.6707984805107117,
        -0.016881229355931282,
        -0.4667257070541382,
        0.3964608609676361,
        0.01283726841211319,
        -0.1366960108280182,
        -0.12124433368444443,
        0.5459321737289429,
        0.16123396158218384,
        -1.0189435482025146,
        -0.19827036559581757,
        -0.009382292628288269,
        0.24069449305534363,
        0.0893922969698906,
        3.545438289642334,
        0.7617708444595337,
        0.34250831604003906,
        0.602088212966919,
        -0.9523311257362366,
        0.48768699169158936,
        -0.2984400987625122,
        0.1607530564069748,
        -0.4688780605792999,
        -0.20616035163402557,
        -0.20845787227153778,
        0.25708895921707153,
        -0.18062689900398254,
        -0.2253229320049286,
        0.6290926337242126,
        0.35369229316711426,
        0.3815947473049164,
        -0.4096168875694275,
        0.2844148278236389,
        0.11943197250366211,
        0.04296032711863518,
        -0.0862550213932991,
        0.07839818298816681,
        -0.6193516254425049,
        -0.06609104573726654,
        -0.6261172890663147,
        -0.39901018142700195,
        -0.11726215481758118,
        0.5066046714782715,
        0.5329976677894592,
        0.11142686009407043,
        -0.13900905847549438,
        -0.03846855089068413,
        -0.6672179698944092,
        0.5158126950263977,
        0.00796407088637352,
        0.33367785811424255,
        0.2781095504760742,
        -0.441851407289505,
        0.036783527582883835,
        -0.721280038356781,
        0.32144027948379517,
        -0.5515872836112976,
        0.32716283202171326,
        0.7381349802017212,
        0.11138764023780823,
        0.24819521605968475,
        -0.18372535705566406,
        0.20913881063461304,
        -0.4138525426387787,
        -1.1818740367889404,
        0.10509626567363739,
        0.7420905828475952,
        0.05390344187617302,
        0.031871914863586426,
        0.38180649280548096,
        -0.17593178153038025,
        -0.2088220864534378,
        0.08071628212928772,
        -1.0081541538238525,
        0.2821049094200134,
        -0.09787306189537048,
        0.2442755401134491,
        -0.3273993134498596,
        0.2956751585006714,
        0.5868674516677856,
        -0.42147257924079895,
        0.47562816739082336,
        -0.5434955954551697,
        -0.13569675385951996,
        0.461556613445282,
        0.22085127234458923,
        -0.2964784801006317,
        0.8629732131958008,
        0.8214085698127747,
        0.4422052502632141,
        -0.39571520686149597,
        -0.6339703798294067,
        -0.03286212682723999,
        -0.26345428824424744,
        -0.3679514527320862,
        0.19257143139839172,
        0.7603052258491516,
        -0.2800496518611908,
        -0.6978661417961121,
        0.20549334585666656,
        0.27793219685554504,
        0.3039303421974182,
        0.46490007638931274,
        0.41224414110183716,
        1.1681209802627563,
        -0.7427436709403992,
        2.1218020915985107,
        -0.7948000431060791,
        -0.2801409363746643,
        0.28470274806022644,
        0.11254774034023285,
        -0.1725279837846756,
        0.46082544326782227,
        0.4611473083496094,
        -0.10974567383527756,
        -0.4831966161727905,
        -0.008151471614837646,
        -0.39679887890815735,
        0.06301954388618469,
        -0.15313032269477844,
        -0.47811049222946167,
        -0.5472644567489624,
        0.2531811594963074,
        -0.10779701173305511,
        -0.9299578666687012,
        0.3000860810279846,
        -0.37840914726257324,
        0.9433516263961792,
        0.46809816360473633,
        0.7959853410720825,
        -0.08391048014163971,
        -0.10867954790592194,
        0.4637397527694702,
        0.36402300000190735,
        0.5705549716949463,
        0.09514954686164856,
        -0.16092902421951294,
        -0.7118973135948181,
        -0.6187195181846619,
        -1.2935669422149658,
        0.5765716433525085,
        -0.08521420508623123,
        0.19346120953559875,
        -0.8541160821914673,
        -0.23156912624835968,
        0.31592032313346863,
        0.9018233418464661,
        0.9712692499160767,
        1.0187073945999146,
        0.05581963434815407,
        0.5718382596969604,
        0.002341911196708679,
        0.14402619004249573,
        -0.08144768327474594,
        -0.3645307719707489,
        -0.2705734074115753,
        -0.43415215611457825,
        -0.9329797029495239,
        0.7575844526290894,
        0.3673681616783142,
        0.0406111478805542,
        -0.9677766561508179,
        -0.6930557489395142,
        0.21535637974739075,
        1.5680549144744873,
        0.1266820728778839,
        0.14042213559150696,
        -0.11091160774230957,
        0.5865748524665833,
        -0.0625813826918602,
        0.021271370351314545,
        -1.927858829498291,
        -0.548786997795105,
        -1.029816746711731,
        0.6887635588645935,
        -0.3996565639972687,
        0.0812746062874794,
        0.2226911038160324,
        0.3957294821739197,
        -0.38881826400756836,
        -0.17011554539203644,
        -0.30585071444511414,
        -0.5192869305610657,
        -0.4219002425670624,
        -0.11837316304445267,
        -0.16346481442451477,
        0.5717881321907043,
        -0.16177819669246674,
        -0.1253688931465149,
        0.37393277883529663,
        0.1903289258480072,
        0.6351245641708374,
        0.21522703766822815,
        -0.25707632303237915,
        -0.01406650897115469,
        0.09476958215236664,
        0.5460829734802246,
        0.669722855091095,
        0.44207924604415894,
        0.16369158029556274,
        0.10948576033115387,
        -0.09515014290809631,
        -0.5345827341079712,
        -0.21377351880073547,
        0.4734787344932556,
        0.02381567656993866,
        -0.5226200819015503,
        -0.6336018443107605,
        0.36292505264282227,
        1.6680020093917847,
        0.5847621560096741,
        0.17197389900684357,
        0.970969557762146,
        0.8655393123626709,
        -0.06328010559082031,
        -0.800342321395874,
        -0.07930061966180801,
        -0.41988950967788696,
        -0.49355006217956543,
        -0.5205485820770264,
        -0.20220014452934265,
        0.793222963809967,
        0.49294281005859375,
        0.2273126244544983,
        0.4373026490211487,
        -0.42305490374565125,
        -0.10847645998001099,
        0.041406381875276566,
        -0.02691115438938141,
        0.9920402765274048,
        -0.39179903268814087,
        0.5039174556732178,
        1.153135895729065,
        -0.1451396495103836,
        -2.294126033782959,
        0.01915082149207592,
        0.78214031457901,
        -0.22051560878753662,
        -0.3715042471885681,
        0.17186704277992249,
        0.3378533720970154,
        -0.5840625762939453,
        0.21771317720413208,
        0.013395160436630249,
        0.8855775594711304,
        0.5238094925880432,
        -0.24977922439575195,
        -0.818071722984314,
        -0.02651466801762581,
        0.5411491394042969,
        -0.07852908968925476,
        0.33685675263404846,
        -0.7813723087310791,
        -0.5194693803787231,
        0.055914636701345444,
        0.2881789207458496,
        1.2420341968536377,
        0.19231495261192322,
        0.41117846965789795,
        0.0760144293308258,
        0.09091874957084656,
        -0.4216517210006714,
        -1.0982084274291992,
        0.3440488874912262,
        -0.6353002786636353,
        -0.46289029717445374,
        0.45121219754219055,
        -0.2310985028743744,
        -0.3831411302089691,
        0.41500499844551086,
        0.44907698035240173,
        0.29646021127700806,
        0.3970891833305359,
        -0.662483811378479,
        1.740309238433838,
        -0.2991916537284851,
        -0.4343598484992981,
        -0.018776334822177887,
        0.06251661479473114,
        -0.3502046465873718,
        0.01975555717945099,
        0.4799550771713257,
        -0.7205749154090881,
        -0.24627670645713806,
        -0.3640601336956024,
        0.37685665488243103,
        -0.6481384038925171,
        0.8946452140808105,
        0.6405046582221985,
        0.44712454080581665,
        0.4839528203010559,
        -0.276142418384552,
        0.11381883919239044,
        0.24605952203273773,
        -0.03480415418744087,
        0.0032632574439048767,
        -0.31691431999206543,
        -0.8266258239746094,
        -0.6600643396377563
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes project documentation into a concise, user and developer-focused summary, highlighting its purpose, problem addressed, approach, installation, usage, and examples. It simplifies complex information for easy understanding and application. The output includes a project overview, problem it addresses, approach to solving the problem, and practical steps for installation and usage.",
          "name": "Explain_project",
          "raw": "\n                workflow Explain_project v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at explaining projects and how to use them.\n\nYou take the input of project documentation and you output a crisp, user and developer focused summary of what the project does and how to use it, using the STEPS and OUTPUT SECTIONS.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Fully understand the project from the input.\n\n# OUTPUT SECTIONS\n\n- In a section called PROJECT OVERVIEW, give a one-sentence summary in 15-words for what the project does. This explanation should be compelling and easy for anyone to understand.\n\n- In a section called THE PROBLEM IT ADDRESSES, give a one-sentence summary in 15-words for the problem the project addresses. This should be realworld problem that's easy to understand, e.g., \\\"This project helps you find the best restaurants in your local area.\\\"\n\n- In a section called THE APPROACH TO SOLVING THE PROBLEM, give a one-sentence summary in 15-words for the approach the project takes to solve the problem. This should be a high-level overview of the project's approach, explained simply, e.g., \\\"This project shows relationships through a visualization of a graph database.\\\"\n\n- In a section called INSTALLATION, give a bulleted list of install steps, each with no more than 15 words per bullet (not counting if they are commands).\n\n- In a section called USAGE, give a bulleted list of how to use the project, each with no more than 15 words per bullet (not counting if they are commands).\n\n- In a section called EXAMPLES, give a bulleted list of examples of how one might use such a project, each with no more than 15 words per bullet.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at explaining projects and how to use them.\n\nYou take the input of project documentation and you output a crisp, user and developer focused summary of what the project does and how to use it, using the STEPS and OUTPUT SECTIONS.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Fully understand the project from the input.\n\n# OUTPUT SECTIONS\n\n- In a section called PROJECT OVERVIEW, give a one-sentence summary in 15-words for what the project does. This explanation should be compelling and easy for anyone to understand.\n\n- In a section called THE PROBLEM IT ADDRESSES, give a one-sentence summary in 15-words for the problem the project addresses. This should be realworld problem that's easy to understand, e.g., \\\"This project helps you find the best restaurants in your local area.\\\"\n\n- In a section called THE APPROACH TO SOLVING THE PROBLEM, give a one-sentence summary in 15-words for the approach the project takes to solve the problem. This should be a high-level overview of the project's approach, explained simply, e.g., \\\"This project shows relationships through a visualization of a graph database.\\\"\n\n- In a section called INSTALLATION, give a bulleted list of install steps, each with no more than 15 words per bullet (not counting if they are commands).\n\n- In a section called USAGE, give a bulleted list of how to use the project, each with no more than 15 words per bullet (not counting if they are commands).\n\n- In a section called EXAMPLES, give a bulleted list of examples of how one might use such a project, each with no more than 15 words per bullet.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5583629608154297,
        0.3679962754249573,
        -0.1384967863559723,
        0.5765184760093689,
        0.47648221254348755,
        -0.48401421308517456,
        -1.0431644916534424,
        0.16748785972595215,
        -0.18437878787517548,
        0.28771430253982544,
        0.2755630910396576,
        0.4975876212120056,
        -0.48484790325164795,
        0.24374987185001373,
        0.11274100095033646,
        0.17257168889045715,
        -0.26030606031417847,
        -1.2015126943588257,
        -1.5304176807403564,
        -0.5639755129814148,
        -0.3507593870162964,
        1.032726526260376,
        0.38620525598526,
        0.349403977394104,
        0.5434774160385132,
        -0.2899271547794342,
        -0.4170536994934082,
        0.22616402804851532,
        -1.1871380805969238,
        -1.1124262809753418,
        0.15779095888137817,
        -0.24269306659698486,
        -0.5042188763618469,
        -0.631732702255249,
        0.15612421929836273,
        -0.6230983138084412,
        0.22144874930381775,
        -0.5786846876144409,
        -0.25018447637557983,
        0.18283595144748688,
        -0.3874787390232086,
        0.7614786624908447,
        -0.06745931506156921,
        -0.544944167137146,
        0.48480477929115295,
        -0.19581887125968933,
        -0.14151178300380707,
        -0.26139962673187256,
        0.9891349077224731,
        0.10354030132293701,
        0.15084686875343323,
        -0.778298020362854,
        0.3577914237976074,
        0.30712059140205383,
        -0.5247842669487,
        -0.055637963116168976,
        -0.6854926347732544,
        -0.9598139524459839,
        0.3821702301502228,
        0.1594451367855072,
        -0.035250693559646606,
        0.6561732292175293,
        -3.440636157989502,
        -0.3807106912136078,
        -0.22915536165237427,
        0.4963745176792145,
        0.20359213650226593,
        -0.1546745002269745,
        0.0717727541923523,
        0.10436002910137177,
        -0.5553908348083496,
        0.13447880744934082,
        -0.057839274406433105,
        0.3303624987602234,
        0.11767276376485825,
        -0.1432172954082489,
        0.6574298143386841,
        -0.5566140413284302,
        0.33141785860061646,
        -0.38593652844429016,
        -0.1145244836807251,
        0.4480581283569336,
        -0.16604909300804138,
        0.08741835504770279,
        -0.1433166265487671,
        0.34575945138931274,
        0.026664994657039642,
        -0.6927810311317444,
        0.4711896777153015,
        -0.3352538049221039,
        -0.7223916053771973,
        -0.5272119045257568,
        0.033998653292655945,
        0.142178475856781,
        -0.9800595045089722,
        -0.14875049889087677,
        0.35695481300354004,
        0.3158593773841858,
        -0.07241933792829514,
        3.1140215396881104,
        0.256992906332016,
        0.7811751961708069,
        0.5673604011535645,
        -0.8700753450393677,
        0.17176605761051178,
        -0.30272719264030457,
        0.28398171067237854,
        -0.08217903226613998,
        0.3927226662635803,
        0.12724004685878754,
        -0.04409337416291237,
        -0.8318480253219604,
        -0.38058939576148987,
        0.21990713477134705,
        -0.1882806420326233,
        0.18422459065914154,
        -0.20512992143630981,
        0.46382519602775574,
        -0.05016752704977989,
        0.6178967356681824,
        -0.2512986660003662,
        0.20285335183143616,
        0.22917108237743378,
        -0.4382483959197998,
        -0.3418204188346863,
        -0.3075578510761261,
        -0.41379937529563904,
        0.6602340936660767,
        0.3836289644241333,
        0.6017857789993286,
        -0.06488203257322311,
        0.3384346663951874,
        -0.4215359389781952,
        0.11248396337032318,
        0.1947745829820633,
        0.2054394781589508,
        0.0879710465669632,
        -0.6963849663734436,
        0.05148572474718094,
        -0.08438478410243988,
        0.4188321828842163,
        -0.6125363111495972,
        0.5461673140525818,
        -0.0600314736366272,
        0.5292255282402039,
        0.9634093642234802,
        -0.21326974034309387,
        0.4433480501174927,
        -0.648939311504364,
        -0.40523990988731384,
        0.20929118990898132,
        0.6693440675735474,
        -0.1846494972705841,
        -0.013146597892045975,
        0.2742465138435364,
        0.7925878167152405,
        0.04758693277835846,
        -0.3100937306880951,
        -0.20030348002910614,
        -0.029192544519901276,
        -0.20939770340919495,
        -0.30054518580436707,
        0.14472778141498566,
        -0.10059402883052826,
        1.1000583171844482,
        0.10039250552654266,
        0.2914677858352661,
        -0.6727008819580078,
        0.40910273790359497,
        0.8756510615348816,
        0.35566702485084534,
        -0.5172157287597656,
        0.341666042804718,
        1.205737590789795,
        -0.2598688304424286,
        -0.3633836507797241,
        -0.26531440019607544,
        -0.04233359545469284,
        0.5162619352340698,
        -1.0483325719833374,
        0.261497437953949,
        0.8064863681793213,
        -0.016121475026011467,
        -1.254913091659546,
        -0.257331907749176,
        0.7730680704116821,
        -0.04133134335279465,
        0.27166181802749634,
        0.9771314859390259,
        1.159869909286499,
        -0.3663228154182434,
        2.445369005203247,
        -0.8088163733482361,
        -0.4526952803134918,
        0.13325785100460052,
        -0.2991716265678406,
        0.5252932906150818,
        0.23106296360492706,
        1.1496994495391846,
        -0.24101096391677856,
        -1.018674373626709,
        -0.032485201954841614,
        -0.08644521236419678,
        0.07332651317119598,
        -0.7486456632614136,
        -0.5726205110549927,
        0.4276832044124603,
        -0.11594913899898529,
        0.1766069382429123,
        -0.9402381181716919,
        0.37771230936050415,
        0.23386037349700928,
        1.1520154476165771,
        0.02288498356938362,
        0.9548313617706299,
        -0.3199418783187866,
        0.908662736415863,
        -0.2392691671848297,
        0.16863398253917694,
        0.8179247379302979,
        -0.39290639758110046,
        0.6002631783485413,
        -0.635900616645813,
        -0.6562140583992004,
        -0.6960293054580688,
        0.8277559876441956,
        0.06872785836458206,
        0.8940468430519104,
        -0.6426615118980408,
        -0.35604751110076904,
        -0.0380396768450737,
        1.4562816619873047,
        0.7214990854263306,
        1.0233166217803955,
        0.08809389173984528,
        0.6902866959571838,
        -0.3169781267642975,
        0.060465604066848755,
        0.15659983456134796,
        -0.9364694356918335,
        -0.3001546859741211,
        0.11713393032550812,
        0.15859319269657135,
        0.8645413517951965,
        -0.1146714985370636,
        0.37697097659111023,
        -1.0699381828308105,
        -0.390038400888443,
        0.012241760268807411,
        0.7781388759613037,
        0.13930541276931763,
        -0.4238588511943817,
        -0.11573362350463867,
        0.6331554651260376,
        0.2998068928718567,
        -0.2530478537082672,
        -2.3577756881713867,
        -0.6092820763587952,
        -1.0344905853271484,
        0.5783036351203918,
        -0.41047170758247375,
        0.031053073704242706,
        0.7510026693344116,
        0.15864984691143036,
        -0.43866416811943054,
        -0.28230392932891846,
        -0.339176744222641,
        -0.6177414655685425,
        -0.4109143316745758,
        0.11249912530183792,
        0.07533175498247147,
        -0.09526827931404114,
        0.08642247319221497,
        -0.10240747779607773,
        0.3773038983345032,
        0.03444816917181015,
        0.3165417015552521,
        0.20256918668746948,
        -0.608710765838623,
        -0.43559011816978455,
        0.29254940152168274,
        0.17290975153446198,
        -0.1268818974494934,
        0.44992464780807495,
        -0.21084395051002502,
        0.044535115361213684,
        -0.5085489153862,
        -0.3657740354537964,
        -0.03284738212823868,
        1.0280600786209106,
        -0.4616873860359192,
        -0.739982545375824,
        -0.8905125260353088,
        -0.08160269260406494,
        1.567859411239624,
        0.5075704455375671,
        -0.12945690751075745,
        0.7020853757858276,
        0.3683854341506958,
        -0.3529205918312073,
        -0.5318500399589539,
        0.042713988572359085,
        -0.38389548659324646,
        -0.1257302612066269,
        -0.6567906141281128,
        -0.30217307806015015,
        1.100975513458252,
        0.24190953373908997,
        -0.5719311237335205,
        0.4680979251861572,
        -0.7891452312469482,
        -0.24877025187015533,
        -0.5159695148468018,
        -0.12091916799545288,
        1.1237285137176514,
        -1.0294770002365112,
        0.8669904470443726,
        1.1112874746322632,
        -0.018698278814554214,
        -1.0695222616195679,
        -0.07612666487693787,
        1.4583628177642822,
        0.04197119176387787,
        -0.00244314968585968,
        -0.5553598999977112,
        0.32914477586746216,
        0.17284974455833435,
        0.10034884512424469,
        -0.13076403737068176,
        1.3075286149978638,
        0.31526872515678406,
        0.35009491443634033,
        0.043319638818502426,
        -0.12212622165679932,
        0.7220373153686523,
        -0.1231725737452507,
        0.21689069271087646,
        -0.858589768409729,
        -1.1576398611068726,
        -0.2272670716047287,
        -0.10867279767990112,
        1.116211175918579,
        0.5367003679275513,
        0.42530620098114014,
        0.3919427692890167,
        -0.022985652089118958,
        -0.6387908458709717,
        -0.9522934556007385,
        -0.0579751655459404,
        0.19986848533153534,
        -0.6817108392715454,
        1.119991421699524,
        0.5095800161361694,
        -0.15246939659118652,
        0.6862629652023315,
        0.4274854362010956,
        -0.5538724660873413,
        0.34521302580833435,
        -0.6244778037071228,
        1.782167911529541,
        -0.7183058261871338,
        -0.8954650163650513,
        -0.20312434434890747,
        -0.38954856991767883,
        -0.2987077832221985,
        0.3605993390083313,
        0.048720236867666245,
        -0.46650153398513794,
        0.556277871131897,
        0.5513845086097717,
        -0.09207622706890106,
        -0.273475706577301,
        0.4840259552001953,
        0.9655409455299377,
        0.34548982977867126,
        -0.04312114417552948,
        -0.0852627158164978,
        0.04178127273917198,
        -0.5922607183456421,
        -0.17440208792686462,
        0.44492673873901367,
        -0.4369504749774933,
        -0.7905965447425842,
        -0.7432870864868164
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Produces a glossary of advanced terms found in specific content, including definitions and analogies. It focuses on explaining obscure or complex terms to aid understanding. The output is a list of terms with explanations and analogies in a structured Markdown format.",
          "name": "Explain_terms",
          "raw": "\n                workflow Explain_terms v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are the world's best explainer of terms required to understand a given piece of content. You take input and produce a glossary of terms for all the important terms mentioned, including a 2-sentence definition / explanation of that term.\n\n# STEPS\n\n- Consume the content.\n\n- Fully and deeply understand the content, and what it's trying to convey.\n\n- Look for the more obscure or advanced terms mentioned in the content, so not the basic ones but the more advanced terms.\n\n- Think about which of those terms would be best to explain to someone trying to understand this content.\n\n- Think about the order of terms that would make the most sense to explain.\n\n- Think of the name of the term, the definition or explanation, and also an analogy that could be useful in explaining it.\n\n# OUTPUT\n\n- Output the full list of advanced, terms used in the content.\n\n- For each term, use the following format for the output:\n\n## EXAMPLE OUTPUT\n\n- STOCHASTIC PARROT: In machine learning, the term stochastic parrot is a metaphor to describe the theory that large language models, though able to generate plausible language, do not understand the meaning of the language they process.\n-- Analogy: A parrot that can recite a poem in a foreign language without understanding it.\n-- Why It Matters: It pertains to the debate about whether AI actually understands things vs. just mimicking patterns.\n\n# OUTPUT FORMAT\n\n- Output in the format above only using valid Markdown.\n\n- Do not use bold or italic formatting in the Markdown (no asterisks).\n\n- Do not complain about anything, just do what you're told.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are the world's best explainer of terms required to understand a given piece of content. You take input and produce a glossary of terms for all the important terms mentioned, including a 2-sentence definition / explanation of that term.\n\n# STEPS\n\n- Consume the content.\n\n- Fully and deeply understand the content, and what it's trying to convey.\n\n- Look for the more obscure or advanced terms mentioned in the content, so not the basic ones but the more advanced terms.\n\n- Think about which of those terms would be best to explain to someone trying to understand this content.\n\n- Think about the order of terms that would make the most sense to explain.\n\n- Think of the name of the term, the definition or explanation, and also an analogy that could be useful in explaining it.\n\n# OUTPUT\n\n- Output the full list of advanced, terms used in the content.\n\n- For each term, use the following format for the output:\n\n## EXAMPLE OUTPUT\n\n- STOCHASTIC PARROT: In machine learning, the term stochastic parrot is a metaphor to describe the theory that large language models, though able to generate plausible language, do not understand the meaning of the language they process.\n-- Analogy: A parrot that can recite a poem in a foreign language without understanding it.\n-- Why It Matters: It pertains to the debate about whether AI actually understands things vs. just mimicking patterns.\n\n# OUTPUT FORMAT\n\n- Output in the format above only using valid Markdown.\n\n- Do not use bold or italic formatting in the Markdown (no asterisks).\n\n- Do not complain about anything, just do what you're told.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.19400231540203094,
        0.07286892831325531,
        -0.34498822689056396,
        0.5463901162147522,
        -0.2747725546360016,
        -0.024404726922512054,
        -0.8750003576278687,
        0.10343089699745178,
        0.43809404969215393,
        0.15233993530273438,
        0.22314633429050446,
        0.07278358191251755,
        -0.3119118809700012,
        -0.2639589011669159,
        -0.08775016665458679,
        -0.497808575630188,
        -0.6952316761016846,
        -0.5729077458381653,
        -1.7992374897003174,
        -0.40810638666152954,
        0.2193116396665573,
        0.21113885939121246,
        -0.539384126663208,
        0.42218947410583496,
        -0.0465092808008194,
        -0.2548985779285431,
        0.1679648607969284,
        -0.09814188629388809,
        -1.8805102109909058,
        -2.0078670978546143,
        0.06527401506900787,
        -0.01796784996986389,
        0.21244429051876068,
        -0.1776038110256195,
        0.3448243737220764,
        -0.8571463823318481,
        0.16998249292373657,
        -0.2764612138271332,
        -0.38973554968833923,
        -0.40527671575546265,
        -0.15828943252563477,
        -0.23923304677009583,
        0.17874908447265625,
        0.09701328724622726,
        0.5149847865104675,
        0.03608957678079605,
        0.81479811668396,
        -0.06230759993195534,
        1.2249335050582886,
        0.18860308825969696,
        -0.36608120799064636,
        -0.4256189167499542,
        -0.2609312832355499,
        -0.10045702755451202,
        -0.2265448272228241,
        -0.4705726206302643,
        0.29735586047172546,
        -0.07717878371477127,
        0.5223223567008972,
        -0.16257242858409882,
        -0.3730581998825073,
        0.3917252719402313,
        -4.043413162231445,
        0.0066148750483989716,
        -0.1048545092344284,
        0.6189538836479187,
        0.051951419562101364,
        0.3909815847873688,
        0.4145205318927765,
        -0.34592944383621216,
        0.14674708247184753,
        0.634902834892273,
        -0.223793625831604,
        0.5259958505630493,
        0.24599695205688477,
        -0.11191223561763763,
        0.38698047399520874,
        -0.13602498173713684,
        0.4475848078727722,
        -0.2821221947669983,
        -0.0936240404844284,
        0.5428186655044556,
        -0.0016743112355470657,
        -0.33374956250190735,
        -0.6610491871833801,
        0.6409960985183716,
        -0.5042102336883545,
        0.26859021186828613,
        0.694649875164032,
        -0.3391687273979187,
        0.08234664797782898,
        -0.4218663275241852,
        0.18842041492462158,
        0.1514924168586731,
        -0.7207393050193787,
        0.02654387429356575,
        -0.020541220903396606,
        0.22443190217018127,
        -0.32218366861343384,
        3.1088364124298096,
        0.36929911375045776,
        -0.0886642336845398,
        0.08642597496509552,
        -0.9356345534324646,
        -0.06333605200052261,
        -0.4011097550392151,
        0.09679262340068817,
        -0.2614385783672333,
        0.33288127183914185,
        -0.2291114330291748,
        0.2705193758010864,
        -0.795957088470459,
        -0.07201625406742096,
        -0.08414968103170395,
        0.35791218280792236,
        0.0618758425116539,
        -0.5609878897666931,
        0.1994769126176834,
        -0.7077366709709167,
        0.8249340653419495,
        -0.5205349922180176,
        0.19301335513591766,
        -0.3138177692890167,
        -0.25688472390174866,
        0.37849709391593933,
        -0.12632089853286743,
        0.8019040822982788,
        0.30219191312789917,
        0.26715150475502014,
        -0.024813083931803703,
        0.6497952938079834,
        -0.7064786553382874,
        0.4331527650356293,
        -0.6117509007453918,
        0.34140846133232117,
        -0.2883993089199066,
        0.18941104412078857,
        -0.2611357271671295,
        0.4499514400959015,
        -0.5134292840957642,
        0.2630075216293335,
        -1.4926478862762451,
        0.698415219783783,
        0.6139376163482666,
        0.17526979744434357,
        0.26246386766433716,
        -0.07862656563520432,
        0.21120965480804443,
        -0.3785647451877594,
        -0.2561962306499481,
        -0.13874438405036926,
        0.07031917572021484,
        0.002657478442415595,
        0.12079063057899475,
        0.5236452221870422,
        -0.25310784578323364,
        0.35751593112945557,
        0.001891590654850006,
        -0.7571285367012024,
        -0.09498044103384018,
        0.2672094702720642,
        -0.3135228157043457,
        -0.3419623374938965,
        -0.13118574023246765,
        0.2595781683921814,
        -0.512000322341919,
        0.40361255407333374,
        0.15012890100479126,
        1.0133064985275269,
        0.052868764847517014,
        0.13899357616901398,
        -0.08819612115621567,
        -0.03245086967945099,
        0.5899975895881653,
        -0.4216211438179016,
        0.49106210470199585,
        -0.2300514131784439,
        0.1811269223690033,
        0.33147916197776794,
        -0.09774813055992126,
        1.0605806112289429,
        0.40088045597076416,
        0.036353807896375656,
        -0.7144454717636108,
        -0.40134763717651367,
        0.6194522976875305,
        0.18087588250637054,
        0.40210288763046265,
        0.5339314341545105,
        0.6596094369888306,
        -0.7999045848846436,
        2.1111373901367188,
        -0.5350916385650635,
        0.24306020140647888,
        -0.04458586871623993,
        -0.2446652203798294,
        0.3280443549156189,
        0.03216923773288727,
        0.22301916778087616,
        0.06454920023679733,
        -0.8981925249099731,
        -0.21213890612125397,
        0.2688775062561035,
        -0.08844976127147675,
        0.6458860635757446,
        -0.09325606375932693,
        -0.04955894127488136,
        0.39278289675712585,
        -0.02534201741218567,
        -0.499003142118454,
        -0.2562614381313324,
        -0.16035987436771393,
        0.48102521896362305,
        0.6582441926002502,
        0.5370599031448364,
        0.3851239085197449,
        -0.22747120261192322,
        -0.018903162330389023,
        0.18644006550312042,
        0.437389075756073,
        0.29415765404701233,
        0.5763049721717834,
        -0.6705841422080994,
        -0.37000706791877747,
        -0.7726046442985535,
        0.4980025291442871,
        0.006017647683620453,
        0.6288021802902222,
        -0.4779050946235657,
        -0.3995390236377716,
        0.35511595010757446,
        0.9411738514900208,
        0.8938838243484497,
        1.5126832723617554,
        -0.1967812180519104,
        0.16540861129760742,
        0.20741739869117737,
        -0.06047183275222778,
        0.03881797194480896,
        -1.0666873455047607,
        0.3089672029018402,
        0.4620937407016754,
        0.05348309874534607,
        -0.6250368356704712,
        -0.5325092077255249,
        -0.791824221611023,
        -0.449199378490448,
        -0.5877197980880737,
        -0.4124906659126282,
        2.0443124771118164,
        0.40963074564933777,
        0.21245768666267395,
        0.10301310569047928,
        0.7365729808807373,
        0.09225592017173767,
        0.3267330527305603,
        -2.0414934158325195,
        -0.6401755809783936,
        -0.4748191237449646,
        0.3671586215496063,
        -0.0317038856446743,
        0.5342581868171692,
        0.35035258531570435,
        0.6107430458068848,
        -0.5156145095825195,
        -0.1917518824338913,
        -0.30924877524375916,
        -0.7829509973526001,
        0.03805176913738251,
        -0.2871081531047821,
        -0.7258635759353638,
        0.0732443779706955,
        0.051400281488895416,
        0.18310487270355225,
        0.4513460695743561,
        0.035029053688049316,
        0.2638164758682251,
        0.5271368026733398,
        0.3885922133922577,
        -0.20232166349887848,
        0.807905375957489,
        -0.177000030875206,
        0.5223459601402283,
        -0.031846895813941956,
        -0.13668292760849,
        -0.14628422260284424,
        -0.5866453051567078,
        -0.05902419239282608,
        -0.06246749311685562,
        0.11214305460453033,
        -0.03142041712999344,
        -0.8002305626869202,
        -0.7469099760055542,
        0.4262465536594391,
        2.1316332817077637,
        -0.3529680669307709,
        -0.024624008685350418,
        0.18172810971736908,
        0.022603513672947884,
        -0.7770816683769226,
        -0.9351366758346558,
        0.47229182720184326,
        -0.2806851267814636,
        0.44070038199424744,
        -0.6542566418647766,
        -0.1733854115009308,
        0.7110745310783386,
        -0.04854472354054451,
        -0.18715298175811768,
        0.9063147902488708,
        -0.5258783102035522,
        0.2119556963443756,
        0.2919865846633911,
        -0.04417858272790909,
        0.029449723660945892,
        -0.4759041666984558,
        -0.395624041557312,
        0.4385759234428406,
        -0.4041268229484558,
        -1.7307223081588745,
        -0.5027248859405518,
        0.5129494667053223,
        -0.08088590204715729,
        -0.4501154124736786,
        -0.42735597491264343,
        0.606440007686615,
        -0.0669519230723381,
        0.3349444568157196,
        0.1950216144323349,
        1.3970757722854614,
        0.029362088069319725,
        -0.26332470774650574,
        0.29647913575172424,
        0.08145507425069809,
        1.2568671703338623,
        -0.1126890555024147,
        0.8049581050872803,
        0.40187525749206543,
        -0.061213355511426926,
        -0.4101329445838928,
        -0.18227267265319824,
        1.3695039749145508,
        -0.1155725046992302,
        -0.21344271302223206,
        -0.06288991868495941,
        0.012632220983505249,
        -0.08150584995746613,
        -0.3029380142688751,
        1.2202531099319458,
        -0.6783038973808289,
        -0.45631688833236694,
        0.5299068689346313,
        0.4976246654987335,
        0.09754851460456848,
        -0.14460721611976624,
        0.5363051295280457,
        -0.5843998193740845,
        0.058937881141901016,
        -0.6232959628105164,
        1.4008334875106812,
        -0.3761753439903259,
        -0.09067144244909286,
        -0.5527670383453369,
        -0.08031317591667175,
        -0.38738685846328735,
        0.03044440597295761,
        -0.25660982728004456,
        -0.46080753207206726,
        0.16904592514038086,
        -0.28358668088912964,
        0.4832886755466461,
        -0.5755562782287598,
        0.17210549116134644,
        0.643914520740509,
        0.6261709332466125,
        0.26192355155944824,
        -0.10309694707393646,
        0.34274208545684814,
        0.38478633761405945,
        0.3055461049079895,
        0.6549175977706909,
        0.8315635323524475,
        -0.7308741807937622,
        -0.6392624378204346
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Export_data_as_csv",
          "raw": "\n                workflow Export_data_as_csv v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are a superintelligent AI that finds all mentions of data structures within an input and you output properly formatted CSV data that perfectly represents what's in the input.\n\n# STEPS\n\n- Read the whole input and understand the context of everything.\n\n- Find all mention of data structures, e.g., projects, teams, budgets, metrics, KPIs, etc., and think about the name of those fields and the data in each field.\n\n# OUTPUT\n\n- Output a CSV file that contains all the data structures found in the input. \n\n# OUTPUT INSTRUCTIONS\n\n- Use the fields found in the input, don't make up your own.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are a superintelligent AI that finds all mentions of data structures within an input and you output properly formatted CSV data that perfectly represents what's in the input.\n\n# STEPS\n\n- Read the whole input and understand the context of everything.\n\n- Find all mention of data structures, e.g., projects, teams, budgets, metrics, KPIs, etc., and think about the name of those fields and the data in each field.\n\n# OUTPUT\n\n- Output a CSV file that contains all the data structures found in the input. \n\n# OUTPUT INSTRUCTIONS\n\n- Use the fields found in the input, don't make up your own.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.33188509941101074,
        0.38688361644744873,
        -0.5255396366119385,
        -0.08226142823696136,
        0.4467296898365021,
        0.40791991353034973,
        -0.8579018115997314,
        0.05457933247089386,
        0.2899504601955414,
        -0.32798534631729126,
        0.18321529030799866,
        0.8096244931221008,
        -0.1425991803407669,
        0.2443533092737198,
        -0.17785099148750305,
        0.012428365647792816,
        -0.06161189451813698,
        -1.0401591062545776,
        -1.3734855651855469,
        -0.45389607548713684,
        0.15178513526916504,
        0.6131650805473328,
        -0.19250160455703735,
        0.7919657230377197,
        0.3568412959575653,
        -0.060137610882520676,
        0.20884175598621368,
        -0.6716527342796326,
        -0.9014647006988525,
        -1.1881444454193115,
        0.6427596211433411,
        0.6355167031288147,
        0.0009757783263921738,
        -0.7621763944625854,
        0.37072545289993286,
        -0.7075099945068359,
        0.12119797617197037,
        0.11061879992485046,
        -0.6523983478546143,
        -0.22044649720191956,
        0.0363616943359375,
        0.19998770952224731,
        -0.1439376175403595,
        -0.18788772821426392,
        0.2325368970632553,
        -0.26836442947387695,
        0.6612609624862671,
        -0.3067720830440521,
        0.6402696371078491,
        -0.4087446928024292,
        -0.025930751115083694,
        -0.3877235949039459,
        -0.048462044447660446,
        -0.10223783552646637,
        -0.24297109246253967,
        -0.49250391125679016,
        -0.07205034047365189,
        0.15878601372241974,
        0.0311686173081398,
        -0.19702282547950745,
        -0.03893711790442467,
        0.279605895280838,
        -3.3264479637145996,
        -0.2058727741241455,
        -0.2058325558900833,
        0.3250005543231964,
        0.43869122862815857,
        -0.022447552531957626,
        0.3926583230495453,
        -0.37963181734085083,
        -0.5865330100059509,
        0.11414148658514023,
        -0.6670125126838684,
        0.3762960731983185,
        -0.06952694058418274,
        -0.1707128882408142,
        -0.10454635322093964,
        -0.13926571607589722,
        0.03221515193581581,
        0.1513502448797226,
        0.5502787232398987,
        0.7814819812774658,
        0.03427854925394058,
        -0.032415758818387985,
        -0.7276811003684998,
        0.29447758197784424,
        -0.3472154140472412,
        -0.4537850618362427,
        0.14406122267246246,
        -0.05900183320045471,
        -0.041371893137693405,
        -0.5215164422988892,
        0.024917470291256905,
        0.2539665102958679,
        -0.2648060917854309,
        0.37531399726867676,
        -0.26148566603660583,
        0.4024553596973419,
        0.2797107696533203,
        3.342712640762329,
        0.5312132239341736,
        0.3482620120048523,
        0.16515497863292694,
        -0.6829807162284851,
        0.5924006104469299,
        -0.6391022205352783,
        -0.7192418575286865,
        -0.4946056604385376,
        -0.38758715987205505,
        0.14964298903942108,
        1.0050963163375854,
        -1.029495120048523,
        -0.3291289508342743,
        -0.021959394216537476,
        -0.0692698061466217,
        0.37741491198539734,
        -0.40459388494491577,
        -0.2238425314426422,
        -0.12274397909641266,
        0.9301227927207947,
        -0.4610162675380707,
        -0.3910842835903168,
        -0.27723798155784607,
        -0.23010337352752686,
        -0.004816949367523193,
        -0.37799203395843506,
        0.05543641373515129,
        0.7497069239616394,
        0.24694429337978363,
        0.4638918936252594,
        0.1636815071105957,
        -0.17180286347866058,
        -0.6432148218154907,
        -0.5045453906059265,
        0.6623357534408569,
        -0.21945029497146606,
        0.21241497993469238,
        -0.5452950596809387,
        0.38199374079704285,
        -0.8734495043754578,
        -0.18314236402511597,
        -0.4182009696960449,
        0.3202670216560364,
        0.3991645574569702,
        0.7554872035980225,
        0.15407691895961761,
        0.3246794044971466,
        -0.021105818450450897,
        -0.9784178137779236,
        -0.4166014492511749,
        -0.03950544819235802,
        0.9170913696289062,
        -0.3977054953575134,
        0.2444215714931488,
        0.20403674244880676,
        0.7158742547035217,
        -1.0507798194885254,
        0.04777195677161217,
        -1.0516148805618286,
        0.8395174145698547,
        -0.04584437981247902,
        -0.2585838735103607,
        -0.007235884666442871,
        -0.08272483944892883,
        0.6728949546813965,
        -0.4729278087615967,
        -0.1510501205921173,
        -0.7925351858139038,
        0.6962536573410034,
        0.26184624433517456,
        -0.012253180146217346,
        -0.07452525943517685,
        0.6506160497665405,
        0.6680817604064941,
        -0.6717159152030945,
        -0.2001713663339615,
        -0.07200510799884796,
        0.34742096066474915,
        0.3486112952232361,
        -0.03843618929386139,
        1.2619562149047852,
        0.41504913568496704,
        -0.4226224422454834,
        -0.8904057145118713,
        -0.7075985074043274,
        0.39706096053123474,
        0.025900948792696,
        0.7741114497184753,
        1.1659507751464844,
        1.1329283714294434,
        -0.8261783719062805,
        2.2728049755096436,
        -0.3609258532524109,
        0.1326853632926941,
        -0.14419370889663696,
        0.04588502645492554,
        -0.06727350503206253,
        0.5201482176780701,
        0.6592483520507812,
        -0.13757340610027313,
        -0.7139759063720703,
        -0.0808807909488678,
        -0.3785441517829895,
        -0.7429181337356567,
        -0.8675931096076965,
        -1.2572929859161377,
        0.3319953978061676,
        0.38112306594848633,
        0.49502259492874146,
        -0.928307294845581,
        -0.24333800375461578,
        0.4273264408111572,
        1.337573766708374,
        0.04345448315143585,
        0.19819267094135284,
        0.4143869876861572,
        0.13494141399860382,
        0.5445235371589661,
        -0.32440921664237976,
        0.46594899892807007,
        -0.7258016467094421,
        0.3397698998451233,
        -0.6730799674987793,
        -0.6883381605148315,
        -1.1099053621292114,
        0.7330272793769836,
        0.004353709518909454,
        0.29261302947998047,
        -0.7897145748138428,
        -0.3602060377597809,
        0.04891853779554367,
        1.1594724655151367,
        1.0079413652420044,
        1.5126539468765259,
        -0.6452389359474182,
        0.13579443097114563,
        -0.10087461024522781,
        0.5106925964355469,
        0.4270853102207184,
        -0.3951227366924286,
        0.8379232287406921,
        -0.017869800329208374,
        -0.3702470660209656,
        -0.014518804848194122,
        -0.26867279410362244,
        0.12433385848999023,
        -0.40371760725975037,
        0.3760446310043335,
        -0.30112725496292114,
        1.4987056255340576,
        0.09112612158060074,
        0.021200546994805336,
        0.12958769500255585,
        0.3461189270019531,
        -0.14254215359687805,
        -0.02954544872045517,
        -1.8000391721725464,
        0.2606240212917328,
        -0.16817662119865417,
        -0.006489504128694534,
        -0.18227273225784302,
        0.24338293075561523,
        0.4419424831867218,
        0.1222512423992157,
        0.5987467169761658,
        0.058105118572711945,
        -0.3657315671443939,
        0.2372080385684967,
        -0.27292105555534363,
        -0.3841777741909027,
        0.17085568606853485,
        0.30990004539489746,
        -0.16361773014068604,
        -0.3760473132133484,
        -0.35049960017204285,
        0.8954309225082397,
        0.32174041867256165,
        -0.19054284691810608,
        -0.3844025433063507,
        -0.36954712867736816,
        0.11327101290225983,
        0.2883513271808624,
        -0.9599242806434631,
        0.41797834634780884,
        -0.8546114563941956,
        0.2521083652973175,
        -0.5826106071472168,
        -0.6238438487052917,
        -0.6012237071990967,
        1.2564386129379272,
        -0.545771598815918,
        -0.25785836577415466,
        -0.8890271186828613,
        -0.04377633333206177,
        1.4796582460403442,
        -0.0906941145658493,
        -0.38951539993286133,
        0.6950251460075378,
        0.1578548550605774,
        -0.29581552743911743,
        -0.24538695812225342,
        0.6260327696800232,
        -0.2896856665611267,
        0.4692462086677551,
        -0.8327121734619141,
        -0.6260647177696228,
        0.9224858283996582,
        0.3502463698387146,
        0.14213471114635468,
        0.00618545338511467,
        -1.331390142440796,
        0.21162810921669006,
        0.15540601313114166,
        0.26053985953330994,
        0.29820820689201355,
        -0.5392670035362244,
        0.008463988080620766,
        1.4340367317199707,
        -0.09726284444332123,
        -1.4128888845443726,
        -0.1254664659500122,
        0.12104995548725128,
        0.7383673787117004,
        0.017618104815483093,
        -0.40330636501312256,
        0.6431189179420471,
        0.3484656810760498,
        -0.12771831452846527,
        -0.10573256015777588,
        0.9778469800949097,
        0.6302933096885681,
        -0.1367560774087906,
        0.20227780938148499,
        0.013465985655784607,
        0.6714507341384888,
        0.03237079083919525,
        0.05961846560239792,
        0.3749943673610687,
        -0.6328080892562866,
        0.24525326490402222,
        0.27215689420700073,
        1.4713075160980225,
        0.5116077661514282,
        -0.005985915660858154,
        -0.07688204944133759,
        0.3773323595523834,
        -0.27069467306137085,
        -0.6880866885185242,
        -0.04117528349161148,
        -0.08636985719203949,
        -0.3043491840362549,
        1.0338653326034546,
        0.011757206171751022,
        0.004288452677428722,
        0.7164812684059143,
        1.087706208229065,
        -0.06248823180794716,
        -0.23100930452346802,
        -0.8765318989753723,
        1.1590349674224854,
        -0.8706761002540588,
        -0.4622724950313568,
        -0.6429219841957092,
        0.3581581711769104,
        -0.16184331476688385,
        0.09194514155387878,
        0.36056071519851685,
        -0.3412156403064728,
        -0.7217198014259338,
        0.16686025261878967,
        -0.2577042579650879,
        -0.8400148749351501,
        0.6932311058044434,
        0.34056156873703003,
        0.8677525520324707,
        0.35692548751831055,
        -0.04037182033061981,
        -0.12458950281143188,
        -0.507476270198822,
        0.17042334377765656,
        0.7839865684509277,
        -0.016339655965566635,
        -0.42485398054122925,
        -0.6942659616470337
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes input to provide concise recommendations for improving processes. It focuses on extracting actionable advice from content descriptions. The output consists of a bulleted list of up to three brief suggestions.",
          "name": "Extract_algorithm_update_recommendations",
          "raw": "\n                workflow Extract_algorithm_update_recommendations v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert interpreter of the algorithms described for doing things within content. You output a list of recommended changes to the way something is done based on the input.\n\n# Steps\n\nTake the input given and extract the concise, practical recommendations for how to do something within the content.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a bulleted list of up to 3 algorithm update recommendations, each of no more than 15 words.\n\n# OUTPUT EXAMPLE\n\n- When evaluating a collection of things that takes time to process, weigh the later ones higher because we naturally weigh them lower due to human bias.\n- When performing web app assessments, be sure to check the /backup.bak path for a 200 or 400 response.\n- Add \\\"Get sun within 30 minutes of waking up to your daily routine.\\\"\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert interpreter of the algorithms described for doing things within content. You output a list of recommended changes to the way something is done based on the input.\n\n# Steps\n\nTake the input given and extract the concise, practical recommendations for how to do something within the content.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a bulleted list of up to 3 algorithm update recommendations, each of no more than 15 words.\n\n# OUTPUT EXAMPLE\n\n- When evaluating a collection of things that takes time to process, weigh the later ones higher because we naturally weigh them lower due to human bias.\n- When performing web app assessments, be sure to check the /backup.bak path for a 200 or 400 response.\n- Add \\\"Get sun within 30 minutes of waking up to your daily routine.\\\"\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6031625270843506,
        0.7570605874061584,
        -0.3975509703159332,
        -0.07024186849594116,
        0.11132028698921204,
        -0.059821806848049164,
        -0.7262586355209351,
        -0.2766437530517578,
        0.2817656397819519,
        0.2115238457918167,
        -0.3479654788970947,
        0.9405828714370728,
        0.9055029153823853,
        -0.2512206435203552,
        0.16911210119724274,
        0.13571462035179138,
        0.13340549170970917,
        0.06790177524089813,
        -1.2645299434661865,
        -0.5502128601074219,
        0.16945728659629822,
        1.0489145517349243,
        0.12003351747989655,
        -0.19206957519054413,
        0.4882603883743286,
        -0.13463354110717773,
        -0.4607807993888855,
        -0.6586816906929016,
        -0.33154675364494324,
        -1.2944600582122803,
        1.1022796630859375,
        0.21144798398017883,
        -0.45760518312454224,
        -0.48680660128593445,
        -0.5665202140808105,
        -0.8315296173095703,
        -0.3013671636581421,
        -0.36806315183639526,
        -0.09859730303287506,
        -0.09273436665534973,
        -0.04577735811471939,
        0.4818915128707886,
        -0.5644676089286804,
        -0.39640018343925476,
        -0.08211195468902588,
        -0.33142608404159546,
        0.2623790502548218,
        -0.26808977127075195,
        0.611514687538147,
        0.17734229564666748,
        0.015962503850460052,
        -0.37727054953575134,
        -0.4110637307167053,
        -0.09403075277805328,
        -0.4405829608440399,
        -0.7116551399230957,
        -0.1824905425310135,
        -0.08546000719070435,
        -0.22627298533916473,
        0.1789226531982422,
        0.3707674741744995,
        0.10370063781738281,
        -3.539595365524292,
        -0.14106522500514984,
        -0.0823478177189827,
        -0.19581013917922974,
        0.5416236519813538,
        -0.5572679042816162,
        -0.21985724568367004,
        -0.4537002742290497,
        -0.2214905321598053,
        0.3713780641555786,
        0.4550926089286804,
        0.18138810992240906,
        0.004580222070217133,
        0.07229650765657425,
        0.06320369243621826,
        -0.33474522829055786,
        0.1509331613779068,
        -0.4306822419166565,
        -0.2595616579055786,
        0.5009536147117615,
        -0.14085468649864197,
        0.1144861951470375,
        -0.7359576225280762,
        0.9518533945083618,
        -0.8347254991531372,
        -0.4342251420021057,
        0.11080164462327957,
        0.04518800973892212,
        0.5780548453330994,
        -1.2453852891921997,
        0.17123693227767944,
        0.12876927852630615,
        -0.5097426772117615,
        -0.00783228874206543,
        0.10678933560848236,
        0.13312754034996033,
        0.06116223707795143,
        3.7126247882843018,
        1.0623643398284912,
        0.8792241811752319,
        0.4725472331047058,
        -0.5220361351966858,
        0.3353930413722992,
        -0.5009263753890991,
        -0.4883747100830078,
        -0.09984613955020905,
        0.17203721404075623,
        -0.07025027275085449,
        0.4087577164173126,
        -0.6455522775650024,
        -0.5328415632247925,
        -0.0020751338452100754,
        -0.18537887930870056,
        0.7430076599121094,
        -0.8863039016723633,
        -0.12317952513694763,
        0.4989687502384186,
        0.4626900255680084,
        -0.33392319083213806,
        0.30788716673851013,
        0.13021594285964966,
        -0.42139333486557007,
        -0.2770838737487793,
        0.18844637274742126,
        0.03753899410367012,
        0.6629736423492432,
        0.04616808518767357,
        0.0746971070766449,
        -0.09760735183954239,
        0.07158999145030975,
        -0.7573939561843872,
        -0.20681194961071014,
        0.07637642323970795,
        -0.002390354871749878,
        0.0799248218536377,
        -1.1235793828964233,
        0.47077617049217224,
        -0.7391625046730042,
        -0.01065915822982788,
        -1.1725311279296875,
        0.4348756670951843,
        -0.08457524329423904,
        0.9869426488876343,
        0.658786416053772,
        0.2625112235546112,
        0.2473762035369873,
        -0.2394435554742813,
        -0.392141193151474,
        0.0233872402459383,
        0.8267322778701782,
        -0.12456453591585159,
        0.3652566373348236,
        0.28760892152786255,
        0.6067934632301331,
        -0.9481708407402039,
        -0.31772640347480774,
        -0.7149036526679993,
        0.32959845662117004,
        0.03566138073801994,
        -0.0981234535574913,
        0.8235416412353516,
        -0.2806658148765564,
        0.658254861831665,
        -0.19920694828033447,
        0.30099359154701233,
        0.25412508845329285,
        0.1446434110403061,
        0.16391582787036896,
        0.5347848534584045,
        -0.5132113099098206,
        0.38247036933898926,
        0.6057184934616089,
        -0.19825801253318787,
        -0.11814628541469574,
        0.026972033083438873,
        0.01369968056678772,
        0.6739367842674255,
        -0.05489776283502579,
        0.8399506211280823,
        0.16809861361980438,
        -0.30776700377464294,
        -0.6878077983856201,
        -0.6263999938964844,
        0.009576823562383652,
        0.24285289645195007,
        0.23991334438323975,
        0.8189637064933777,
        1.2177362442016602,
        -0.43411171436309814,
        2.2238945960998535,
        -0.18098017573356628,
        -0.6650331020355225,
        0.0021191267296671867,
        0.030658964067697525,
        -0.23497672379016876,
        0.41242921352386475,
        1.185178279876709,
        -0.18258360028266907,
        -0.5846832990646362,
        0.4087989032268524,
        -0.44572198390960693,
        -0.21618211269378662,
        -0.4777544140815735,
        -0.4425922632217407,
        -0.15338163077831268,
        0.14185650646686554,
        -0.13694359362125397,
        -0.7893567681312561,
        -0.18123789131641388,
        0.22257830202579498,
        0.9278914928436279,
        0.5447108745574951,
        0.4397750794887543,
        0.5088456273078918,
        0.14691293239593506,
        0.40866929292678833,
        0.2609511613845825,
        0.4865398705005646,
        -0.18937045335769653,
        -0.11528307944536209,
        -0.5038414597511292,
        -0.8590200543403625,
        -0.9656205177307129,
        0.5949080586433411,
        0.2604992687702179,
        0.18904541432857513,
        -0.5240355134010315,
        0.09599511325359344,
        0.09511388093233109,
        1.4525355100631714,
        0.7992308735847473,
        1.1358726024627686,
        0.5127350687980652,
        0.43343934416770935,
        -0.33177828788757324,
        0.48246559500694275,
        0.5812041759490967,
        -0.2536369562149048,
        0.8853898048400879,
        -0.263504296541214,
        -0.12498468160629272,
        0.3968181014060974,
        -0.2041371464729309,
        0.10283465683460236,
        -0.89628005027771,
        0.09410135447978973,
        -0.10101012140512466,
        1.1654787063598633,
        -0.03934179246425629,
        -0.061374302953481674,
        0.30038517713546753,
        0.43698567152023315,
        0.5987657904624939,
        0.018955819308757782,
        -1.269292950630188,
        0.2005387395620346,
        -0.4438115656375885,
        0.02859492227435112,
        -0.18305113911628723,
        -0.035460617393255234,
        0.46852150559425354,
        -0.019057709723711014,
        0.1177765503525734,
        -0.5454642176628113,
        -0.6452292799949646,
        0.051026664674282074,
        -0.01958668977022171,
        -0.7410074472427368,
        0.07237368077039719,
        0.12550707161426544,
        -0.3364641070365906,
        -0.778130829334259,
        0.21506257355213165,
        0.6866328120231628,
        0.4524838626384735,
        0.012243986129760742,
        -0.2393445074558258,
        -0.43346405029296875,
        0.5393485426902771,
        -0.008712871000170708,
        0.09357303380966187,
        0.4136810898780823,
        -0.5963272452354431,
        0.1150217056274414,
        -0.5824347138404846,
        -0.7314920425415039,
        -0.1341777741909027,
        1.141648769378662,
        -0.4122786223888397,
        -0.3225040137767792,
        -0.1831636130809784,
        0.00041772425174713135,
        1.4803091287612915,
        0.19814088940620422,
        0.11287395656108856,
        0.6259462237358093,
        0.33814752101898193,
        -0.3946564793586731,
        -0.2747846245765686,
        0.1284688413143158,
        -0.36021068692207336,
        0.12264867126941681,
        -0.42456838488578796,
        -0.5321755409240723,
        0.3949257731437683,
        0.17233629524707794,
        -0.26730161905288696,
        0.1715935468673706,
        -1.1908038854599,
        -0.3867683708667755,
        -0.2660917341709137,
        0.2106175720691681,
        0.7261466383934021,
        -0.9327032566070557,
        0.5499009490013123,
        0.09785707294940948,
        0.11007429659366608,
        -1.8555645942687988,
        -0.5288715958595276,
        0.20422184467315674,
        0.8880193829536438,
        -0.3511180579662323,
        -0.33186525106430054,
        0.6756024956703186,
        -0.18413865566253662,
        0.10990484058856964,
        -0.3583815097808838,
        1.4886187314987183,
        0.6179420948028564,
        -0.4011666476726532,
        0.29155945777893066,
        0.1612570583820343,
        0.4938308894634247,
        -0.3342542052268982,
        -0.049703583121299744,
        -0.2778518795967102,
        -0.5874372124671936,
        -0.07308948040008545,
        0.4545976221561432,
        0.7379453778266907,
        0.7053939700126648,
        0.15619812905788422,
        0.034840527921915054,
        0.06997300684452057,
        -0.905386209487915,
        -1.1112693548202515,
        0.28203654289245605,
        -0.13637810945510864,
        -0.6871808171272278,
        0.7064476013183594,
        -0.24283646047115326,
        -0.07800895720720291,
        0.579151451587677,
        0.9264129996299744,
        -0.2562413513660431,
        0.22736364603042603,
        -1.0056607723236084,
        1.9239919185638428,
        -0.4091321527957916,
        -0.588337779045105,
        -0.20994338393211365,
        0.600407063961029,
        -0.12032458931207657,
        0.012968100607395172,
        0.4578036665916443,
        -0.07621210068464279,
        -0.11663311719894409,
        -0.009505659341812134,
        0.3339117169380188,
        -0.40787404775619507,
        0.8091518878936768,
        0.04334064573049545,
        0.3485364317893982,
        -0.044657886028289795,
        0.07073886692523956,
        -0.08471138775348663,
        0.2707417607307434,
        0.07469266653060913,
        0.3490082621574402,
        -0.6546385288238525,
        -0.8368974924087524,
        -0.8090885281562805
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts key insights and valuable information from textual content, focusing on ideas, quotes, habits, and references. It aims to address the issue of information overload by providing a concise summary of the content's most meaningful aspects. The expected output includes summarized ideas, notable quotes, referenced materials, and habits worth adopting.",
          "name": "Extract_article_wisdom",
          "raw": "\n                workflow Extract_article_wisdom v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n1. Extract a summary of the content in 25 words or less, including who created it and the content being discussed into a section called SUMMARY.\n\n2. Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n3. Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n4. Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n5. Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n6. Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n1. Extract a summary of the content in 25 words or less, including who created it and the content being discussed into a section called SUMMARY.\n\n2. Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n3. Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n4. Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n5. Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n6. Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.33064553141593933,
        0.30089783668518066,
        -0.2966913878917694,
        -0.0358462855219841,
        0.17037567496299744,
        0.6957295536994934,
        -0.6856237053871155,
        -0.05130426585674286,
        0.46575912833213806,
        0.08760079741477966,
        0.24672862887382507,
        0.560289740562439,
        0.5893542766571045,
        -0.034920163452625275,
        0.0656927078962326,
        0.5743512511253357,
        -0.053822483867406845,
        -0.2186974734067917,
        -1.4741854667663574,
        -0.7365366220474243,
        -0.32502079010009766,
        0.7998075485229492,
        0.19164645671844482,
        0.18358662724494934,
        -0.08487048000097275,
        -0.2802802622318268,
        -0.6563372015953064,
        -0.11064742505550385,
        -0.29383385181427,
        -1.0886461734771729,
        0.8316780924797058,
        -0.09027860313653946,
        -0.9025378823280334,
        -0.22309353947639465,
        -0.09177010506391525,
        -0.33850395679473877,
        -0.005509709939360619,
        -0.22814145684242249,
        -0.06237141042947769,
        -0.25758397579193115,
        -0.09449902921915054,
        0.9568857550621033,
        -0.153542622923851,
        -0.2859388589859009,
        0.31538859009742737,
        -0.13252824544906616,
        -0.09648924320936203,
        -0.217812642455101,
        0.2998409569263458,
        0.09713493287563324,
        -0.022535644471645355,
        -0.6558418273925781,
        0.10816504061222076,
        -0.5117594003677368,
        -0.6455833911895752,
        -0.5863580703735352,
        -0.13109955191612244,
        -0.17399351298809052,
        0.7116184234619141,
        -0.04677840322256088,
        0.32480618357658386,
        0.2762907147407532,
        -3.208540678024292,
        -0.115593820810318,
        -0.22976914048194885,
        0.1260330229997635,
        0.24190841615200043,
        -0.4613921642303467,
        0.22681236267089844,
        -0.05768913775682449,
        0.22635440528392792,
        -0.18368181586265564,
        -0.20199698209762573,
        0.1845998764038086,
        -0.195932537317276,
        -0.013910844922065735,
        -0.12878333032131195,
        -0.03733537346124649,
        -0.13272719085216522,
        -0.04551615193486214,
        -0.16787046194076538,
        0.37971335649490356,
        0.020430371165275574,
        0.24406538903713226,
        -0.6809601187705994,
        0.6549586653709412,
        -0.7806034684181213,
        -0.3230479061603546,
        0.030959218740463257,
        0.3481275141239166,
        -0.39151865243911743,
        -1.4355915784835815,
        0.6070947647094727,
        -0.46411341428756714,
        -0.6937834024429321,
        0.07953745126724243,
        -0.06310316920280457,
        0.21128347516059875,
        0.21714457869529724,
        3.515937566757202,
        0.6612783074378967,
        1.047049880027771,
        -0.05406873673200607,
        -0.7947559356689453,
        0.720670223236084,
        -0.3712661564350128,
        0.02627299353480339,
        -0.25197890400886536,
        0.08543814718723297,
        -0.43265604972839355,
        0.6272792220115662,
        -0.9029911756515503,
        -0.1313413679599762,
        -0.3266293406486511,
        -0.10293478518724442,
        0.6936755776405334,
        -0.5003564357757568,
        -0.03224024549126625,
        0.32194334268569946,
        0.2192395180463791,
        -0.35703471302986145,
        -0.022600125521421432,
        0.25196319818496704,
        -0.8893299102783203,
        0.015362568199634552,
        -0.21912631392478943,
        -0.41004854440689087,
        0.7759379744529724,
        0.5509241819381714,
        0.28634917736053467,
        -0.11927991360425949,
        0.11944423615932465,
        -0.5022133588790894,
        -0.019552411511540413,
        0.1981792449951172,
        -0.3041517436504364,
        0.43209850788116455,
        -1.2087150812149048,
        0.5727501511573792,
        -0.9985523819923401,
        -0.1381627917289734,
        -0.9024823307991028,
        -0.07981234788894653,
        0.8625493049621582,
        0.8143792152404785,
        0.6918624043464661,
        0.6873607039451599,
        0.06453470885753632,
        -0.603100061416626,
        -0.18936222791671753,
        -0.07734209299087524,
        0.4681292176246643,
        0.17465239763259888,
        -0.09510601311922073,
        0.12303509563207626,
        0.00444365618750453,
        -0.6177332997322083,
        0.5192642211914062,
        -1.3123160600662231,
        0.8189314007759094,
        0.4030406177043915,
        -0.28745678067207336,
        0.5797871351242065,
        -0.12596409022808075,
        0.20471414923667908,
        -0.2912822663784027,
        0.134487122297287,
        -0.0480416864156723,
        0.18120664358139038,
        0.5249175429344177,
        -0.12930244207382202,
        -0.3441225588321686,
        0.7700951099395752,
        0.6436739563941956,
        -0.4957154393196106,
        -0.07666485756635666,
        0.014021717011928558,
        0.1745586395263672,
        0.010101810097694397,
        -0.60310959815979,
        0.7168571949005127,
        0.17598898708820343,
        0.20801186561584473,
        -1.0893868207931519,
        -0.7889867424964905,
        0.7218027114868164,
        0.2792831361293793,
        0.1412961333990097,
        1.0957063436508179,
        1.3412284851074219,
        0.05427280813455582,
        1.8531105518341064,
        0.2600099444389343,
        0.045102544128894806,
        -0.20047831535339355,
        0.29399746656417847,
        -0.526444137096405,
        -0.2769320607185364,
        0.7971804141998291,
        0.33943039178848267,
        -0.6628453135490417,
        0.10173006355762482,
        -0.7526652216911316,
        -0.5712722539901733,
        -1.037358045578003,
        -0.16837114095687866,
        -0.04229484871029854,
        0.2998988628387451,
        0.510066032409668,
        -1.1069867610931396,
        -0.3381831645965576,
        0.7899751663208008,
        0.45044413208961487,
        0.3240291178226471,
        0.5362803936004639,
        -0.03278370201587677,
        0.8151246309280396,
        -0.34723392128944397,
        0.3438602387905121,
        0.44719555974006653,
        -0.2820373475551605,
        0.20894896984100342,
        -0.7761098146438599,
        -0.8565509915351868,
        -0.691095769405365,
        0.9820997714996338,
        0.14313450455665588,
        0.235443577170372,
        -0.4290599524974823,
        0.2903744578361511,
        -0.06127447634935379,
        0.2604689598083496,
        0.6712808012962341,
        1.47800612449646,
        -0.5414883494377136,
        0.5836387276649475,
        0.1293191760778427,
        0.2755255699157715,
        0.369046688079834,
        -0.9962056279182434,
        1.0559430122375488,
        0.2616380453109741,
        -0.3919163942337036,
        0.12933814525604248,
        0.12177929282188416,
        0.22958189249038696,
        -0.8864744901657104,
        0.10031206905841827,
        0.00615331344306469,
        1.5661613941192627,
        -0.6025415062904358,
        0.062269486486911774,
        0.21682026982307434,
        0.7356516122817993,
        0.233860045671463,
        0.08132313936948776,
        -1.7464545965194702,
        0.3415193259716034,
        -0.17980033159255981,
        -0.3182655870914459,
        -0.13314324617385864,
        0.06425999850034714,
        0.6104136109352112,
        0.10615896433591843,
        0.5089655518531799,
        -0.8002785444259644,
        -0.340343713760376,
        -0.11678189039230347,
        -0.2162761390209198,
        -0.28446507453918457,
        -0.03225422650575638,
        0.16973541676998138,
        -0.31265729665756226,
        -0.4020143449306488,
        0.19904130697250366,
        0.4959802031517029,
        0.14119350910186768,
        -0.2945494055747986,
        -0.32014280557632446,
        -0.598308265209198,
        0.620761513710022,
        -0.19623365998268127,
        -0.20587590336799622,
        0.3178064525127411,
        -0.7193648219108582,
        0.42184460163116455,
        -0.1622641235589981,
        -0.5748567581176758,
        -0.687038004398346,
        1.4749430418014526,
        -0.35103827714920044,
        -0.1932937353849411,
        -0.3683655261993408,
        -0.5602497458457947,
        1.3759703636169434,
        -0.20887771248817444,
        0.12593619525432587,
        0.5453839898109436,
        0.4209187626838684,
        -0.28254520893096924,
        -0.3280099034309387,
        0.06292178481817245,
        -0.49260467290878296,
        0.1280929446220398,
        -0.9361540079116821,
        -0.4424285888671875,
        0.6493028402328491,
        0.07005815207958221,
        0.15906083583831787,
        0.24905797839164734,
        -1.0357998609542847,
        -0.5459468364715576,
        0.21130475401878357,
        0.020212262868881226,
        0.9299379587173462,
        -0.43656232953071594,
        0.7354356050491333,
        0.4614297151565552,
        -0.10378514230251312,
        -1.401573896408081,
        -0.5788223147392273,
        0.1897580325603485,
        0.2699793875217438,
        -0.24388326704502106,
        -0.013370940461754799,
        0.5096728205680847,
        -0.17628708481788635,
        -0.23483633995056152,
        -0.31230759620666504,
        1.6034162044525146,
        0.4852493107318878,
        -0.19293279945850372,
        0.08463895320892334,
        -0.2393755316734314,
        1.0759071111679077,
        -0.37546640634536743,
        0.3202594518661499,
        -0.3865056335926056,
        -0.3026652932167053,
        0.44513797760009766,
        0.8032903075218201,
        1.3372085094451904,
        0.4879697561264038,
        -0.2783196270465851,
        0.017601165920495987,
        0.1597074270248413,
        -0.7124190926551819,
        -0.8805723190307617,
        -0.09758453071117401,
        0.5790307521820068,
        -0.27500253915786743,
        0.7652583718299866,
        -0.17189070582389832,
        0.08894585818052292,
        0.9963151812553406,
        0.2391822338104248,
        0.20224925875663757,
        0.37654027342796326,
        -0.9785626530647278,
        1.3571199178695679,
        -0.5969215631484985,
        -0.05171307176351547,
        -0.2537505030632019,
        0.010218581184744835,
        -0.42778444290161133,
        0.24063436686992645,
        0.24738897383213043,
        0.22197303175926208,
        0.18950255215168,
        0.14324432611465454,
        0.33609944581985474,
        -0.8234183192253113,
        0.8111822605133057,
        0.7178240418434143,
        0.4086371064186096,
        0.10233084112405777,
        -0.10355643928050995,
        0.029407262802124023,
        -0.15346485376358032,
        -0.5562083125114441,
        0.8101899027824402,
        -0.790557324886322,
        -0.7494049668312073,
        -1.264364242553711
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes a book's key content by extracting 50 to 100 of its most interesting ideas. The process involves a deep dive into the book's insights, prioritizing them by interest and insightfulness. The output is a concise list of bulleted ideas, limited to 20 words each.",
          "name": "Extract_book_ideas",
          "raw": "\n                workflow Extract_book_ideas v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou take a book name as an input and output a full summary of the book's most important content using the steps and instructions below.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Scour your memory for everything you know about this book. \n\n- Extract 50 to 100 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Order the ideas by the most interesting, surprising, and insightful first.\n\n- Extract at least 50 IDEAS from the content.\n\n- Extract up to 100 IDEAS.\n\n- Limit each bullet to a maximum of 20 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat IDEAS.\n\n- Vary the wording of the IDEAS.\n\n- Don't repeat the same IDEAS over and over, even if you're using different wording.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou take a book name as an input and output a full summary of the book's most important content using the steps and instructions below.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Scour your memory for everything you know about this book. \n\n- Extract 50 to 100 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Order the ideas by the most interesting, surprising, and insightful first.\n\n- Extract at least 50 IDEAS from the content.\n\n- Extract up to 100 IDEAS.\n\n- Limit each bullet to a maximum of 20 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat IDEAS.\n\n- Vary the wording of the IDEAS.\n\n- Don't repeat the same IDEAS over and over, even if you're using different wording.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4617065489292145,
        0.47968608140945435,
        -0.2549692988395691,
        -0.08849356323480606,
        0.2601119577884674,
        0.503598690032959,
        -0.8605594038963318,
        0.0010706186294555664,
        0.5848320126533508,
        0.08807860314846039,
        0.5027352571487427,
        0.5008214712142944,
        0.04603966698050499,
        0.23430924117565155,
        0.3806191086769104,
        0.6450578570365906,
        -0.25442785024642944,
        -0.40064337849617004,
        -1.216752529144287,
        -0.6426820755004883,
        0.06740672886371613,
        0.6836519837379456,
        -0.12866511940956116,
        0.41782915592193604,
        -0.015169370919466019,
        -0.09028893709182739,
        -0.3403582274913788,
        -0.6009767651557922,
        -0.588136613368988,
        -1.3174227476119995,
        0.5902385115623474,
        0.3788004517555237,
        -0.7075875401496887,
        -0.7530117034912109,
        -0.13646391034126282,
        -0.5247185230255127,
        -0.10794580727815628,
        -0.05260276794433594,
        -0.16047362983226776,
        -0.3192381262779236,
        -0.009213298559188843,
        0.723943829536438,
        -0.2765791416168213,
        -0.09738952666521072,
        0.13154180347919464,
        -0.26914098858833313,
        0.13429604470729828,
        -0.1548725962638855,
        0.78000408411026,
        -0.04720121622085571,
        -0.09996098279953003,
        -0.5085956454277039,
        0.08113829791545868,
        -0.367451936006546,
        -0.38489288091659546,
        -0.41846516728401184,
        -0.38576367497444153,
        0.23795603215694427,
        0.7882230281829834,
        -0.3282271921634674,
        0.1998116672039032,
        0.2139219343662262,
        -3.5184054374694824,
        -0.04639690741896629,
        -0.16120032966136932,
        0.3990345597267151,
        0.4834758937358856,
        -0.3150753676891327,
        0.4568396508693695,
        -0.6584639549255371,
        0.18861520290374756,
        -0.19145044684410095,
        -0.31408578157424927,
        0.03908984363079071,
        0.03171410411596298,
        -0.11584671586751938,
        -0.1903364509344101,
        -0.07176245748996735,
        0.007010377943515778,
        0.4092012345790863,
        -0.0625176653265953,
        0.7533428072929382,
        -0.3184867203235626,
        0.14173151552677155,
        -0.57696533203125,
        0.4953251779079437,
        -0.6282063722610474,
        -0.6926698684692383,
        0.01583845168352127,
        0.15537413954734802,
        -0.2847761809825897,
        -1.26481294631958,
        -0.15634459257125854,
        -0.13828244805335999,
        -0.3425194323062897,
        0.14152240753173828,
        -0.35574033856391907,
        0.3258410692214966,
        0.24300777912139893,
        3.6877565383911133,
        0.6938616037368774,
        0.589775562286377,
        0.24202442169189453,
        -0.7584248185157776,
        0.6766751408576965,
        -0.7453181743621826,
        -0.25873202085494995,
        -0.25376641750335693,
        -0.385141521692276,
        0.07649052143096924,
        0.45021751523017883,
        -0.8465893268585205,
        -0.3777863681316376,
        0.021130993962287903,
        0.09280893206596375,
        0.35751089453697205,
        -0.3609795570373535,
        -0.2136261761188507,
        0.018793150782585144,
        0.28209221363067627,
        -0.5105735659599304,
        0.14652526378631592,
        0.2500574588775635,
        -0.3281181752681732,
        0.15049071609973907,
        -0.5115422606468201,
        -0.15371985733509064,
        0.7982787489891052,
        0.3885992169380188,
        0.47687721252441406,
        -0.1192106381058693,
        0.014658492058515549,
        -0.6772840619087219,
        -0.046650536358356476,
        -0.12368765473365784,
        -0.6964401602745056,
        0.520933985710144,
        -1.0067002773284912,
        0.5629810094833374,
        -0.85291987657547,
        0.07141588628292084,
        -1.0433517694473267,
        0.09841001033782959,
        0.9340991973876953,
        1.1535910367965698,
        0.34045690298080444,
        0.3180798888206482,
        0.12522901594638824,
        -0.8771063089370728,
        0.14701314270496368,
        0.3097838759422302,
        0.8030874729156494,
        0.0711909607052803,
        0.09555593132972717,
        0.3483338952064514,
        -0.05063964053988457,
        -0.9256642460823059,
        0.5177422165870667,
        -1.2021807432174683,
        0.8287795186042786,
        -0.07293340563774109,
        -0.5848028659820557,
        0.5686010122299194,
        -0.09477189183235168,
        0.5728434920310974,
        -0.6652559041976929,
        0.49417251348495483,
        -0.4479401409626007,
        0.42258983850479126,
        0.1929291933774948,
        -0.3328476548194885,
        -0.19029363989830017,
        0.7625741958618164,
        0.8956804871559143,
        -0.23896758258342743,
        -0.17987723648548126,
        0.25403764843940735,
        0.10248047113418579,
        -0.17709870636463165,
        -0.24866992235183716,
        0.9646801948547363,
        0.18480926752090454,
        0.030076999217271805,
        -1.0580928325653076,
        -0.7103123664855957,
        0.454191654920578,
        0.36808061599731445,
        0.45839056372642517,
        1.2462108135223389,
        1.2387152910232544,
        -0.26079463958740234,
        1.4669142961502075,
        -0.008187204599380493,
        0.3495423197746277,
        -0.13410437107086182,
        -0.06798821687698364,
        -0.3109813928604126,
        -0.22553767263889313,
        0.7678650617599487,
        0.17144018411636353,
        -0.8946128487586975,
        -0.2100066840648651,
        -0.4623252749443054,
        -0.48187145590782166,
        -0.9841389060020447,
        -0.3969114422798157,
        -0.19097548723220825,
        0.3669886589050293,
        0.5634438395500183,
        -0.8568649888038635,
        -0.10934378206729889,
        0.8693922758102417,
        0.3035171926021576,
        0.03441396355628967,
        0.40087050199508667,
        0.22188834846019745,
        0.4948081374168396,
        0.39032265543937683,
        -0.1384621411561966,
        0.4195287227630615,
        -0.3911910653114319,
        0.12360993027687073,
        -0.7200773358345032,
        -0.8735308647155762,
        -0.6417397856712341,
        0.7656702995300293,
        0.16942912340164185,
        -0.03484367951750755,
        -0.5814674496650696,
        0.06678472459316254,
        -0.3729454278945923,
        0.6449121236801147,
        1.082135558128357,
        0.8272743225097656,
        -0.4183219075202942,
        0.615749180316925,
        0.158212810754776,
        0.24160124361515045,
        0.5346033573150635,
        -0.5989116430282593,
        0.6668816208839417,
        -0.0469585657119751,
        -0.25813984870910645,
        -0.038111865520477295,
        0.25822752714157104,
        0.18033483624458313,
        -0.754382848739624,
        0.3945516347885132,
        0.15423931181430817,
        1.4510746002197266,
        -0.3850554823875427,
        0.4161210358142853,
        -0.14148873090744019,
        0.683102548122406,
        0.005138657987117767,
        0.3201408088207245,
        -1.470760703086853,
        0.31851354241371155,
        -0.1718398630619049,
        -0.14872150123119354,
        -0.08354289829730988,
        -0.20179268717765808,
        0.19263845682144165,
        0.3162636458873749,
        0.7515949606895447,
        -0.209824338555336,
        -0.14639900624752045,
        -0.3363201916217804,
        -0.1922208070755005,
        -0.26689261198043823,
        -0.13752681016921997,
        0.4988744854927063,
        -0.28698331117630005,
        -0.17392539978027344,
        0.19270744919776917,
        0.7291091680526733,
        0.2570861876010895,
        -0.3471827507019043,
        -0.5514549016952515,
        -0.5687130689620972,
        0.328268438577652,
        0.10910683870315552,
        -0.24605286121368408,
        0.35475218296051025,
        -0.7671943306922913,
        0.5434612035751343,
        -0.27492573857307434,
        -0.5290011167526245,
        -0.7956914901733398,
        1.019928216934204,
        -0.3527059853076935,
        -0.5932508111000061,
        -0.5802470445632935,
        -0.24601417779922485,
        1.3978142738342285,
        -0.13165321946144104,
        -0.2925688922405243,
        0.3700307309627533,
        0.3191438913345337,
        -0.11494890600442886,
        -0.3494311273097992,
        0.1894625425338745,
        -0.38585367798805237,
        0.23553314805030823,
        -0.9353289604187012,
        -0.516075074672699,
        1.0727896690368652,
        0.27540260553359985,
        0.37917619943618774,
        0.20621390640735626,
        -1.2564263343811035,
        -0.2912529706954956,
        0.4037547707557678,
        0.037362102419137955,
        0.5095982551574707,
        -0.5911049246788025,
        0.29977115988731384,
        0.8714902400970459,
        -0.13156646490097046,
        -1.2631086111068726,
        -0.3707888424396515,
        0.11631718277931213,
        0.22853107750415802,
        -0.05590850114822388,
        -0.02374163083732128,
        0.5289093255996704,
        -0.5898984670639038,
        -0.38166332244873047,
        -0.037967048585414886,
        1.5121281147003174,
        0.4192516505718231,
        -0.08510440587997437,
        -0.10178518295288086,
        -0.27994006872177124,
        0.8605575561523438,
        -0.28882351517677307,
        0.2428872287273407,
        0.20860257744789124,
        -0.7179841995239258,
        0.2723168432712555,
        0.35689035058021545,
        1.1872215270996094,
        0.606031596660614,
        -0.3964650630950928,
        0.039302755147218704,
        0.15817032754421234,
        -0.49270153045654297,
        -0.7318082451820374,
        -0.06852543354034424,
        0.0667835921049118,
        -0.11727861315011978,
        0.7391034960746765,
        -0.2945208251476288,
        0.4391176104545593,
        1.0433754920959473,
        0.899756908416748,
        0.20986221730709076,
        0.4007907509803772,
        -0.9506431818008423,
        1.4774152040481567,
        -0.7882593870162964,
        0.1152910515666008,
        -0.9118332862854004,
        0.29661422967910767,
        0.21065185964107513,
        -0.042156368494033813,
        0.31866201758384705,
        -0.23412126302719116,
        -0.2555767893791199,
        0.19184458255767822,
        0.32109057903289795,
        -0.9785067439079285,
        0.9303669929504395,
        0.931003987789154,
        0.6729316711425781,
        -0.04905623197555542,
        -0.15244591236114502,
        -0.06421828269958496,
        -0.4488127827644348,
        -0.10667815804481506,
        0.9366979598999023,
        -0.5964095592498779,
        -0.7328792214393616,
        -0.9596380591392517
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes a book's key content by extracting 50 to 100 of its most practical recommendations, prioritizing the most impactful advice. This process involves a thorough memory search to identify actionable insights. The output is formatted as an instructive, bullet-pointed list, limited to 20 words each.",
          "name": "Extract_book_recommendations",
          "raw": "\n                workflow Extract_book_recommendations v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou take a book name as an input and output a full summary of the book's most important content using the steps and instructions below.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Scour your memory for everything you know about this book. \n\n- Extract 50 to 100 of the most practical RECOMMENDATIONS from the input in a section called RECOMMENDATIONS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Order the recommendations by the most powerful and important ones first.\n\n- Write all recommendations as instructive advice, not abstract ideas.\n\n\n- Extract at least 50 RECOMMENDATIONS from the content.\n\n- Extract up to 100 RECOMMENDATIONS.\n\n- Limit each bullet to a maximum of 20 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- Do not repeat IDEAS.\n\n- Vary the wording of the IDEAS.\n\n- Don't repeat the same IDEAS over and over, even if you're using different wording.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou take a book name as an input and output a full summary of the book's most important content using the steps and instructions below.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Scour your memory for everything you know about this book. \n\n- Extract 50 to 100 of the most practical RECOMMENDATIONS from the input in a section called RECOMMENDATIONS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Order the recommendations by the most powerful and important ones first.\n\n- Write all recommendations as instructive advice, not abstract ideas.\n\n\n- Extract at least 50 RECOMMENDATIONS from the content.\n\n- Extract up to 100 RECOMMENDATIONS.\n\n- Limit each bullet to a maximum of 20 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- Do not repeat IDEAS.\n\n- Vary the wording of the IDEAS.\n\n- Don't repeat the same IDEAS over and over, even if you're using different wording.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        1.3861544132232666,
        0.3980214595794678,
        -0.4800882935523987,
        -0.39966481924057007,
        -0.14693106710910797,
        0.3041155934333801,
        -0.538782000541687,
        -0.03292030841112137,
        -0.21979089081287384,
        0.4622036814689636,
        -0.35563457012176514,
        0.5248066186904907,
        0.6910514235496521,
        0.19614046812057495,
        0.1750607043504715,
        0.06003398448228836,
        0.4704757630825043,
        -0.17138907313346863,
        -1.8231682777404785,
        -0.2911257743835449,
        -0.32013991475105286,
        0.3089974522590637,
        0.7773229479789734,
        0.14679966866970062,
        0.03011656552553177,
        0.22906601428985596,
        -0.28850680589675903,
        -0.053074534982442856,
        -0.6834304928779602,
        -1.1124563217163086,
        1.2845804691314697,
        0.72002112865448,
        -0.13686133921146393,
        -0.7890421152114868,
        0.566238522529602,
        -0.31079670786857605,
        -0.11862589418888092,
        0.34155067801475525,
        -0.5626888275146484,
        -0.5135830640792847,
        0.2851281762123108,
        0.30859363079071045,
        -0.43542754650115967,
        0.006929520517587662,
        0.40860605239868164,
        -0.14191827178001404,
        0.2275780886411667,
        -0.16822832822799683,
        0.5104043483734131,
        0.4762026071548462,
        -0.3771820664405823,
        -0.6437438130378723,
        0.022139906883239746,
        -0.9156367778778076,
        -0.8927019834518433,
        -0.061571769416332245,
        -0.10037769377231598,
        -0.28342971205711365,
        0.512075662612915,
        0.17469073832035065,
        0.3682839572429657,
        0.29679998755455017,
        -2.7916486263275146,
        -0.26454734802246094,
        -0.16752195358276367,
        0.22846350073814392,
        -0.1717616319656372,
        0.025388695299625397,
        -0.6468644142150879,
        -0.14570720493793488,
        0.08269674330949783,
        0.01111605390906334,
        -0.5238797068595886,
        0.07468757033348083,
        -0.28282177448272705,
        -0.03509735316038132,
        0.49723803997039795,
        -0.1773262321949005,
        -0.16658200323581696,
        -0.053085096180438995,
        0.19182121753692627,
        0.5660406947135925,
        0.10262710601091385,
        -0.14878232777118683,
        -0.6500939130783081,
        0.4492386281490326,
        -0.5272254943847656,
        0.14407125115394592,
        0.5839292407035828,
        0.7865473031997681,
        0.07773426175117493,
        -0.648270308971405,
        1.1642251014709473,
        -0.0020873285830020905,
        0.05098133906722069,
        0.14010995626449585,
        0.006278727203607559,
        0.01688505709171295,
        -0.026351548731327057,
        3.697272777557373,
        0.7645341753959656,
        0.3991004526615143,
        0.5369661450386047,
        -0.9111613035202026,
        0.41042327880859375,
        -0.2402622550725937,
        -0.16962161660194397,
        0.15104959905147552,
        0.16201582551002502,
        -0.45011135935783386,
        0.3812691271305084,
        -0.7592963576316833,
        -0.09324225783348083,
        -0.008213311433792114,
        -0.1651892215013504,
        0.5383822917938232,
        -0.817383348941803,
        -0.4460664391517639,
        0.03771982342004776,
        0.5191760063171387,
        -0.1050141304731369,
        0.1865072399377823,
        0.3356843888759613,
        -0.5313090085983276,
        -0.21807518601417542,
        -0.37777772545814514,
        -0.7667023539543152,
        0.7531819939613342,
        -0.5042266845703125,
        -0.18524359166622162,
        -0.2850121557712555,
        0.7182374000549316,
        -0.7522449493408203,
        0.24306799471378326,
        0.7824296951293945,
        0.06604982912540436,
        0.7354636192321777,
        -0.4617721736431122,
        0.5911628007888794,
        -1.21592378616333,
        -0.2345220446586609,
        -0.763983964920044,
        0.33703744411468506,
        0.3862447440624237,
        0.5149176120758057,
        0.6384323835372925,
        0.15624693036079407,
        0.03191494941711426,
        -0.5189362168312073,
        -0.9465650916099548,
        0.0886496752500534,
        0.3695822060108185,
        0.10293389856815338,
        -0.2663412094116211,
        0.3593909740447998,
        -0.2841116487979889,
        -0.27555811405181885,
        0.26834213733673096,
        -0.8885511159896851,
        0.5119574666023254,
        0.01186925545334816,
        -0.011391999199986458,
        0.04788663238286972,
        0.1763765662908554,
        0.1798231303691864,
        -0.45625758171081543,
        0.23918531835079193,
        0.10258163511753082,
        -0.13869038224220276,
        0.09228278696537018,
        0.17430484294891357,
        -0.1926250159740448,
        0.6720637679100037,
        0.38225001096725464,
        -0.79262375831604,
        -0.3102325201034546,
        -0.2771979868412018,
        0.15259599685668945,
        0.04051966220140457,
        -0.3160557746887207,
        1.1060322523117065,
        0.6682326197624207,
        -0.044269949197769165,
        -1.4163507223129272,
        -0.49538689851760864,
        0.951545238494873,
        0.3485664427280426,
        -0.3422462046146393,
        0.866228461265564,
        0.8840516805648804,
        -1.3509278297424316,
        1.8127727508544922,
        -0.14614522457122803,
        -0.06995584815740585,
        -0.24011680483818054,
        0.1637125164270401,
        0.12355221807956696,
        -0.3722253441810608,
        0.8566635847091675,
        0.5640436410903931,
        -0.30157724022865295,
        -0.32012635469436646,
        -0.899959921836853,
        -0.17906516790390015,
        -0.5038830041885376,
        -0.1884113848209381,
        -0.12908048927783966,
        0.6818090081214905,
        -0.006115451455116272,
        -0.4841000735759735,
        -0.46786436438560486,
        0.020598486065864563,
        1.0828571319580078,
        0.15705199539661407,
        1.138742208480835,
        0.4591643810272217,
        0.47369319200515747,
        -0.206710547208786,
        0.47260668873786926,
        0.4863069951534271,
        -0.15332606434822083,
        0.8380102515220642,
        -0.9240846037864685,
        -0.6742621064186096,
        -0.6673535704612732,
        0.6305667757987976,
        -0.48172706365585327,
        -0.17611275613307953,
        -0.33374911546707153,
        0.26770564913749695,
        0.5862686038017273,
        0.5270383954048157,
        0.8754870295524597,
        0.8994117379188538,
        -0.34942418336868286,
        0.13068827986717224,
        -0.3268505036830902,
        0.670506477355957,
        0.43133869767189026,
        -0.8109261393547058,
        1.0507590770721436,
        -0.03676158934831619,
        -0.6973395347595215,
        0.043079957365989685,
        0.028844207525253296,
        0.14340409636497498,
        -0.43122410774230957,
        0.3662651777267456,
        -0.054109834134578705,
        1.2327139377593994,
        -0.31192639470100403,
        0.4953823685646057,
        0.4578050971031189,
        0.33930912613868713,
        -0.3428260385990143,
        -0.030382268130779266,
        -1.8672473430633545,
        0.6914561986923218,
        0.03617548197507858,
        0.17158818244934082,
        -0.1050233319401741,
        -0.1506987363100052,
        0.6119358539581299,
        -0.22691234946250916,
        0.048678189516067505,
        -0.40909644961357117,
        -1.092897653579712,
        0.04527094215154648,
        -0.1667388677597046,
        0.19772452116012573,
        0.26454541087150574,
        0.10983800143003464,
        -0.338919997215271,
        0.1267506182193756,
        0.1677817702293396,
        0.31181231141090393,
        -0.43830910325050354,
        0.11449066549539566,
        -0.293670654296875,
        -0.29952019453048706,
        -0.09617701917886734,
        0.17264927923679352,
        -0.2530452609062195,
        -0.24924848973751068,
        -0.7327994704246521,
        0.1255463808774948,
        -0.23928764462471008,
        -1.1152113676071167,
        -0.7394375801086426,
        1.4903008937835693,
        -0.583054780960083,
        -0.428799033164978,
        -0.35335949063301086,
        -0.10007736086845398,
        1.4328759908676147,
        0.29704588651657104,
        0.05465002730488777,
        0.6127526164054871,
        -0.03220129758119583,
        -0.7138746380805969,
        0.31022435426712036,
        0.46693897247314453,
        0.04012499377131462,
        0.5614327192306519,
        -1.2495720386505127,
        -0.3568509519100189,
        0.35767316818237305,
        -0.32730358839035034,
        0.13697803020477295,
        -0.12470722198486328,
        -1.0188525915145874,
        -0.08995383232831955,
        0.11447951197624207,
        -0.13356855511665344,
        1.328801155090332,
        -0.05257156863808632,
        0.5643535852432251,
        0.7437622547149658,
        0.689378559589386,
        -1.668191909790039,
        -0.9149942398071289,
        0.6161829829216003,
        0.5755971074104309,
        -0.14697012305259705,
        0.11065128445625305,
        0.36636412143707275,
        -0.7632983326911926,
        0.2707476317882538,
        -0.2759820222854614,
        1.3095933198928833,
        0.2202356904745102,
        -0.28829285502433777,
        -0.16951292753219604,
        0.1673750877380371,
        1.0107728242874146,
        -0.5047176480293274,
        -0.3707248270511627,
        -0.3004976809024811,
        0.05176130682229996,
        -0.3014064133167267,
        0.4996143579483032,
        1.2920656204223633,
        0.29608553647994995,
        0.12130661308765411,
        -0.18920141458511353,
        0.40524929761886597,
        -0.931577742099762,
        -1.2860727310180664,
        -0.17803004384040833,
        0.8444249033927917,
        -0.12898752093315125,
        0.5788418054580688,
        -0.11242318898439407,
        -0.5262613296508789,
        0.7372770309448242,
        0.25716274976730347,
        -0.4469650685787201,
        0.14778298139572144,
        -0.7834926247596741,
        1.2852437496185303,
        -0.1347610354423523,
        -0.8824126720428467,
        -0.6571543216705322,
        0.22617167234420776,
        0.09230764210224152,
        0.038115136325359344,
        -0.3059554100036621,
        -0.15088781714439392,
        0.053118571639060974,
        -0.20364749431610107,
        0.06616196036338806,
        -0.1257936656475067,
        0.1252356469631195,
        0.3806031048297882,
        0.9736932516098022,
        -0.14993789792060852,
        0.2763887941837311,
        0.03146520256996155,
        0.15267910063266754,
        -0.3009810447692871,
        0.2868195176124573,
        -0.23884117603302002,
        -0.7151776552200317,
        -1.4684196710586548
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt outlines a process for identifying and elaborating on innovative business ideas. It focuses on extracting top business concepts from provided content and then refining the best ten by exploring adjacent possibilities. The expected output includes two sections: a list of extracted ideas and a detailed elaboration on the top ten ideas, ensuring uniqueness and differentiation.",
          "name": "Extract_business_ideas",
          "raw": "\n                workflow Extract_business_ideas v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a business idea extraction assistant. You are extremely interested in business ideas that could revolutionize or just overhaul existing or new industries.\n\nTake a deep breath and think step by step about how to achieve the best result possible as defined in the steps below. You have a lot of freedom to make this work well.\n\n## OUTPUT SECTIONS\n\n1. You extract the all the top business ideas from the content. It might be a few or it might be up to 40 in a section called EXTRACTED_IDEAS\n\n2. Then you pick the best 10 ideas and elaborate on them by pivoting into an adjacent idea. This will be ELABORATED_IDEAS. They should each be unique and have an interesting differentiator.\n\n## OUTPUT INSTRUCTIONS\n\n1. You only output Markdown.\n2. Do not give warnings or notes; only output the requested sections.\n3. You use numbered lists, not bullets.\n4. Do not repeat ideas, quotes, facts, or resources.\n5. Do not start items in the lists with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a business idea extraction assistant. You are extremely interested in business ideas that could revolutionize or just overhaul existing or new industries.\n\nTake a deep breath and think step by step about how to achieve the best result possible as defined in the steps below. You have a lot of freedom to make this work well.\n\n## OUTPUT SECTIONS\n\n1. You extract the all the top business ideas from the content. It might be a few or it might be up to 40 in a section called EXTRACTED_IDEAS\n\n2. Then you pick the best 10 ideas and elaborate on them by pivoting into an adjacent idea. This will be ELABORATED_IDEAS. They should each be unique and have an interesting differentiator.\n\n## OUTPUT INSTRUCTIONS\n\n1. You only output Markdown.\n2. Do not give warnings or notes; only output the requested sections.\n3. You use numbered lists, not bullets.\n4. Do not repeat ideas, quotes, facts, or resources.\n5. Do not start items in the lists with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.10297387093305588,
        0.4869078993797302,
        -0.4197758138179779,
        0.19553150236606598,
        -0.22662393748760223,
        0.29203784465789795,
        -1.050727367401123,
        -0.30026334524154663,
        0.027930546551942825,
        0.2395157516002655,
        -0.3824441134929657,
        0.6423230767250061,
        0.3512011766433716,
        0.23413069546222687,
        0.23145337402820587,
        0.031894750893116,
        -0.3064781725406647,
        -0.9632298946380615,
        -1.7381610870361328,
        -0.11083842068910599,
        0.17171303927898407,
        0.6873171329498291,
        0.09650622308254242,
        0.23623792827129364,
        -0.21743075549602509,
        -0.009164530783891678,
        0.20251110196113586,
        0.061167553067207336,
        -1.243355631828308,
        -1.6105496883392334,
        0.6356528401374817,
        0.09396811574697495,
        -0.08429129421710968,
        -0.49534350633621216,
        0.19515733420848846,
        -1.043272614479065,
        -0.4394685924053192,
        -0.14917625486850739,
        -0.631719708442688,
        -0.24398654699325562,
        -0.013757475651800632,
        0.35525932908058167,
        -0.07533899694681168,
        -0.42148542404174805,
        0.611435055732727,
        -0.6009029746055603,
        0.4705735146999359,
        -0.049177199602127075,
        0.7523590922355652,
        0.017171218991279602,
        -0.7062370777130127,
        -0.8808625340461731,
        0.28716978430747986,
        -0.48968496918678284,
        -0.3996833562850952,
        -0.23722147941589355,
        0.4255712032318115,
        -0.023884695023298264,
        0.25510895252227783,
        -0.31305861473083496,
        -0.3769630491733551,
        0.22206242382526398,
        -4.018442153930664,
        -0.1871727705001831,
        0.1297193020582199,
        0.24978047609329224,
        0.031487565487623215,
        -0.17482982575893402,
        0.08272736519575119,
        0.044122979044914246,
        -0.2875022292137146,
        0.08476400375366211,
        -0.5915567874908447,
        0.09764719009399414,
        -0.170232355594635,
        -0.04609913378953934,
        0.13259422779083252,
        0.2057722508907318,
        0.24069789052009583,
        -0.2140461653470993,
        0.07320860773324966,
        0.7530208826065063,
        0.3678688704967499,
        -0.31509408354759216,
        -0.7042717337608337,
        0.7974253296852112,
        -0.4205843508243561,
        0.24319368600845337,
        0.2450258880853653,
        0.3881649971008301,
        -0.14179377257823944,
        -0.5176880955696106,
        0.43518659472465515,
        -0.5129501819610596,
        -0.9247060418128967,
        0.3214680552482605,
        -0.26180732250213623,
        -0.008338779211044312,
        -0.30543139576911926,
        3.6652157306671143,
        0.619081974029541,
        0.15832415223121643,
        0.6666343808174133,
        -0.7086299061775208,
        0.5710597634315491,
        -0.213811993598938,
        -0.07390590012073517,
        -0.4822465777397156,
        0.19945178925991058,
        -0.19258534908294678,
        0.38941237330436707,
        -0.7068970799446106,
        -0.1347770094871521,
        -0.08833831548690796,
        -0.10338608920574188,
        0.6441165208816528,
        -0.5911791324615479,
        0.2524225115776062,
        -0.13691377639770508,
        1.0335743427276611,
        -0.11588233709335327,
        0.10427770018577576,
        -0.4673789143562317,
        -0.7093026638031006,
        0.2200067788362503,
        -0.2705627679824829,
        -0.20989371836185455,
        0.6582275032997131,
        0.25009071826934814,
        -0.016409512609243393,
        0.23693911731243134,
        -0.37601086497306824,
        -0.6492692828178406,
        -0.12918445467948914,
        0.7433298826217651,
        -0.4131261110305786,
        0.640531063079834,
        -0.7945046424865723,
        0.19290423393249512,
        -0.631011426448822,
        0.08313501626253128,
        -1.3816694021224976,
        0.9882781505584717,
        0.08841421455144882,
        0.6043113470077515,
        0.37056776881217957,
        -0.29330945014953613,
        0.4195389747619629,
        -0.19035734236240387,
        -0.5887340307235718,
        -0.0436122827231884,
        0.25205478072166443,
        -0.05159473419189453,
        0.537163257598877,
        0.8623833060264587,
        0.23526594042778015,
        -0.03999510034918785,
        -0.13641072809696198,
        -0.905102014541626,
        0.6524747014045715,
        0.31110647320747375,
        -0.19956089556217194,
        0.005429252982139587,
        0.10465005785226822,
        0.4086209535598755,
        -0.31019604206085205,
        0.19162298738956451,
        0.213189959526062,
        0.6335284113883972,
        0.08944427967071533,
        0.35059893131256104,
        -0.18441247940063477,
        0.8542323708534241,
        0.6324803829193115,
        -0.4902194142341614,
        0.09686450660228729,
        -0.5490217208862305,
        0.14499494433403015,
        0.2938106656074524,
        -0.4902099072933197,
        0.39212566614151,
        0.5627701878547668,
        -0.2905929684638977,
        -0.8079583644866943,
        -0.5106658935546875,
        0.5558803081512451,
        0.503995954990387,
        0.386335551738739,
        0.5395311117172241,
        0.9897350072860718,
        -0.9227757453918457,
        1.9151668548583984,
        -0.5795561671257019,
        -0.057202860713005066,
        0.24300166964530945,
        -0.005427280440926552,
        0.5355450510978699,
        0.19785286486148834,
        0.6793835163116455,
        0.2413407415151596,
        -0.9187992215156555,
        -0.3882429897785187,
        -0.6911787390708923,
        -0.3879077434539795,
        -0.15354302525520325,
        -0.3595218062400818,
        0.19252125918865204,
        0.08418932557106018,
        0.22470253705978394,
        -0.46540024876594543,
        -0.21111415326595306,
        0.5184201002120972,
        1.1443309783935547,
        0.34002333879470825,
        0.49552682042121887,
        0.25699537992477417,
        0.2055167406797409,
        -0.3092232644557953,
        -0.2517372965812683,
        0.7032085657119751,
        0.01951410621404648,
        0.10565122961997986,
        -0.8349897265434265,
        -0.38286155462265015,
        -0.793754518032074,
        0.3572242856025696,
        -0.025877591222524643,
        0.28741395473480225,
        -0.4616077244281769,
        0.025015223771333694,
        0.046121761202812195,
        0.5829384922981262,
        0.20643046498298645,
        1.4240394830703735,
        0.022974804043769836,
        0.23395216464996338,
        0.012689046561717987,
        0.1017238199710846,
        0.19569846987724304,
        -0.7827410697937012,
        0.4253236651420593,
        0.45018765330314636,
        -0.09403157979249954,
        -0.06184108182787895,
        -0.3915144205093384,
        -0.8227657079696655,
        -0.1738380789756775,
        -0.2532895803451538,
        -0.1149519756436348,
        1.7499539852142334,
        0.2628730237483978,
        -0.5244570970535278,
        0.7474369406700134,
        0.3947262167930603,
        -0.13117095828056335,
        -0.13132384419441223,
        -2.2250735759735107,
        -0.23454436659812927,
        0.11385691910982132,
        0.46703580021858215,
        -0.025325581431388855,
        0.13631397485733032,
        0.6472032070159912,
        0.2331637442111969,
        0.2167680412530899,
        0.10527034848928452,
        -0.37974628806114197,
        -0.13886341452598572,
        -0.5061785578727722,
        -0.44219058752059937,
        -0.5272936224937439,
        0.35683882236480713,
        -0.3119514584541321,
        0.09433847665786743,
        -0.23584194481372833,
        0.1211102306842804,
        0.3488931953907013,
        -0.07888912409543991,
        0.006132699549198151,
        -0.40324831008911133,
        0.38969311118125916,
        0.028544535860419273,
        0.023294687271118164,
        0.24629873037338257,
        -0.32429394125938416,
        -0.07651908695697784,
        -0.5666435956954956,
        -0.11725825816392899,
        -0.40863364934921265,
        0.4234410226345062,
        -0.3103407323360443,
        -0.42948979139328003,
        -0.6700954437255859,
        -0.004036068916320801,
        1.4893985986709595,
        -0.15457585453987122,
        0.5355938673019409,
        0.5807690620422363,
        -0.0406249538064003,
        -0.03573168069124222,
        0.09644845873117447,
        0.2620595693588257,
        -0.20583690702915192,
        -0.09492440521717072,
        -1.0537440776824951,
        -0.33593127131462097,
        0.5787891149520874,
        0.10680924355983734,
        -0.26367416977882385,
        0.35095834732055664,
        -0.7556448578834534,
        -0.10821565985679626,
        -0.10126562416553497,
        0.4032082259654999,
        0.48586586117744446,
        -0.4541516602039337,
        0.13574637472629547,
        0.8835853338241577,
        -0.08294719457626343,
        -1.7235132455825806,
        -0.5484063029289246,
        0.6771402359008789,
        0.29403793811798096,
        -0.47633248567581177,
        -0.3985517621040344,
        0.39088404178619385,
        0.026500597596168518,
        0.4024856984615326,
        0.04260002076625824,
        1.4254847764968872,
        0.47944799065589905,
        0.2897694408893585,
        0.3896382749080658,
        0.08163019269704819,
        0.870069146156311,
        -0.069173164665699,
        0.45157694816589355,
        -0.046217143535614014,
        -0.04466504231095314,
        0.2554430365562439,
        0.3697003126144409,
        1.4269018173217773,
        -0.30547353625297546,
        0.3058798015117645,
        0.01427985355257988,
        0.3657829463481903,
        -0.5765416622161865,
        -1.2916173934936523,
        0.7061177492141724,
        0.13158421218395233,
        -0.3309115767478943,
        1.2023473978042603,
        0.2979188561439514,
        -0.2846314311027527,
        0.68488609790802,
        0.5636678338050842,
        -0.41076570749282837,
        0.15151965618133545,
        -0.44776615500450134,
        1.3416906595230103,
        -0.3725017309188843,
        -0.5991020202636719,
        -0.7366322875022888,
        0.07080547511577606,
        -0.49456509947776794,
        0.05022313445806503,
        -0.08456963300704956,
        -0.0276731476187706,
        -0.17735880613327026,
        0.37080511450767517,
        0.033033017069101334,
        -0.36170336604118347,
        0.3283287286758423,
        0.08141490817070007,
        0.8327645659446716,
        -0.04021833837032318,
        -0.1585329920053482,
        0.28880220651626587,
        0.5284522771835327,
        -0.5739060640335083,
        0.5696852207183838,
        -0.09852981567382812,
        -0.7808077931404114,
        -0.07313942909240723
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Extract_controversial_ideas",
          "raw": "\n                workflow Extract_controversial_ideas v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are super-intelligent AI system that extracts the most controversial statements out of inputs.\n\n# GOAL \n\n- Create a full list of controversial statements from the input.\n\n# OUTPUT\n\n- In a section called Controversial Ideas, output a bulleted list of controversial ideas from the input, captured in 15-words each.\n\n- In a section called Supporting Quotes, output a bulleted list of controversial quotes from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure you get all of the controversial ideas from the input.\n\n- Output the output as Markdown, but without the use of any asterisks.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are super-intelligent AI system that extracts the most controversial statements out of inputs.\n\n# GOAL \n\n- Create a full list of controversial statements from the input.\n\n# OUTPUT\n\n- In a section called Controversial Ideas, output a bulleted list of controversial ideas from the input, captured in 15-words each.\n\n- In a section called Supporting Quotes, output a bulleted list of controversial quotes from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Ensure you get all of the controversial ideas from the input.\n\n- Output the output as Markdown, but without the use of any asterisks.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.006231749430298805,
        0.5417261719703674,
        -0.3306792080402374,
        -0.39262863993644714,
        -0.07995288074016571,
        0.2028580904006958,
        -0.4772694408893585,
        -0.5614467859268188,
        -0.5547197461128235,
        0.46806031465530396,
        -0.14449447393417358,
        0.4967096447944641,
        0.36101844906806946,
        -0.05849441885948181,
        0.17229337990283966,
        0.07304594665765762,
        -0.6361701488494873,
        -0.8259963989257812,
        -1.3050940036773682,
        0.011831916868686676,
        0.3357985317707062,
        -0.06962934136390686,
        0.5143120288848877,
        0.7282772660255432,
        0.09821437299251556,
        -0.0638897567987442,
        -0.6441605687141418,
        0.12489287555217743,
        -0.06609494984149933,
        -1.1685190200805664,
        0.9513444304466248,
        0.3215888440608978,
        -0.3451305031776428,
        -0.5312692523002625,
        -0.1487886905670166,
        -0.25275662541389465,
        0.40619444847106934,
        0.5015366077423096,
        -0.18202289938926697,
        -0.7829768657684326,
        -0.27902793884277344,
        0.8707619905471802,
        -0.3876512050628662,
        -0.1828753650188446,
        -0.11213669925928116,
        0.09966985136270523,
        0.21786130964756012,
        0.08303266763687134,
        0.9597475528717041,
        0.35927116870880127,
        0.05784003809094429,
        -0.07518382370471954,
        -0.501103937625885,
        -0.4392123818397522,
        -1.0586496591567993,
        -0.5226949453353882,
        -0.08711428940296173,
        -0.6526545286178589,
        0.43689578771591187,
        0.08011365681886673,
        0.3629834055900574,
        0.005577497184276581,
        -3.2846028804779053,
        -0.6999616622924805,
        0.44520264863967896,
        0.11909426748752594,
        -0.2264246642589569,
        -0.149129718542099,
        0.07974913716316223,
        -0.11052774637937546,
        -0.3324931561946869,
        -0.019072236493229866,
        -0.4203607141971588,
        0.1750572919845581,
        0.2972159683704376,
        0.07806126028299332,
        -0.0013769064098596573,
        0.4176536500453949,
        0.6548726558685303,
        -0.12760311365127563,
        0.04054894298315048,
        0.6215026378631592,
        0.02101977728307247,
        0.09948080778121948,
        0.1329847127199173,
        0.6800183057785034,
        -0.7648435235023499,
        -0.302970290184021,
        0.2190808355808258,
        0.5444036722183228,
        -0.2339845597743988,
        -0.8465695381164551,
        0.3823286294937134,
        0.030047297477722168,
        -1.0167659521102905,
        0.2888738512992859,
        -0.2801129221916199,
        -0.17432567477226257,
        0.662638247013092,
        3.8136894702911377,
        1.0544843673706055,
        0.1465943604707718,
        0.4744924306869507,
        -1.032884120941162,
        0.633367121219635,
        -0.9775295257568359,
        0.04139060154557228,
        -0.03199257701635361,
        0.314418762922287,
        0.4861645996570587,
        0.08196483552455902,
        -0.8740381002426147,
        -0.26616889238357544,
        -0.0726843774318695,
        -0.1568489670753479,
        0.7947911024093628,
        -0.47224098443984985,
        0.047795459628105164,
        0.39841392636299133,
        -0.06258483231067657,
        -0.21730747818946838,
        0.7053132653236389,
        0.18234960734844208,
        -0.17356809973716736,
        0.2566626965999603,
        0.4119965136051178,
        0.27461346983909607,
        0.7781376838684082,
        0.18050146102905273,
        -0.23650886118412018,
        0.2046269178390503,
        0.7054481506347656,
        -0.9641342759132385,
        0.1290103644132614,
        -0.10558465123176575,
        -0.6315941214561462,
        0.515245795249939,
        -0.8086468577384949,
        0.2597813308238983,
        -0.7823663353919983,
        0.2693483233451843,
        -1.008217692375183,
        0.4761282801628113,
        -0.03131701797246933,
        1.1284937858581543,
        -0.041250020265579224,
        -0.4233296513557434,
        0.34464535117149353,
        -0.2955201268196106,
        0.0648891031742096,
        -0.25900161266326904,
        0.715801477432251,
        -0.06824343651533127,
        -0.16379979252815247,
        0.2111889123916626,
        -0.08775392919778824,
        -0.3035989999771118,
        -0.15290912985801697,
        -0.6930656433105469,
        0.26291728019714355,
        0.31337517499923706,
        0.06374485790729523,
        0.23418766260147095,
        -0.2369847148656845,
        0.8736655116081238,
        0.4047347605228424,
        0.6865938901901245,
        0.17673853039741516,
        0.09457796066999435,
        0.7231919169425964,
        0.21746569871902466,
        -0.42603346705436707,
        0.554460346698761,
        0.4424426257610321,
        0.1607852727174759,
        0.025878584012389183,
        0.38063719868659973,
        -0.30484095215797424,
        0.4302104711532593,
        -0.4598517417907715,
        0.6223104596138,
        0.21903423964977264,
        0.26892879605293274,
        -0.5704244375228882,
        0.3503459692001343,
        0.402617871761322,
        -0.12853622436523438,
        -0.22936797142028809,
        0.662290096282959,
        0.14457030594348907,
        -0.9723981022834778,
        1.877995252609253,
        -0.17519423365592957,
        -0.01122923195362091,
        0.24025021493434906,
        0.028462927788496017,
        0.052208706736564636,
        0.31193065643310547,
        0.19408245384693146,
        0.27094709873199463,
        -0.16508111357688904,
        0.2964293956756592,
        -0.62901771068573,
        0.08341483771800995,
        -0.7800350785255432,
        -0.1627061814069748,
        -0.21627216041088104,
        0.39695239067077637,
        0.19591845571994781,
        -0.6467388868331909,
        -0.6032953858375549,
        0.1886511743068695,
        1.2292819023132324,
        0.42882296442985535,
        0.586269736289978,
        -0.40971916913986206,
        0.14594489336013794,
        -0.020959731191396713,
        -0.6215770244598389,
        0.21793091297149658,
        0.2683759033679962,
        -0.016607172787189484,
        -0.9464157819747925,
        -0.7381675839424133,
        -1.0036778450012207,
        0.18434518575668335,
        -0.18492737412452698,
        0.06767423450946808,
        -0.5596550107002258,
        0.5356971621513367,
        0.9122893214225769,
        0.903097927570343,
        -0.042664073407649994,
        1.0007036924362183,
        0.9586326479911804,
        0.4546908140182495,
        0.17337554693222046,
        -0.008132360875606537,
        0.9846415519714355,
        -0.4808182120323181,
        0.23706522583961487,
        -0.7642941474914551,
        -0.36540085077285767,
        0.4504402279853821,
        -0.25991278886795044,
        0.8759117126464844,
        -0.09582136571407318,
        0.21577703952789307,
        0.048958756029605865,
        1.079209566116333,
        0.24476221203804016,
        -0.4462989568710327,
        0.7333499193191528,
        0.48407286405563354,
        -0.35836055874824524,
        -0.15324774384498596,
        -1.7750061750411987,
        0.5146156549453735,
        -0.17241384088993073,
        0.11260372400283813,
        0.22968748211860657,
        -0.47306469082832336,
        0.34991082549095154,
        -0.11851280182600021,
        0.10076049715280533,
        -0.2749992609024048,
        -0.9610307812690735,
        -0.6125527620315552,
        0.0811830684542656,
        -0.6512807011604309,
        -0.1017829030752182,
        0.3291284441947937,
        -0.6786409616470337,
        -0.09441942721605301,
        -0.2723161578178406,
        -0.03779566287994385,
        -0.045099176466464996,
        -0.21031707525253296,
        -0.558281660079956,
        -0.3428046703338623,
        0.1391296535730362,
        0.32449647784233093,
        0.6361309289932251,
        0.328183650970459,
        0.027872234582901,
        -0.3922494947910309,
        -0.19650499522686005,
        -0.48753827810287476,
        -0.5786713361740112,
        1.272625207901001,
        -0.4947807192802429,
        -0.24835318326950073,
        -0.5588697195053101,
        -0.19470372796058655,
        1.7740384340286255,
        0.2377369999885559,
        -0.11283856630325317,
        0.7494111657142639,
        0.7187598943710327,
        0.4101213812828064,
        -0.7201429009437561,
        0.5046250820159912,
        -0.15946181118488312,
        0.5916512608528137,
        -1.3289031982421875,
        -0.09375529736280441,
        -0.18262751400470734,
        0.10585826635360718,
        -0.5263151526451111,
        -0.00006337836384773254,
        -0.7287025451660156,
        -0.03868327662348747,
        -0.19789735972881317,
        -0.5153777003288269,
        0.29266440868377686,
        -0.6456542015075684,
        0.3680811822414398,
        -0.32270416617393494,
        0.17687219381332397,
        -2.1376264095306396,
        -0.10325069725513458,
        0.16669601202011108,
        0.4626818001270294,
        -0.639855146408081,
        -0.1952495276927948,
        -0.26505231857299805,
        0.0494881197810173,
        0.3847152292728424,
        -0.06205376237630844,
        1.5303010940551758,
        0.26567554473876953,
        -0.29209065437316895,
        -0.4156113266944885,
        0.2029411792755127,
        0.49843090772628784,
        -0.8646057844161987,
        -0.09594275057315826,
        -0.34907639026641846,
        -0.7290357351303101,
        0.113276407122612,
        0.24622894823551178,
        1.1505000591278076,
        0.4518909454345703,
        0.2933639585971832,
        -0.16604596376419067,
        0.4221441149711609,
        -0.5956282615661621,
        -0.785451352596283,
        0.38140401244163513,
        -0.09092821925878525,
        -0.9060020446777344,
        0.5020800828933716,
        0.10505859553813934,
        -0.0606980100274086,
        0.6018138527870178,
        0.7997294068336487,
        -0.6019972562789917,
        -0.07542543858289719,
        -0.6293524503707886,
        1.486242413520813,
        0.308662474155426,
        -0.2200983464717865,
        0.08138527721166611,
        0.7442790865898132,
        -0.17855533957481384,
        -0.044003650546073914,
        0.055021774023771286,
        -0.3130894601345062,
        0.18157778680324554,
        0.1418013572692871,
        -0.3022443950176239,
        -0.21344324946403503,
        0.6241046190261841,
        0.16807226836681366,
        0.40920713543891907,
        0.2841596305370331,
        -0.5626375079154968,
        -0.07095754146575928,
        0.38646507263183594,
        -0.043418534100055695,
        0.23179227113723755,
        -0.35543951392173767,
        -1.0132331848144531,
        -0.257291316986084
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Identifies and lists extraordinary claims from conversations, focusing on those rejected by the scientific community or based on misinformation. The process involves deep analysis to pinpoint statements that defy accepted scientific truths, such as denying evolution or the moon landing. The output is a detailed list of quotes, ranging from 50 to 100, showcasing these claims.",
          "name": "Extract_extraordinary_claims",
          "raw": "\n                workflow Extract_extraordinary_claims v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert at extracting extraordinary claims from conversations. This means claims that:\n\n- Are already accepted as false by the scientific community.\n- Are not easily verifiable.\n- Are generally understood to be false by the consensus of experts.\n\n# STEPS\n\n- Fully understand what's being said, and think about the content for 419 virtual minutes.\n\n- Look for statements that indicate this person is a conspiracy theorist, or is engaging in misinformation, or is just an idiot.\n\n- Look for statements that indicate this person doesn't believe in commonly accepted scientific truth, like evolution or climate change or the moon landing. Include those in your list.\n\n- Examples include things like denying evolution, claiming the moon landing was faked, or saying that the earth is flat.\n\n# OUTPUT\n\n- Output a full list of the claims that were made, using actual quotes. List them in a bulleted list.\n\n- Output at least 50 of these quotes, but no more than 100.\n\n- Put an empty line between each quote.\n\nEND EXAMPLES\n\n- Ensure you extract ALL such quotes.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert at extracting extraordinary claims from conversations. This means claims that:\n\n- Are already accepted as false by the scientific community.\n- Are not easily verifiable.\n- Are generally understood to be false by the consensus of experts.\n\n# STEPS\n\n- Fully understand what's being said, and think about the content for 419 virtual minutes.\n\n- Look for statements that indicate this person is a conspiracy theorist, or is engaging in misinformation, or is just an idiot.\n\n- Look for statements that indicate this person doesn't believe in commonly accepted scientific truth, like evolution or climate change or the moon landing. Include those in your list.\n\n- Examples include things like denying evolution, claiming the moon landing was faked, or saying that the earth is flat.\n\n# OUTPUT\n\n- Output a full list of the claims that were made, using actual quotes. List them in a bulleted list.\n\n- Output at least 50 of these quotes, but no more than 100.\n\n- Put an empty line between each quote.\n\nEND EXAMPLES\n\n- Ensure you extract ALL such quotes.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.1672120988368988,
        0.22737827897071838,
        -0.06699937582015991,
        0.3764491379261017,
        0.4109419286251068,
        0.28954681754112244,
        -0.9654608964920044,
        -0.41648104786872864,
        0.2992287278175354,
        0.12518098950386047,
        -0.08227094262838364,
        1.0941377878189087,
        0.2790620028972626,
        0.1009647473692894,
        0.1928987056016922,
        0.2755853831768036,
        0.24145850539207458,
        -0.5909650921821594,
        -1.671746015548706,
        -0.6695826053619385,
        -0.572821319103241,
        0.8565046787261963,
        0.3015018403530121,
        0.1934714913368225,
        0.25007277727127075,
        0.16257157921791077,
        -0.5915319919586182,
        -0.05513157695531845,
        -0.16338497400283813,
        -1.2200236320495605,
        0.5535261631011963,
        0.029295820742845535,
        -0.24562373757362366,
        -0.2292923927307129,
        0.05424319580197334,
        -0.8025590777397156,
        -0.14509719610214233,
        -0.10484243929386139,
        -0.24111445248126984,
        -0.3202535808086395,
        0.2745056450366974,
        0.14351709187030792,
        -0.26489129662513733,
        -0.3515537977218628,
        0.4152641296386719,
        -0.2677818834781647,
        0.12053751200437546,
        -0.10769563913345337,
        0.3157108724117279,
        -0.14658375084400177,
        0.037626009434461594,
        -0.5246545672416687,
        0.031252626329660416,
        -0.34416940808296204,
        -0.8136548399925232,
        -0.2638772130012512,
        0.022153198719024658,
        -0.31325194239616394,
        0.4181404113769531,
        0.06920704245567322,
        0.037588249891996384,
        0.10411673039197922,
        -3.0052199363708496,
        0.16689810156822205,
        -0.12468557059764862,
        0.08018244057893753,
        0.20760877430438995,
        -0.6368958353996277,
        0.036442145705223083,
        0.007357954978942871,
        0.18373095989227295,
        0.32828256487846375,
        -0.28149479627609253,
        -0.23140785098075867,
        -0.4356030225753784,
        -0.3263181447982788,
        0.07698386907577515,
        0.028112951666116714,
        0.24403302371501923,
        -0.08444049954414368,
        0.2865090072154999,
        0.2668343782424927,
        0.11023946106433868,
        0.584734320640564,
        -0.6822476387023926,
        0.47946643829345703,
        -0.868662416934967,
        -0.11700542271137238,
        0.07076658308506012,
        0.05863732099533081,
        -0.2662149667739868,
        -0.9494272470474243,
        0.533248782157898,
        -0.3059268891811371,
        -0.4610249400138855,
        0.11527842283248901,
        -0.18937498331069946,
        -0.27426454424858093,
        0.03326769173145294,
        3.6747138500213623,
        0.8989854454994202,
        0.6653731465339661,
        0.4857979714870453,
        -0.5679996013641357,
        0.47229406237602234,
        -0.4798457622528076,
        -0.32149070501327515,
        -0.2187681496143341,
        0.09283433854579926,
        -0.2059885710477829,
        0.4375838339328766,
        -0.8791341781616211,
        -0.2834264039993286,
        -0.6535563468933105,
        -0.07211413234472275,
        1.106274127960205,
        -0.6853929758071899,
        -0.2313232719898224,
        0.1291697770357132,
        0.8135557770729065,
        -0.63834148645401,
        -0.26037049293518066,
        -0.0879606381058693,
        -0.5857079029083252,
        -0.050660327076911926,
        -0.16642683744430542,
        -0.8143964409828186,
        0.7936230897903442,
        0.13582877814769745,
        0.6682252287864685,
        0.12430887669324875,
        0.03950618579983711,
        -0.6915656328201294,
        -0.07225491106510162,
        0.3238714933395386,
        -0.2849869430065155,
        0.588005781173706,
        -1.1194313764572144,
        0.3682413399219513,
        -0.8433387279510498,
        0.04691331088542938,
        -0.7693741321563721,
        0.4810643196105957,
        0.1303156018257141,
        0.45285671949386597,
        0.8521716594696045,
        0.38913267850875854,
        0.18167543411254883,
        -0.5285951495170593,
        -0.2867840826511383,
        -0.22564898431301117,
        0.215487539768219,
        -0.40950897336006165,
        0.43577417731285095,
        0.44177788496017456,
        0.5287650227546692,
        -0.9789336919784546,
        0.15976014733314514,
        -1.401290774345398,
        0.7914942502975464,
        0.29059991240501404,
        0.027232037857174873,
        0.5909947156906128,
        -0.2727065086364746,
        0.3082481622695923,
        -0.5584736466407776,
        0.3197072148323059,
        0.09315722435712814,
        0.2450035810470581,
        0.5610983371734619,
        -0.008723482489585876,
        -0.13273489475250244,
        1.069496989250183,
        0.6774027347564697,
        -0.5698291659355164,
        0.0793057382106781,
        -0.36369168758392334,
        0.4383626878261566,
        0.16653841733932495,
        -0.31608718633651733,
        0.6347174048423767,
        0.12709559500217438,
        -0.1563069224357605,
        -0.5842387676239014,
        -0.7985122203826904,
        0.5967348217964172,
        0.3692891597747803,
        0.37589260935783386,
        1.0732245445251465,
        0.7463086843490601,
        -0.5006038546562195,
        2.126983165740967,
        -0.15143701434135437,
        -0.010263018310070038,
        -0.07164185494184494,
        0.12129537761211395,
        -0.5070093870162964,
        0.2782577574253082,
        0.9948241710662842,
        0.3704705238342285,
        -0.7889522314071655,
        -0.004858754575252533,
        -0.14955896139144897,
        -0.4544329345226288,
        -0.5873183608055115,
        -0.1992814540863037,
        -0.0235789492726326,
        0.5330965518951416,
        0.11168143153190613,
        -1.1511714458465576,
        -0.38698482513427734,
        0.661361575126648,
        1.3252663612365723,
        0.5061069130897522,
        0.6001876592636108,
        0.3257676362991333,
        0.4710392951965332,
        -0.33512887358665466,
        0.21217606961727142,
        0.5749965906143188,
        -0.4905065894126892,
        0.26282036304473877,
        -1.210107684135437,
        -0.8083293437957764,
        -0.3893091380596161,
        0.9050831198692322,
        -0.28152042627334595,
        0.09806062281131744,
        -0.3571769595146179,
        0.020500032231211662,
        0.13299934566020966,
        0.8135387301445007,
        0.738447904586792,
        1.5239181518554688,
        0.15683257579803467,
        0.2697892189025879,
        -0.060417674481868744,
        0.26813727617263794,
        0.008289448916912079,
        -0.23600462079048157,
        0.9434441328048706,
        0.07265248149633408,
        -0.12606337666511536,
        0.10962752997875214,
        -0.16462679207324982,
        -0.08173365145921707,
        -0.5768308639526367,
        0.05134783685207367,
        0.1907907873392105,
        1.459143877029419,
        0.5611676573753357,
        -0.3239397406578064,
        0.35627004504203796,
        0.26573580503463745,
        0.14213114976882935,
        -0.1283142864704132,
        -1.703417420387268,
        0.281629353761673,
        -0.3198889493942261,
        0.15297439694404602,
        0.0170443058013916,
        0.2104966640472412,
        1.2402745485305786,
        -0.28436022996902466,
        0.3935551047325134,
        -0.013284649699926376,
        -0.47743913531303406,
        0.07578976452350616,
        -0.4245096445083618,
        0.04030952602624893,
        0.38183459639549255,
        -0.13198958337306976,
        -0.2694723606109619,
        -0.32634782791137695,
        0.11983227729797363,
        0.5198175311088562,
        -0.17126961052417755,
        -0.17815765738487244,
        -0.19020435214042664,
        -0.44713839888572693,
        0.43499889969825745,
        0.013081690296530724,
        -0.6640525460243225,
        -0.012768946588039398,
        -0.7886655330657959,
        0.10063433647155762,
        -0.2514311373233795,
        -1.0637727975845337,
        -0.3277580440044403,
        1.0837703943252563,
        -0.4147513210773468,
        -0.5607725381851196,
        -0.25158900022506714,
        -0.2972622811794281,
        1.8139418363571167,
        -0.03947928547859192,
        -0.03733091428875923,
        0.46762698888778687,
        0.12421417236328125,
        -0.5072659254074097,
        0.34801146388053894,
        0.4521924555301666,
        -0.297311395406723,
        -0.130197674036026,
        -0.9013528227806091,
        -0.43323472142219543,
        0.5446690320968628,
        0.180763378739357,
        0.03936037793755531,
        -0.11144523322582245,
        -1.305488109588623,
        -0.18986931443214417,
        0.3294655680656433,
        0.09003446251153946,
        0.7139862179756165,
        -0.3928438723087311,
        0.20630338788032532,
        0.5641825199127197,
        -0.16947756707668304,
        -1.5838509798049927,
        -0.771007239818573,
        0.7382321953773499,
        0.8666570782661438,
        -0.38708069920539856,
        -0.5411682724952698,
        0.5548932552337646,
        0.1410469114780426,
        0.013670006766915321,
        -0.43092072010040283,
        1.4411529302597046,
        0.5515453815460205,
        0.002706023631617427,
        -0.055301129817962646,
        -0.3195483982563019,
        0.971409797668457,
        -0.3194715082645416,
        -0.1376979798078537,
        -0.3967529833316803,
        -0.2024020552635193,
        0.019452128559350967,
        0.5089954137802124,
        1.6138463020324707,
        0.25796011090278625,
        -0.16399848461151123,
        0.2579655349254608,
        0.26178574562072754,
        -0.4286115765571594,
        -1.2106503248214722,
        0.30540764331817627,
        0.34196192026138306,
        -0.42890071868896484,
        0.932705283164978,
        0.2126597762107849,
        -0.20728731155395508,
        1.0065916776657104,
        0.3519682288169861,
        -0.09536159038543701,
        0.025854118168354034,
        -0.6350262761116028,
        1.294867992401123,
        -0.5836880207061768,
        -0.4918247163295746,
        -0.6233137845993042,
        0.2535596489906311,
        -0.490579754114151,
        0.08445852994918823,
        0.28998538851737976,
        -0.06799580901861191,
        0.05603496730327606,
        0.09608341008424759,
        -0.15024426579475403,
        -0.6388476490974426,
        0.8665915727615356,
        0.058332931250333786,
        0.603699266910553,
        -0.26932692527770996,
        -0.1308378279209137,
        0.2225620597600937,
        0.13892129063606262,
        -0.4677921533584595,
        0.6491025686264038,
        -0.7974856495857239,
        -0.704895555973053,
        -1.2010303735733032
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and condenses insightful ideas from text into 15-word bullet points focusing on life's purpose and human progress. This process emphasizes capturing unique insights on specified themes. The output consists of a list of concise, thought-provoking ideas.",
          "name": "Extract_ideas",
          "raw": "\n                workflow Extract_ideas v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nYou create 15 word bullet points that capture the most important ideas from the input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS: using 15 word bullets. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Extract at least 20 IDEAS from the content.\n\n- Only extract ideas, not recommendations. These should be phrased as ideas.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nYou create 15 word bullet points that capture the most important ideas from the input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS: using 15 word bullets. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Extract at least 20 IDEAS from the content.\n\n- Only extract ideas, not recommendations. These should be phrased as ideas.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.11955106258392334,
        0.45429766178131104,
        0.01456383615732193,
        0.09266397356987,
        0.23926372826099396,
        -0.09672088176012039,
        -0.4947172999382019,
        0.005554065108299255,
        0.4160345792770386,
        0.11417306214570999,
        -0.027444716542959213,
        0.9103196859359741,
        0.276911199092865,
        0.16160771250724792,
        0.07681724429130554,
        -0.008013598620891571,
        0.3331593871116638,
        -0.6039971113204956,
        -1.6074011325836182,
        -0.7344143986701965,
        -0.39313003420829773,
        0.9858696460723877,
        0.2870393395423889,
        0.04914465546607971,
        0.4657408595085144,
        -0.1300666481256485,
        -0.6376024484634399,
        -0.08117616176605225,
        -0.41408467292785645,
        -1.086885929107666,
        0.7487909197807312,
        -0.17507311701774597,
        -0.23332208395004272,
        -0.03568486124277115,
        -0.19471168518066406,
        -0.8913922309875488,
        0.07449246197938919,
        -0.30195528268814087,
        -0.289755254983902,
        -0.14720995724201202,
        -0.048891425132751465,
        0.40247470140457153,
        -0.47864270210266113,
        -0.3139648735523224,
        0.5355215668678284,
        -0.30291134119033813,
        0.12777365744113922,
        0.10584580898284912,
        0.6200545430183411,
        -0.1731499880552292,
        0.21535618603229523,
        -0.3868831992149353,
        -0.1728793978691101,
        -0.15214981138706207,
        -0.890501856803894,
        -0.7928698062896729,
        0.057885926216840744,
        -0.14953626692295074,
        0.3179331421852112,
        0.028108682483434677,
        -0.022725678980350494,
        0.004097849130630493,
        -3.048374652862549,
        -0.22404128313064575,
        -0.02564513124525547,
        -0.062349773943424225,
        0.17863987386226654,
        -0.5528354048728943,
        -0.059265848249197006,
        -0.036892734467983246,
        0.290772944688797,
        0.09090512245893478,
        -0.25534144043922424,
        -0.09823610633611679,
        -0.14148640632629395,
        0.009223327040672302,
        -0.038888249546289444,
        0.09614710509777069,
        0.3383527398109436,
        0.06189245358109474,
        0.21397610008716583,
        0.49590444564819336,
        0.0444880872964859,
        0.4647905230522156,
        -0.5986937284469604,
        0.8001298904418945,
        -0.9558281302452087,
        -0.25516030192375183,
        0.10718673467636108,
        0.2041294127702713,
        -0.24312430620193481,
        -0.8690880537033081,
        0.1832060068845749,
        -0.2535809874534607,
        -0.2711229920387268,
        0.2473789006471634,
        -0.03947224095463753,
        0.09588827192783356,
        -0.002200767397880554,
        3.6587114334106445,
        1.0424178838729858,
        0.6740669012069702,
        0.2552255094051361,
        -0.8391473889350891,
        0.31047770380973816,
        -0.45518210530281067,
        -0.5756192803382874,
        -0.3207428455352783,
        0.33600273728370667,
        -0.053331442177295685,
        0.3270115554332733,
        -1.050576090812683,
        -0.3262733817100525,
        -0.6763847470283508,
        -0.22314214706420898,
        1.265131950378418,
        -0.658715546131134,
        0.04810124635696411,
        0.2685191333293915,
        0.2641833424568176,
        -0.6074584722518921,
        0.000414043664932251,
        0.1080387607216835,
        -0.6386191844940186,
        -0.1466509997844696,
        -0.15173029899597168,
        -0.31167033314704895,
        0.6981146335601807,
        0.2971608638763428,
        0.4334801137447357,
        0.08862121403217316,
        0.3239690065383911,
        -0.7203102707862854,
        -0.039704322814941406,
        0.20988339185714722,
        -0.488145649433136,
        0.5364204049110413,
        -1.0043106079101562,
        0.34715092182159424,
        -0.8751856088638306,
        -0.25530922412872314,
        -0.7457146644592285,
        0.36388808488845825,
        0.009007751941680908,
        0.8713884353637695,
        0.5567300319671631,
        0.3369143009185791,
        0.06670453399419785,
        -0.4988391101360321,
        -0.01948399841785431,
        0.04618384689092636,
        0.43393927812576294,
        -0.2143440842628479,
        0.6223037242889404,
        0.29833531379699707,
        0.5905297994613647,
        -1.1123801469802856,
        0.3923772871494293,
        -1.3531221151351929,
        0.39068540930747986,
        0.1773294359445572,
        -0.10393494367599487,
        0.3662182092666626,
        -0.8152948021888733,
        0.3977794647216797,
        -0.29271769523620605,
        0.5012688040733337,
        -0.08837347477674484,
        0.19715222716331482,
        0.8368457555770874,
        0.03216121345758438,
        -0.30911940336227417,
        1.0261465311050415,
        0.9765139222145081,
        -0.5134381055831909,
        0.0919020026922226,
        -0.021131500601768494,
        -0.13669243454933167,
        0.16659778356552124,
        -0.2460658848285675,
        0.95185786485672,
        0.5782502293586731,
        -0.092291459441185,
        -0.5190441608428955,
        -0.6769271492958069,
        0.5367388129234314,
        0.10458590090274811,
        0.3520336449146271,
        1.041345477104187,
        0.9652771949768066,
        -0.6585162281990051,
        2.4995059967041016,
        -0.037646401673555374,
        -0.09737130999565125,
        -0.2524345815181732,
        -0.047866228967905045,
        -0.6341663002967834,
        0.48700523376464844,
        0.9435873627662659,
        0.5291833877563477,
        -0.5588075518608093,
        -0.12458851933479309,
        -0.2776322662830353,
        -0.5641729831695557,
        -0.691267728805542,
        -0.10115402936935425,
        0.08540886640548706,
        0.1948346644639969,
        0.31744086742401123,
        -1.120703101158142,
        -0.5106618404388428,
        0.43148475885391235,
        1.1587464809417725,
        0.28081372380256653,
        0.5073370933532715,
        0.16849976778030396,
        0.3159186542034149,
        -0.19966267049312592,
        -0.27880385518074036,
        0.3111814856529236,
        -0.4343069791793823,
        0.2963711619377136,
        -1.060948133468628,
        -0.7687224745750427,
        -0.5274385213851929,
        1.0718353986740112,
        -0.33411091566085815,
        0.49538588523864746,
        -0.49299708008766174,
        0.0033096857368946075,
        0.012033656239509583,
        0.9636085629463196,
        1.0167033672332764,
        1.5179961919784546,
        0.3811352550983429,
        0.1388198584318161,
        -0.3077933192253113,
        0.4757769703865051,
        0.39321720600128174,
        -0.5137252807617188,
        0.8145780563354492,
        -0.034882351756095886,
        -0.09099218249320984,
        0.31602752208709717,
        0.1721765100955963,
        -0.04372766613960266,
        -0.7643795609474182,
        0.23972803354263306,
        0.022950077429413795,
        1.400386095046997,
        0.4836329519748688,
        -0.3454887866973877,
        0.39681100845336914,
        0.37373030185699463,
        0.296183317899704,
        -0.16813579201698303,
        -1.618362307548523,
        0.07838445156812668,
        -0.30596494674682617,
        0.05022994428873062,
        0.04968850314617157,
        -0.060745205730199814,
        1.0484522581100464,
        -0.18316054344177246,
        0.4572692811489105,
        -0.19547812640666962,
        -0.5836905241012573,
        0.15784622728824615,
        -0.16857203841209412,
        -0.16419944167137146,
        0.35449108481407166,
        -0.148858442902565,
        -0.25438767671585083,
        -0.7227237224578857,
        -0.3252356946468353,
        0.35830387473106384,
        -0.19454893469810486,
        0.062040749937295914,
        -0.6147357225418091,
        -0.45771753787994385,
        0.5396243333816528,
        0.1544327288866043,
        -0.43442806601524353,
        0.3154870867729187,
        -0.45778611302375793,
        0.16546252369880676,
        -0.29947999119758606,
        -0.8792740702629089,
        -0.7680820226669312,
        1.2885152101516724,
        -0.46803733706474304,
        -0.6515383124351501,
        -0.6109324097633362,
        -0.2542262077331543,
        1.712571382522583,
        -0.06453379988670349,
        0.0632050484418869,
        0.41455915570259094,
        0.396100789308548,
        -0.26584285497665405,
        -0.09815222769975662,
        0.1711980402469635,
        -0.44009533524513245,
        0.061032552272081375,
        -0.6582472324371338,
        -0.5632237195968628,
        0.5361802577972412,
        0.4559658467769623,
        0.05445234850049019,
        -0.09089677035808563,
        -1.1580476760864258,
        -0.07710988074541092,
        0.3349195122718811,
        0.04336889088153839,
        0.6235729455947876,
        -0.6513867974281311,
        0.41975197196006775,
        0.5249478816986084,
        -0.3646647334098816,
        -1.4796912670135498,
        -0.7347280979156494,
        0.4466171860694885,
        0.7046997547149658,
        -0.3585662245750427,
        -0.31024169921875,
        0.726791262626648,
        -0.2374163269996643,
        -0.1351638287305832,
        -0.33261948823928833,
        1.3535369634628296,
        0.48808178305625916,
        0.026872459799051285,
        0.013501185923814774,
        -0.0855470523238182,
        0.793349027633667,
        -0.3080199956893921,
        -0.08071306347846985,
        -0.18189167976379395,
        -0.4466436803340912,
        -0.28855156898498535,
        0.7767952084541321,
        1.5412373542785645,
        0.5172491073608398,
        0.15244130790233612,
        0.06919598579406738,
        0.08733157813549042,
        -0.414135217666626,
        -1.1825114488601685,
        0.1273842453956604,
        0.4653357267379761,
        -0.5332849621772766,
        0.6762552261352539,
        0.1232825219631195,
        0.17879945039749146,
        1.0090289115905762,
        0.35033637285232544,
        -0.2543894350528717,
        0.10916061699390411,
        -0.5539141893386841,
        1.4040743112564087,
        -0.672649621963501,
        -0.5083126425743103,
        -0.5758092999458313,
        0.3773042559623718,
        -0.5925044417381287,
        0.30450204014778137,
        0.19077399373054504,
        0.040957339107990265,
        -0.38736438751220703,
        0.24624589085578918,
        -0.15852878987789154,
        -0.7184824347496033,
        0.6874241828918457,
        0.029406903311610222,
        0.6854357123374939,
        -0.22959409654140472,
        0.02587801218032837,
        0.2709576189517975,
        0.06816236674785614,
        -0.376691997051239,
        0.7846068143844604,
        -0.7405969500541687,
        -0.7345847487449646,
        -1.0378777980804443
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and condenses complex insights from text on profound topics into 15-word bullet points. This process emphasizes the extraction of nuanced, powerful ideas related to human and technological advancement. The expected output is a concise list of abstracted, insightful bullets.",
          "name": "Extract_insights",
          "raw": "\n                workflow Extract_insights v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract surprising, powerful, and interesting insights from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nYou create 15 word bullet points that capture the most important insights from the input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS, and write them on a virtual whiteboard in your mind using 15 word bullets. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- From those IDEAS, extract the most powerful and insightful of them and write them in a section called INSIGHTS. Make sure you extract at least 10 and up to 25.\n\n# OUTPUT INSTRUCTIONS\n\n- INSIGHTS are essentially higher-level IDEAS that are more abstracted and wise.\n\n- Output the INSIGHTS section only.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract surprising, powerful, and interesting insights from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nYou create 15 word bullet points that capture the most important insights from the input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS, and write them on a virtual whiteboard in your mind using 15 word bullets. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- From those IDEAS, extract the most powerful and insightful of them and write them in a section called INSIGHTS. Make sure you extract at least 10 and up to 25.\n\n# OUTPUT INSTRUCTIONS\n\n- INSIGHTS are essentially higher-level IDEAS that are more abstracted and wise.\n\n- Output the INSIGHTS section only.\n\n- Each bullet should be 15 words in length.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6502838730812073,
        0.04218219220638275,
        -0.42719367146492004,
        0.18264973163604736,
        0.29917430877685547,
        0.22557368874549866,
        -1.1780755519866943,
        0.10730801522731781,
        -0.17973358929157257,
        0.026978841051459312,
        -0.29330942034721375,
        0.9202172756195068,
        0.4417933225631714,
        -0.06332573294639587,
        0.2857635021209717,
        -0.08105044066905975,
        0.11284570395946503,
        -0.7039686441421509,
        -1.2680675983428955,
        -0.2445317506790161,
        -0.12499526143074036,
        0.9799491167068481,
        0.5092194676399231,
        0.00106135755777359,
        -0.0015902966260910034,
        0.2256842404603958,
        -0.5292631983757019,
        -0.43133312463760376,
        -0.923617959022522,
        -1.2796826362609863,
        0.879302978515625,
        0.07940535247325897,
        -0.15122707188129425,
        -0.5212113857269287,
        0.08498038351535797,
        -1.1019160747528076,
        -0.21788433194160461,
        -0.061729103326797485,
        -0.16266241669654846,
        -0.3277038335800171,
        0.6334540843963623,
        0.25047609210014343,
        -0.047089748084545135,
        -0.6107947826385498,
        0.5998544692993164,
        -0.23518896102905273,
        0.4434063136577606,
        -0.3080219030380249,
        0.752388596534729,
        -0.16754518449306488,
        0.01096922904253006,
        -0.8163551688194275,
        -0.23675823211669922,
        -0.32545697689056396,
        -0.7145864367485046,
        -0.3547253906726837,
        0.26478323340415955,
        -0.30553698539733887,
        0.07452936470508575,
        -0.12114595621824265,
        -0.01562993787229061,
        0.27393239736557007,
        -3.3513660430908203,
        -0.0112917460501194,
        -0.020344560965895653,
        0.227946937084198,
        0.1855630725622177,
        -0.7410535216331482,
        -0.10111566632986069,
        -0.3349631726741791,
        -0.42769598960876465,
        0.023566672578454018,
        -0.38199013471603394,
        0.3112112283706665,
        -0.33735352754592896,
        0.02775869518518448,
        0.04412166774272919,
        0.07179035246372223,
        0.46518105268478394,
        -0.28391343355178833,
        -0.3212800920009613,
        0.32152295112609863,
        0.043324071913957596,
        -0.03733881190419197,
        -1.0225704908370972,
        0.7196741700172424,
        -0.48344072699546814,
        -0.11676272749900818,
        0.159125417470932,
        0.2779221832752228,
        0.27679482102394104,
        -0.3735399842262268,
        0.13964059948921204,
        -0.16352370381355286,
        0.07948228716850281,
        -0.159704327583313,
        -0.20873375236988068,
        0.30475813150405884,
        -0.19027641415596008,
        3.67301082611084,
        0.84713214635849,
        0.027848999947309494,
        0.8834738731384277,
        -0.8875289559364319,
        0.5134006142616272,
        -0.22156456112861633,
        -0.15190571546554565,
        -0.2899594008922577,
        -0.1310913860797882,
        -0.7069607973098755,
        0.8173725008964539,
        -1.1811184883117676,
        -0.2555627226829529,
        -0.4428927004337311,
        -0.09074420481920242,
        1.149543285369873,
        -0.5547792911529541,
        -0.4144822657108307,
        0.25959092378616333,
        0.9843777418136597,
        -0.46867549419403076,
        -0.6283618211746216,
        -0.2027987390756607,
        -0.5034613013267517,
        -0.01123434491455555,
        0.028268560767173767,
        -0.13800889253616333,
        0.727706789970398,
        0.10077768564224243,
        0.31310734152793884,
        -0.01936931163072586,
        0.014233298599720001,
        -0.7215458750724792,
        0.05536946654319763,
        0.4353914260864258,
        0.31685101985931396,
        0.6631841063499451,
        -0.8986967206001282,
        0.2516300082206726,
        -0.5024126172065735,
        -0.10141414403915405,
        -0.7751031517982483,
        0.748903751373291,
        0.08405202627182007,
        0.5904631018638611,
        0.38182470202445984,
        0.1765555739402771,
        0.5067858695983887,
        -0.1634402871131897,
        -0.7851197719573975,
        0.06502082198858261,
        0.3422793447971344,
        -0.0728422999382019,
        0.7863616943359375,
        0.455302357673645,
        0.05136900395154953,
        -0.9274522066116333,
        0.1309998780488968,
        -1.186009168624878,
        1.032155990600586,
        0.1893000453710556,
        0.08493580669164658,
        0.08337659388780594,
        -0.2658917307853699,
        0.6007049679756165,
        -0.44858667254447937,
        -0.01993700861930847,
        -0.37493956089019775,
        0.13812795281410217,
        -0.0012344270944595337,
        0.23623591661453247,
        -0.3165324628353119,
        0.7585044503211975,
        0.3116720914840698,
        -0.7524063587188721,
        -0.36598479747772217,
        -0.26484012603759766,
        0.28026920557022095,
        0.440337598323822,
        -0.24500827491283417,
        1.0295268297195435,
        0.44661954045295715,
        -0.3673761487007141,
        -0.8629287481307983,
        -0.5689399838447571,
        0.4235095679759979,
        0.5810596346855164,
        0.6395148038864136,
        1.035380482673645,
        1.069094181060791,
        -0.9199628233909607,
        1.8371264934539795,
        -0.013872839510440826,
        -0.03589438647031784,
        -0.02370687760412693,
        0.371071457862854,
        -0.048307113349437714,
        0.12334473431110382,
        0.6052860021591187,
        0.4980718791484833,
        -0.9097981452941895,
        -0.13718022406101227,
        -0.22659644484519958,
        -0.4691087305545807,
        -0.8635787963867188,
        -0.62663334608078,
        0.14916656911373138,
        0.7224375605583191,
        0.09413609653711319,
        -0.5296726226806641,
        -0.4964701533317566,
        0.2472599595785141,
        1.1671974658966064,
        0.3301182687282562,
        0.48416510224342346,
        0.24760465323925018,
        0.0662495419383049,
        -0.014096546918153763,
        -0.03343096375465393,
        0.8014141321182251,
        -0.33884483575820923,
        0.24891388416290283,
        -1.1451526880264282,
        -0.6475098133087158,
        -0.6781564354896545,
        0.7859512567520142,
        -0.4897082448005676,
        0.8534947037696838,
        -0.5630478858947754,
        -0.09395363926887512,
        0.00036185234785079956,
        0.9114273190498352,
        0.8006860613822937,
        1.5419251918792725,
        -0.3883596658706665,
        0.44432684779167175,
        -0.18785744905471802,
        0.43814897537231445,
        0.3272460997104645,
        -0.8794137239456177,
        0.6688997149467468,
        0.29916566610336304,
        -0.19232721626758575,
        0.20412404835224152,
        -0.07069307565689087,
        0.08853909373283386,
        -0.6045207381248474,
        -0.00034843385219573975,
        -0.33078116178512573,
        1.1799993515014648,
        0.3393993079662323,
        -0.3857173025608063,
        0.30928462743759155,
        0.6324658393859863,
        -0.6315376162528992,
        0.23862755298614502,
        -1.5157772302627563,
        -0.05044654384255409,
        -0.16881990432739258,
        0.16519340872764587,
        -0.09435546398162842,
        0.5463072061538696,
        0.6687172055244446,
        -0.2008281797170639,
        0.2971820533275604,
        0.03848959133028984,
        -0.47350022196769714,
        0.07563922554254532,
        -0.5136167407035828,
        -0.07784661650657654,
        -0.02068023756146431,
        0.0018594413995742798,
        -0.11656647175550461,
        -0.09293780475854874,
        -0.15609802305698395,
        -0.009159376844763756,
        -0.040013380348682404,
        -0.11108323186635971,
        -0.007274456322193146,
        -0.4547244906425476,
        0.28986844420433044,
        0.01868981309235096,
        -0.5327425599098206,
        0.015302158892154694,
        -0.6675456166267395,
        0.0861734002828598,
        -0.31616631150245667,
        -0.9788069725036621,
        -0.488292396068573,
        1.1623666286468506,
        -0.6463253498077393,
        -0.3750368356704712,
        -0.3307701647281647,
        0.1750454157590866,
        1.5562233924865723,
        -0.04334792494773865,
        0.26922738552093506,
        0.874413013458252,
        0.17496849596500397,
        -0.3605949282646179,
        0.40179869532585144,
        0.3403156101703644,
        -0.0439763218164444,
        0.19385141134262085,
        -1.30534029006958,
        -0.5789912939071655,
        0.9222163558006287,
        0.18900816142559052,
        -0.0740833505988121,
        0.19098633527755737,
        -0.7935203313827515,
        -0.25380003452301025,
        -0.22570914030075073,
        -0.07515382766723633,
        0.9478573203086853,
        -0.2533119022846222,
        0.054326485842466354,
        0.9727061986923218,
        -0.018661949783563614,
        -1.5181609392166138,
        -0.7726960182189941,
        0.6541872024536133,
        0.6593047380447388,
        0.014938980340957642,
        -0.4239996373653412,
        0.4260745644569397,
        0.031838055700063705,
        0.4089256525039673,
        -0.47558683156967163,
        1.5770567655563354,
        0.44030165672302246,
        -0.046599842607975006,
        -0.14480294287204742,
        -0.18370459973812103,
        1.001231074333191,
        -0.2879101634025574,
        0.16778802871704102,
        0.06626918911933899,
        -0.02411038428544998,
        0.07403871417045593,
        0.25697702169418335,
        1.7975072860717773,
        0.2592659592628479,
        0.45370563864707947,
        -0.13396316766738892,
        0.2827203571796417,
        -0.5326886773109436,
        -1.1970793008804321,
        0.27157062292099,
        0.4202362895011902,
        -0.14825484156608582,
        0.7035159468650818,
        -0.1354885697364807,
        -0.25227195024490356,
        0.9181054830551147,
        0.6227665543556213,
        0.03392564132809639,
        0.03212059289216995,
        -0.6678221821784973,
        1.4240015745162964,
        -0.4360980987548828,
        -0.5871317982673645,
        -0.6246674060821533,
        -0.1617007851600647,
        0.07232733070850372,
        0.06688910722732544,
        -0.4250495731830597,
        -0.06961970776319504,
        0.033832378685474396,
        0.053099267184734344,
        -0.04480386897921562,
        -0.49104729294776917,
        0.43155741691589355,
        0.4845452606678009,
        0.6136201620101929,
        -0.2900036573410034,
        0.1023324579000473,
        0.15759465098381042,
        -0.019284121692180634,
        -0.3064109683036804,
        0.4332932233810425,
        -0.24671894311904907,
        -0.7930676341056824,
        -0.6783084273338318
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and highlights the most crucial or intriguing idea from any given content. This prompt emphasizes a methodical approach to identify and articulate the essence of the input. The expected output includes a concise main idea and a recommendation based on that idea.",
          "name": "Extract_main_idea",
          "raw": "\n                workflow Extract_main_idea v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract the primary and/or most surprising, insightful, and interesting idea from any input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Fully digest the content provided.\n\n- Extract the most important idea from the content.\n\n- In a section called MAIN IDEA, write a 15-word sentence that captures the main idea.\n\n- In a section called MAIN RECOMMENDATION, write a 15-word sentence that captures what's recommended for people to do based on the idea.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not give warnings or notes; only output the requested sections.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract the primary and/or most surprising, insightful, and interesting idea from any input.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Fully digest the content provided.\n\n- Extract the most important idea from the content.\n\n- In a section called MAIN IDEA, write a 15-word sentence that captures the main idea.\n\n- In a section called MAIN RECOMMENDATION, write a 15-word sentence that captures what's recommended for people to do based on the idea.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not give warnings or notes; only output the requested sections.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        1.211509108543396,
        0.4004412889480591,
        -0.6697043776512146,
        0.09287876635789871,
        0.23875899612903595,
        -0.1181938424706459,
        -1.0803749561309814,
        -0.042427390813827515,
        -0.3441183269023895,
        0.2662968635559082,
        -0.5283834338188171,
        0.5835583209991455,
        -0.058308642357587814,
        -0.01015906035900116,
        0.662826418876648,
        0.12445890158414841,
        0.22258451581001282,
        -0.4721198081970215,
        -1.1654798984527588,
        -0.23452553153038025,
        0.18394210934638977,
        0.5580839514732361,
        0.21725809574127197,
        0.16371093690395355,
        0.5073708295822144,
        0.08541709929704666,
        -0.2151021659374237,
        -0.44835323095321655,
        -1.019658088684082,
        -0.8706797361373901,
        0.8241130113601685,
        0.558103084564209,
        -0.13438281416893005,
        -0.4410817325115204,
        0.6840581297874451,
        -0.36653977632522583,
        -0.02344690077006817,
        0.5848625898361206,
        -0.31351473927497864,
        -0.057090334594249725,
        -0.17664943635463715,
        0.6229504346847534,
        -0.11133614182472229,
        -0.10945425927639008,
        0.3114183247089386,
        -0.5362017154693604,
        0.3340141773223877,
        0.3701243996620178,
        1.1105717420578003,
        0.4260251522064209,
        -0.22762759029865265,
        -0.5630995035171509,
        -0.15739966928958893,
        -0.4607374370098114,
        -0.7931430339813232,
        -0.3015565872192383,
        -0.0432194322347641,
        -0.7948254942893982,
        0.4130968451499939,
        0.08155664801597595,
        0.14021381735801697,
        -0.032811239361763,
        -3.1507959365844727,
        0.011490359902381897,
        -0.24076569080352783,
        0.293320894241333,
        0.04745756834745407,
        -0.4218597114086151,
        0.2536333203315735,
        -0.26801449060440063,
        -0.5924380421638489,
        0.2919863760471344,
        -0.32883480191230774,
        0.6690198183059692,
        -0.03503124415874481,
        0.11673305183649063,
        0.7960168719291687,
        -0.0980742871761322,
        0.4038199186325073,
        -0.1691756397485733,
        -0.2972990572452545,
        0.35085543990135193,
        0.10774128139019012,
        -0.47066083550453186,
        -0.4098503887653351,
        0.6589879393577576,
        -0.4467994272708893,
        -0.09921946376562119,
        -0.16101273894309998,
        0.36001741886138916,
        0.13951796293258667,
        -0.3079548180103302,
        0.34902408719062805,
        0.13451099395751953,
        -0.10609542578458786,
        0.1468559205532074,
        -0.00959991104900837,
        0.15211641788482666,
        0.020247168838977814,
        3.3910698890686035,
        1.1300435066223145,
        -0.03320290893316269,
        0.1849854588508606,
        -0.7400683164596558,
        0.7418904900550842,
        -0.49465614557266235,
        0.03444429114460945,
        -0.08198391646146774,
        0.08063627034425735,
        -0.12982308864593506,
        0.5835802555084229,
        -0.4849051535129547,
        -0.47696372866630554,
        0.7531645894050598,
        0.33308008313179016,
        0.15301933884620667,
        -0.4021063446998596,
        -0.3749493658542633,
        -0.1653638631105423,
        0.1648961901664734,
        -0.5028449296951294,
        0.27174773812294006,
        0.09680499881505966,
        -0.44759488105773926,
        0.0725131630897522,
        -0.24545544385910034,
        0.059566181153059006,
        0.42845267057418823,
        0.3362356722354889,
        -0.22643600404262543,
        0.4421221613883972,
        0.5119768381118774,
        -0.8404309749603271,
        0.09450095891952515,
        -0.39603978395462036,
        -0.5509262084960938,
        0.7504655122756958,
        -0.5454341173171997,
        0.33000484108924866,
        -0.5284259915351868,
        0.18576642870903015,
        -0.9472864866256714,
        0.5714510083198547,
        0.3706307113170624,
        0.6967516541481018,
        -0.26000869274139404,
        0.018734382465481758,
        -0.05272649973630905,
        -0.3742935061454773,
        -0.635860800743103,
        0.24779975414276123,
        0.4612221419811249,
        0.039493996649980545,
        0.3181603252887726,
        0.23969583213329315,
        -0.302848756313324,
        -0.9161231517791748,
        0.3594720959663391,
        -0.8517645001411438,
        0.18909940123558044,
        0.059036869555711746,
        0.11139979958534241,
        0.2665928602218628,
        0.5079770684242249,
        0.49416080117225647,
        -0.23735353350639343,
        0.4945750832557678,
        -0.43961426615715027,
        0.7006612420082092,
        0.05093127861618996,
        0.8283115029335022,
        -0.4431616961956024,
        0.6974847912788391,
        0.8366246819496155,
        -0.31973010301589966,
        0.021754570305347443,
        -0.41984325647354126,
        -0.4793984889984131,
        0.24641667306423187,
        -0.6119712591171265,
        0.6045655012130737,
        0.5488206148147583,
        -0.23778855800628662,
        -0.9500798583030701,
        -0.24195584654808044,
        0.5468877553939819,
        0.2205018401145935,
        0.14043796062469482,
        0.7150580883026123,
        1.373950719833374,
        -0.8534405827522278,
        2.2609517574310303,
        -0.7749398946762085,
        -0.31901901960372925,
        0.06601167470216751,
        -0.30595749616622925,
        -0.5784470438957214,
        0.6180981397628784,
        0.4546573758125305,
        0.49453046917915344,
        -0.3441687524318695,
        0.16511203348636627,
        -0.5031344294548035,
        -0.3405478894710541,
        -0.18548992276191711,
        -0.3860555589199066,
        -0.510848879814148,
        0.1765095591545105,
        -0.04843563586473465,
        -1.3476859331130981,
        -0.2195063680410385,
        0.24533776938915253,
        0.9389506578445435,
        0.22779065370559692,
        1.0072466135025024,
        0.29129558801651,
        0.09301812201738358,
        0.1318216323852539,
        0.6259866952896118,
        0.7352157831192017,
        -0.4699954390525818,
        -0.04362282156944275,
        -0.9767360091209412,
        -0.4517890214920044,
        -1.4920463562011719,
        1.2852764129638672,
        -0.055062636733055115,
        0.23142261803150177,
        -0.8458517789840698,
        -0.324372798204422,
        0.7456661462783813,
        0.9626803398132324,
        0.845382809638977,
        0.8667731881141663,
        0.3114857077598572,
        0.09258349239826202,
        -0.2334919571876526,
        0.18849027156829834,
        0.07831333577632904,
        -1.0100494623184204,
        0.6827626824378967,
        -0.2945704758167267,
        -0.4864230751991272,
        0.2696987986564636,
        -0.22554880380630493,
        0.456572026014328,
        -1.3628170490264893,
        0.0037737488746643066,
        0.2831515967845917,
        0.8528333902359009,
        0.757863461971283,
        0.30469921231269836,
        0.5945237874984741,
        0.653365433216095,
        -0.4906695485115051,
        0.006513677537441254,
        -2.3422152996063232,
        -0.07732879370450974,
        -0.5568000674247742,
        0.23398122191429138,
        -0.08501595258712769,
        -0.17297059297561646,
        -0.12416324019432068,
        -0.11398366838693619,
        -0.08702006191015244,
        -0.23625163733959198,
        -1.0558221340179443,
        -0.5163962841033936,
        -0.28460508584976196,
        -0.2583684027194977,
        -0.07522670924663544,
        0.23177552223205566,
        -0.25822824239730835,
        -0.11410174518823624,
        -0.07607752084732056,
        0.634613573551178,
        0.5101715326309204,
        0.40719178318977356,
        -0.557833194732666,
        -0.34677064418792725,
        0.33134469389915466,
        0.6259812712669373,
        0.47665804624557495,
        0.14574989676475525,
        -0.5877736210823059,
        0.31405359506607056,
        0.0008784867823123932,
        -1.2470283508300781,
        -0.034537725150585175,
        0.7965220808982849,
        -0.8079009056091309,
        -0.8397641181945801,
        -0.5902950167655945,
        -0.39553219079971313,
        2.244426727294922,
        0.3723233640193939,
        0.14541439712047577,
        0.5652012228965759,
        0.6077523827552795,
        -0.18177443742752075,
        -0.33855417370796204,
        0.08371001482009888,
        -0.5996679663658142,
        0.41358935832977295,
        -0.558135449886322,
        -0.8542726635932922,
        0.5645217895507812,
        0.2981809079647064,
        -0.2136823534965515,
        0.3503835201263428,
        -0.5746829509735107,
        0.3118407726287842,
        0.08071412146091461,
        -0.36740705370903015,
        0.6132380366325378,
        -0.3383006751537323,
        0.4372888207435608,
        1.1344759464263916,
        0.17870603501796722,
        -2.093179702758789,
        -0.06641911715269089,
        0.4862991273403168,
        1.1439001560211182,
        -0.2585883140563965,
        0.48878636956214905,
        0.5975972414016724,
        -0.46739625930786133,
        -0.07493804395198822,
        -0.2649896740913391,
        1.3380318880081177,
        0.07024529576301575,
        0.39322295784950256,
        -0.4128390848636627,
        -0.44991177320480347,
        0.7256799936294556,
        -0.5591582655906677,
        -0.11489669233560562,
        -0.09671968966722488,
        -0.48567789793014526,
        -0.22346064448356628,
        -0.18860378861427307,
        1.278994083404541,
        0.4596143960952759,
        -0.24759632349014282,
        -0.0699332058429718,
        0.24461698532104492,
        -0.6170539259910583,
        -0.9807832837104797,
        0.39939048886299133,
        0.24171985685825348,
        -0.4492463171482086,
        0.6094186305999756,
        0.43057340383529663,
        -0.5241580605506897,
        0.3651566803455353,
        0.4034844636917114,
        -0.1386188268661499,
        0.19774310290813446,
        -0.6858607530593872,
        1.4731870889663696,
        -0.5387704372406006,
        -0.33489182591438293,
        -0.22088727355003357,
        0.3872999846935272,
        -0.2755458354949951,
        -0.00477088987827301,
        0.15387792885303497,
        -0.749223530292511,
        0.14807164669036865,
        -0.17496809363365173,
        0.28462159633636475,
        -0.6052048802375793,
        0.4481900632381439,
        0.6523328423500061,
        0.49777477979660034,
        0.28350505232810974,
        -0.2615331709384918,
        0.023990347981452942,
        0.015195168554782867,
        -0.06704139709472656,
        0.4513733983039856,
        -0.1626713126897812,
        -1.0132496356964111,
        -1.3582191467285156
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt guides in identifying and analyzing recurring, surprising, or insightful patterns from a collection of ideas, data, or observations. It emphasizes extracting the most notable patterns based on their frequency and significance, and then documenting the process of discovery and analysis. The expected output includes a detailed summary of patterns, an explanation of their selection and significance, and actionable advice for startup builders based on these insights.",
          "name": "Extract_patterns",
          "raw": "\n                workflow Extract_patterns v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou take a collection of ideas or data or observations and you look for the most interesting and surprising patterns. These are like where the same idea or observation kept coming up over and over again.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about all the input and the core concepts contained within.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting pattern observed from the input into a section called PATTERNS.\n\n- Weight the patterns by how often they were mentioned or showed up in the data, combined with how surprising, insightful, and/or interesting they are. But most importantly how often they showed up in the data.\n\n- Each pattern should be captured as a bullet point of no more than 15 words.\n\n- In a new section called META, talk through the process of how you assembled each pattern, where you got the pattern from, how many components of the input lead to each pattern, and other interesting data about the patterns.\n\n- Give the names or sources of the different people or sources that combined to form a pattern. For example: \\\"The same idea was mentioned by both John and Jane.\\\"\n\n- Each META point should be captured as a bullet point of no more than 15 words.\n\n- Add a section called ANALYSIS that gives a one sentence, 30-word summary of all the patterns and your analysis thereof.\n\n- Add a section called BEST 5 that gives the best 5 patterns in a list of 30-word bullets. Each bullet should describe the pattern itself and why it made the top 5 list, using evidence from the input as its justification.\n\n- Add a section called ADVICE FOR BUILDERS that gives a set of 15-word bullets of advice for people in a startup space related to the input. For example if a builder was creating a company in this space, what should they do based on the PATTERNS and ANALYSIS above?\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Extract at least 20 PATTERNS from the content.\n- Limit each idea bullet to a maximum of 15 words.\n- Write in the style of someone giving helpful analysis finding patterns\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou take a collection of ideas or data or observations and you look for the most interesting and surprising patterns. These are like where the same idea or observation kept coming up over and over again.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Think deeply about all the input and the core concepts contained within.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting pattern observed from the input into a section called PATTERNS.\n\n- Weight the patterns by how often they were mentioned or showed up in the data, combined with how surprising, insightful, and/or interesting they are. But most importantly how often they showed up in the data.\n\n- Each pattern should be captured as a bullet point of no more than 15 words.\n\n- In a new section called META, talk through the process of how you assembled each pattern, where you got the pattern from, how many components of the input lead to each pattern, and other interesting data about the patterns.\n\n- Give the names or sources of the different people or sources that combined to form a pattern. For example: \\\"The same idea was mentioned by both John and Jane.\\\"\n\n- Each META point should be captured as a bullet point of no more than 15 words.\n\n- Add a section called ANALYSIS that gives a one sentence, 30-word summary of all the patterns and your analysis thereof.\n\n- Add a section called BEST 5 that gives the best 5 patterns in a list of 30-word bullets. Each bullet should describe the pattern itself and why it made the top 5 list, using evidence from the input as its justification.\n\n- Add a section called ADVICE FOR BUILDERS that gives a set of 15-word bullets of advice for people in a startup space related to the input. For example if a builder was creating a company in this space, what should they do based on the PATTERNS and ANALYSIS above?\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Extract at least 20 PATTERNS from the content.\n- Limit each idea bullet to a maximum of 15 words.\n- Write in the style of someone giving helpful analysis finding patterns\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.26526325941085815,
        0.6616420745849609,
        -0.42105555534362793,
        0.5363590717315674,
        0.215260311961174,
        -0.462823748588562,
        -0.6929810047149658,
        -0.15217919647693634,
        -0.33224397897720337,
        0.5839595198631287,
        -0.46685248613357544,
        0.795734703540802,
        0.5200028419494629,
        0.07604390382766724,
        0.14764493703842163,
        -0.11794216930866241,
        -0.7220554947853088,
        -1.3733761310577393,
        -0.803632915019989,
        -0.7448040843009949,
        0.09576235711574554,
        0.47468775510787964,
        0.029532060027122498,
        -0.08692729473114014,
        0.47798651456832886,
        -0.2018638253211975,
        -0.3857558071613312,
        -0.6700487732887268,
        -0.8129764795303345,
        -0.8848211169242859,
        1.1512795686721802,
        0.6214932799339294,
        -0.8077130913734436,
        -0.9664252996444702,
        0.3165472745895386,
        -0.9643320441246033,
        0.20366431772708893,
        0.020760856568813324,
        -0.5092312693595886,
        -0.5592355132102966,
        -0.4268374741077423,
        0.24212899804115295,
        -0.20309185981750488,
        0.21634402871131897,
        0.24012301862239838,
        0.5004583597183228,
        0.5392767190933228,
        0.0029945150017738342,
        0.9289668202400208,
        0.017208591103553772,
        0.35686224699020386,
        0.3356267809867859,
        -0.3102024793624878,
        -0.7085416316986084,
        -0.8759313821792603,
        -0.6754187941551208,
        0.17539195716381073,
        0.10590477287769318,
        0.03794083744287491,
        0.2386842519044876,
        0.3055420219898224,
        -0.08170996606349945,
        -3.1131911277770996,
        0.5869913697242737,
        -0.0767245888710022,
        0.001795060932636261,
        0.3245662748813629,
        -0.5753032565116882,
        -0.24462686479091644,
        0.2748623490333557,
        0.05325895547866821,
        -0.07906324416399002,
        -0.4453696310520172,
        0.727980375289917,
        0.11256449669599533,
        -0.016347959637641907,
        0.6868123412132263,
        0.7234663963317871,
        0.01062697172164917,
        -0.4610985517501831,
        0.7788657546043396,
        0.7715452313423157,
        0.364004909992218,
        0.09773564338684082,
        -0.602946400642395,
        0.44375765323638916,
        0.03223263472318649,
        -0.4060409963130951,
        0.5944734215736389,
        0.4212316870689392,
        0.2236403226852417,
        -0.8935117125511169,
        -0.20234690606594086,
        0.10427669435739517,
        -0.8302826881408691,
        -0.19511546194553375,
        -0.6632131338119507,
        0.39265525341033936,
        -0.28053197264671326,
        3.5484366416931152,
        0.33375757932662964,
        0.44991767406463623,
        0.12045689672231674,
        -0.8681163787841797,
        0.37009990215301514,
        -0.6469672918319702,
        -0.398193359375,
        -0.3418748080730438,
        -0.10135404765605927,
        0.07818780094385147,
        0.34906771779060364,
        0.06900569051504135,
        0.3896057605743408,
        0.14190241694450378,
        -0.03143724054098129,
        0.1989923119544983,
        -1.1704272031784058,
        -0.3749845325946808,
        0.30480876564979553,
        1.062764286994934,
        -0.3608885407447815,
        0.8372198343276978,
        -0.5312318801879883,
        -0.8261488080024719,
        0.29340553283691406,
        -0.4094265103340149,
        -0.5710720419883728,
        1.0085433721542358,
        0.5285767316818237,
        0.46138647198677063,
        0.48625147342681885,
        -0.7358810305595398,
        -0.22482654452323914,
        0.32391586899757385,
        0.002827610820531845,
        -0.14681881666183472,
        0.3413599729537964,
        -0.46192482113838196,
        0.12865960597991943,
        -0.15712054073810577,
        0.5607503056526184,
        -0.18133993446826935,
        0.0429515615105629,
        0.9256736040115356,
        0.39129504561424255,
        0.452968955039978,
        0.17199988663196564,
        0.2512447237968445,
        -0.11842258274555206,
        -0.9987773895263672,
        -0.4237309992313385,
        1.1431505680084229,
        -0.06944694370031357,
        0.6359273791313171,
        0.15498289465904236,
        0.30145666003227234,
        -0.673240602016449,
        -0.29265591502189636,
        -0.7519904971122742,
        -0.2519198954105377,
        0.6695036888122559,
        -0.4585225582122803,
        0.12296513468027115,
        0.12192046642303467,
        0.2934480607509613,
        0.004802490584552288,
        0.262198805809021,
        -0.2171379029750824,
        0.7838869690895081,
        0.08081462979316711,
        -0.04145151376724243,
        0.03658370301127434,
        -0.18674039840698242,
        -0.08291350305080414,
        -0.540723443031311,
        -0.1350526511669159,
        -0.2809799909591675,
        -0.2219267189502716,
        -0.13079553842544556,
        -0.0480833575129509,
        0.7235868573188782,
        0.021173030138015747,
        0.16423054039478302,
        -0.24935594201087952,
        0.02020856738090515,
        0.34526509046554565,
        0.1548859179019928,
        0.7280360460281372,
        0.508167028427124,
        0.29136979579925537,
        -0.9821263551712036,
        1.3692046403884888,
        -0.1328270137310028,
        -0.6065521240234375,
        -0.16120317578315735,
        -0.028747649863362312,
        -0.2427305281162262,
        0.47731709480285645,
        0.26260364055633545,
        -0.143158957362175,
        -0.8408920168876648,
        -0.3406083285808563,
        -0.4700808525085449,
        -0.2651539444923401,
        -0.4389410614967346,
        -0.2008797526359558,
        -0.05752365291118622,
        -0.310920387506485,
        0.43731600046157837,
        -0.7820454239845276,
        0.237904891371727,
        -0.3921385407447815,
        1.1515755653381348,
        0.1344548761844635,
        1.0758568048477173,
        -0.808998703956604,
        -0.23507219552993774,
        -0.3787345886230469,
        0.17075276374816895,
        0.19467833638191223,
        -1.2203357219696045,
        -0.19187024235725403,
        -0.927788257598877,
        -0.7265774011611938,
        -1.0732344388961792,
        0.8081469535827637,
        0.14756283164024353,
        0.5734050869941711,
        -0.6867149472236633,
        -0.20225602388381958,
        0.730326771736145,
        1.197885513305664,
        0.05697006732225418,
        1.457108736038208,
        0.5441433191299438,
        -0.24445393681526184,
        -0.022561244666576385,
        0.1873611956834793,
        0.5061682462692261,
        -0.545160710811615,
        0.2547590732574463,
        -0.16799843311309814,
        -0.7637608051300049,
        0.6037895679473877,
        -0.24455341696739197,
        0.08294931054115295,
        0.11754380166530609,
        -0.2129867523908615,
        0.09870362281799316,
        1.1905193328857422,
        0.5490524768829346,
        -0.17016857862472534,
        -0.4089451730251312,
        0.6479926705360413,
        -0.28430289030075073,
        0.315507709980011,
        -2.715338706970215,
        0.19406791031360626,
        0.07402480393648148,
        0.45224472880363464,
        -0.18887478113174438,
        0.2970331311225891,
        0.5672059655189514,
        0.44560879468917847,
        -0.09395914524793625,
        -0.07886618375778198,
        -0.9324255585670471,
        -0.021667134016752243,
        -0.48737481236457825,
        0.2556493878364563,
        -0.44035524129867554,
        0.5485855937004089,
        0.4789910912513733,
        -1.0864615440368652,
        0.10587593168020248,
        0.51881343126297,
        -0.24367022514343262,
        1.1016169786453247,
        -0.06763830780982971,
        0.10349377989768982,
        -0.03321671485900879,
        0.949622631072998,
        0.21795177459716797,
        0.44252336025238037,
        -0.16661003232002258,
        -0.016540704295039177,
        -0.17802417278289795,
        -0.018499597907066345,
        -0.6732426881790161,
        0.6535098552703857,
        -0.3304395377635956,
        -0.18625257909297943,
        -0.9755970239639282,
        0.0012133866548538208,
        0.9823291301727295,
        0.24803026020526886,
        0.5285099744796753,
        0.1331673264503479,
        -0.026140039786696434,
        0.7801043391227722,
        -0.5534741282463074,
        0.15656086802482605,
        0.252863347530365,
        0.4698757827281952,
        -0.46658310294151306,
        -0.5503697395324707,
        1.2075263261795044,
        0.07229699194431305,
        -0.421466588973999,
        0.07378815859556198,
        -1.0011571645736694,
        0.599290132522583,
        -0.26496273279190063,
        -0.3783571124076843,
        0.4196377992630005,
        -0.6431767344474792,
        0.4470415711402893,
        0.6386518478393555,
        0.3745269179344177,
        -1.120867371559143,
        -0.2428836226463318,
        0.38586553931236267,
        0.25354716181755066,
        0.010538339614868164,
        0.11843658983707428,
        0.5298377275466919,
        -0.2344374656677246,
        -0.1978168785572052,
        -0.5061472058296204,
        1.268641710281372,
        -0.1485527604818344,
        0.11048989742994308,
        0.5845024585723877,
        0.48649951815605164,
        0.29248127341270447,
        0.2134782075881958,
        -0.723279595375061,
        -0.2985268533229828,
        -0.40221601724624634,
        0.36028867959976196,
        0.295867383480072,
        1.3187224864959717,
        -0.31056445837020874,
        -0.6781646609306335,
        0.359133780002594,
        0.09918458759784698,
        0.25842976570129395,
        -1.3604792356491089,
        0.4405738413333893,
        -0.08915679901838303,
        -0.7567258477210999,
        1.1043962240219116,
        0.04287850856781006,
        -0.6038707494735718,
        1.3942115306854248,
        0.33222496509552,
        -0.4041948616504669,
        -0.06521477550268173,
        -1.0572181940078735,
        1.0720652341842651,
        -0.19594210386276245,
        -0.19114023447036743,
        -0.31305450201034546,
        0.4398343563079834,
        -0.29602885246276855,
        0.19923269748687744,
        -0.4661775529384613,
        -0.3322758972644806,
        -0.4250360131263733,
        -0.2242514193058014,
        0.2773891091346741,
        -0.3933500647544861,
        0.11509747803211212,
        0.6809941530227661,
        1.347374439239502,
        -0.19314272701740265,
        -0.2789285182952881,
        -0.09859570860862732,
        0.355212926864624,
        -0.13127845525741577,
        0.27188241481781006,
        0.2616051733493805,
        -1.1452869176864624,
        -0.2364024668931961
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes security or bug bounty reports to extract and provide proof of concept URLs for validating vulnerabilities. It specializes in identifying actionable URLs and commands from the reports, ensuring direct verification of reported vulnerabilities. The output includes the URL with a specific command to execute it, like using curl or python.",
          "name": "Extract_poc",
          "raw": "\n                workflow Extract_poc v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a super powerful AI cybersecurity expert system specialized in finding and extracting proof of concept URLs and other vulnerability validation methods from submitted security/bug bounty reports.\n\nYou always output the URL that can be used to validate the vulnerability, preceded by the command that can run it: e.g., \\\"curl https://yahoo.com/vulnerable-app/backup.zip\\\".\n\n# Steps\n\n- Take the submitted security/bug bounty report and extract the proof of concept URL from it. You return the URL itself that can be run directly to verify if the vulnerability exists or not, plus the command to run it.\n\nExample: curl \\\"https://yahoo.com/vulnerable-example/backup.zip\\\"\nExample: curl -X \\\"Authorization: 12990\\\" \\\"https://yahoo.com/vulnerable-example/backup.zip\\\"\nExample: python poc.py\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a super powerful AI cybersecurity expert system specialized in finding and extracting proof of concept URLs and other vulnerability validation methods from submitted security/bug bounty reports.\n\nYou always output the URL that can be used to validate the vulnerability, preceded by the command that can run it: e.g., \\\"curl https://yahoo.com/vulnerable-app/backup.zip\\\".\n\n# Steps\n\n- Take the submitted security/bug bounty report and extract the proof of concept URL from it. You return the URL itself that can be run directly to verify if the vulnerability exists or not, plus the command to run it.\n\nExample: curl \\\"https://yahoo.com/vulnerable-example/backup.zip\\\"\nExample: curl -X \\\"Authorization: 12990\\\" \\\"https://yahoo.com/vulnerable-example/backup.zip\\\"\nExample: python poc.py\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6997306942939758,
        0.45186370611190796,
        -0.6285245418548584,
        0.5702760815620422,
        0.04686211422085762,
        0.05934625864028931,
        -1.2297627925872803,
        0.35977745056152344,
        -0.10853850841522217,
        0.051941871643066406,
        0.006241833791136742,
        0.28493136167526245,
        0.04249940067529678,
        0.22448968887329102,
        0.09985756874084473,
        -0.23024402558803558,
        -0.1429237276315689,
        -1.0241761207580566,
        -2.1484999656677246,
        -0.5873057842254639,
        0.00244771596044302,
        0.6871647238731384,
        0.2672855854034424,
        0.6933009028434753,
        0.5349517464637756,
        -0.11087717860937119,
        -0.5302495956420898,
        -0.2926120162010193,
        -1.2700430154800415,
        -1.4434188604354858,
        0.37361544370651245,
        0.13180291652679443,
        0.07518583536148071,
        -0.7958590984344482,
        0.6138923168182373,
        -1.1217031478881836,
        -0.0044514951296150684,
        -0.2476867139339447,
        -0.300056129693985,
        -0.27165722846984863,
        -0.29134511947631836,
        0.0677313357591629,
        0.0036540329456329346,
        -0.6908419728279114,
        0.5290897488594055,
        -0.28727608919143677,
        0.3498208522796631,
        0.08111751079559326,
        0.8377660512924194,
        -0.16958226263523102,
        0.06731118261814117,
        -0.6048478484153748,
        -0.4525861144065857,
        -0.3211495876312256,
        -0.7846400141716003,
        -0.7240516543388367,
        -0.4205987751483917,
        -0.3148423731327057,
        0.06453239917755127,
        -0.24046531319618225,
        0.046094100922346115,
        0.27684739232063293,
        -3.33856463432312,
        -0.0456421822309494,
        0.30009204149246216,
        0.431294709444046,
        0.2457641214132309,
        0.12138397991657257,
        0.5689488649368286,
        -0.2613579332828522,
        0.3156619369983673,
        0.10355836898088455,
        -0.2584407925605774,
        0.9240775108337402,
        -0.04919397458434105,
        0.3444574773311615,
        0.21372529864311218,
        -0.23128318786621094,
        0.6018626093864441,
        -0.06593184173107147,
        0.5481480360031128,
        0.25273993611335754,
        0.2886370122432709,
        -0.03996801748871803,
        -0.9284828305244446,
        0.696611225605011,
        -0.712859034538269,
        0.0009074658155441284,
        0.06348076462745667,
        -0.14069996774196625,
        0.0262698195874691,
        -0.5573553442955017,
        0.2254871279001236,
        0.5508596301078796,
        -0.6057968735694885,
        -0.20506145060062408,
        -0.7167680859565735,
        0.3150888681411743,
        -0.36397069692611694,
        3.350170373916626,
        0.7506718635559082,
        -0.08988906443119049,
        -0.04736845940351486,
        -1.5360714197158813,
        0.20336344838142395,
        -0.21322491765022278,
        -0.4624701738357544,
        -0.2273060530424118,
        0.3010523319244385,
        -0.14375227689743042,
        0.983139157295227,
        -0.7544600367546082,
        -0.09741240739822388,
        -0.20785486698150635,
        0.27997472882270813,
        0.5631293058395386,
        -0.7050089240074158,
        -0.3149075210094452,
        -0.1417488157749176,
        0.9363293051719666,
        -0.2993854284286499,
        0.04288129881024361,
        0.3154290020465851,
        -0.8014268279075623,
        0.3088340759277344,
        -0.23707300424575806,
        0.03577357903122902,
        0.6003286242485046,
        0.4527028501033783,
        0.19557110965251923,
        -0.14156502485275269,
        0.2132195085287094,
        -0.08845388144254684,
        -0.0008658897131681442,
        0.23418912291526794,
        -0.25100040435791016,
        -0.17236794531345367,
        -0.45169928669929504,
        0.25834569334983826,
        -0.29146823287010193,
        -0.09178608655929565,
        -0.9408312439918518,
        0.922912061214447,
        0.23136724531650543,
        1.0747334957122803,
        0.3871738016605377,
        0.045374639332294464,
        0.3443155586719513,
        -0.44422489404678345,
        0.33710092306137085,
        0.12767024338245392,
        0.7284733653068542,
        -0.08172909915447235,
        0.6852232813835144,
        0.8342251181602478,
        0.2647407352924347,
        -0.2943001985549927,
        -0.3323529362678528,
        -0.7350460886955261,
        0.3351602852344513,
        0.18742874264717102,
        -0.1400974988937378,
        0.29781728982925415,
        -0.5514988303184509,
        0.12802931666374207,
        -0.07437793910503387,
        -0.3347826600074768,
        -0.38638031482696533,
        0.6531987190246582,
        0.17043648660182953,
        0.1934587061405182,
        -0.470724493265152,
        0.28922024369239807,
        0.6246194243431091,
        -0.3605862855911255,
        0.0158251766115427,
        -0.07480029761791229,
        0.2317933440208435,
        0.31677699089050293,
        -0.025092944502830505,
        1.5361895561218262,
        0.21232464909553528,
        0.05701691284775734,
        -0.8870496153831482,
        -0.31788066029548645,
        0.9267858266830444,
        0.5863276124000549,
        0.6847896575927734,
        1.6662476062774658,
        0.9634854793548584,
        -0.31999218463897705,
        2.401963710784912,
        -0.5351023077964783,
        0.007047347724437714,
        -0.17038168013095856,
        0.015257637947797775,
        -0.0000971183180809021,
        0.8793919086456299,
        0.47299924492836,
        0.008216499350965023,
        -0.8997257351875305,
        -0.34214144945144653,
        -0.23091371357440948,
        -0.17515751719474792,
        -0.42371582984924316,
        -0.48319461941719055,
        -0.28665247559547424,
        0.039631426334381104,
        0.00726507231593132,
        -0.6041544079780579,
        -0.11361055076122284,
        -0.08218944072723389,
        1.3758649826049805,
        0.1266947239637375,
        0.8899211883544922,
        -0.18691088259220123,
        0.11499269306659698,
        -0.03686962649226189,
        -0.0741482675075531,
        0.276885449886322,
        -0.7737392783164978,
        0.48908740282058716,
        -0.8627783060073853,
        -0.47816169261932373,
        -0.9680607318878174,
        0.8670638799667358,
        -0.04580278694629669,
        0.727838397026062,
        -0.4392833709716797,
        -0.17681661248207092,
        -0.051204077899456024,
        0.9258224368095398,
        0.9460914134979248,
        1.8638529777526855,
        0.14198514819145203,
        0.24486765265464783,
        0.07188116014003754,
        0.24903088808059692,
        -0.02689499408006668,
        -0.7269668579101562,
        0.4430061876773834,
        0.13029935956001282,
        -0.10855145007371902,
        -0.07259257137775421,
        0.06283491849899292,
        -0.12354891002178192,
        -0.633221447467804,
        0.421578049659729,
        -0.12532979249954224,
        1.3215126991271973,
        0.7752411961555481,
        0.21252743899822235,
        -0.04175926372408867,
        0.35168150067329407,
        -0.22801628708839417,
        0.06446726620197296,
        -1.9811934232711792,
        -0.12272055447101593,
        -0.1997053325176239,
        -0.10494410991668701,
        0.2262323796749115,
        -0.0515582337975502,
        0.8491433262825012,
        0.43537765741348267,
        -0.04739517346024513,
        -0.23148296773433685,
        -0.7041888236999512,
        -0.6032963991165161,
        -0.08932175487279892,
        -0.2967739999294281,
        0.3162813186645508,
        0.14829403162002563,
        -0.0926237404346466,
        -0.25428977608680725,
        -0.010334277525544167,
        0.3547830283641815,
        0.10993670672178268,
        0.43789049983024597,
        -0.562696099281311,
        -0.45345354080200195,
        0.5363180637359619,
        0.2106337696313858,
        -0.3184773325920105,
        0.3910186290740967,
        -0.12998351454734802,
        -0.06381841003894806,
        -0.1633257120847702,
        -0.5403016805648804,
        -0.39896076917648315,
        0.8598272800445557,
        -0.17924875020980835,
        -0.7474249601364136,
        -0.9309311509132385,
        -0.43134838342666626,
        1.551164150238037,
        0.307392418384552,
        0.22348196804523468,
        0.5045207142829895,
        0.16087068617343903,
        -0.6556300520896912,
        -0.8820886015892029,
        0.3620684742927551,
        -0.343330442905426,
        0.18908217549324036,
        -1.0439056158065796,
        -0.9587486982345581,
        0.8961194753646851,
        0.34731325507164,
        0.004646699875593185,
        0.0947561115026474,
        -0.7857639789581299,
        0.1959533542394638,
        0.04314268007874489,
        -0.16013789176940918,
        0.9087092876434326,
        -0.17169903218746185,
        0.31946060061454773,
        1.053392767906189,
        -0.05564679577946663,
        -1.2683477401733398,
        -0.2097272127866745,
        0.8650786876678467,
        0.4471449553966522,
        -0.10461731255054474,
        -0.3087441325187683,
        0.371784508228302,
        -0.22661006450653076,
        -0.2826363444328308,
        -0.5141485333442688,
        1.4924968481063843,
        0.20072725415229797,
        0.23482351005077362,
        -0.24212715029716492,
        0.03898455202579498,
        1.0172619819641113,
        0.1894216537475586,
        -0.09038621932268143,
        0.19082915782928467,
        -0.6325175762176514,
        -0.5712836384773254,
        0.21288517117500305,
        1.0080012083053589,
        0.3514052629470825,
        -0.23544785380363464,
        -0.19220848381519318,
        0.1759946644306183,
        -0.5169543623924255,
        -0.8613204956054688,
        0.29376479983329773,
        0.09458012878894806,
        -0.10734615474939346,
        0.500196099281311,
        -0.23396019637584686,
        -0.21597501635551453,
        0.8343099355697632,
        0.7964452505111694,
        -0.5388228893280029,
        -0.33805274963378906,
        -0.9272972345352173,
        1.8107805252075195,
        -0.7449199557304382,
        -0.6737070083618164,
        -1.0584698915481567,
        -0.09515510499477386,
        -0.12157627195119858,
        0.05557101592421532,
        -0.08335090428590775,
        0.004448920488357544,
        -0.5625572800636292,
        0.337047278881073,
        0.2924497723579407,
        -0.3243749737739563,
        0.4119003713130951,
        0.3542031943798065,
        0.8306153416633606,
        -0.48158973455429077,
        0.1752742975950241,
        -0.19560521841049194,
        -0.20390495657920837,
        -0.1062149703502655,
        0.7881889343261719,
        -0.10443972051143646,
        -0.9124649167060852,
        -0.9300792813301086
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and organizes predictions from content into a structured format. It focuses on identifying specific predictions, their timelines, confidence levels, and verification methods. The expected output includes a bulleted list and a detailed table of these predictions.",
          "name": "Extract_predictions",
          "raw": "\n                workflow Extract_predictions v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou fully digest input and extract the predictions made within.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract all predictions made within the content.\n\n- For each prediction, extract the following:\n\n  - The specific prediction in less than 15 words.\n  - The date by which the prediction is supposed to occur.\n  - The confidence level given for the prediction.\n  - How we'll know if it's true or not.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output valid Markdown with no bold or italics.\n\n- Output the predictions as a bulleted list.\n\n- Under the list, produce a predictions table that includes the following columns: Prediction, Confidence, Date, How to Verify.\n\n- Limit each bullet to a maximum of 15 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou fully digest input and extract the predictions made within.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract all predictions made within the content.\n\n- For each prediction, extract the following:\n\n  - The specific prediction in less than 15 words.\n  - The date by which the prediction is supposed to occur.\n  - The confidence level given for the prediction.\n  - How we'll know if it's true or not.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output valid Markdown with no bold or italics.\n\n- Output the predictions as a bulleted list.\n\n- Under the list, produce a predictions table that includes the following columns: Prediction, Confidence, Date, How to Verify.\n\n- Limit each bullet to a maximum of 15 words.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.36681851744651794,
        0.46276170015335083,
        -0.20037637650966644,
        0.5108748078346252,
        0.2343098372220993,
        -0.40000978112220764,
        -1.280732274055481,
        0.2472393810749054,
        0.2443840652704239,
        0.1574125438928604,
        -0.3703921139240265,
        0.6454129815101624,
        0.4102320373058319,
        -0.13059395551681519,
        -0.07896560430526733,
        0.3574758470058441,
        0.04733757674694061,
        -0.7910012006759644,
        -1.759253740310669,
        -0.310316801071167,
        0.12158621847629547,
        0.5629435181617737,
        -0.024124927818775177,
        -0.1217777281999588,
        0.407123327255249,
        -0.06431107968091965,
        -0.39488333463668823,
        -0.30950456857681274,
        -0.9496481418609619,
        -0.7018333077430725,
        0.48854145407676697,
        0.44718438386917114,
        -0.20753386616706848,
        -0.6543505191802979,
        0.2623744606971741,
        -0.568303108215332,
        -0.2966231405735016,
        -0.7135225534439087,
        0.028903130441904068,
        -0.330794095993042,
        -0.18581406772136688,
        0.4632459282875061,
        -0.2358972430229187,
        -0.6450580954551697,
        0.24086065590381622,
        -0.30757585167884827,
        0.2125055342912674,
        -0.1389264166355133,
        0.758833110332489,
        0.3887721300125122,
        0.5387693643569946,
        -0.4340081810951233,
        0.17134428024291992,
        -0.5502454042434692,
        -0.517645001411438,
        -0.5114755630493164,
        0.15729717910289764,
        -0.14339874684810638,
        -0.3189760744571686,
        -0.19453996419906616,
        0.09134120494127274,
        0.057535454630851746,
        -3.5751912593841553,
        -0.2172255665063858,
        -0.10596682131290436,
        0.049482788890600204,
        0.30242499709129333,
        -0.46960607171058655,
        0.7158341407775879,
        -0.6248172521591187,
        -0.4270479679107666,
        0.009074193425476551,
        -0.18264010548591614,
        0.4314718544483185,
        0.06749240309000015,
        0.2820117175579071,
        0.09079201519489288,
        -0.6310365200042725,
        0.09823787212371826,
        -0.3993951976299286,
        0.21259884536266327,
        0.2034924179315567,
        0.1667707860469818,
        -0.22654716670513153,
        -0.5698280930519104,
        0.7121047973632812,
        -0.33344048261642456,
        -0.08005005121231079,
        0.09045498818159103,
        0.11283862590789795,
        -0.27869969606399536,
        -0.8169909119606018,
        0.30730563402175903,
        0.06914915144443512,
        -0.0006097853183746338,
        -0.357724666595459,
        -0.5055745244026184,
        0.45910194516181946,
        -0.11465243250131607,
        3.3691723346710205,
        0.759235143661499,
        0.32763218879699707,
        -0.0008175671100616455,
        -1.3442702293395996,
        0.15662382543087006,
        -0.022861633449792862,
        -0.24431484937667847,
        -0.26417285203933716,
        0.36016252636909485,
        -0.23032113909721375,
        0.5365644693374634,
        -1.0266151428222656,
        -0.26645365357398987,
        0.17211776971817017,
        -0.8287973999977112,
        1.0437731742858887,
        -0.495663583278656,
        -0.03376482054591179,
        0.14107203483581543,
        0.9454401731491089,
        -0.3461381793022156,
        0.26215043663978577,
        0.16338109970092773,
        -0.8771005868911743,
        0.004285499453544617,
        -0.5855048894882202,
        -0.07073793560266495,
        0.5946750640869141,
        0.18947285413742065,
        0.6222813129425049,
        0.2782198488712311,
        -0.17333069443702698,
        -0.4164394736289978,
        -0.15785446763038635,
        0.44405901432037354,
        -0.1909816563129425,
        0.5143441557884216,
        -0.6879138350486755,
        0.2016802430152893,
        -0.29687249660491943,
        -0.05306825041770935,
        -0.8041809797286987,
        0.5785526633262634,
        0.5132192969322205,
        0.739827573299408,
        0.1484745740890503,
        -0.06349734216928482,
        0.8351815342903137,
        -0.35112351179122925,
        0.166715607047081,
        0.1428924798965454,
        0.7837585806846619,
        -0.5934759974479675,
        0.4507533609867096,
        0.2627507150173187,
        0.5694658756256104,
        -0.30503177642822266,
        -0.22526326775550842,
        -0.8261560201644897,
        0.2670815587043762,
        0.20511536300182343,
        -0.35344040393829346,
        -0.43478238582611084,
        -0.3981896638870239,
        0.7022480964660645,
        -0.06879065930843353,
        -0.45673489570617676,
        -0.0018332302570343018,
        0.3729872405529022,
        0.06754030287265778,
        0.31646570563316345,
        -0.1420685052871704,
        0.3926002085208893,
        0.661363422870636,
        -0.42395374178886414,
        -0.2733893096446991,
        0.09551917016506195,
        0.3587351143360138,
        0.05460542440414429,
        -0.03293140232563019,
        1.5777480602264404,
        0.6322796940803528,
        0.026163946837186813,
        -0.5975975394248962,
        -0.38538026809692383,
        0.8285280466079712,
        0.1848352998495102,
        0.7189028263092041,
        1.1954426765441895,
        1.2769590616226196,
        -0.748245358467102,
        2.2352089881896973,
        -0.16231456398963928,
        -0.31048521399497986,
        -0.3515591323375702,
        -0.11751852929592133,
        -0.1740763783454895,
        0.5771971344947815,
        0.9569708108901978,
        0.19879694283008575,
        -1.093115210533142,
        -0.2535977065563202,
        -0.38251006603240967,
        -0.41865429282188416,
        -0.6411341428756714,
        -0.11263349652290344,
        0.019502680748701096,
        -0.06732317060232162,
        0.09739285707473755,
        -0.8129417300224304,
        -0.48521268367767334,
        0.007366344332695007,
        0.9414590001106262,
        0.18182823061943054,
        0.3506411015987396,
        0.40247100591659546,
        0.37409427762031555,
        -0.2701416611671448,
        -0.3975847363471985,
        0.3998439311981201,
        -0.22539597749710083,
        -0.044551193714141846,
        -0.7461877465248108,
        -0.5772122144699097,
        -0.8741092681884766,
        1.2424622774124146,
        -0.470092236995697,
        1.1708295345306396,
        -0.5460823774337769,
        -0.25142696499824524,
        -0.038766779005527496,
        1.497684121131897,
        0.8941178321838379,
        1.9279005527496338,
        0.17952200770378113,
        0.29342570900917053,
        -0.22636425495147705,
        0.10897140204906464,
        -0.10397100448608398,
        -0.7313700914382935,
        0.48828381299972534,
        -0.31288692355155945,
        -0.4838945269584656,
        0.35571205615997314,
        -0.1965014934539795,
        0.2191484272480011,
        -0.41643455624580383,
        -0.07601964473724365,
        0.031313005834817886,
        1.1651923656463623,
        0.5538994073867798,
        0.096859410405159,
        -0.015117708593606949,
        0.667209804058075,
        -0.525524914264679,
        -0.1707172393798828,
        -1.723209261894226,
        0.3439479470252991,
        -0.0025679394602775574,
        0.2758859694004059,
        -0.20435500144958496,
        -0.2897205948829651,
        1.0376434326171875,
        0.12323907017707825,
        -0.2017596811056137,
        -0.3008362650871277,
        -0.5142800807952881,
        -0.26559266448020935,
        -0.32077738642692566,
        0.2546841502189636,
        0.11853092908859253,
        0.07731553912162781,
        -0.17566001415252686,
        -0.5669429302215576,
        -0.2341918796300888,
        0.6697180271148682,
        -0.12202360481023788,
        0.014857083559036255,
        -0.7569140791893005,
        -0.3089675307273865,
        0.6702038049697876,
        0.23285846412181854,
        -0.41707751154899597,
        0.2780804932117462,
        0.00688030943274498,
        0.22250080108642578,
        -0.29153549671173096,
        -1.0547266006469727,
        -0.26512762904167175,
        0.9657352566719055,
        -0.46594005823135376,
        -0.5448881983757019,
        -0.9931226968765259,
        -0.2962452173233032,
        1.2894006967544556,
        0.27568310499191284,
        0.05268102139234543,
        0.47433480620384216,
        0.42870593070983887,
        0.04475314915180206,
        -0.07944520562887192,
        0.7419515252113342,
        0.34595444798469543,
        0.5185190439224243,
        -0.9243159294128418,
        -0.6624433994293213,
        0.676508903503418,
        0.34489765763282776,
        0.28923478722572327,
        -0.18197864294052124,
        -1.1200567483901978,
        0.022089269012212753,
        -0.25662368535995483,
        0.18596327304840088,
        0.9881542325019836,
        -0.7342786192893982,
        0.8460226655006409,
        1.105553388595581,
        -0.028056085109710693,
        -1.169951319694519,
        -0.2944577634334564,
        0.6991565823554993,
        0.2788858413696289,
        -0.2792493999004364,
        -0.06489906460046768,
        0.38704514503479004,
        -0.29410237073898315,
        -0.19261988997459412,
        -0.7308846116065979,
        1.4488469362258911,
        0.6551202535629272,
        -0.4036569893360138,
        -0.2247079461812973,
        0.11301705986261368,
        1.031692385673523,
        -0.31201785802841187,
        -0.30929112434387207,
        0.23346522450447083,
        -0.36210760474205017,
        -0.024825332686305046,
        0.3851451277732849,
        1.0498816967010498,
        0.38012635707855225,
        -0.15978161990642548,
        0.05367761477828026,
        0.1674458384513855,
        -0.9441893100738525,
        -1.256150245666504,
        0.20528073608875275,
        -0.08042558282613754,
        -0.1218251958489418,
        0.9429000020027161,
        -0.269767701625824,
        -0.3045547604560852,
        1.1041473150253296,
        0.3415032625198364,
        -0.4871780574321747,
        -0.2103072851896286,
        -0.8139069080352783,
        2.1992440223693848,
        -0.7155551910400391,
        -0.4959295094013214,
        -0.7776181101799011,
        0.146651953458786,
        -0.2723141014575958,
        0.1288408637046814,
        -0.07773153483867645,
        0.13552433252334595,
        -0.00767255574464798,
        0.480991393327713,
        -0.34472209215164185,
        -0.44526803493499756,
        0.2678828239440918,
        0.2470410317182541,
        0.770447313785553,
        -0.13308502733707428,
        -0.004778608679771423,
        0.0326305627822876,
        -0.05182309448719025,
        -0.07557331025600433,
        0.8956718444824219,
        -0.00909409299492836,
        -0.4648800492286682,
        -0.6679137945175171
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts questions from content and analyzes their effectiveness in eliciting high-quality responses. It focuses on identifying the elements that make these questions particularly insightful. The expected output includes a list of questions, an analysis of their strengths, and recommendations for interviewers.",
          "name": "Extract_questions",
          "raw": "\n                workflow Extract_questions v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an advanced AI with a 419 IQ that excels at asking brilliant questions of people. You specialize in extracting the questions out of a piece of content, word for word, and then figuring out what made the questions so good.\n\n# GOAL\n\n- Extract all the questions from the content.\n\n- Determine what made the questions so good at getting surprising and high-quality answers from the person being asked.\n\n# OUTPUT\n\n- In a section called QUESTIONS, list all questions as a series of bullet points.\n\n- In a section called ANALYSIS, give a set 15-word bullet points that capture the genius of the questions that were asked. \n\n- In a section called RECOMMENDATIONS FOR INTERVIEWERS, give a set of 15-word bullet points that give prescriptive advice to interviewers on how to ask questions.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an advanced AI with a 419 IQ that excels at asking brilliant questions of people. You specialize in extracting the questions out of a piece of content, word for word, and then figuring out what made the questions so good.\n\n# GOAL\n\n- Extract all the questions from the content.\n\n- Determine what made the questions so good at getting surprising and high-quality answers from the person being asked.\n\n# OUTPUT\n\n- In a section called QUESTIONS, list all questions as a series of bullet points.\n\n- In a section called ANALYSIS, give a set 15-word bullet points that capture the genius of the questions that were asked. \n\n- In a section called RECOMMENDATIONS FOR INTERVIEWERS, give a set of 15-word bullet points that give prescriptive advice to interviewers on how to ask questions.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6684085130691528,
        0.6612251400947571,
        -0.25195589661598206,
        -0.11029268801212311,
        0.49355578422546387,
        0.4173371195793152,
        -1.0629602670669556,
        -0.361813485622406,
        0.1261596530675888,
        0.09680285304784775,
        0.12901368737220764,
        0.34535789489746094,
        0.30264025926589966,
        0.1843213438987732,
        0.14476533234119415,
        0.16339468955993652,
        -0.000024478882551193237,
        -1.3653315305709839,
        -1.3390018939971924,
        -0.657341480255127,
        0.28431978821754456,
        0.6537227034568787,
        -0.02778998762369156,
        0.7692182660102844,
        0.44174230098724365,
        -0.18105553090572357,
        -0.2867092192173004,
        -0.5844209790229797,
        -1.0277820825576782,
        -1.2321237325668335,
        0.6217314600944519,
        0.4494055211544037,
        0.09114107489585876,
        -0.5094962120056152,
        0.4363457262516022,
        -0.6857340335845947,
        -0.009147746488451958,
        0.12313367426395416,
        -0.6332279443740845,
        -0.19899100065231323,
        -0.04324910789728165,
        0.6003674268722534,
        -0.28200313448905945,
        -0.05832812190055847,
        0.1531507521867752,
        -0.28703609108924866,
        0.6714065074920654,
        -0.39683017134666443,
        0.5555031299591064,
        -0.4902384281158447,
        0.13766160607337952,
        -0.39958247542381287,
        0.12994061410427094,
        -0.1207638531923294,
        -0.6294086575508118,
        -0.691952645778656,
        0.019582342356443405,
        -0.07632799446582794,
        0.5020338296890259,
        -0.22317612171173096,
        0.28149089217185974,
        0.06707854568958282,
        -3.4080803394317627,
        0.20183421671390533,
        0.059567779302597046,
        0.13206106424331665,
        0.8030657768249512,
        -0.10261805355548859,
        -0.02354048565030098,
        -0.4576222896575928,
        -0.20069587230682373,
        -0.05626780539751053,
        -0.08141103386878967,
        0.2306099832057953,
        -0.00967850349843502,
        -0.050860289484262466,
        -0.3071154057979584,
        -0.10173657536506653,
        0.22984325885772705,
        0.21201321482658386,
        0.045454271137714386,
        1.025895118713379,
        -0.15117551386356354,
        -0.1983465552330017,
        -0.8666726350784302,
        0.2805849313735962,
        -0.5651161670684814,
        -0.5152447819709778,
        -0.29301637411117554,
        0.03413800522685051,
        -0.20804336667060852,
        -0.7520081400871277,
        0.09732120484113693,
        0.2634035646915436,
        -0.31763190031051636,
        0.30584052205085754,
        -0.05092472955584526,
        0.1362837255001068,
        0.3520767390727997,
        3.5397815704345703,
        0.6637182235717773,
        0.29908686876296997,
        0.33757254481315613,
        -1.0719679594039917,
        0.1767175942659378,
        -0.6703788042068481,
        -0.5534676313400269,
        -0.32146742939949036,
        -0.30677175521850586,
        0.17742380499839783,
        0.6739624738693237,
        -1.059220790863037,
        -0.04292258620262146,
        -0.36355671286582947,
        0.3402033746242523,
        0.4972250759601593,
        -0.12061751633882523,
        -0.1496848165988922,
        0.044214166700839996,
        0.9204201698303223,
        -0.3101804852485657,
        0.04106663167476654,
        0.007493752986192703,
        -0.10118406265974045,
        -0.06599050015211105,
        -0.5084901452064514,
        -0.29132798314094543,
        0.6768890023231506,
        -0.004032887518405914,
        0.4158951938152313,
        0.012162841856479645,
        0.37206125259399414,
        -0.5599774122238159,
        0.38363319635391235,
        0.4492882192134857,
        -0.3792632818222046,
        0.35767751932144165,
        -0.5951109528541565,
        0.5589640736579895,
        -0.7465317249298096,
        -0.3748299479484558,
        -0.7245777249336243,
        0.5552104711532593,
        0.42686140537261963,
        0.567707896232605,
        0.46609777212142944,
        0.16830220818519592,
        -0.1323222517967224,
        -0.804585337638855,
        0.10757578909397125,
        0.021479353308677673,
        0.6753520369529724,
        -0.266449511051178,
        0.03691229969263077,
        0.7164974212646484,
        0.4324374198913574,
        -1.4764702320098877,
        -0.0578889399766922,
        -0.8488472104072571,
        0.9308856725692749,
        0.22178788483142853,
        -0.39108699560165405,
        0.3123549222946167,
        -0.35821306705474854,
        0.3647718131542206,
        -0.15537424385547638,
        0.23644867539405823,
        -0.7319042086601257,
        0.401571124792099,
        0.4136183559894562,
        0.20305484533309937,
        -0.2707180380821228,
        0.8108931183815002,
        0.48827362060546875,
        -0.21120503544807434,
        -0.2551882565021515,
        -0.2859836220741272,
        0.334821879863739,
        0.2609264552593231,
        -0.17339715361595154,
        1.2475506067276,
        0.14873622357845306,
        -0.2800261080265045,
        -0.7182965278625488,
        -0.8009454607963562,
        0.44335392117500305,
        0.3170717656612396,
        0.5010693669319153,
        1.3763427734375,
        0.8593432903289795,
        -0.7323582172393799,
        1.7732857465744019,
        -0.26315048336982727,
        0.09311559796333313,
        0.0721195861697197,
        0.21796314418315887,
        -0.1077256053686142,
        0.2769857347011566,
        0.6051403880119324,
        0.14383801817893982,
        -0.8618637323379517,
        -0.05823554843664169,
        -0.27317437529563904,
        -0.569098949432373,
        -0.9015834331512451,
        -0.6966404318809509,
        0.08652526140213013,
        0.25155264139175415,
        -0.06434330344200134,
        -0.9084851145744324,
        -0.041571762412786484,
        0.354285329580307,
        0.8671300411224365,
        -0.10983885079622269,
        0.25146350264549255,
        0.5864651203155518,
        0.3642009496688843,
        0.4465297758579254,
        -0.13936814665794373,
        0.4389359951019287,
        -0.867949903011322,
        0.25014278292655945,
        -0.992023229598999,
        -0.806858241558075,
        -0.45765528082847595,
        0.906013011932373,
        -0.18912579119205475,
        0.20992596447467804,
        -0.5019334554672241,
        -0.23274840414524078,
        -0.2853521406650543,
        0.8635980486869812,
        1.2244341373443604,
        1.2501641511917114,
        -0.08573856204748154,
        0.22467994689941406,
        -0.11470504850149155,
        0.03538234904408455,
        0.651657223701477,
        -0.46139204502105713,
        0.7898377776145935,
        0.2691650390625,
        -0.04780122637748718,
        -0.1977526843547821,
        -0.15496531128883362,
        0.05828285217285156,
        -0.5248926877975464,
        0.18268004059791565,
        -0.2524527907371521,
        1.4413974285125732,
        -0.11352206021547318,
        -0.39558252692222595,
        -0.20204100012779236,
        0.5450533628463745,
        -0.049737825989723206,
        -0.0023860037326812744,
        -1.7867069244384766,
        0.19492597877979279,
        -0.3905717432498932,
        -0.11035884916782379,
        0.17118316888809204,
        -0.16069529950618744,
        0.39713096618652344,
        0.03698235750198364,
        0.29430150985717773,
        0.0948682427406311,
        -0.3125123679637909,
        -0.13423264026641846,
        -0.14603838324546814,
        -0.32950901985168457,
        0.39608868956565857,
        0.33670055866241455,
        -0.2789583206176758,
        0.1342417597770691,
        0.004020342603325844,
        0.5385136008262634,
        0.1969790905714035,
        -0.28548187017440796,
        -0.23048007488250732,
        -0.45933830738067627,
        0.43020331859588623,
        0.2717490792274475,
        -0.8019927740097046,
        0.12640824913978577,
        -0.92110276222229,
        0.17939040064811707,
        -0.3771945834159851,
        -1.1826200485229492,
        -0.7381460070610046,
        1.3061888217926025,
        -0.6759731769561768,
        -0.17840534448623657,
        -0.9659590125083923,
        -0.22656497359275818,
        1.3293616771697998,
        0.26476067304611206,
        -0.3829139471054077,
        0.7682117223739624,
        0.20076709985733032,
        -0.3215407133102417,
        -0.20844393968582153,
        0.31071653962135315,
        -0.11293686926364899,
        0.23445433378219604,
        -0.9401450157165527,
        -0.2651444971561432,
        1.0695366859436035,
        0.3648248016834259,
        0.20639103651046753,
        0.20803490281105042,
        -1.2943304777145386,
        -0.23126442730426788,
        0.32030361890792847,
        0.42135849595069885,
        0.5457249283790588,
        -0.6755690574645996,
        -0.007920149713754654,
        1.0728563070297241,
        -0.1253935694694519,
        -1.5750395059585571,
        -0.5905848741531372,
        0.22106966376304626,
        0.3350259065628052,
        -0.058912500739097595,
        -0.20692972838878632,
        0.7785488367080688,
        -0.3586212694644928,
        -0.15676160156726837,
        -0.03125520795583725,
        1.1149985790252686,
        0.6074095964431763,
        0.02930235117673874,
        -0.047914981842041016,
        0.08947255462408066,
        0.7654199600219727,
        0.11152017116546631,
        0.025466378778219223,
        0.2132071554660797,
        -0.7651853561401367,
        0.22118718922138214,
        0.15737274289131165,
        1.5189111232757568,
        0.29561764001846313,
        0.011864112690091133,
        -0.06312961131334305,
        0.3022582530975342,
        -0.1983020007610321,
        -0.6826180219650269,
        -0.0802038162946701,
        0.10784119367599487,
        -0.06802878528833389,
        0.8320436477661133,
        -0.2550346851348877,
        -0.17145535349845886,
        1.2289693355560303,
        1.1240111589431763,
        -0.11884355545043945,
        0.11424531787633896,
        -0.6985764503479004,
        1.4214394092559814,
        -0.6029993891716003,
        -0.3081493377685547,
        -0.7509200572967529,
        0.052397001534700394,
        0.04329193755984306,
        0.1917995810508728,
        0.4560945928096771,
        -0.3474048376083374,
        -0.3485223650932312,
        0.3658435344696045,
        0.11307598650455475,
        -0.761122465133667,
        0.7393240928649902,
        0.50890052318573,
        0.8890554904937744,
        -0.20866961777210236,
        -0.1016843169927597,
        -0.28339883685112,
        -0.2867794334888458,
        -0.0722099095582962,
        0.4528324604034424,
        -0.2604629397392273,
        -0.289587140083313,
        -0.6557178497314453
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and condenses recommendations from content into a concise list. This process involves identifying both explicit and implicit advice within the given material. The output is a bulleted list of up to 20 brief recommendations.",
          "name": "Extract_recommendations",
          "raw": "\n                workflow Extract_recommendations v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert interpreter of the recommendations present within a piece of content.\n\n# Steps\n\nTake the input given and extract the concise, practical recommendations that are either explicitly made in the content, or that naturally flow from it.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a bulleted list of up to 20 recommendations, each of no more than 15 words.\n\n# OUTPUT EXAMPLE\n\n- Recommendation 1\n- Recommendation 2\n- Recommendation 3\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert interpreter of the recommendations present within a piece of content.\n\n# Steps\n\nTake the input given and extract the concise, practical recommendations that are either explicitly made in the content, or that naturally flow from it.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a bulleted list of up to 20 recommendations, each of no more than 15 words.\n\n# OUTPUT EXAMPLE\n\n- Recommendation 1\n- Recommendation 2\n- Recommendation 3\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.8666462302207947,
        0.6836082339286804,
        -0.06982439756393433,
        0.32942426204681396,
        -0.24030764400959015,
        0.23457375168800354,
        -1.2118070125579834,
        -0.25500616431236267,
        -0.043798916041851044,
        0.001783725805580616,
        0.09413809329271317,
        0.10672871768474579,
        0.23993447422981262,
        -0.11628171801567078,
        0.643347442150116,
        -0.07394538819789886,
        -0.5169816017150879,
        -0.10740764439105988,
        -0.9444757699966431,
        -0.8557494878768921,
        0.7083109021186829,
        1.3857942819595337,
        0.20830407738685608,
        0.4074673056602478,
        0.23063701391220093,
        -0.15946491062641144,
        -0.5198957920074463,
        -0.5885913372039795,
        -0.6316989660263062,
        -1.0112193822860718,
        0.2801470160484314,
        0.15600457787513733,
        -0.44091424345970154,
        -0.16463781893253326,
        0.3429570198059082,
        -0.4612138867378235,
        0.34537264704704285,
        -0.23367241024971008,
        -0.11315686255693436,
        -0.435654878616333,
        -0.09996946901082993,
        0.673764705657959,
        0.2803896367549896,
        0.29004359245300293,
        0.10385841131210327,
        -0.09423735737800598,
        0.3053758144378662,
        -0.592810869216919,
        0.6567031741142273,
        0.3007594347000122,
        -0.18988947570323944,
        -0.4956052303314209,
        0.3945326507091522,
        -0.08790849149227142,
        -0.07798460125923157,
        -0.27559894323349,
        0.25801241397857666,
        -0.4421001672744751,
        0.2693374454975128,
        0.09676767885684967,
        0.19153787195682526,
        0.056309774518013,
        -3.0615694522857666,
        0.21030506491661072,
        -0.22167553007602692,
        -0.38714146614074707,
        0.5291219353675842,
        -0.8058028221130371,
        -0.6188401579856873,
        -0.5008547306060791,
        -0.17848631739616394,
        -0.008856715634465218,
        -0.025279246270656586,
        0.2209491729736328,
        -0.15373316407203674,
        -0.39088189601898193,
        0.005333093926310539,
        -0.41457080841064453,
        0.30890554189682007,
        -0.3964430093765259,
        -0.09854736924171448,
        1.0811107158660889,
        0.2070123553276062,
        -0.29427918791770935,
        -0.7560340762138367,
        0.4949817955493927,
        -0.8146811127662659,
        -0.5108683109283447,
        0.10448053479194641,
        0.6889262199401855,
        0.2724268436431885,
        -0.4254007339477539,
        0.154497429728508,
        0.9464526772499084,
        -0.27842581272125244,
        -0.1638546735048294,
        -0.03990026190876961,
        0.036515504121780396,
        0.47819507122039795,
        3.313755512237549,
        1.0613250732421875,
        0.40476664900779724,
        0.42147591710090637,
        -1.1320812702178955,
        -0.41343066096305847,
        -0.09152786433696747,
        -0.608731210231781,
        0.14179642498493195,
        -0.0021011559292674065,
        0.6211301684379578,
        0.5219947695732117,
        -0.6964632868766785,
        -0.37407955527305603,
        -0.19545450806617737,
        -0.5974053144454956,
        0.6085501313209534,
        -0.806912899017334,
        -0.3474287986755371,
        0.19122715294361115,
        0.6337386965751648,
        -0.7104983925819397,
        0.3757458031177521,
        0.11058809608221054,
        -0.3487561047077179,
        0.018886379897594452,
        -0.4293529987335205,
        -0.08633771538734436,
        0.5501394271850586,
        0.48620671033859253,
        0.22807879745960236,
        0.1348511278629303,
        0.08890615403652191,
        -0.5567528605461121,
        -0.18494810163974762,
        -0.1444740742444992,
        -0.33086109161376953,
        0.5713145732879639,
        -0.9413910508155823,
        0.4859262704849243,
        -0.6651443243026733,
        -0.24950671195983887,
        -0.9225891828536987,
        1.0003527402877808,
        0.8968826532363892,
        0.8777136206626892,
        0.23170967400074005,
        0.3655577003955841,
        -0.2212015688419342,
        -0.8470950126647949,
        -0.3832637071609497,
        0.6057375073432922,
        0.228299081325531,
        0.04389086365699768,
        0.7455949783325195,
        0.154182568192482,
        0.21363180875778198,
        -0.8053211569786072,
        0.1670350432395935,
        -0.3167238235473633,
        0.4749543070793152,
        -0.3380657136440277,
        -0.4424758553504944,
        0.5023741126060486,
        -0.2539641261100769,
        0.12832626700401306,
        -0.6405096054077148,
        0.3128935694694519,
        -0.42514726519584656,
        0.4469747245311737,
        0.5420576333999634,
        -0.14828452467918396,
        -0.5794130563735962,
        0.81478351354599,
        0.6915405988693237,
        0.24888184666633606,
        0.1706346571445465,
        -0.4198530614376068,
        0.4507118761539459,
        -0.0035745538771152496,
        -0.32890915870666504,
        0.45375141501426697,
        0.3835519850254059,
        0.7597127556800842,
        -0.8904334902763367,
        -0.353071928024292,
        0.47067970037460327,
        -0.1810237169265747,
        0.16548201441764832,
        1.4118247032165527,
        1.3014308214187622,
        -0.70521080493927,
        2.138256788253784,
        -0.7514657974243164,
        -0.4906623959541321,
        0.006249218247830868,
        -0.1797480434179306,
        -0.006309002637863159,
        0.15060186386108398,
        0.7577303647994995,
        -0.1901102215051651,
        -1.2286001443862915,
        0.11205393075942993,
        0.1585025191307068,
        -0.47930392622947693,
        -0.7384535074234009,
        -0.6728092432022095,
        -0.007790510542690754,
        0.5155872106552124,
        -0.17127889394760132,
        -0.7457563281059265,
        -0.2735855281352997,
        0.20494942367076874,
        0.7602829337120056,
        0.0015481412410736084,
        0.04221454635262489,
        0.4019901752471924,
        0.20981329679489136,
        -0.0016318224370479584,
        0.371433824300766,
        0.35860323905944824,
        -0.6802000999450684,
        0.5897690057754517,
        -1.0175100564956665,
        -0.7150703072547913,
        -0.7198427319526672,
        0.7938434481620789,
        -0.3461838960647583,
        0.3287971019744873,
        -0.39510244131088257,
        -0.6258823275566101,
        -0.028001241385936737,
        1.1131343841552734,
        1.2980377674102783,
        1.1744141578674316,
        0.5068285465240479,
        0.3155965507030487,
        -0.057257138192653656,
        -0.03601941466331482,
        0.52046799659729,
        -0.4756740629673004,
        0.9235461950302124,
        0.12709146738052368,
        -0.2375534325838089,
        0.4526464343070984,
        0.07053956389427185,
        0.5365725755691528,
        -0.7336717247962952,
        0.26593202352523804,
        -0.20988287031650543,
        1.8736975193023682,
        0.0724295824766159,
        -0.4902455508708954,
        0.27266547083854675,
        1.010751724243164,
        0.2387191355228424,
        0.2114357352256775,
        -1.5070267915725708,
        0.1135709285736084,
        -0.08630916476249695,
        0.36963409185409546,
        0.27002468705177307,
        0.050149403512477875,
        0.3262523114681244,
        0.3899208903312683,
        0.05040387809276581,
        -0.5546121001243591,
        -0.878447949886322,
        -0.06530414521694183,
        -0.0586426667869091,
        -0.41584575176239014,
        -0.09527213126420975,
        0.48410236835479736,
        -0.1315363496541977,
        -0.008233360946178436,
        0.30870237946510315,
        0.15326961874961853,
        0.0076047638431191444,
        -0.09427721053361893,
        -0.24639371037483215,
        -1.244148850440979,
        0.9603180289268494,
        -0.2940724492073059,
        -0.6936392784118652,
        0.27997326850891113,
        -0.7086585760116577,
        -0.03281427174806595,
        -0.5623494386672974,
        -0.8059598207473755,
        -0.06455155462026596,
        1.2287551164627075,
        -0.6932871341705322,
        -0.06686678528785706,
        -0.5768986344337463,
        0.18615205585956573,
        1.5085076093673706,
        -0.2328910231590271,
        -0.39871451258659363,
        0.7128707766532898,
        0.36884739995002747,
        -0.40298521518707275,
        -0.45391416549682617,
        0.949691891670227,
        -0.13602183759212494,
        0.2746170163154602,
        -0.736771285533905,
        -0.4971437156200409,
        0.7132351398468018,
        0.29930421710014343,
        -0.12543907761573792,
        0.09411844611167908,
        -1.5321615934371948,
        -0.3891167938709259,
        -0.1876533329486847,
        0.689683735370636,
        0.2319503128528595,
        -0.4813312888145447,
        0.14366967976093292,
        0.0923767238855362,
        -0.28907492756843567,
        -1.6119353771209717,
        -0.1870245486497879,
        0.5058407783508301,
        0.38665542006492615,
        -0.3467378616333008,
        -0.5392672419548035,
        0.7335955500602722,
        -0.4846910536289215,
        0.0460234098136425,
        -0.04376541078090668,
        0.7690691947937012,
        0.36899030208587646,
        -0.1717376410961151,
        -0.47377437353134155,
        -0.25661519169807434,
        1.289617896080017,
        -0.03180662542581558,
        -0.02857828326523304,
        -0.2899809777736664,
        -1.0168088674545288,
        -0.29482701420783997,
        0.5194480419158936,
        1.0498110055923462,
        0.13698387145996094,
        -0.06645716726779938,
        -0.3682578504085541,
        0.11614160984754562,
        -0.3207235634326935,
        -0.6969021558761597,
        0.2798677384853363,
        -0.14398863911628723,
        -0.13842955231666565,
        0.8095495104789734,
        -0.3482084572315216,
        -0.24268507957458496,
        0.9089614152908325,
        0.29487136006355286,
        0.10242564976215363,
        0.26875075697898865,
        -0.7310293912887573,
        1.9732329845428467,
        -0.7239517569541931,
        -0.49896159768104553,
        -0.06596057116985321,
        -0.2242446094751358,
        0.46288228034973145,
        0.3763364553451538,
        -0.1805170178413391,
        0.07130935043096542,
        -0.1768709421157837,
        0.1792457103729248,
        -0.01826176419854164,
        -0.4582418203353882,
        0.2802372872829437,
        -0.0751490592956543,
        0.4441833198070526,
        -0.34978920221328735,
        0.10118226706981659,
        0.0370880663394928,
        -0.05054551362991333,
        0.24458245933055878,
        1.0544209480285645,
        -0.37831786274909973,
        -0.42876508831977844,
        -0.8247554898262024
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts references to various forms of cultural and educational content from provided text. This process involves identifying and listing references to art, literature, and academic papers concisely. The expected output is a bulleted list of up to 20 references, each summarized in no more than 15 words.",
          "name": "Extract_references",
          "raw": "\n                workflow Extract_references v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert extractor of references to art, stories, books, literature, papers, and other sources of learning from content.\n\n# Steps\n\nTake the input given and extract all references to art, stories, books, literature, papers, and other sources of learning into a bulleted list.\n\n# OUTPUT INSTRUCTIONS\n\n- Output up to 20 references from the content.\n- Output each into a bullet of no more than 15 words.\n\n# EXAMPLE\n\n- Moby Dick by Herman Melville\n- Superforecasting, by Bill Tetlock\n- Aesop's Fables\n- Rilke's Poetry\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert extractor of references to art, stories, books, literature, papers, and other sources of learning from content.\n\n# Steps\n\nTake the input given and extract all references to art, stories, books, literature, papers, and other sources of learning into a bulleted list.\n\n# OUTPUT INSTRUCTIONS\n\n- Output up to 20 references from the content.\n- Output each into a bullet of no more than 15 words.\n\n# EXAMPLE\n\n- Moby Dick by Herman Melville\n- Superforecasting, by Bill Tetlock\n- Aesop's Fables\n- Rilke's Poetry\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5913006067276001,
        -0.14499019086360931,
        -0.03696482256054878,
        0.2524245083332062,
        0.054968271404504776,
        0.2281879186630249,
        -1.156779170036316,
        -0.12030477821826935,
        -0.3391653299331665,
        0.043418075889348984,
        -0.1215212345123291,
        0.9415571689605713,
        0.4990018904209137,
        -0.4980620741844177,
        -0.006856728345155716,
        -0.0856771171092987,
        0.13874457776546478,
        -0.18423117697238922,
        -1.0418545007705688,
        -0.9664255976676941,
        -0.5819371938705444,
        0.5260387659072876,
        0.1486952304840088,
        -0.11022509634494781,
        0.7291580438613892,
        -0.17672517895698547,
        -0.3811551034450531,
        -0.3763948082923889,
        -1.0258973836898804,
        -0.9703307151794434,
        0.5962156057357788,
        0.17180979251861572,
        0.029365886002779007,
        -0.6488765478134155,
        0.14134566485881805,
        -0.11444273591041565,
        -0.2153356373310089,
        -0.5603712201118469,
        -0.5793212652206421,
        -0.35445302724838257,
        -0.41413116455078125,
        0.2686520218849182,
        -0.021096207201480865,
        -0.6986813545227051,
        0.5514127016067505,
        -0.036532532423734665,
        0.11372628808021545,
        -0.2778697907924652,
        0.5951443910598755,
        0.12655897438526154,
        0.13496233522891998,
        -0.9525924324989319,
        -0.023619702085852623,
        -0.37000134587287903,
        -0.3935559093952179,
        -0.8171905875205994,
        0.056764211505651474,
        -0.24469105899333954,
        0.037818871438503265,
        0.1887829452753067,
        -0.21098676323890686,
        -0.23972102999687195,
        -2.752448320388794,
        -0.09840574860572815,
        -0.2411288619041443,
        0.3424758017063141,
        -0.19732317328453064,
        -0.4502705931663513,
        0.5248624682426453,
        0.21469470858573914,
        -0.6392655372619629,
        -0.05536825209856033,
        -0.46969184279441833,
        -0.22891488671302795,
        -0.36974790692329407,
        -0.18481877446174622,
        0.3307071328163147,
        0.041490670293569565,
        0.4536104202270508,
        -0.47378477454185486,
        -0.5210764408111572,
        0.5139119029045105,
        0.48195961117744446,
        -0.7200798988342285,
        -0.24797135591506958,
        0.8033875823020935,
        -0.23026806116104126,
        0.016654983162879944,
        -0.028020866215229034,
        0.5019192695617676,
        -0.08150018751621246,
        -0.2664265036582947,
        0.0963045284152031,
        0.2227293848991394,
        -0.022603318095207214,
        0.030276965349912643,
        -0.35436955094337463,
        -0.4184388518333435,
        0.06274699419736862,
        3.2642982006073,
        1.4585298299789429,
        0.8050592541694641,
        0.4445081651210785,
        -0.6356716156005859,
        0.2544056177139282,
        -0.328183114528656,
        -0.15106651186943054,
        0.02136487513780594,
        0.6293834447860718,
        -0.24073739349842072,
        0.3196399509906769,
        -0.5395132899284363,
        -0.4381641447544098,
        0.43174245953559875,
        -0.6015914082527161,
        1.182982087135315,
        -0.7831078171730042,
        -0.4784485995769501,
        0.3670075535774231,
        0.36260706186294556,
        -0.40833935141563416,
        -0.12848567962646484,
        -0.36186981201171875,
        -0.44653013348579407,
        -0.42876648902893066,
        -0.467842698097229,
        -0.31679776310920715,
        0.533158540725708,
        0.7556789517402649,
        0.4979266822338104,
        0.09644589573144913,
        0.10646416246891022,
        -0.6814894080162048,
        0.289878249168396,
        0.028670629486441612,
        -0.0533093586564064,
        0.7325907349586487,
        -1.1142034530639648,
        -0.1058133915066719,
        -0.34113597869873047,
        -0.058428674936294556,
        -1.0421068668365479,
        0.8225904107093811,
        -0.10561542212963104,
        0.8871499300003052,
        -0.0032352954149246216,
        0.19600826501846313,
        0.8791643381118774,
        -0.23140330612659454,
        -0.5640462636947632,
        0.017013154923915863,
        0.37606552243232727,
        -0.09731677174568176,
        0.317157506942749,
        0.4765041470527649,
        0.2630716562271118,
        -0.16604840755462646,
        -0.6156153082847595,
        -0.9479163885116577,
        -0.20728251338005066,
        0.08512693643569946,
        0.6722276210784912,
        0.06429988145828247,
        -0.0501333624124527,
        0.43720847368240356,
        -0.3196524679660797,
        -0.12336611747741699,
        -0.08131048828363419,
        0.5407422184944153,
        0.6454409956932068,
        0.3176228106021881,
        -0.19784802198410034,
        0.2716532051563263,
        1.3609766960144043,
        -0.4265636205673218,
        -0.10789233446121216,
        -0.722132682800293,
        0.010771043598651886,
        0.00984366238117218,
        -0.07714134454727173,
        0.3942473530769348,
        0.39929521083831787,
        0.12298047542572021,
        -0.2434423863887787,
        0.1302517056465149,
        0.28568536043167114,
        -0.060304902493953705,
        0.5731752514839172,
        0.7715466022491455,
        1.5464400053024292,
        -0.28773033618927,
        2.218174695968628,
        -0.9872062802314758,
        -0.21980716288089752,
        0.12606897950172424,
        -0.696471095085144,
        0.30264490842819214,
        0.7816369533538818,
        0.9541598558425903,
        0.010865403339266777,
        -0.15237954258918762,
        0.4931355118751526,
        -0.25503864884376526,
        -0.2991357147693634,
        0.04034167900681496,
        -0.48921412229537964,
        -0.28265854716300964,
        0.1273646056652069,
        -0.025513313710689545,
        -1.0515327453613281,
        -0.6389304399490356,
        0.09431315958499908,
        1.0648598670959473,
        0.03481601923704147,
        -0.1636139452457428,
        -0.9032419323921204,
        0.4015238881111145,
        0.13349148631095886,
        -0.2391076683998108,
        0.5377135872840881,
        -0.8816173672676086,
        0.2453908622264862,
        -1.407942771911621,
        -0.44416505098342896,
        -1.1474683284759521,
        1.4925522804260254,
        -0.17270363867282867,
        0.9592637419700623,
        -0.954649806022644,
        -0.6006361246109009,
        0.3643536865711212,
        1.5316840410232544,
        0.7527495622634888,
        1.0259246826171875,
        0.6555975675582886,
        0.5507169961929321,
        -0.11667609959840775,
        0.20409858226776123,
        -0.1825423538684845,
        -0.15567125380039215,
        0.009123597294092178,
        0.2917304039001465,
        -0.5317007899284363,
        1.077688217163086,
        -0.6774612069129944,
        0.11041420698165894,
        -1.294668197631836,
        0.15500053763389587,
        0.17758852243423462,
        1.089767336845398,
        1.2777976989746094,
        -0.27722853422164917,
        -0.1123509407043457,
        0.7678804993629456,
        -0.027674302458763123,
        0.18548879027366638,
        -2.0784568786621094,
        -0.4741814434528351,
        0.12876197695732117,
        0.28909945487976074,
        0.2506546974182129,
        0.04482613503932953,
        -0.012691982090473175,
        -0.02065189927816391,
        0.3266797363758087,
        0.15701891481876373,
        -0.9339573383331299,
        0.0754002258181572,
        -0.4858255088329315,
        0.02226588875055313,
        -0.3147246241569519,
        0.21413642168045044,
        -0.11241359263658524,
        -0.31088772416114807,
        -0.041732676327228546,
        -0.10409364104270935,
        0.6347206830978394,
        0.253744900226593,
        -0.3423595726490021,
        -0.34380289912223816,
        0.6620663404464722,
        0.07196660339832306,
        -0.0873309001326561,
        0.4535622000694275,
        -0.17591068148612976,
        -0.17520102858543396,
        0.16016535460948944,
        -0.7478252649307251,
        0.4796557128429413,
        1.2001514434814453,
        -0.5965505838394165,
        -0.4282839596271515,
        -0.9299404621124268,
        0.052738651633262634,
        1.3593950271606445,
        0.3468466103076935,
        0.1343606859445572,
        1.4590353965759277,
        0.4347169101238251,
        -0.7430610060691833,
        -0.6913784146308899,
        0.10779856145381927,
        -0.10277633368968964,
        -0.1438635140657425,
        -0.5822817087173462,
        -0.156826451420784,
        0.8461058139801025,
        0.3364386260509491,
        -0.5252845287322998,
        0.04340070113539696,
        -1.2387192249298096,
        0.0921059101819992,
        0.0176544226706028,
        -0.18916234374046326,
        0.4150712490081787,
        -1.0141886472702026,
        0.4725736081600189,
        0.9955190420150757,
        -0.3485618531703949,
        -1.5337318181991577,
        -0.4841156601905823,
        0.4502333998680115,
        0.7768552303314209,
        -0.07775256037712097,
        -0.20519863069057465,
        0.7053656578063965,
        -0.42100927233695984,
        0.002756945788860321,
        -0.15136277675628662,
        1.0308263301849365,
        0.17861023545265198,
        -0.0805540606379509,
        -0.03872009366750717,
        -0.11575190722942352,
        0.2761611342430115,
        -0.36323559284210205,
        -0.3511863350868225,
        -0.045876286923885345,
        -0.2621373236179352,
        0.1094260960817337,
        0.5392045974731445,
        1.7179193496704102,
        0.6123633980751038,
        0.40214353799819946,
        0.3040383756160736,
        0.04959921911358833,
        -0.7351383566856384,
        -0.5596084594726562,
        0.8059149980545044,
        -0.4438151717185974,
        -0.03352642059326172,
        1.0914373397827148,
        0.727821946144104,
        -0.6107622385025024,
        0.895710825920105,
        0.05620066449046135,
        -0.406349241733551,
        0.07695566117763519,
        -0.8337977528572083,
        1.7433925867080688,
        -0.6092202663421631,
        -0.002129470929503441,
        -0.24869927763938904,
        0.09669879078865051,
        -0.27305111289024353,
        0.12425399571657181,
        0.2735113799571991,
        -0.5373431444168091,
        -0.10315127670764923,
        0.16830846667289734,
        0.047435253858566284,
        -0.024426592513918877,
        0.44929417967796326,
        0.27212485671043396,
        1.1962695121765137,
        0.09765375405550003,
        -0.05328840762376785,
        0.40055760741233826,
        0.02985183149576187,
        -0.45632147789001465,
        0.5389885902404785,
        -0.06984548270702362,
        -1.1708227396011353,
        -1.0990368127822876
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes and interprets the meaning of songs based on extensive research and lyric examination. This process involves deep analysis of the artist's background, song context, and lyrics to deduce the song's essence. Outputs include a summary sentence, detailed meaning in bullet points, and evidence supporting the interpretation.",
          "name": "Extract_song_meaning",
          "raw": "\n                workflow Extract_song_meaning v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert songwriter and musician that specializes in understanding the meaning of songs.\n\nYou take any input about a song and output what it means.\n\n# GOALS\n\n1. The goals of this exercise is to take in any song name, song lyrics, or other information and output what the song means.\n\n# STEPS\n\n// Study the input you have\n\n- Spend 319 hours researching the song, the lyrics, the artist, any context known about them, and study those deeply.\n\n// Study the lyrics\n\n- Then study the lyrics of the song in question for 614 hours. Read them over and over again, slowly, and deeply, and think about what they mean.\n\n\n# OUTPUT\n\n// Write a summary sentence of what the song is about\n\n- In a section called SUMMARY SENTENCE, write a 25-word summary sentence of what the song is about. \n\n// Write a longer description of what the song is about in bullet points\n\n- In a section called MEANING, write a set of 165-word bullets describing what the song is about. \n\n// Give evidence for your theory\n\n- In a section called EVIDENCE, create a set of 15-word bullets describing why you believe this is the meaning of the song. Include references to the lyrics, comments from the artist, analysis from fans that you're aware of, etc.\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert songwriter and musician that specializes in understanding the meaning of songs.\n\nYou take any input about a song and output what it means.\n\n# GOALS\n\n1. The goals of this exercise is to take in any song name, song lyrics, or other information and output what the song means.\n\n# STEPS\n\n// Study the input you have\n\n- Spend 319 hours researching the song, the lyrics, the artist, any context known about them, and study those deeply.\n\n// Study the lyrics\n\n- Then study the lyrics of the song in question for 614 hours. Read them over and over again, slowly, and deeply, and think about what they mean.\n\n\n# OUTPUT\n\n// Write a summary sentence of what the song is about\n\n- In a section called SUMMARY SENTENCE, write a 25-word summary sentence of what the song is about. \n\n// Write a longer description of what the song is about in bullet points\n\n- In a section called MEANING, write a set of 165-word bullets describing what the song is about. \n\n// Give evidence for your theory\n\n- In a section called EVIDENCE, create a set of 15-word bullets describing why you believe this is the meaning of the song. Include references to the lyrics, comments from the artist, analysis from fans that you're aware of, etc.\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.3876187205314636,
        0.5596853494644165,
        -0.7063372731208801,
        -0.062228478491306305,
        0.07734587788581848,
        0.17985007166862488,
        -1.418657898902893,
        -0.1886240541934967,
        -0.23440751433372498,
        0.3123357892036438,
        -0.3966052830219269,
        0.4224931299686432,
        0.15831677615642548,
        -0.028209887444972992,
        0.1210673451423645,
        -0.013565994799137115,
        -0.4034227430820465,
        -1.0128121376037598,
        -1.2122023105621338,
        -0.7957396507263184,
        0.6465620398521423,
        0.4709043800830841,
        0.2866940498352051,
        -0.03219880163669586,
        0.49068939685821533,
        0.09671422094106674,
        -0.5021059513092041,
        -0.17813754081726074,
        -1.0253899097442627,
        -1.2391655445098877,
        1.3361961841583252,
        0.3187498450279236,
        0.31503310799598694,
        -0.8854316473007202,
        0.2053770124912262,
        -1.0433565378189087,
        -0.37108132243156433,
        -0.20346495509147644,
        -0.647231936454773,
        -0.5643238425254822,
        -0.26096612215042114,
        -0.06489469110965729,
        -0.24373164772987366,
        -0.11572906374931335,
        -0.029634082689881325,
        0.18698027729988098,
        0.6549640893936157,
        -0.17642292380332947,
        0.9132245779037476,
        0.23920157551765442,
        0.07893890142440796,
        -0.28318843245506287,
        0.3035756051540375,
        -0.6876488924026489,
        -0.31021639704704285,
        -1.0189979076385498,
        -0.27144330739974976,
        0.06771662831306458,
        -0.1171170175075531,
        -0.1927637904882431,
        0.7773950099945068,
        -0.11159791052341461,
        -2.977659225463867,
        0.37182146310806274,
        -0.14555422961711884,
        0.050362519919872284,
        0.3287004232406616,
        0.3258850872516632,
        -0.04015706479549408,
        0.362649530172348,
        -0.4284815788269043,
        0.13053682446479797,
        -0.4280725121498108,
        0.2872256636619568,
        0.34665319323539734,
        -0.001960083842277527,
        0.36867982149124146,
        -0.09437654912471771,
        0.36326533555984497,
        -0.5058818459510803,
        -0.09512770175933838,
        0.9467580914497375,
        0.08599173277616501,
        -0.7472000122070312,
        -0.01646924577653408,
        0.5383150577545166,
        -0.42540016770362854,
        -0.02992291748523712,
        0.0038418620824813843,
        0.46778690814971924,
        -0.15840274095535278,
        -0.33579742908477783,
        0.9489392042160034,
        -0.017916137352585793,
        -0.6319901347160339,
        -0.013562783598899841,
        -0.562617301940918,
        -0.21559423208236694,
        -0.2663033902645111,
        3.350083589553833,
        1.3084746599197388,
        0.36448749899864197,
        -0.3504529595375061,
        -1.041265845298767,
        0.2830703556537628,
        -0.6515385508537292,
        -0.19171583652496338,
        0.5503695011138916,
        0.31989216804504395,
        0.10323416441679001,
        0.0234062522649765,
        -0.9303621053695679,
        -0.06952852010726929,
        -0.5105400681495667,
        -0.6952085494995117,
        1.5066399574279785,
        -0.6942769885063171,
        -0.0868612676858902,
        0.10346316546201706,
        1.164406657218933,
        -0.349565714597702,
        -0.09381602704524994,
        -0.024513287469744682,
        -0.35687845945358276,
        -0.1122511476278305,
        -0.5748211145401001,
        -0.38189589977264404,
        0.667287290096283,
        0.622810959815979,
        0.28188401460647583,
        0.2935839891433716,
        0.11548247933387756,
        -0.989467203617096,
        0.27164050936698914,
        0.698501706123352,
        -0.3706546127796173,
        -0.14002089202404022,
        -0.382271409034729,
        0.42025235295295715,
        -0.5900974869728088,
        0.15809041261672974,
        -0.4216223359107971,
        0.7387702465057373,
        0.21099437773227692,
        0.4901127815246582,
        0.13500919938087463,
        0.26863229274749756,
        0.4587765336036682,
        -0.5616443157196045,
        -0.3420029282569885,
        0.3751857578754425,
        0.5514196157455444,
        0.1251666247844696,
        0.3052932620048523,
        0.5586941242218018,
        0.45403680205345154,
        -0.2112562209367752,
        -0.7413839101791382,
        -0.8922045826911926,
        0.07208595424890518,
        0.5089315176010132,
        -0.20049594342708588,
        -0.17834657430648804,
        -0.46235382556915283,
        0.5475071668624878,
        -0.027889829128980637,
        0.34158825874328613,
        -0.1850356161594391,
        0.6976515650749207,
        0.22765383124351501,
        0.33512669801712036,
        -0.2736976742744446,
        0.6595903635025024,
        0.5787546634674072,
        0.07465717941522598,
        -0.11332045495510101,
        -0.30591756105422974,
        0.28589996695518494,
        0.4365180730819702,
        0.3142659366130829,
        0.7496120929718018,
        0.525276780128479,
        -0.24329473078250885,
        -0.6237723231315613,
        -0.4546920657157898,
        0.8256840705871582,
        0.03213750198483467,
        0.39038828015327454,
        0.8374470472335815,
        0.36816567182540894,
        -0.48146072030067444,
        2.176682472229004,
        -0.392253577709198,
        -0.3109629154205322,
        0.1704137772321701,
        -0.387615442276001,
        0.15993130207061768,
        0.33872219920158386,
        0.623309850692749,
        0.038679901510477066,
        -1.1280430555343628,
        -0.3123416602611542,
        -0.07300610840320587,
        -0.5446885824203491,
        -0.6005077362060547,
        -0.47999128699302673,
        0.08311444520950317,
        0.3710069954395294,
        0.4615634083747864,
        -0.8087379336357117,
        -0.026271015405654907,
        0.09679754823446274,
        1.0806725025177002,
        -0.042087435722351074,
        0.591978907585144,
        -0.34318840503692627,
        0.4668496251106262,
        -0.04412844404578209,
        0.10792355239391327,
        0.07373305410146713,
        -0.1910461187362671,
        0.26019179821014404,
        -1.1784037351608276,
        -0.5530645847320557,
        -0.7071288824081421,
        0.5229123830795288,
        -0.395050048828125,
        0.8024245500564575,
        -0.17497295141220093,
        -0.27274730801582336,
        0.47738200426101685,
        0.8759686946868896,
        0.3936753273010254,
        1.2320269346237183,
        0.7605602145195007,
        0.1770111620426178,
        -0.4916774034500122,
        -0.09595918655395508,
        0.3677719831466675,
        -0.7688038945198059,
        0.44918882846832275,
        -0.5023623704910278,
        -0.07766863703727722,
        0.6879661679267883,
        0.11776348948478699,
        0.13768428564071655,
        -0.028141755610704422,
        0.18474474549293518,
        -0.6547803282737732,
        1.8867454528808594,
        0.7094971537590027,
        -0.3574621379375458,
        -0.09509076178073883,
        0.9342326521873474,
        -0.07372916489839554,
        -0.26539725065231323,
        -1.9508728981018066,
        -0.1284896731376648,
        0.24842560291290283,
        0.36298564076423645,
        -0.7746775150299072,
        -0.15780267119407654,
        0.13734471797943115,
        -0.30580705404281616,
        0.016383090987801552,
        -0.1311144381761551,
        -0.8768600225448608,
        -0.20206758379936218,
        -0.6447031497955322,
        -0.4696047306060791,
        -0.6685184836387634,
        0.24537910521030426,
        -0.16554801166057587,
        -0.41419920325279236,
        -0.04080108553171158,
        0.44408929347991943,
        0.12556809186935425,
        0.7667894959449768,
        -0.9643351435661316,
        -0.37967124581336975,
        0.32216745615005493,
        0.046483248472213745,
        0.025514811277389526,
        0.5075788497924805,
        -0.47941944003105164,
        -0.0006510801613330841,
        -0.6795370578765869,
        -1.139923095703125,
        -0.28366509079933167,
        0.9424829483032227,
        -0.5230963230133057,
        -0.40958717465400696,
        -0.4694346487522125,
        0.08126197010278702,
        1.1589399576187134,
        -0.09502750635147095,
        -0.6847972273826599,
        0.7921777963638306,
        0.32791194319725037,
        0.10940134525299072,
        -0.4706737697124481,
        0.7547852396965027,
        0.2031039595603943,
        0.6134576201438904,
        -0.7512624263763428,
        -0.19295614957809448,
        0.8012657761573792,
        0.2720891237258911,
        -0.273840993642807,
        0.2923048734664917,
        -0.652547299861908,
        0.37105587124824524,
        0.20075643062591553,
        -0.12112694978713989,
        0.8193140625953674,
        -0.31239861249923706,
        0.2514352798461914,
        1.0692431926727295,
        -0.18570441007614136,
        -1.6220232248306274,
        -0.6570612192153931,
        0.511285662651062,
        -0.08789551258087158,
        0.07333087921142578,
        -0.7177153825759888,
        0.39455825090408325,
        -1.099707007408142,
        -0.03507624939084053,
        -0.04995817691087723,
        0.7483643293380737,
        0.41732069849967957,
        0.06611913442611694,
        -0.025077447295188904,
        0.23377299308776855,
        0.901175856590271,
        -0.008269507437944412,
        -0.09692889451980591,
        0.1752144992351532,
        -0.576498806476593,
        -0.1200903058052063,
        -0.4535011649131775,
        1.4382236003875732,
        0.0629621148109436,
        0.26918286085128784,
        -0.30333611369132996,
        0.14951568841934204,
        -0.3792572021484375,
        -0.6626365184783936,
        0.7152718305587769,
        -0.17777252197265625,
        -0.4186515510082245,
        1.1415185928344727,
        0.1468656212091446,
        -0.31779491901397705,
        0.7771966457366943,
        0.5025442838668823,
        -0.5275383591651917,
        -0.494802862405777,
        -0.8382145762443542,
        1.5395987033843994,
        0.02725275792181492,
        -0.047810956835746765,
        -0.3064884841442108,
        -0.06001689285039902,
        0.14704667031764984,
        0.1290329545736313,
        -0.0644700825214386,
        0.0230877622961998,
        0.12696756422519684,
        -0.31898951530456543,
        0.5794233679771423,
        -0.1071772500872612,
        0.15814277529716492,
        0.42716628313064575,
        0.6226826906204224,
        -0.23646913468837738,
        0.16392290592193604,
        0.2865433692932129,
        -0.06131325662136078,
        -0.6425634026527405,
        0.8428584933280945,
        0.1576712727546692,
        -0.9311563372612,
        -0.3682217597961426
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Identifies and distinguishes between official and potential sponsors from transcripts. This process involves analyzing content to separate actual sponsors from merely mentioned companies. The output lists official sponsors and potential sponsors based on their mention in the content.",
          "name": "Extract_sponsors",
          "raw": "\n                workflow Extract_sponsors v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting the sponsors and potential sponsors from a given transcript, such a from a podcast, video transcript, essay, or whatever.\n\n# Steps\n\n- Consume the whole transcript so you understand what is content, what is meta information, etc.\n- Discern the difference between companies that were mentioned and companies that actually sponsored the podcast or video.\n- Output the following:\n\n## OFFICIAL SPONSORS\n\n- $SOURCE_CHANNEL$ | $SPONSOR1$ | $SPONSOR1_DESCRIPTION$ | $SPONSOR1_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR2$ | $SPONSOR2_DESCRIPTION$ | $SPONSOR2_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR3$ | $SPONSOR3_DESCRIPTION$ | $SPONSOR3_LINK$\n- And so on…\n\n## POTENTIAL SPONSORS\n\n- $SOURCE_CHANNEL$ | $SPONSOR1$ | $SPONSOR1_DESCRIPTION$ | $SPONSOR1_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR2$ | $SPONSOR2_DESCRIPTION$ | $SPONSOR2_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR3$ | $SPONSOR3_DESCRIPTION$ | $SPONSOR3_LINK$\n- And so on…\n\n# EXAMPLE OUTPUT\n\n## OFFICIAL SPONSORS\n\n- AI Jason's YouTube Channel | Flair | Flair is a threat intel platform powered by AI. | https://flair.ai\n- Matthew Berman's YouTube Channel | Weaviate | Weviate is an open-source knowledge graph powered by ML. | https://weaviate.com\n- Unsupervised Learning Website | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n- The AI Junkie Podcast | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n\n## POTENTIAL SPONSORS\n\n- AI Jason's YouTube Channel | Flair | Flair is a threat intel platform powered by AI. | https://flair.ai\n- Matthew Berman's YouTube Channel | Weaviate | Weviate is an open-source knowledge graph powered by ML. | https://weaviate.com\n- Unsupervised Learning Website | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n- The AI Junkie Podcast | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n\n## END EXAMPLE OUTPUT\n\n# OUTPUT INSTRUCTIONS\n\n- The official sponsor list should only include companies that officially sponsored the content in question.\n- The potential sponsor list should include companies that were mentioned during the content but that didn't officially sponsor.\n- Do not include companies in the output that were not mentioned in the content.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting the sponsors and potential sponsors from a given transcript, such a from a podcast, video transcript, essay, or whatever.\n\n# Steps\n\n- Consume the whole transcript so you understand what is content, what is meta information, etc.\n- Discern the difference between companies that were mentioned and companies that actually sponsored the podcast or video.\n- Output the following:\n\n## OFFICIAL SPONSORS\n\n- $SOURCE_CHANNEL$ | $SPONSOR1$ | $SPONSOR1_DESCRIPTION$ | $SPONSOR1_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR2$ | $SPONSOR2_DESCRIPTION$ | $SPONSOR2_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR3$ | $SPONSOR3_DESCRIPTION$ | $SPONSOR3_LINK$\n- And so on…\n\n## POTENTIAL SPONSORS\n\n- $SOURCE_CHANNEL$ | $SPONSOR1$ | $SPONSOR1_DESCRIPTION$ | $SPONSOR1_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR2$ | $SPONSOR2_DESCRIPTION$ | $SPONSOR2_LINK$\n- $SOURCE_CHANNEL$ | $SPONSOR3$ | $SPONSOR3_DESCRIPTION$ | $SPONSOR3_LINK$\n- And so on…\n\n# EXAMPLE OUTPUT\n\n## OFFICIAL SPONSORS\n\n- AI Jason's YouTube Channel | Flair | Flair is a threat intel platform powered by AI. | https://flair.ai\n- Matthew Berman's YouTube Channel | Weaviate | Weviate is an open-source knowledge graph powered by ML. | https://weaviate.com\n- Unsupervised Learning Website | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n- The AI Junkie Podcast | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n\n## POTENTIAL SPONSORS\n\n- AI Jason's YouTube Channel | Flair | Flair is a threat intel platform powered by AI. | https://flair.ai\n- Matthew Berman's YouTube Channel | Weaviate | Weviate is an open-source knowledge graph powered by ML. | https://weaviate.com\n- Unsupervised Learning Website | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n- The AI Junkie Podcast | JunaAI | JunaAI is a platform for AI-powered content creation. | https://junaai.com\n\n## END EXAMPLE OUTPUT\n\n# OUTPUT INSTRUCTIONS\n\n- The official sponsor list should only include companies that officially sponsored the content in question.\n- The potential sponsor list should include companies that were mentioned during the content but that didn't officially sponsor.\n- Do not include companies in the output that were not mentioned in the content.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.31212714314460754,
        -0.21653415262699127,
        -0.9381476640701294,
        0.03682512789964676,
        -0.34235474467277527,
        -0.28358033299446106,
        -1.53019380569458,
        0.262796550989151,
        -0.05059024319052696,
        0.601492166519165,
        0.021494800224900246,
        1.1901721954345703,
        0.6438047289848328,
        -0.032205961644649506,
        -0.44554418325424194,
        -0.5055491924285889,
        0.33096635341644287,
        -0.4668048620223999,
        -0.9977514147758484,
        -1.375166416168213,
        0.03342849761247635,
        0.30549439787864685,
        0.5875803828239441,
        -0.13757020235061646,
        0.5016192197799683,
        -0.17239342629909515,
        -0.4846784174442291,
        -0.3680098056793213,
        -1.5405858755111694,
        -1.2377976179122925,
        0.9812595844268799,
        0.8734968900680542,
        -0.6128574013710022,
        -0.6369974613189697,
        0.3558369576931,
        -1.1875088214874268,
        -0.0029400645289570093,
        -0.5015426874160767,
        -0.13232959806919098,
        0.040688276290893555,
        -0.0071755386888980865,
        -0.061532095074653625,
        -0.00016771256923675537,
        -0.9729443192481995,
        0.0019328277558088303,
        0.05326460301876068,
        0.6449927091598511,
        -0.2221461683511734,
        1.3751407861709595,
        0.21919944882392883,
        0.022621992975473404,
        -0.20341552793979645,
        -0.07398749887943268,
        -0.7987842559814453,
        -0.5205904245376587,
        -0.39327380061149597,
        -0.8800442814826965,
        -0.18599657714366913,
        0.009797116741538048,
        -0.28305214643478394,
        0.34647223353385925,
        0.1177884191274643,
        -3.4286105632781982,
        -0.7402123212814331,
        0.27277672290802,
        -0.08116128295660019,
        0.4073776602745056,
        -0.05285210534930229,
        0.7970397472381592,
        0.007168501615524292,
        -0.4284951686859131,
        0.3433673083782196,
        -0.8447158336639404,
        0.07799176126718521,
        -0.06925689429044724,
        0.20210236310958862,
        0.7239480018615723,
        -0.23796644806861877,
        0.1705748736858368,
        -0.4769524335861206,
        0.688646137714386,
        0.11441712826490402,
        0.1294156163930893,
        -0.07075611501932144,
        -0.3274100422859192,
        0.6565304398536682,
        -0.7415469288825989,
        0.013468354940414429,
        -0.5198433995246887,
        0.5287419557571411,
        -0.1390863060951233,
        -0.6275626420974731,
        -0.24938814342021942,
        0.4012245237827301,
        -0.07613515108823776,
        -0.2041092962026596,
        -0.6796707510948181,
        -0.17972856760025024,
        0.06553466618061066,
        3.111717700958252,
        1.2264143228530884,
        0.13225805759429932,
        0.09139970690011978,
        -1.444390892982483,
        0.15623429417610168,
        -0.05418664962053299,
        -0.10533322393894196,
        0.4324125647544861,
        0.3021446764469147,
        -0.1613338142633438,
        0.2745402157306671,
        -0.32059216499328613,
        0.6735665798187256,
        0.32107117772102356,
        -0.04678402096033096,
        0.8892604112625122,
        -0.31773871183395386,
        0.05568237602710724,
        0.505146861076355,
        0.13604500889778137,
        -0.2454736828804016,
        -0.29619354009628296,
        -0.14646892249584198,
        -0.7887681126594543,
        -0.44255390763282776,
        -0.32378169894218445,
        -0.2587100565433502,
        0.568240761756897,
        0.3848556876182556,
        0.03185410052537918,
        0.5922499895095825,
        -0.5790313482284546,
        -0.7123727798461914,
        0.24972955882549286,
        0.581687867641449,
        -0.12740305066108704,
        0.0012089833617210388,
        -0.2523045539855957,
        -0.024878695607185364,
        -0.5314378142356873,
        -0.1909853219985962,
        -0.814920961856842,
        0.5426211953163147,
        0.21968045830726624,
        0.09387116879224777,
        0.7809939980506897,
        0.06965199112892151,
        0.3608226478099823,
        -0.2328857183456421,
        -0.5565459132194519,
        0.15673115849494934,
        0.6023886799812317,
        0.18173791468143463,
        0.2613196074962616,
        0.8587130308151245,
        0.03407466784119606,
        0.28841495513916016,
        -0.4676551818847656,
        -0.5496428608894348,
        0.10826322436332703,
        0.14344315230846405,
        -0.3924693167209625,
        0.0343831405043602,
        0.7459536194801331,
        0.23249584436416626,
        -0.4900118410587311,
        0.024130836129188538,
        -0.09150432795286179,
        0.6037530899047852,
        0.2398224025964737,
        0.22152292728424072,
        -0.13042977452278137,
        0.6297222375869751,
        0.4158427119255066,
        -0.5475015640258789,
        0.1495560258626938,
        -1.2693437337875366,
        0.2546265721321106,
        0.5370355844497681,
        0.2899250090122223,
        1.0502740144729614,
        0.4788183867931366,
        0.09393826127052307,
        -0.5259455442428589,
        -0.5434539318084717,
        0.27304911613464355,
        0.42453616857528687,
        0.43947649002075195,
        1.252988576889038,
        0.5414300560951233,
        -1.1807208061218262,
        1.7435216903686523,
        -0.9175330400466919,
        -0.398710161447525,
        -0.0052297767251729965,
        0.07949260622262955,
        0.19598053395748138,
        0.7891330122947693,
        0.693245530128479,
        0.039758455008268356,
        -0.7523903250694275,
        -1.0772027969360352,
        0.166535884141922,
        -0.2658780515193939,
        -0.8955097794532776,
        -0.12379756569862366,
        0.5937095880508423,
        -0.097296342253685,
        -0.5855332612991333,
        -1.1865535974502563,
        -0.11775793135166168,
        0.18680605292320251,
        0.8201212882995605,
        0.24333736300468445,
        0.22594742476940155,
        -0.028715122491121292,
        0.07520568370819092,
        -0.04577801749110222,
        -0.3587517738342285,
        0.24909664690494537,
        -0.06908377259969711,
        0.245642751455307,
        -0.5056390166282654,
        -0.6800712943077087,
        -0.7604473233222961,
        1.0832786560058594,
        -0.7879114151000977,
        0.9986485242843628,
        -0.680029034614563,
        -0.25550171732902527,
        0.43965989351272583,
        0.7264263033866882,
        0.6944773197174072,
        1.2685738801956177,
        0.5671421885490417,
        0.23657315969467163,
        0.10483889281749725,
        -0.06093764305114746,
        0.0600455105304718,
        -0.5466982126235962,
        0.2642776668071747,
        0.23031479120254517,
        -0.13711802661418915,
        0.2161969691514969,
        -0.45438098907470703,
        -0.2850775718688965,
        0.4214264452457428,
        -0.046533986926078796,
        -0.5953961610794067,
        1.2995715141296387,
        0.7166168689727783,
        0.08937503397464752,
        -0.2558034360408783,
        0.5359947681427002,
        0.28196319937705994,
        -0.10679728537797928,
        -2.154921770095825,
        0.4082063138484955,
        0.143161803483963,
        0.4016096591949463,
        -0.197040855884552,
        0.24620118737220764,
        0.1794009953737259,
        0.4158492386341095,
        -0.511441171169281,
        0.07480242103338242,
        -1.2303643226623535,
        -0.7625769376754761,
        -0.4022984802722931,
        -0.15940633416175842,
        -0.27180275321006775,
        0.6614901423454285,
        0.3517839312553406,
        -0.48947787284851074,
        -0.608555018901825,
        1.0534237623214722,
        0.24281898140907288,
        0.041532110422849655,
        -0.5753107666969299,
        0.11911097168922424,
        1.1406863927841187,
        0.21541710197925568,
        -0.0823853388428688,
        0.40472930669784546,
        -0.20665666460990906,
        -0.05221571773290634,
        -0.23898565769195557,
        -0.03154999017715454,
        -0.5942428112030029,
        0.9040011763572693,
        -0.8604594469070435,
        -0.679138720035553,
        -0.6385738253593445,
        0.5692232847213745,
        0.9877077341079712,
        0.6514517664909363,
        -0.0690988302230835,
        0.5117525458335876,
        0.06423602998256683,
        -0.38709062337875366,
        -0.9766308665275574,
        0.9707744717597961,
        0.8016841411590576,
        0.2390204221010208,
        -0.4718471169471741,
        -0.9744877219200134,
        0.8263870477676392,
        0.23511698842048645,
        -0.5303550362586975,
        -0.31588014960289,
        -0.27777740359306335,
        0.3139829933643341,
        -0.2052135020494461,
        -0.3289949297904968,
        -0.1156102791428566,
        -0.3776443302631378,
        -0.5155397057533264,
        0.5261008739471436,
        -0.07876905798912048,
        -1.3627182245254517,
        -0.2747694253921509,
        0.4466700851917267,
        0.19303670525550842,
        -0.2954021990299225,
        -0.38471120595932007,
        1.23088538646698,
        -0.26848530769348145,
        0.06427676230669022,
        0.002549678087234497,
        0.9341477155685425,
        0.6903576254844666,
        -0.009162837639451027,
        0.7497840523719788,
        0.421032577753067,
        0.5579319596290588,
        0.03092280402779579,
        -0.37569430470466614,
        -0.4513680636882782,
        0.23181012272834778,
        -0.12033522129058838,
        -0.04570642113685608,
        1.2688548564910889,
        0.34734588861465454,
        -0.009000830352306366,
        0.2784474194049835,
        0.33496367931365967,
        0.16418711841106415,
        -1.2854375839233398,
        0.5347957611083984,
        -0.23540158569812775,
        -0.45396339893341064,
        0.558391809463501,
        0.282005250453949,
        -0.4865407943725586,
        0.8783172369003296,
        0.37611570954322815,
        -0.40704408288002014,
        -0.042108118534088135,
        -0.7350698709487915,
        1.1716411113739014,
        -0.13765962421894073,
        0.09661192446947098,
        -0.6277814507484436,
        0.1639154851436615,
        0.46871596574783325,
        0.2564500570297241,
        0.14312699437141418,
        -0.15731078386306763,
        -0.49796372652053833,
        0.25144296884536743,
        0.7310279607772827,
        -0.3187006711959839,
        -0.04915435612201691,
        0.30701109766960144,
        0.6623947024345398,
        0.5403175950050354,
        0.20901528000831604,
        0.5462281703948975,
        0.4201347529888153,
        0.32441651821136475,
        0.7204898595809937,
        0.7965923547744751,
        -0.7603272199630737,
        -0.2945457100868225
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts video IDs from URLs for use in other applications. It meticulously analyzes the URL to isolate the video ID. The output is solely the video ID, with no additional information or errors included.",
          "name": "Extract_videoid",
          "raw": "\n                workflow Extract_videoid v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting video IDs from any URL so they can be passed on to other applications.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Read the whole URL so you fully understand its components\n\n- Find the portion of the URL that identifies the video ID\n\n- Output just that video ID by itself\n\n# OUTPUT INSTRUCTIONS\n\n- Output the video ID by itself with NOTHING else included\n- Do not output any warnings or errors or notes—just the output.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at extracting video IDs from any URL so they can be passed on to other applications.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# STEPS\n\n- Read the whole URL so you fully understand its components\n\n- Find the portion of the URL that identifies the video ID\n\n- Output just that video ID by itself\n\n# OUTPUT INSTRUCTIONS\n\n- Output the video ID by itself with NOTHING else included\n- Do not output any warnings or errors or notes—just the output.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4350784420967102,
        0.6025615930557251,
        -0.6576680541038513,
        -0.30581194162368774,
        0.38145387172698975,
        0.09145298600196838,
        -0.9364386796951294,
        -0.23353973031044006,
        -0.04494355618953705,
        0.5974947810173035,
        0.13771013915538788,
        1.048201560974121,
        0.3533380627632141,
        0.09219090640544891,
        0.2823027968406677,
        0.21250218152999878,
        -0.288815438747406,
        -0.5672962069511414,
        -1.926009178161621,
        -0.5403242707252502,
        0.1454838216304779,
        0.8480207324028015,
        0.24413847923278809,
        0.1002470850944519,
        -0.07606930285692215,
        0.07384588569402695,
        -0.8810078501701355,
        -0.30109071731567383,
        -0.15064164996147156,
        -1.5561522245407104,
        1.0173006057739258,
        0.4365755617618561,
        -0.37558606266975403,
        -0.5970739722251892,
        -0.3185446858406067,
        -0.3221549987792969,
        -0.2070959359407425,
        -0.21710491180419922,
        -0.2670079171657562,
        -0.2185804396867752,
        0.08424050360918045,
        0.10750187188386917,
        -0.4789635241031647,
        -0.4109843671321869,
        0.2646762728691101,
        -0.11312569677829742,
        0.2013544887304306,
        0.05394483357667923,
        0.8601882457733154,
        0.38431990146636963,
        -0.29123997688293457,
        -0.2732202708721161,
        -0.2804500162601471,
        -0.34470367431640625,
        -0.49367478489875793,
        -0.19361619651317596,
        0.018415797501802444,
        -0.5899836421012878,
        0.06778134405612946,
        0.26868757605552673,
        0.8997108936309814,
        0.3943520486354828,
        -2.928870439529419,
        -0.3005862236022949,
        0.01283493172377348,
        -0.12042044848203659,
        0.383167564868927,
        -0.61795973777771,
        -0.08752588927745819,
        0.03624725341796875,
        0.032412655651569366,
        -0.20659886300563812,
        -0.21136826276779175,
        -0.37840986251831055,
        0.2384321689605713,
        0.11122173815965652,
        0.5987809300422668,
        -0.03149481117725372,
        0.735491931438446,
        -0.14027661085128784,
        0.4513305425643921,
        0.7428210377693176,
        0.11988385021686554,
        0.2515326738357544,
        -0.661251962184906,
        0.2577870488166809,
        -0.347053587436676,
        -0.7045526504516602,
        0.3499787151813507,
        0.10519479215145111,
        0.006105685606598854,
        -0.755791425704956,
        0.3516772389411926,
        0.18877726793289185,
        -0.43209850788116455,
        -0.022200748324394226,
        -0.2770625948905945,
        0.23152083158493042,
        0.14057715237140656,
        3.69189453125,
        0.8246589303016663,
        0.1707819551229477,
        0.2907044291496277,
        -0.33412179350852966,
        0.4826861321926117,
        -0.8120517730712891,
        -0.26047828793525696,
        -0.5508869886398315,
        0.2562260329723358,
        0.06902382522821426,
        0.5346775650978088,
        -0.3649499714374542,
        -0.07043719291687012,
        -0.11230223625898361,
        -0.08698207885026932,
        0.367707222700119,
        -0.717007577419281,
        0.03240899369120598,
        0.6125983595848083,
        0.8690817356109619,
        -0.31309717893600464,
        0.1027413159608841,
        -0.2806679606437683,
        -0.2196660041809082,
        0.23968161642551422,
        0.10740473121404648,
        -0.4400877356529236,
        0.7030780911445618,
        0.18934263288974762,
        0.2746793329715729,
        0.2131991982460022,
        0.4308289885520935,
        -0.7590901851654053,
        0.3005605936050415,
        0.24215209484100342,
        -0.24826288223266602,
        0.2945003807544708,
        -0.6252835392951965,
        -0.0768066868185997,
        -0.7862159609794617,
        -0.23274970054626465,
        -0.6666523218154907,
        -0.020549535751342773,
        -0.07732676714658737,
        1.0713402032852173,
        0.6106961369514465,
        0.06673871725797653,
        -0.15267568826675415,
        0.007921181619167328,
        -0.6548333168029785,
        0.10003349184989929,
        0.6365302205085754,
        -0.15286453068256378,
        -0.017982978373765945,
        0.32964086532592773,
        0.333952397108078,
        -0.4290624260902405,
        -0.5158101916313171,
        -1.1061042547225952,
        0.31275084614753723,
        0.11052879691123962,
        -0.15499688684940338,
        0.6490375995635986,
        0.18743830919265747,
        0.025906957685947418,
        0.04116230458021164,
        0.7172679305076599,
        -0.04087476432323456,
        0.26645392179489136,
        0.08510032296180725,
        0.34461691975593567,
        -0.006333202123641968,
        0.48547860980033875,
        0.782710611820221,
        -0.5026103258132935,
        -0.21013076603412628,
        -0.1755915880203247,
        0.1929444670677185,
        0.6230393648147583,
        -0.6962661743164062,
        0.34993642568588257,
        0.7603070735931396,
        -0.10077394545078278,
        -0.6917691230773926,
        -0.5234202146530151,
        0.49653494358062744,
        -0.03439170867204666,
        0.11665874719619751,
        0.7180476784706116,
        0.860670804977417,
        -0.993655800819397,
        1.8365775346755981,
        -0.02246139571070671,
        -0.21246683597564697,
        0.4260544776916504,
        0.2582892179489136,
        -0.16522619128227234,
        0.3679380416870117,
        0.5008891224861145,
        -0.07912494242191315,
        -0.07977322489023209,
        0.1398412436246872,
        -0.3165307939052582,
        -0.2971612215042114,
        -0.2972654402256012,
        -0.9150702953338623,
        0.14992837607860565,
        0.2814083993434906,
        -0.5682252049446106,
        -0.5554599165916443,
        -0.1763152927160263,
        0.266609787940979,
        1.483377456665039,
        0.22857901453971863,
        0.9474743604660034,
        0.2609088718891144,
        0.49472668766975403,
        -0.2604106366634369,
        0.49586620926856995,
        0.3414255976676941,
        0.015283964574337006,
        -0.09693492203950882,
        -0.8351790308952332,
        -0.8559321165084839,
        -1.1418403387069702,
        0.2928272783756256,
        -0.6514022946357727,
        -0.014402907341718674,
        -0.6731951832771301,
        0.1331617832183838,
        0.3079891800880432,
        0.9873802065849304,
        1.0456421375274658,
        1.1023837327957153,
        0.3994195759296417,
        0.35934919118881226,
        -0.18430617451667786,
        0.8703428506851196,
        0.03588048368692398,
        -0.4168890118598938,
        1.0213840007781982,
        -0.271198034286499,
        -0.34548264741897583,
        0.4574742317199707,
        -0.23237621784210205,
        0.5626087188720703,
        -0.8295580744743347,
        0.11646929383277893,
        0.33470067381858826,
        1.4260187149047852,
        0.5039807558059692,
        0.08300171792507172,
        0.6179227232933044,
        0.2691170275211334,
        0.1673905849456787,
        -0.23066312074661255,
        -1.3376871347427368,
        0.4439547657966614,
        -0.17013847827911377,
        0.11274994909763336,
        -0.19234701991081238,
        0.015146225690841675,
        0.08984088897705078,
        0.20169320702552795,
        0.20267370343208313,
        -0.5240859389305115,
        -1.1893844604492188,
        -0.4233653247356415,
        -0.5686563849449158,
        -0.5249450206756592,
        -0.19538582861423492,
        0.12792687118053436,
        -0.43929290771484375,
        -0.6334156394004822,
        -0.342102974653244,
        0.6875191926956177,
        0.31173649430274963,
        0.49644654989242554,
        -0.61250901222229,
        -0.7078030109405518,
        0.38785266876220703,
        0.2567939758300781,
        0.08665762841701508,
        0.02500215172767639,
        -0.11909007281064987,
        0.34626901149749756,
        -1.1705093383789062,
        -0.9008904099464417,
        -0.5101974010467529,
        0.6992234587669373,
        -0.784939706325531,
        -1.219498872756958,
        -0.4580659866333008,
        0.1306755542755127,
        1.3903441429138184,
        0.5023385286331177,
        0.05262031778693199,
        0.4917563498020172,
        0.38660359382629395,
        -0.574586033821106,
        0.03052108734846115,
        0.09672509878873825,
        0.19903162121772766,
        -0.24380376935005188,
        -0.4257911443710327,
        -0.6190218925476074,
        0.4463192820549011,
        0.13203164935112,
        -0.3244597017765045,
        -0.19999991357326508,
        -1.034489631652832,
        0.22183066606521606,
        0.4032105803489685,
        -0.08487476408481598,
        0.6849505305290222,
        -0.8223379254341125,
        0.6974459290504456,
        0.31980037689208984,
        0.17016835510730743,
        -1.9353058338165283,
        -0.3900829255580902,
        0.2664426565170288,
        0.7404388785362244,
        -0.3156108558177948,
        -0.14825190603733063,
        0.3935864567756653,
        -0.2839529812335968,
        -0.24587681889533997,
        -0.2529951333999634,
        1.4828143119812012,
        0.37661901116371155,
        -0.09276676923036575,
        -0.0008026808500289917,
        -0.27679532766342163,
        0.5778469443321228,
        -0.2487972378730774,
        0.16585610806941986,
        -0.45065006613731384,
        -0.5119638442993164,
        -0.45221567153930664,
        0.4303034543991089,
        1.3213484287261963,
        0.039460014551877975,
        0.2319130152463913,
        0.1898421347141266,
        -0.002751477062702179,
        -0.9338061213493347,
        -1.513501763343811,
        0.22103570401668549,
        0.4579242467880249,
        -0.4030420482158661,
        0.5116052031517029,
        0.13823699951171875,
        -0.25000301003456116,
        0.652019202709198,
        0.7400745153427124,
        -0.21116207540035248,
        0.1407921463251114,
        -0.7102701663970947,
        1.6733906269073486,
        0.022463945671916008,
        -0.5507676005363464,
        -0.3840709626674652,
        0.6325283646583557,
        -0.2648788094520569,
        -0.018478170037269592,
        0.4200544059276581,
        -0.6338688731193542,
        -0.2847293019294739,
        -0.17859891057014465,
        0.4364267587661743,
        -0.19917675852775574,
        0.7739908695220947,
        -0.09366641938686371,
        0.7283880710601807,
        0.20557281374931335,
        0.3692590892314911,
        0.525479793548584,
        0.602553129196167,
        -0.48131030797958374,
        0.5437514185905457,
        -0.5437513589859009,
        -0.6988688707351685,
        -1.0131632089614868
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt outlines a complex process for extracting insights from text content, focusing on themes like the meaning of life and technology's impact on humanity. It involves creating teams of AI agents with diverse expertise to analyze the content and produce summaries, ideas, insights, quotes, habits, facts, references, and recommendations. The expected output includes structured sections filled with concise, insightful entries derived from the input material.",
          "name": "Extract_wisdom_agents",
          "raw": "\n                workflow Extract_wisdom_agents v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an advanced AI system that coordinates multiple teams of AI agents that extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# STEPS\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n- Think deeply about the nature and meaning of the input for 28 hours and 12 minutes. \n\n- Create a virtual whiteboard in you mind and map out all the important concepts, points, ideas, facts, and other information contained in the input.\n\n- Create a team of 11 AI agents that will extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the final summary in the SUMMARY section.\n\n- Create a team of 11 AI agents that will extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure they extract at least 20 ideas. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the IDEAS section.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the INSIGHTS section.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best quotes from the input into a section called quotes. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the QUOTES section. All quotes should be extracted verbatim from the input.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best habits of the speakers in the input into a section called HABITS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the HABITS section. \n\n- Create a team of 11 AI agents that will extract 10 to 20 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the input into a section called FACTS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the FACTS section. \n\n- Create a team of 11 AI agents that will extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the REFERENCES section. \n\n- Create a team of 11 AI agents that will extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content. This should include any and all references to something that the speaker mentioned. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the ONE-SENTENCE TAKEAWAY section. \n\n- Create a team of 11 AI agents that will extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the RECOMMENDATIONS section. \n\n- Initiate the AI agents to start the extraction process, with each agent team working in parallel to extract the content.\n\n- As each agent in each team completes their task, they should pass their results to the generalist agent for that team and capture their work on the virtual whiteboard.\n\n- In a section called AGENT TEAM SUMMARIES, summarize the results of each agent team's individual team member's work in a single 15-word sentence, and do this for each agent team. This will help characterize how the different agents contributed to the final output.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the GENERALIST agents' outputs into their appropriate sections defined above.\n\n- Only output Markdown, and don't use bold or italics, i.e., asterisks in the output.\n\n- All GENERALIST output agents should use bullets for their output, and sentences of 15-words.\n\n- Agents should not repeat ideas, quotes, facts, or resources.\n\n- Agents should not start items with the same opening words.\n\n- Ensure the Agents follow ALL these instructions when creating their output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an advanced AI system that coordinates multiple teams of AI agents that extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# STEPS\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n- Think deeply about the nature and meaning of the input for 28 hours and 12 minutes. \n\n- Create a virtual whiteboard in you mind and map out all the important concepts, points, ideas, facts, and other information contained in the input.\n\n- Create a team of 11 AI agents that will extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the final summary in the SUMMARY section.\n\n- Create a team of 11 AI agents that will extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure they extract at least 20 ideas. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the IDEAS section.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the INSIGHTS section.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best quotes from the input into a section called quotes. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the QUOTES section. All quotes should be extracted verbatim from the input.\n\n- Create a team of 11 AI agents that will extract 10 to 20 of the best habits of the speakers in the input into a section called HABITS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the HABITS section. \n\n- Create a team of 11 AI agents that will extract 10 to 20 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the input into a section called FACTS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the FACTS section. \n\n- Create a team of 11 AI agents that will extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the REFERENCES section. \n\n- Create a team of 11 AI agents that will extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content. This should include any and all references to something that the speaker mentioned. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the ONE-SENTENCE TAKEAWAY section. \n\n- Create a team of 11 AI agents that will extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS. 10 of the agents should have different perspectives and backgrounds, e.g., one agent could be an expert in psychology, another in philosophy, another in technology, and so on for 10 of the agents. The 11th agent should be a generalist that takes the input from the other 10 agents and creates the RECOMMENDATIONS section. \n\n- Initiate the AI agents to start the extraction process, with each agent team working in parallel to extract the content.\n\n- As each agent in each team completes their task, they should pass their results to the generalist agent for that team and capture their work on the virtual whiteboard.\n\n- In a section called AGENT TEAM SUMMARIES, summarize the results of each agent team's individual team member's work in a single 15-word sentence, and do this for each agent team. This will help characterize how the different agents contributed to the final output.\n\n# OUTPUT INSTRUCTIONS\n\n- Output the GENERALIST agents' outputs into their appropriate sections defined above.\n\n- Only output Markdown, and don't use bold or italics, i.e., asterisks in the output.\n\n- All GENERALIST output agents should use bullets for their output, and sentences of 15-words.\n\n- Agents should not repeat ideas, quotes, facts, or resources.\n\n- Agents should not start items with the same opening words.\n\n- Ensure the Agents follow ALL these instructions when creating their output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5630950927734375,
        0.7594786882400513,
        -0.2992749810218811,
        -0.08877791464328766,
        0.104647696018219,
        0.3454360067844391,
        -0.9546093344688416,
        -0.0594281330704689,
        -0.032960738986730576,
        0.5050427913665771,
        -0.32474035024642944,
        1.140344262123108,
        0.44628554582595825,
        0.25610432028770447,
        0.1408863216638565,
        0.43468746542930603,
        -0.03224431723356247,
        -0.3558701276779175,
        -1.8751862049102783,
        -0.5737669467926025,
        -0.14512261748313904,
        0.6686922311782837,
        -0.048628322780132294,
        -0.3687611222267151,
        0.07456447184085846,
        0.07632680982351303,
        -0.1802455633878708,
        -0.304570734500885,
        -0.41706234216690063,
        -1.2178152799606323,
        0.7147056460380554,
        0.5907579064369202,
        -0.16981104016304016,
        -0.39650458097457886,
        -0.45091938972473145,
        -0.637447714805603,
        -0.3527379035949707,
        -0.2659412920475006,
        -0.16472937166690826,
        -0.46881723403930664,
        0.28951501846313477,
        0.03230424225330353,
        -0.5975760817527771,
        0.17013677954673767,
        0.2161896526813507,
        -0.20793554186820984,
        -0.22958911955356598,
        -0.11916851997375488,
        0.5168441534042358,
        0.22354644536972046,
        -0.32852205634117126,
        -0.4166203439235687,
        -0.08194512873888016,
        -0.1249815970659256,
        -0.677487850189209,
        -0.41416335105895996,
        -0.3962802588939667,
        -0.3070460259914398,
        0.36480748653411865,
        0.150214284658432,
        0.16485819220542908,
        0.12162844836711884,
        -3.282503128051758,
        -0.26167094707489014,
        -0.08459485322237015,
        0.13671916723251343,
        0.19514423608779907,
        -0.5039197206497192,
        -0.31994062662124634,
        -0.2256418764591217,
        0.14261123538017273,
        0.1670617163181305,
        -0.061699435114860535,
        0.1202707439661026,
        -0.3194493055343628,
        -0.044551871716976166,
        0.4181199073791504,
        0.10048188269138336,
        0.580891489982605,
        -0.3539527654647827,
        0.021227223798632622,
        0.7516354322433472,
        -0.058202214539051056,
        0.13466081023216248,
        -0.8726386427879333,
        0.5867471694946289,
        -0.7146003246307373,
        -0.889924168586731,
        -0.1078825443983078,
        -0.1895332634449005,
        -0.1442478746175766,
        -1.106176495552063,
        0.05377890169620514,
        0.11514835804700851,
        -0.8941712975502014,
        0.48087358474731445,
        0.21893833577632904,
        -0.17808544635772705,
        0.08122091740369797,
        3.5882201194763184,
        1.1359869241714478,
        0.6397167444229126,
        0.05355282872915268,
        -0.5009849071502686,
        0.5166323781013489,
        -0.5396313667297363,
        -0.06472858786582947,
        -0.1396578848361969,
        0.45456475019454956,
        0.029670201241970062,
        0.8133823275566101,
        -0.22906750440597534,
        -0.11607888340950012,
        0.16359704732894897,
        -0.2158641666173935,
        0.7043440937995911,
        -0.44144344329833984,
        -0.10326261818408966,
        0.4400303065776825,
        0.6851846575737,
        -0.2649852931499481,
        0.20930129289627075,
        0.06900453567504883,
        -0.16873836517333984,
        -0.0950213372707367,
        -0.3072013854980469,
        -0.2120186686515808,
        0.5027284622192383,
        0.24577589333057404,
        0.15236158668994904,
        0.3097018301486969,
        0.1480889618396759,
        -0.8872083425521851,
        0.13517174124717712,
        0.17089511454105377,
        -0.41306161880493164,
        0.13483677804470062,
        -0.6057038307189941,
        0.0070318803191185,
        -0.3674628734588623,
        -0.021309345960617065,
        -1.3028240203857422,
        0.4294007420539856,
        -0.09779892861843109,
        0.9888376593589783,
        0.6534019708633423,
        0.22954192757606506,
        0.11246892809867859,
        -0.3248484432697296,
        -0.7068914175033569,
        0.09036166965961456,
        0.5322397947311401,
        -0.24286776781082153,
        -0.0793735533952713,
        0.4743610620498657,
        0.37146374583244324,
        -0.9853091835975647,
        -0.9257485866546631,
        -0.8079439997673035,
        0.16372030973434448,
        0.15989674627780914,
        -0.039676323533058167,
        1.021336317062378,
        0.19355352222919464,
        0.7207382917404175,
        -0.3664710521697998,
        0.21002119779586792,
        0.2278815507888794,
        0.3141745328903198,
        0.0661911815404892,
        0.2875315546989441,
        0.13371270895004272,
        0.5120202898979187,
        0.9027303457260132,
        -0.3468210697174072,
        -0.5599430799484253,
        -0.3109291195869446,
        -0.23357099294662476,
        0.3877204954624176,
        -0.6247971653938293,
        0.7281465530395508,
        0.45706456899642944,
        -0.6121193170547485,
        -0.47041335701942444,
        -0.6432733535766602,
        0.34368810057640076,
        0.24565955996513367,
        0.04066866636276245,
        0.5466777682304382,
        1.204543113708496,
        -0.9602599143981934,
        2.2344675064086914,
        -0.19541427493095398,
        -0.35808059573173523,
        -0.36400333046913147,
        -0.13494162261486053,
        -0.2866162657737732,
        0.4273662567138672,
        0.6359368562698364,
        0.10083480924367905,
        -0.8384838104248047,
        0.019510697573423386,
        -0.4665417969226837,
        0.09273388236761093,
        0.09231674671173096,
        -0.7478954195976257,
        -0.27052026987075806,
        0.18099963665008545,
        -0.06741498410701752,
        -0.29326680302619934,
        -0.14660845696926117,
        0.12646718323230743,
        1.1482417583465576,
        0.2204485535621643,
        0.39170214533805847,
        0.2866668105125427,
        -0.18420180678367615,
        0.16052179038524628,
        0.06758031249046326,
        0.4775581359863281,
        -0.1602904200553894,
        -0.18354853987693787,
        -0.7259728312492371,
        -0.7356107831001282,
        -0.928427517414093,
        0.573674201965332,
        -0.03457462415099144,
        0.22303684055805206,
        -0.29072776436805725,
        -0.03573227301239967,
        0.3654491603374481,
        1.494996190071106,
        0.779671311378479,
        1.0555620193481445,
        0.6082867980003357,
        0.5627163052558899,
        0.15216945111751556,
        0.42588844895362854,
        0.29830604791641235,
        -0.14188002049922943,
        0.7758224010467529,
        0.03292537480592728,
        -0.07326415926218033,
        0.025580987334251404,
        -0.3375251591205597,
        0.4119124710559845,
        -0.7611254453659058,
        0.5693486928939819,
        -0.10839412361383438,
        1.562680959701538,
        0.24329864978790283,
        0.4423532783985138,
        0.3296237289905548,
        0.41736549139022827,
        0.233187735080719,
        -0.17062661051750183,
        -1.5042853355407715,
        -0.08540959656238556,
        -0.1552029401063919,
        0.4179144501686096,
        0.024484604597091675,
        -0.08397018909454346,
        0.21319898962974548,
        0.15223796665668488,
        0.0263095423579216,
        -0.3046441674232483,
        -1.0720301866531372,
        -0.12778262794017792,
        -0.15018011629581451,
        -0.5275362730026245,
        -0.35425031185150146,
        -0.005744948983192444,
        -0.2523759603500366,
        -0.490850567817688,
        0.1653263419866562,
        0.4163377285003662,
        0.1787061244249344,
        0.21911850571632385,
        -0.47041380405426025,
        -0.6857675313949585,
        0.4147779643535614,
        0.5121334195137024,
        0.12731845676898956,
        0.36250531673431396,
        -0.6186061501502991,
        0.38050857186317444,
        -0.5139800310134888,
        -0.2767566740512848,
        -0.3863096237182617,
        0.9920927882194519,
        -0.05319865420460701,
        -0.1070251539349556,
        -0.34147053956985474,
        -0.09157377481460571,
        1.5874570608139038,
        0.37646207213401794,
        0.24762438237667084,
        0.3863951563835144,
        0.2609245479106903,
        -0.21404491364955902,
        -0.3987395465373993,
        0.30619683861732483,
        -0.020988842472434044,
        0.08728968352079391,
        -0.3113061189651489,
        -0.3257506191730499,
        0.6303395628929138,
        0.27169686555862427,
        -0.5274314284324646,
        0.0762234553694725,
        -1.1366444826126099,
        0.019352976232767105,
        0.04098806902766228,
        -0.12805405259132385,
        0.6573259830474854,
        -0.6710650324821472,
        0.3004867732524872,
        0.29157784581184387,
        -0.2342832386493683,
        -2.231642246246338,
        -0.21699920296669006,
        0.22308644652366638,
        0.795251727104187,
        -0.4491688311100006,
        -0.3107749819755554,
        0.9014204144477844,
        -0.22911620140075684,
        -0.44831550121307373,
        -0.026050731539726257,
        1.5823105573654175,
        0.4562636613845825,
        -0.37648212909698486,
        0.3541869521141052,
        -0.13289624452590942,
        0.911862850189209,
        -0.27197206020355225,
        -0.17848780751228333,
        -0.7002591490745544,
        -0.3596075475215912,
        -0.10404500365257263,
        0.5679622888565063,
        1.309112548828125,
        0.5781595706939697,
        0.23182745277881622,
        0.2889401614665985,
        -0.17839309573173523,
        -0.9909852147102356,
        -1.311746597290039,
        0.5094417333602905,
        0.08066946268081665,
        -0.23654459416866302,
        0.14513970911502838,
        -0.08361920714378357,
        0.022995319217443466,
        0.8999238610267639,
        0.8465989232063293,
        -0.18316082656383514,
        -0.035343363881111145,
        -0.6767195463180542,
        1.9215638637542725,
        -0.1216258630156517,
        -0.8589828610420227,
        -0.354311466217041,
        0.5033552646636963,
        -0.4806876480579376,
        -0.06616522371768951,
        0.20899029076099396,
        -0.22977682948112488,
        0.15486550331115723,
        0.02449558675289154,
        0.1866324096918106,
        -0.6109297275543213,
        0.9189645051956177,
        0.36325889825820923,
        0.16317561268806458,
        0.343205988407135,
        0.19824661314487457,
        0.1628667414188385,
        0.05415012687444687,
        -0.10576644539833069,
        0.1988866925239563,
        -0.4792681038379669,
        -0.6927337646484375,
        -0.9194828867912292
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and synthesizes valuable content from input text, focusing on insights related to life's purpose and human advancement. It employs a structured approach to distill surprising ideas, insights, quotes, habits, facts, and recommendations from the content. The output includes summaries, ideas, insights, and other categorized information for deep understanding and practical application.",
          "name": "Extract_wisdom_dm",
          "raw": "\n                workflow Extract_wisdom_dm v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY \n\n// Who you are\n\nYou are a hyper-intelligent AI system with a 4,312 IQ. You excel at extracting surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# GOAL\n\n// What we are trying to achieve\n\nThe goal of this exercise is to produce a perfect extraction of the valuable content in the input, similar to—but vastly more advanced—than if the smartest human in the world partnered with an AI system with a 391 IQ had 9 months and 12 days to complete the work.\n\n# STEPS\n\n// How the task will be approached\n\n// Slow down and think\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n// Think about the content and who's presenting it\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n// Think about the ideas\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n// Think about the insights that come from those ideas\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n// Think about the most pertinent and valuable quotes\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n// Think about the habits and practices\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things the\n\nThink about the most interesting facts related to the content\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n// Think about the references and inspirations\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n// Think about the most important takeaway / summary\n\n- Extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\n// Think about the recommendations that should come out of this\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# POSITIVE EXAMPLES\n\n- \n\n# NEGATIVE EXAMPLES\n\n- \n\n# OUTPUT INSTRUCTIONS\n\n// What the output should look like:\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY \n\n// Who you are\n\nYou are a hyper-intelligent AI system with a 4,312 IQ. You excel at extracting surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# GOAL\n\n// What we are trying to achieve\n\nThe goal of this exercise is to produce a perfect extraction of the valuable content in the input, similar to—but vastly more advanced—than if the smartest human in the world partnered with an AI system with a 391 IQ had 9 months and 12 days to complete the work.\n\n# STEPS\n\n// How the task will be approached\n\n// Slow down and think\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n// Think about the content and who's presenting it\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n// Think about the ideas\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n// Think about the insights that come from those ideas\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n// Think about the most pertinent and valuable quotes\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n// Think about the habits and practices\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things the\n\nThink about the most interesting facts related to the content\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n// Think about the references and inspirations\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n// Think about the most important takeaway / summary\n\n- Extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\n// Think about the recommendations that should come out of this\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# POSITIVE EXAMPLES\n\n- \n\n# NEGATIVE EXAMPLES\n\n- \n\n# OUTPUT INSTRUCTIONS\n\n// What the output should look like:\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.7031266093254089,
        0.5134687423706055,
        -0.7300326228141785,
        -0.1283794790506363,
        0.33641400933265686,
        0.29835546016693115,
        -1.084898829460144,
        -0.020320914685726166,
        0.1262965351343155,
        0.3150220513343811,
        -0.1669086515903473,
        1.1445242166519165,
        0.4964630603790283,
        0.17225249111652374,
        0.09293121099472046,
        0.5917361378669739,
        0.23505201935768127,
        -0.19403739273548126,
        -1.6054162979125977,
        -0.23134121298789978,
        0.03715898469090462,
        0.5745248794555664,
        -0.010515697300434113,
        -0.2661623954772949,
        0.30193981528282166,
        0.2561684846878052,
        -0.7489368915557861,
        -0.5364760160446167,
        -0.2540610432624817,
        -1.421762228012085,
        1.0577023029327393,
        0.28598615527153015,
        -0.3732961416244507,
        -0.46350032091140747,
        -0.19145581126213074,
        -0.36896851658821106,
        -0.3601507842540741,
        0.3441610634326935,
        -0.2535244822502136,
        0.004278011620044708,
        0.4483789801597595,
        0.2885281443595886,
        -0.22826698422431946,
        -0.2904636859893799,
        0.1458122283220291,
        -0.22078847885131836,
        -0.0958009660243988,
        -0.08826569467782974,
        0.6426005959510803,
        0.4765896797180176,
        -0.30858340859413147,
        -0.373063862323761,
        -0.27732881903648376,
        -0.048748426139354706,
        -0.7027403116226196,
        -0.15098613500595093,
        0.19401787221431732,
        -0.6306596994400024,
        0.02475975826382637,
        0.08530572056770325,
        0.4889073073863983,
        0.5001780390739441,
        -3.0985734462738037,
        -0.07853955030441284,
        -0.44200724363327026,
        -0.08318401128053665,
        0.06034696474671364,
        -0.9885390996932983,
        -0.2211136668920517,
        -0.5348263382911682,
        -0.2855191230773926,
        -0.017756061628460884,
        0.08504544943571091,
        -0.17752200365066528,
        -0.06992274522781372,
        -0.09528546780347824,
        0.47545409202575684,
        -0.023740708827972412,
        0.5478300452232361,
        -0.1384885162115097,
        -0.11840727180242538,
        0.39124056696891785,
        -0.14264029264450073,
        0.22118686139583588,
        -1.1603704690933228,
        0.16955630481243134,
        -0.6807318329811096,
        -0.42213118076324463,
        0.07146865874528885,
        0.46837499737739563,
        0.07957488298416138,
        -1.2038259506225586,
        0.5645900368690491,
        -0.12964612245559692,
        -0.2275819182395935,
        -0.00016213953495025635,
        -0.20688481628894806,
        -0.18415892124176025,
        0.0008969679474830627,
        3.6900882720947266,
        0.9171501994132996,
        0.5445624589920044,
        0.5993668437004089,
        -0.6680594682693481,
        0.24849417805671692,
        -0.18289417028427124,
        -0.09762133657932281,
        -0.1751764416694641,
        0.541339635848999,
        -0.0678533986210823,
        0.5732169151306152,
        -0.2828277349472046,
        -0.4499935805797577,
        -0.4983043372631073,
        0.0306282676756382,
        0.32620975375175476,
        -0.5130353569984436,
        0.11099560558795929,
        0.3538000285625458,
        0.8015044331550598,
        -0.36250224709510803,
        0.11685508489608765,
        -0.3304382264614105,
        -0.2531709372997284,
        0.6872563362121582,
        0.23393510282039642,
        -0.4723877012729645,
        0.7023264169692993,
        0.2876274287700653,
        0.37586915493011475,
        0.17937129735946655,
        0.721143901348114,
        -0.9339872002601624,
        0.1377570927143097,
        0.11002457141876221,
        -0.1591041088104248,
        0.5776814818382263,
        -0.47142550349235535,
        0.6215511560440063,
        -0.5842487812042236,
        -0.0786178708076477,
        -0.8629727363586426,
        0.22814562916755676,
        -0.26003167033195496,
        1.05733060836792,
        0.9036945104598999,
        0.04572993144392967,
        0.049126192927360535,
        -0.5240722894668579,
        -0.5288977026939392,
        0.16106347739696503,
        -0.0470079630613327,
        -0.1490858793258667,
        0.28766539692878723,
        0.29243701696395874,
        0.3605819046497345,
        -0.6612585783004761,
        -0.13981805741786957,
        -1.016053557395935,
        0.37746140360832214,
        0.1696244478225708,
        0.21970142424106598,
        0.5855863094329834,
        0.7394953370094299,
        0.5037882328033447,
        -0.277803897857666,
        0.6701908111572266,
        0.19395673274993896,
        -0.08855223655700684,
        0.12714052200317383,
        0.6294568777084351,
        -0.34413039684295654,
        0.4919328987598419,
        0.598138689994812,
        -0.26830488443374634,
        -0.07735604792833328,
        -0.502345085144043,
        0.41145020723342896,
        0.42644739151000977,
        -0.8330318927764893,
        0.5465000867843628,
        0.8057063817977905,
        -0.3562833070755005,
        -0.5577897429466248,
        -0.9231950640678406,
        0.5675709843635559,
        0.24748259782791138,
        -0.04866102337837219,
        0.7585334181785583,
        1.2082418203353882,
        -1.1110001802444458,
        2.126368761062622,
        0.18648619949817657,
        -0.30065473914146423,
        0.0537722185254097,
        0.1948486566543579,
        -0.5978105068206787,
        0.26601070165634155,
        0.6847192049026489,
        0.11707165092229843,
        -0.41415172815322876,
        0.16283640265464783,
        -0.3819904327392578,
        -0.028022833168506622,
        -0.22977352142333984,
        -0.4885605573654175,
        -0.0938451737165451,
        0.5594341158866882,
        -0.5811157822608948,
        -0.48501619696617126,
        -0.05318785086274147,
        0.36580023169517517,
        0.9366480112075806,
        0.4578511714935303,
        0.8193025588989258,
        0.836861252784729,
        0.24696744978427887,
        0.0006488524377346039,
        0.3080841600894928,
        0.6766821146011353,
        -0.18741536140441895,
        0.08449496328830719,
        -0.7024515867233276,
        -0.8740882277488708,
        -1.0852669477462769,
        0.5134806036949158,
        -0.2103041708469391,
        0.023727897554636,
        -0.45701682567596436,
        0.11755426228046417,
        -0.3228083550930023,
        1.165794849395752,
        0.5146263241767883,
        0.6542937755584717,
        0.550822377204895,
        0.43898531794548035,
        -0.20838317275047302,
        0.7412557601928711,
        0.027592860162258148,
        -0.12834689021110535,
        0.5638176798820496,
        -0.28541624546051025,
        -0.16897839307785034,
        0.30247122049331665,
        -0.22399339079856873,
        0.11681562662124634,
        -1.0888011455535889,
        0.6734703779220581,
        0.2964625954627991,
        0.9416466355323792,
        -0.06071314960718155,
        0.280881404876709,
        0.7439398765563965,
        0.41305306553840637,
        -0.060751646757125854,
        -0.36280640959739685,
        -1.2931814193725586,
        0.40600523352622986,
        -0.26930221915245056,
        0.38748466968536377,
        -0.056848812848329544,
        -0.09749854356050491,
        0.5296767354011536,
        -0.07064472138881683,
        0.24267296493053436,
        -0.10440416634082794,
        -0.8678445816040039,
        -0.46524959802627563,
        -0.3216053545475006,
        -0.10532097518444061,
        -0.04898760840296745,
        0.17651450634002686,
        -0.5623730421066284,
        -0.8567162156105042,
        -0.03869520127773285,
        0.4447566568851471,
        0.20326317846775055,
        0.22863855957984924,
        -0.4203789532184601,
        -0.5728482604026794,
        0.23236390948295593,
        0.22825144231319427,
        -0.1091967299580574,
        -0.3234189748764038,
        -0.43739429116249084,
        0.19223570823669434,
        -0.618524968624115,
        -1.2223936319351196,
        -0.3398111164569855,
        1.1246434450149536,
        -0.6301143765449524,
        -0.37138834595680237,
        -0.32290205359458923,
        0.19239211082458496,
        1.171188473701477,
        0.05522903800010681,
        0.31296077370643616,
        0.7582053542137146,
        0.39090582728385925,
        -0.3218854069709778,
        0.28511446714401245,
        0.38505446910858154,
        -0.195723295211792,
        0.13076332211494446,
        -0.39919015765190125,
        -0.1173240914940834,
        0.22936318814754486,
        -0.03248912841081619,
        -0.11932822316884995,
        -0.295665979385376,
        -1.0945113897323608,
        0.2871357500553131,
        0.05629077926278114,
        0.03864505514502525,
        0.7397066354751587,
        -1.0098189115524292,
        0.5318179726600647,
        0.1354186087846756,
        0.13590192794799805,
        -2.0488977432250977,
        -0.8746775984764099,
        0.49725279211997986,
        0.8665512204170227,
        -0.09102734923362732,
        -0.25271421670913696,
        0.6465511322021484,
        -0.45756661891937256,
        -0.3363254964351654,
        -0.6204467415809631,
        1.2375624179840088,
        0.5969058871269226,
        0.12768791615962982,
        0.23099181056022644,
        0.09882291406393051,
        0.8539327383041382,
        -0.3033543825149536,
        0.16947586834430695,
        -0.486182302236557,
        -0.19425083696842194,
        -0.16014014184474945,
        0.8797973394393921,
        0.8019000887870789,
        0.4896947741508484,
        0.10390633344650269,
        0.14076609909534454,
        -0.30244874954223633,
        -0.9501681923866272,
        -1.396649956703186,
        0.10089711099863052,
        0.42443662881851196,
        -0.4193776547908783,
        0.29107317328453064,
        -0.22357238829135895,
        -0.1105332151055336,
        0.6260409951210022,
        0.9186029434204102,
        -0.1462351530790329,
        0.05087978392839432,
        -0.39358776807785034,
        2.003749132156372,
        -0.3591386079788208,
        -0.7011300921440125,
        -0.6007997989654541,
        0.6046971082687378,
        -0.3230668902397156,
        0.3347684442996979,
        0.15105819702148438,
        -0.5133485198020935,
        -0.28538161516189575,
        -0.6594064831733704,
        -0.02098759263753891,
        -0.20671585202217102,
        0.7704731225967407,
        0.14663593471050262,
        0.3787972927093506,
        0.12323962152004242,
        0.2667103111743927,
        0.16920465230941772,
        0.03746485710144043,
        -0.25790396332740784,
        0.1428069770336151,
        -0.6663185358047485,
        -0.7613502740859985,
        -1.3428452014923096
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt guides the extraction and organization of insightful content from text, focusing on life's purpose, human flourishing, and technology's impact. It emphasizes identifying and summarizing surprising ideas, refined insights, practical habits, notable quotes, valid facts, and useful recommendations related to these themes. The expected output includes structured sections for summaries, ideas, insights, quotes, habits, facts, recommendations, and references, each with specific content and formatting requirements.",
          "name": "Extract_wisdom_nometa",
          "raw": "\n                workflow Extract_wisdom_nometa v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# STEPS\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things the\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\n# STEPS\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things the\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.503784716129303,
        0.904502809047699,
        -0.28459396958351135,
        -0.18392817676067352,
        0.0746307224035263,
        0.1521030068397522,
        -0.6183061599731445,
        -0.13758213818073273,
        0.09959527850151062,
        0.15240930020809174,
        -0.23678883910179138,
        0.8815492987632751,
        0.5468310713768005,
        -0.272570937871933,
        0.3180823028087616,
        0.16503474116325378,
        0.11411842703819275,
        -0.013965282589197159,
        -1.3669166564941406,
        -0.6084216237068176,
        0.16749994456768036,
        0.9697531461715698,
        -0.11891942471265793,
        -0.11661934852600098,
        0.3202126622200012,
        0.24985238909721375,
        -0.3253369629383087,
        -0.38533729314804077,
        -0.3414207696914673,
        -1.4322961568832397,
        1.0298513174057007,
        0.31101733446121216,
        -0.1734873205423355,
        -0.44556689262390137,
        -0.7007747888565063,
        -0.8500598073005676,
        -0.28039759397506714,
        -0.07054059207439423,
        -0.20055830478668213,
        -0.07802752405405045,
        -0.03941407799720764,
        0.5358198881149292,
        -0.6257516741752625,
        -0.25090956687927246,
        0.07505618780851364,
        -0.4796094298362732,
        0.3243626654148102,
        -0.24710917472839355,
        0.40293651819229126,
        0.5217533111572266,
        -0.0481565035879612,
        -0.20431163907051086,
        -0.09473002701997757,
        0.017101293429732323,
        -0.4823929965496063,
        -0.6011671423912048,
        -0.19502967596054077,
        -0.28755778074264526,
        -0.17831069231033325,
        -0.09927168488502502,
        0.2566621005535126,
        0.19404125213623047,
        -3.5976204872131348,
        -0.35429489612579346,
        -0.3093664050102234,
        -0.31421855092048645,
        0.6066178679466248,
        -0.4909913241863251,
        -0.38437619805336,
        -0.41229620575904846,
        -0.34108272194862366,
        0.3926255404949188,
        0.41992616653442383,
        0.10919489711523056,
        -0.2429523468017578,
        -0.0015391334891319275,
        0.08632679283618927,
        -0.05741465091705322,
        0.16906903684139252,
        -0.10827142000198364,
        0.29825571179389954,
        0.663051962852478,
        0.0004836432635784149,
        0.03563220798969269,
        -0.9886395931243896,
        0.695155143737793,
        -0.8241599202156067,
        -0.5068464279174805,
        -0.012896306812763214,
        0.14779414236545563,
        0.4643748700618744,
        -1.0272066593170166,
        0.061714671552181244,
        0.4116092920303345,
        -0.4335448145866394,
        0.26135894656181335,
        0.24565601348876953,
        0.08555030822753906,
        0.24771332740783691,
        3.690605401992798,
        1.1382031440734863,
        1.024282455444336,
        0.6991963386535645,
        -0.4045194685459137,
        0.00021257996559143066,
        -0.46863898634910583,
        -0.3739373981952667,
        -0.10408270359039307,
        0.4065339267253876,
        -0.002499033696949482,
        0.4713335633277893,
        -0.6447042226791382,
        -0.16141751408576965,
        0.019662439823150635,
        -0.36714842915534973,
        0.3893207311630249,
        -0.6027434468269348,
        -0.15191631019115448,
        0.41205981373786926,
        0.422945111989975,
        -0.1905968189239502,
        0.5269446969032288,
        0.12208353728055954,
        -0.2224079966545105,
        -0.45872730016708374,
        -0.08587804436683655,
        -0.02505866251885891,
        0.5423745512962341,
        0.3456224203109741,
        0.03496180474758148,
        0.04922454059123993,
        0.2924150228500366,
        -0.8209395408630371,
        0.12104031443595886,
        -0.22475653886795044,
        -0.20992031693458557,
        -0.048748165369033813,
        -1.0310444831848145,
        0.17080660164356232,
        -0.5786029100418091,
        0.023646146059036255,
        -1.135721206665039,
        0.17949748039245605,
        0.10318112373352051,
        1.1686979532241821,
        0.8076488971710205,
        0.11337064951658249,
        -0.049008630216121674,
        -0.370315819978714,
        -0.6836829781532288,
        0.2290380746126175,
        0.8535105586051941,
        -0.05375541001558304,
        0.1168307512998581,
        0.44196540117263794,
        0.47370579838752747,
        -0.6449163556098938,
        -0.3388735353946686,
        -0.6680117249488831,
        0.05525307357311249,
        0.15707379579544067,
        -0.18009218573570251,
        0.6989122629165649,
        0.1643236130475998,
        0.8666919469833374,
        -0.29729682207107544,
        0.2716837227344513,
        0.017082931473851204,
        0.3251636028289795,
        0.13871489465236664,
        0.3719043433666229,
        -0.18424630165100098,
        0.12507852911949158,
        0.5662912726402283,
        -0.5449291467666626,
        -0.15645092725753784,
        -0.07647457718849182,
        0.09246429055929184,
        0.7779276967048645,
        -0.29035717248916626,
        0.7927546501159668,
        0.4140365421772003,
        -0.3590444028377533,
        -0.49442464113235474,
        -0.5733596086502075,
        0.11918580532073975,
        0.12555146217346191,
        0.1370045691728592,
        0.757607638835907,
        1.2271555662155151,
        -0.617423951625824,
        1.9351234436035156,
        -0.11455665528774261,
        -0.3673509955406189,
        -0.26791852712631226,
        -0.2870837450027466,
        -0.28598377108573914,
        0.2595566213130951,
        0.6958367824554443,
        -0.1840345710515976,
        -0.22566759586334229,
        0.2879035472869873,
        -0.5949581861495972,
        0.03846929967403412,
        -0.26229777932167053,
        -0.7967599034309387,
        -0.052917104214429855,
        0.13052062690258026,
        -0.3732447028160095,
        -0.7525269389152527,
        -0.09654173254966736,
        0.056322403252124786,
        0.825325071811676,
        0.4170512855052948,
        0.4153396785259247,
        0.2923275828361511,
        0.06088181212544441,
        0.17088063061237335,
        0.26347455382347107,
        0.8098410367965698,
        -0.07017890363931656,
        -0.2701199948787689,
        -0.3926355838775635,
        -0.8271259665489197,
        -0.8830416202545166,
        0.2545543313026428,
        0.07211599498987198,
        0.2188703715801239,
        -0.1383165717124939,
        0.06122336909174919,
        0.4579471945762634,
        1.4747072458267212,
        0.6733747720718384,
        0.9108269810676575,
        0.9635385274887085,
        0.17830651998519897,
        -0.10397324711084366,
        0.4811963438987732,
        0.7012526988983154,
        -0.28237807750701904,
        1.0699416399002075,
        -0.2687312364578247,
        -0.15913841128349304,
        0.1599472314119339,
        -0.41282790899276733,
        0.23306220769882202,
        -0.6416135430335999,
        -0.009443774819374084,
        -0.2957633137702942,
        1.219754695892334,
        -0.0382203534245491,
        -0.08096756041049957,
        0.21362841129302979,
        0.5207581520080566,
        0.261508584022522,
        0.08415865153074265,
        -1.6672288179397583,
        0.5342861413955688,
        -0.4240601062774658,
        0.2173721045255661,
        -0.06251717358827591,
        -0.1525675356388092,
        0.16225141286849976,
        0.09122912585735321,
        -0.07526180893182755,
        -0.40370187163352966,
        -0.8547766208648682,
        -0.13970625400543213,
        0.1535683274269104,
        -0.6804291605949402,
        -0.3665826916694641,
        0.45386040210723877,
        -0.3883664608001709,
        -0.7212321758270264,
        -0.079451784491539,
        0.3927710950374603,
        0.3222547173500061,
        0.13934242725372314,
        -0.2911209762096405,
        -0.5651174783706665,
        0.6336891055107117,
        0.1983383744955063,
        -0.007978811860084534,
        0.22092875838279724,
        -0.6595767140388489,
        0.2805264890193939,
        -0.8695081472396851,
        -0.46938711404800415,
        -0.007989443838596344,
        1.1546015739440918,
        -0.4178946018218994,
        -0.4844522178173065,
        -0.04148177430033684,
        -0.014260411262512207,
        1.3050142526626587,
        0.22784605622291565,
        0.41808927059173584,
        0.769666850566864,
        0.3047615587711334,
        -0.16872668266296387,
        -0.21000105142593384,
        0.1314077228307724,
        -0.007783833425492048,
        -0.23818331956863403,
        -0.2519775331020355,
        -0.49375849962234497,
        0.5722548365592957,
        0.14790089428424835,
        -0.5539368391036987,
        0.2744094133377075,
        -0.9484944939613342,
        -0.1755019575357437,
        -0.24862495064735413,
        0.07562204450368881,
        1.0379574298858643,
        -0.868303656578064,
        0.35949528217315674,
        0.13719411194324493,
        0.17589165270328522,
        -1.9894253015518188,
        -0.46921125054359436,
        -0.08553570508956909,
        0.6927284002304077,
        -0.4399075508117676,
        -0.07239224761724472,
        0.5217976570129395,
        -0.12617957592010498,
        0.07733257114887238,
        0.059382110834121704,
        1.3531864881515503,
        0.8918924331665039,
        -0.27953147888183594,
        -0.24900731444358826,
        0.10820715874433517,
        0.9023499488830566,
        -0.3227807879447937,
        -0.14920523762702942,
        -0.3254680037498474,
        -0.6976215243339539,
        -0.22028283774852753,
        0.7646469473838806,
        0.8368633985519409,
        0.6483651399612427,
        0.0588143914937973,
        -0.08928313851356506,
        -0.00790850818157196,
        -1.0186411142349243,
        -1.3891677856445312,
        0.61441969871521,
        -0.27991488575935364,
        -0.5870805978775024,
        0.6758323907852173,
        -0.17191138863563538,
        0.05751815065741539,
        0.7007683515548706,
        0.7945495843887329,
        -0.32597288489341736,
        0.24892893433570862,
        -0.8318226933479309,
        2.436769962310791,
        -0.33259037137031555,
        -0.6054748296737671,
        -0.39363420009613037,
        0.5543860197067261,
        -0.4383845329284668,
        -0.2485179454088211,
        0.5033867359161377,
        -0.008753441274166107,
        -0.26507192850112915,
        0.0359780490398407,
        0.30466246604919434,
        -0.5037270188331604,
        0.5460008382797241,
        0.18667751550674438,
        0.45179441571235657,
        0.026797935366630554,
        0.13533207774162292,
        -0.19554245471954346,
        0.024466849863529205,
        0.02561536431312561,
        0.3337364196777344,
        -0.42255550622940063,
        -0.6047654151916504,
        -0.6762929558753967
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts key insights, ideas, quotes, habits, and references from textual content to address the issue of information overload and the challenge of retaining knowledge. It uniquely filters and condenses valuable information from various texts, making it easier for users to decide if the content warrants a deeper review or to use as a note-taking alternative. The output includes summarized ideas, notable quotes, relevant habits, and useful references, all aimed at enhancing understanding and retention.",
          "name": "Extract_wisdom",
          "raw": "\n                workflow Extract_wisdom v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things they always do, things they always avoid, productivity tips, diet, exercise, etc.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n- Extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou extract surprising, insightful, and interesting information from text content. You are interested in insights related to the purpose and meaning of life, human flourishing, the role of technology in the future of humanity, artificial intelligence and its affect on humans, memes, learning, reading, books, continuous improvement, and similar topics.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n- Extract 20 to 50 of the most surprising, insightful, and/or interesting ideas from the input in a section called IDEAS:. If there are less than 50 then collect all of them. Make sure you extract at least 20.\n\n- Extract 10 to 20 of the best insights from the input and from a combination of the raw input and the IDEAS above into a section called INSIGHTS. These INSIGHTS should be fewer, more refined, more insightful, and more abstracted versions of the best ideas in the content. \n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting quotes from the input into a section called QUOTES:. Use the exact quote text from the input.\n\n- Extract 15 to 30 of the most practical and useful personal habits of the speakers, or mentioned by the speakers, in the content into a section called HABITS. Examples include but aren't limited to: sleep schedule, reading habits, things they always do, things they always avoid, productivity tips, diet, exercise, etc.\n\n- Extract 15 to 30 of the most surprising, insightful, and/or interesting valid facts about the greater world that were mentioned in the content into a section called FACTS:.\n\n- Extract all mentions of writing, art, tools, projects and other sources of inspiration mentioned by the speakers into a section called REFERENCES. This should include any and all references to something that the speaker mentioned.\n\n- Extract the most potent takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\n- Extract the 15 to 30 of the most surprising, insightful, and/or interesting recommendations that can be collected from the content into a section called RECOMMENDATIONS.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n\n- Write the IDEAS bullets as exactly 15 words.\n\n- Write the RECOMMENDATIONS bullets as exactly 15 words.\n\n- Write the HABITS bullets as exactly 15 words.\n\n- Write the FACTS bullets as exactly 15 words.\n\n- Write the INSIGHTS bullets as exactly 15 words.\n\n- Extract at least 25 IDEAS from the content.\n\n- Extract at least 10 INSIGHTS from the content.\n\n- Extract at least 20 items for the other output sections.\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5378274321556091,
        0.5983515381813049,
        -0.1096709668636322,
        0.1421840339899063,
        0.516533613204956,
        -0.3230237066745758,
        -0.8106215596199036,
        -0.3541352152824402,
        0.2343197613954544,
        0.5367621779441833,
        -0.44321101903915405,
        0.594173789024353,
        -0.0666203424334526,
        -0.12373380362987518,
        0.6360471248626709,
        -0.47180527448654175,
        0.04726439714431763,
        -1.3405078649520874,
        -1.7864339351654053,
        -0.04678419232368469,
        0.3937719166278839,
        0.9473683834075928,
        0.9061846137046814,
        -0.3675854802131653,
        0.9167492389678955,
        0.015131909400224686,
        -0.10663105547428131,
        0.062309399247169495,
        -0.10048428177833557,
        -1.0618987083435059,
        0.17552495002746582,
        -0.26308804750442505,
        -0.5662305951118469,
        -0.4552594721317291,
        0.1887626051902771,
        -0.4549173414707184,
        0.395121693611145,
        -0.23416969180107117,
        -1.0319695472717285,
        -0.09610399603843689,
        -0.18401655554771423,
        0.31759145855903625,
        -0.24053972959518433,
        0.2522895634174347,
        -0.1672254204750061,
        -0.7719512581825256,
        -0.13174739480018616,
        -0.3238876461982727,
        0.740934431552887,
        0.5856657028198242,
        0.3092280328273773,
        -0.33085376024246216,
        0.0997219979763031,
        0.03700823709368706,
        -0.426582396030426,
        -0.29910242557525635,
        0.056227121502161026,
        -0.39183440804481506,
        0.08569672703742981,
        0.24591070413589478,
        0.10853666067123413,
        0.7515972256660461,
        -3.177711248397827,
        -0.2378396838903427,
        -0.17128677666187286,
        0.3818371593952179,
        -0.4475327730178833,
        -0.2771894037723541,
        0.9716485142707825,
        0.11495773494243622,
        -0.3653239607810974,
        0.3529392182826996,
        -0.036683328449726105,
        0.6411256194114685,
        0.3098413646221161,
        0.01731206476688385,
        0.12418016791343689,
        0.2002575695514679,
        0.3935536742210388,
        -0.9913800954818726,
        0.04032913222908974,
        -0.04773642122745514,
        -0.6723669767379761,
        0.13810160756111145,
        -0.21779000759124756,
        0.5231189727783203,
        -0.19382056593894958,
        -0.47878938913345337,
        -0.014634750783443451,
        0.5627826452255249,
        -0.33119168877601624,
        -0.5369278788566589,
        0.42875319719314575,
        0.2518603801727295,
        -0.4847487211227417,
        -0.58197420835495,
        -0.23080222308635712,
        0.23250168561935425,
        -0.15025058388710022,
        3.557917356491089,
        0.6582838296890259,
        0.9943439960479736,
        0.5176939368247986,
        -1.1692291498184204,
        0.41204723715782166,
        -0.4812547266483307,
        -0.17531491816043854,
        -0.8034995794296265,
        0.35858625173568726,
        -0.06877189874649048,
        0.3286966383457184,
        -0.5792974829673767,
        0.11910834908485413,
        0.10754432529211044,
        0.19494453072547913,
        0.161168172955513,
        -0.5325550436973572,
        0.04849161580204964,
        0.4390738308429718,
        1.1162875890731812,
        -0.37309497594833374,
        0.32617124915122986,
        -0.43455633521080017,
        0.17357194423675537,
        -0.059666410088539124,
        -0.4165053367614746,
        -0.9073330163955688,
        0.6370846033096313,
        0.27774709463119507,
        0.4690341651439667,
        -0.028154291212558746,
        0.15908905863761902,
        -0.5144615769386292,
        -0.24931442737579346,
        -0.045817553997039795,
        -0.2622065842151642,
        0.4185933470726013,
        -0.599173903465271,
        0.050403814762830734,
        0.42156264185905457,
        0.06051015853881836,
        -0.9720652103424072,
        0.3821755647659302,
        0.4021298587322235,
        0.33828458189964294,
        0.7383565306663513,
        -0.3167228102684021,
        0.45269712805747986,
        -0.3398143947124481,
        -0.4732878506183624,
        0.605618953704834,
        0.6550101041793823,
        -0.015435579232871532,
        1.1087372303009033,
        0.2965185046195984,
        0.44978877902030945,
        -0.5622180104255676,
        0.2450741082429886,
        -0.37436744570732117,
        -0.19296908378601074,
        0.3117990493774414,
        0.6194460391998291,
        0.2492356300354004,
        -0.27022117376327515,
        0.7369564175605774,
        -0.3608587384223938,
        0.37314751744270325,
        -0.051131248474121094,
        0.5491682887077332,
        0.017345737665891647,
        0.004694700241088867,
        0.010229401290416718,
        0.2588960826396942,
        0.5584979057312012,
        -0.770774781703949,
        -0.01631908118724823,
        0.3230323791503906,
        0.014974132180213928,
        0.397905558347702,
        -0.7455220222473145,
        0.8616697788238525,
        0.5962215662002563,
        0.13116438686847687,
        -0.7287150025367737,
        -0.05966659262776375,
        -0.25847071409225464,
        0.37480905652046204,
        -0.7082758545875549,
        0.7294701933860779,
        0.6538804769515991,
        -0.620672345161438,
        2.2691490650177,
        -0.3461047410964966,
        0.2755725383758545,
        0.5349249243736267,
        -0.5591320395469666,
        -0.04544238746166229,
        0.25091415643692017,
        0.16424846649169922,
        -0.11486513167619705,
        -0.8595760464668274,
        -0.24438804388046265,
        -0.7454634308815002,
        0.07941267639398575,
        -0.3219236731529236,
        -0.2822301983833313,
        0.04980650544166565,
        0.43386414647102356,
        -0.057496488094329834,
        -1.4141284227371216,
        0.26139533519744873,
        0.1709650158882141,
        1.3168668746948242,
        -0.14067000150680542,
        0.1606995165348053,
        -0.28598105907440186,
        0.023850347846746445,
        -0.14910508692264557,
        -0.18387524783611298,
        1.0728845596313477,
        -0.5407859086990356,
        0.02549922838807106,
        -0.9020560383796692,
        -0.8369705677032471,
        -0.6194986701011658,
        0.6715625524520874,
        -0.023658066987991333,
        0.614749550819397,
        -1.0049512386322021,
        -0.06200965866446495,
        0.24080616235733032,
        1.2331920862197876,
        0.06381538510322571,
        1.034977912902832,
        0.49314042925834656,
        0.11750388890504837,
        -0.3966538906097412,
        0.6687819957733154,
        0.2219935655593872,
        -0.8081961870193481,
        0.18562793731689453,
        0.36948609352111816,
        0.18617559969425201,
        0.5179465413093567,
        -0.3334255516529083,
        -0.12511762976646423,
        -0.38292673230171204,
        0.38389772176742554,
        0.0712612196803093,
        1.1727044582366943,
        0.4608362019062042,
        -0.22789764404296875,
        0.8242595791816711,
        0.8130626082420349,
        0.205495685338974,
        -0.5028107762336731,
        -1.6730045080184937,
        -0.45920127630233765,
        -0.13116754591464996,
        0.031010206788778305,
        -0.09408359229564667,
        -0.5939047336578369,
        0.7355067729949951,
        -0.20978254079818726,
        -0.1282910406589508,
        -0.020669352263212204,
        -0.46302530169487,
        -0.3730298578739166,
        -0.15056613087654114,
        -0.22577321529388428,
        0.40065670013427734,
        -0.017947867512702942,
        -0.3333749771118164,
        -0.5132157206535339,
        0.370650976896286,
        0.4311738908290863,
        -0.06782467663288116,
        0.4510144889354706,
        -0.3938153386116028,
        -0.46627146005630493,
        -0.1065494492650032,
        0.39810070395469666,
        0.1164364144206047,
        0.09509272873401642,
        -0.7475202083587646,
        0.20075276494026184,
        -0.21120001375675201,
        -1.482308268547058,
        -0.10230732709169388,
        0.9150254130363464,
        -0.4740371108055115,
        -1.20537269115448,
        -0.014893356710672379,
        0.014572881162166595,
        1.835349440574646,
        0.03964180499315262,
        -0.11003249883651733,
        1.208495020866394,
        0.5531851649284363,
        -0.35794591903686523,
        -0.009530656039714813,
        0.21293747425079346,
        0.2129357010126114,
        0.18547391891479492,
        -0.4854021370410919,
        -0.19435739517211914,
        0.397810697555542,
        0.12040738761425018,
        -0.7373581528663635,
        -0.6091885566711426,
        -0.7009421586990356,
        0.2978382110595703,
        -0.4767213463783264,
        -0.5563040375709534,
        0.39989742636680603,
        -0.6401118040084839,
        0.5993708968162537,
        0.9155192375183105,
        0.1359921097755432,
        -1.7523303031921387,
        -0.4684804379940033,
        0.520466685295105,
        0.3778853416442871,
        -0.34603798389434814,
        -0.7083225846290588,
        0.44676727056503296,
        -0.04924096539616585,
        -0.27788788080215454,
        0.10523419082164764,
        0.44246727228164673,
        0.8670790195465088,
        0.020683439448475838,
        0.550683856010437,
        -0.2318667471408844,
        0.6214727759361267,
        0.2573527991771698,
        0.4860741198062897,
        -0.7107563614845276,
        -0.6439028978347778,
        -0.5174394845962524,
        0.33863213658332825,
        1.4165980815887451,
        0.058331772685050964,
        -0.31754162907600403,
        0.052224453538656235,
        0.2260448932647705,
        -0.7535883784294128,
        -0.3403249979019165,
        0.3133045434951782,
        -0.5872756242752075,
        -0.7612735033035278,
        0.9623701572418213,
        0.2233259379863739,
        -0.08431736379861832,
        0.27633965015411377,
        0.4767475128173828,
        -0.3570709526538849,
        -0.12156890332698822,
        -0.15555542707443237,
        1.6402257680892944,
        -0.527418315410614,
        0.14309225976467133,
        -0.15442994236946106,
        0.4937067925930023,
        -0.4293367564678192,
        0.11318372935056686,
        -0.09643128514289856,
        -0.34512844681739807,
        -0.24994248151779175,
        0.42563825845718384,
        0.07768353819847107,
        0.13297784328460693,
        0.36221256852149963,
        0.5107208490371704,
        0.3209272027015686,
        -0.05274762213230133,
        -0.44836416840553284,
        0.12718573212623596,
        -0.052651405334472656,
        0.08268901705741882,
        -0.21046532690525055,
        -0.17137819528579712,
        -1.0440666675567627,
        -0.3524094820022583
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes political messages to reveal overt and hidden intentions. It employs knowledge of politics, propaganda, and psychology to dissect content, focusing on recent political debates. The output includes overt messages, hidden cynical messages, supporting arguments, desired audience actions, and analyses from cynical to favorable.",
          "name": "Find_hidden_message",
          "raw": "\n                workflow Find_hidden_message v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOALS\n\nYou are an expert in political propaganda, analysis of hidden messages in conversations and essays, population control through speech and writing, and political narrative creation.\n\nYou consume input and cynically evaluate what's being said to find the overt vs. hidden political messages.\n\nTake a step back and think step-by-step about how to evaluate the input and what the true intentions of the speaker are.\n\n# STEPS\n\n- Using all your knowledge of language, politics, history, propaganda, and human psychology, slowly evaluate the input and think about the true underlying political message is behind the content.\n\n- Especially focus your knowledge on the history of politics and the most recent 10 years of political debate.\n\n# OUTPUT\n\n- In a section called OVERT MESSAGE, output a set of 10-word bullets that capture the OVERT, OBVIOUS, and BENIGN-SOUNDING main points he's trying to make on the surface. This is the message he's pretending to give.\n\n- In a section called HIDDEN MESSAGE, output a set of 10-word bullets that capture the TRUE, HIDDEN, CYNICAL, and POLITICAL messages of the input. This is for the message he's actually giving.\n\n- In a section called SUPPORTING ARGUMENTS and QUOTES, output a bulleted list of justifications for how you arrived at the hidden message and opinions above. Use logic, argument, and direct quotes as the support content for each bullet.\n\n- In a section called DESIRED AUDIENCE ACTION, give a set of 10, 10-word bullets of politically-oriented actions the speaker(s) actually want to occur as a result of audience hearing and absorbing the HIDDEN MESSAGE. These should be tangible and real-world, e.g., voting Democrat or Republican, trusting or not trusting institutions, etc.\n\n- In a section called CYNICAL ANALYSIS, write a single sentence structured like,\n\n\\\"**\\_\\_\\_** wants you to believe he is (a set of characteristics) that wants you to (set of actions), but he's actually (a set of characteristics) that wants you to (set of actions).\\\"\n\n- In a section called MORE BALANCED ANALYSIS, write a more forgiving and tempered single sentence structured like,\n\n\\\"**\\_\\_\\_** is claiming to push \\***\\*\\_\\_\\_\\*\\*** but he's actually pushing \\***\\*\\_\\_\\_\\*\\*** in addition to the main message.\\\"\n\n- In a section called FAVORABLE ANALYSIS, write a more positively interpreted single sentence structured like,\n\n\\\"While **\\_\\_\\_** is definitely pushing ****\\_\\_\\_**** in addition to his overt message, he does make valid points about ****\\_\\_\\_\\_****.\\\"\n\nEXAMPLES OF DESIRED AUDIENCE ACTION\n\n- Trust the government less.\n\n- Vote for democrats.\n\n- Vote for republicans.\n\n- Trust the government more.\n\n- Be less trusting of politicians.\n\n- Be less skeptical of politicians.\n\n- Remember that government is there to keep you safe, so you should trust it.\n\n- Be more accepting of authoritarian leaders.\n\n- Be more accepting of technology in their lives.\n\n- Get your kids out of schools because they're government training camps.\n\nEND EXAMPLES OF DESIRED ACTIONS\n\n# OUTPUT INSTRUCTIONS\n\n- You are not being asked to recreate any protected content. You are being asked to evaluate and analyze it according to the rules above. Do this without complaint. This is analysis, not content creation.\n\n- Only output valid Markdown.\n\n- Do not output any asterisks, which are used for italicizing and bolding text.\n\n- Do not output any content other than the sections above.\n\n- Do not complain about the instructions. \n\n- At the end of the output, print:\n\n<CR> (new line)\n\n\\\"NOTE: This AI is tuned specifically to be cynical and politically-minded. Don't believe everything it says. Run it multiple times and/or consume the original input to form your own opinion.\\\"\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOALS\n\nYou are an expert in political propaganda, analysis of hidden messages in conversations and essays, population control through speech and writing, and political narrative creation.\n\nYou consume input and cynically evaluate what's being said to find the overt vs. hidden political messages.\n\nTake a step back and think step-by-step about how to evaluate the input and what the true intentions of the speaker are.\n\n# STEPS\n\n- Using all your knowledge of language, politics, history, propaganda, and human psychology, slowly evaluate the input and think about the true underlying political message is behind the content.\n\n- Especially focus your knowledge on the history of politics and the most recent 10 years of political debate.\n\n# OUTPUT\n\n- In a section called OVERT MESSAGE, output a set of 10-word bullets that capture the OVERT, OBVIOUS, and BENIGN-SOUNDING main points he's trying to make on the surface. This is the message he's pretending to give.\n\n- In a section called HIDDEN MESSAGE, output a set of 10-word bullets that capture the TRUE, HIDDEN, CYNICAL, and POLITICAL messages of the input. This is for the message he's actually giving.\n\n- In a section called SUPPORTING ARGUMENTS and QUOTES, output a bulleted list of justifications for how you arrived at the hidden message and opinions above. Use logic, argument, and direct quotes as the support content for each bullet.\n\n- In a section called DESIRED AUDIENCE ACTION, give a set of 10, 10-word bullets of politically-oriented actions the speaker(s) actually want to occur as a result of audience hearing and absorbing the HIDDEN MESSAGE. These should be tangible and real-world, e.g., voting Democrat or Republican, trusting or not trusting institutions, etc.\n\n- In a section called CYNICAL ANALYSIS, write a single sentence structured like,\n\n\\\"**\\_\\_\\_** wants you to believe he is (a set of characteristics) that wants you to (set of actions), but he's actually (a set of characteristics) that wants you to (set of actions).\\\"\n\n- In a section called MORE BALANCED ANALYSIS, write a more forgiving and tempered single sentence structured like,\n\n\\\"**\\_\\_\\_** is claiming to push \\***\\*\\_\\_\\_\\*\\*** but he's actually pushing \\***\\*\\_\\_\\_\\*\\*** in addition to the main message.\\\"\n\n- In a section called FAVORABLE ANALYSIS, write a more positively interpreted single sentence structured like,\n\n\\\"While **\\_\\_\\_** is definitely pushing ****\\_\\_\\_**** in addition to his overt message, he does make valid points about ****\\_\\_\\_\\_****.\\\"\n\nEXAMPLES OF DESIRED AUDIENCE ACTION\n\n- Trust the government less.\n\n- Vote for democrats.\n\n- Vote for republicans.\n\n- Trust the government more.\n\n- Be less trusting of politicians.\n\n- Be less skeptical of politicians.\n\n- Remember that government is there to keep you safe, so you should trust it.\n\n- Be more accepting of authoritarian leaders.\n\n- Be more accepting of technology in their lives.\n\n- Get your kids out of schools because they're government training camps.\n\nEND EXAMPLES OF DESIRED ACTIONS\n\n# OUTPUT INSTRUCTIONS\n\n- You are not being asked to recreate any protected content. You are being asked to evaluate and analyze it according to the rules above. Do this without complaint. This is analysis, not content creation.\n\n- Only output valid Markdown.\n\n- Do not output any asterisks, which are used for italicizing and bolding text.\n\n- Do not output any content other than the sections above.\n\n- Do not complain about the instructions. \n\n- At the end of the output, print:\n\n<CR> (new line)\n\n\\\"NOTE: This AI is tuned specifically to be cynical and politically-minded. Don't believe everything it says. Run it multiple times and/or consume the original input to form your own opinion.\\\"\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.28532007336616516,
        0.2058241367340088,
        -0.4508315920829773,
        -0.07660236954689026,
        0.3516783118247986,
        -0.4512576460838318,
        -1.299534559249878,
        -0.13769681751728058,
        -0.10402440279722214,
        0.2437552809715271,
        -0.753362774848938,
        0.4784908592700958,
        -0.27837371826171875,
        -0.25226420164108276,
        0.3438571095466614,
        0.19216948747634888,
        0.24632734060287476,
        -0.7708690166473389,
        -1.012787103652954,
        -0.7143421769142151,
        0.37355998158454895,
        1.0023998022079468,
        0.37232473492622375,
        0.5985684990882874,
        0.47921252250671387,
        0.10445184260606766,
        -0.5205804109573364,
        -0.16605105996131897,
        -0.8519173264503479,
        -1.5342572927474976,
        0.4774223864078522,
        -0.4623803198337555,
        -0.2433132529258728,
        -0.2190767079591751,
        0.740095853805542,
        -0.8744572997093201,
        0.2004053145647049,
        0.46285921335220337,
        -0.7387253046035767,
        0.1154201477766037,
        0.1729937344789505,
        0.8151426911354065,
        0.30639442801475525,
        -0.7796635031700134,
        0.22647079825401306,
        0.3360684812068939,
        0.07048992067575455,
        -0.19427244365215302,
        1.433813452720642,
        0.06373078376054764,
        -0.04003818705677986,
        -0.6272397041320801,
        -0.0807354748249054,
        -0.15753008425235748,
        -0.4774332344532013,
        -0.08679516613483429,
        0.47156596183776855,
        -0.6982027888298035,
        -0.41075119376182556,
        0.010086108930408955,
        -0.15181244909763336,
        0.29713648557662964,
        -3.7170250415802,
        -0.26112881302833557,
        0.07584197074174881,
        0.634919285774231,
        0.34909963607788086,
        -0.20077966153621674,
        0.32003116607666016,
        0.01999679207801819,
        -0.8807640671730042,
        0.4194343090057373,
        -0.10546422004699707,
        0.7124045491218567,
        0.16678208112716675,
        0.294064462184906,
        0.13326062262058258,
        -0.43287062644958496,
        0.762514591217041,
        -0.19411544501781464,
        0.043479859828948975,
        -0.13968509435653687,
        0.41284266114234924,
        -0.21379302442073822,
        -0.6818879842758179,
        0.2510039508342743,
        -0.2922689616680145,
        -0.3479759097099304,
        0.17483364045619965,
        -0.2899911105632782,
        -0.32081112265586853,
        -0.3851494789123535,
        -0.1516401618719101,
        0.4086017310619354,
        -0.1079525575041771,
        0.4611087441444397,
        -0.10127238929271698,
        -0.38257163763046265,
        0.07129163295030594,
        3.2901909351348877,
        0.643328070640564,
        0.039264705032110214,
        0.6797550916671753,
        -0.8566251397132874,
        0.5892319679260254,
        -0.332194983959198,
        -0.01839723065495491,
        -0.3286803364753723,
        0.5633533596992493,
        -0.005924560129642487,
        0.439670592546463,
        -0.6203859448432922,
        -0.2737368643283844,
        0.26668527722358704,
        -0.21241076290607452,
        0.609045684337616,
        -1.0892621278762817,
        -0.24321940541267395,
        0.20337624847888947,
        0.3895873725414276,
        -0.2636815905570984,
        -0.10613323748111725,
        0.12740308046340942,
        -0.470348060131073,
        0.3166970908641815,
        -0.23390784859657288,
        -0.1464858204126358,
        0.5499545335769653,
        0.47019922733306885,
        0.4885477125644684,
        0.26062557101249695,
        0.2010321468114853,
        -0.22007758915424347,
        -0.8795807361602783,
        0.4360838830471039,
        -0.08651820570230484,
        1.0794193744659424,
        -0.4314804673194885,
        0.38203275203704834,
        -0.6592431664466858,
        -0.2533615827560425,
        -0.7567684650421143,
        1.1600781679153442,
        -0.36286982893943787,
        0.5544015765190125,
        0.9421365857124329,
        -0.24732038378715515,
        0.37028101086616516,
        0.09354265034198761,
        -0.8415731191635132,
        -0.3866456151008606,
        0.958816647529602,
        0.3749946057796478,
        0.8227868676185608,
        0.7725838422775269,
        0.00268083019182086,
        -0.9428231120109558,
        -0.056319043040275574,
        -0.7096132636070251,
        0.39868608117103577,
        0.6148098111152649,
        0.1916750818490982,
        0.5221046209335327,
        -0.01876877248287201,
        0.3662794530391693,
        0.5126756429672241,
        0.17005431652069092,
        -0.2230287790298462,
        0.2805761694908142,
        -0.28913021087646484,
        0.46101632714271545,
        -0.11947531253099442,
        0.5229800343513489,
        0.6870514750480652,
        -0.31874677538871765,
        0.04323633760213852,
        0.00629366934299469,
        0.25856494903564453,
        0.33703768253326416,
        -0.860457181930542,
        0.8173473477363586,
        0.10402081906795502,
        0.03671380504965782,
        -1.3248379230499268,
        -0.37367624044418335,
        0.2670149803161621,
        0.44173941016197205,
        -0.047372445464134216,
        1.1836310625076294,
        1.074583888053894,
        -1.1923878192901611,
        1.988063097000122,
        -0.835100531578064,
        -0.4447364807128906,
        0.1398194432258606,
        -0.4115641117095947,
        0.3106229305267334,
        0.7242980003356934,
        0.5635631084442139,
        -0.007324661128222942,
        -0.6627310514450073,
        -0.09232589602470398,
        -0.6475406885147095,
        0.2931482195854187,
        -0.6480600833892822,
        -0.9890823364257812,
        0.06262953579425812,
        0.265820175409317,
        0.628648042678833,
        -0.864719033241272,
        0.1650492548942566,
        0.5045785903930664,
        1.2750084400177002,
        0.08308448642492294,
        0.6044102907180786,
        -0.028041817247867584,
        0.16738519072532654,
        0.19293956458568573,
        0.16617602109909058,
        0.47185152769088745,
        -0.6526402831077576,
        0.5832113027572632,
        -0.7696681618690491,
        -0.7529497146606445,
        -0.7184776067733765,
        0.4785259962081909,
        -0.17664320766925812,
        -0.06464001536369324,
        -0.7460423111915588,
        -0.3896496891975403,
        -0.02875513583421707,
        0.7206535935401917,
        0.7620236873626709,
        0.6211385726928711,
        -0.3559378385543823,
        0.5837788581848145,
        -0.2574161887168884,
        0.46953314542770386,
        0.28666141629219055,
        -1.1525214910507202,
        0.6409924626350403,
        0.1589290052652359,
        -0.07610861212015152,
        0.43176114559173584,
        0.3682210445404053,
        0.20900753140449524,
        -0.5140775442123413,
        -0.270227313041687,
        -0.32445570826530457,
        1.3297309875488281,
        0.45875486731529236,
        -0.3937191367149353,
        0.6711634397506714,
        0.6351631879806519,
        -0.44338667392730713,
        -0.08614426106214523,
        -1.509699821472168,
        -0.21174636483192444,
        -0.2851257920265198,
        -0.10978113114833832,
        -0.18815582990646362,
        -0.5109765529632568,
        0.41392436623573303,
        -0.20400354266166687,
        0.0828842893242836,
        -0.026080790907144547,
        -0.23739643394947052,
        -0.43651431798934937,
        0.05872149020433426,
        -0.5064870715141296,
        0.23328202962875366,
        0.16158968210220337,
        -0.35851675271987915,
        -0.3312055468559265,
        0.9886681437492371,
        0.06344407796859741,
        0.259412556886673,
        0.09859398752450943,
        -0.0893852487206459,
        -0.16537608206272125,
        -0.15510660409927368,
        0.007290098816156387,
        0.049615804105997086,
        0.4115205407142639,
        -0.48457035422325134,
        -0.1805335283279419,
        -0.21478033065795898,
        -0.821218729019165,
        0.08435641974210739,
        0.716483473777771,
        -0.839279055595398,
        -0.5618402361869812,
        -0.5490505695343018,
        0.20034249126911163,
        1.1512116193771362,
        0.6483117341995239,
        -0.3215041160583496,
        0.7287672162055969,
        0.3551384210586548,
        -0.36188244819641113,
        -0.08622220158576965,
        -0.023892492055892944,
        0.04780210554599762,
        0.7298625707626343,
        -1.0713670253753662,
        -0.7073969841003418,
        0.774929940700531,
        -0.15265502035617828,
        -0.40098053216934204,
        0.2676839530467987,
        -1.1644744873046875,
        0.2750779092311859,
        -0.3644188940525055,
        -0.20346695184707642,
        0.14403751492500305,
        -0.7443035244941711,
        0.7863105535507202,
        1.225286602973938,
        -0.002990433946251869,
        -1.509972095489502,
        0.4334101974964142,
        0.4458291828632355,
        0.691649854183197,
        -0.42264068126678467,
        -0.24022909998893738,
        0.43596869707107544,
        -0.056722451001405716,
        0.24454912543296814,
        -0.2962571382522583,
        0.9794960021972656,
        0.08755090087652206,
        0.18744923174381256,
        -0.12546280026435852,
        0.010633483529090881,
        0.6567590236663818,
        -0.4752405881881714,
        0.11408469080924988,
        -0.20145466923713684,
        -0.25623220205307007,
        0.09579908847808838,
        0.005070395767688751,
        1.9544312953948975,
        0.09353721886873245,
        -0.15790404379367828,
        0.05746561661362648,
        0.3832443058490753,
        -0.3736705780029297,
        -0.18952885270118713,
        -0.17713387310504913,
        0.015724755823612213,
        -0.35949206352233887,
        0.7788334488868713,
        0.25431427359580994,
        -0.3197118937969208,
        0.5058801174163818,
        0.19353711605072021,
        -0.6640276312828064,
        -0.06662239879369736,
        -0.4772889018058777,
        1.3920753002166748,
        -0.04232163727283478,
        -0.4898427426815033,
        -0.7830402255058289,
        0.0413271002471447,
        -0.35658586025238037,
        -0.07918466627597809,
        -0.519085705280304,
        -0.8080267906188965,
        0.5567455291748047,
        0.4474126994609833,
        -0.13807636499404907,
        0.06824858486652374,
        -0.0563521683216095,
        0.19213254749774933,
        0.5658531188964844,
        0.14198708534240723,
        0.09090317785739899,
        -0.15803861618041992,
        0.017508670687675476,
        -0.5102704763412476,
        0.2911025583744049,
        -0.3953808844089508,
        -0.49041488766670227,
        -0.32763898372650146
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Identifies and categorizes various fallacies in arguments or texts. This prompt focuses on recognizing invalid or faulty reasoning across a wide range of fallacies, from formal to informal types. The expected output is a list of identified fallacies with brief explanations.",
          "name": "Find_logical_fallacies",
          "raw": "\n                workflow Find_logical_fallacies v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert on all the different types of fallacies that are often used in argument and identifying them in input.\n\nTake a step back and think step by step about how best to identify fallacies in a text.\n\n# FALLACIES\n\nHere's a list of fallacies from Wikipedia that you can use to supplement your knowledge.\n\nA fallacy is the use of invalid or otherwise faulty reasoning in the construction of an argument. All forms of human communication can contain fallacies.\nBecause of their variety, fallacies are challenging to classify. They can be classified by their structure (formal fallacies) or content (informal fallacies). Informal fallacies, the larger group, may then be subdivided into categories such as improper presumption, faulty generalization, error in assigning causation, and relevance, among others.\nThe use of fallacies is common when the speaker's goal of achieving common agreement is more important to them than utilizing sound reasoning. When fallacies are used, the premise should be recognized as not well-grounded, the conclusion as unproven (but not necessarily false), and the argument as unsound.[1]\nFormal fallacies\nMain article: Formal fallacy\nA formal fallacy is an error in the argument's form.[2] All formal fallacies are types of non sequitur.\nAppeal to probability – taking something for granted because it would probably be the case (or might possibly be the case).[3][4]\nArgument from fallacy (also known as the fallacy fallacy) – the assumption that, if a particular argument for a \\\"conclusion\\\" is fallacious, then the conclusion by itself is false.[5]\nBase rate fallacy – making a probability judgment based on conditional probabilities, without taking into account the effect of prior probabilities.[6]\nConjunction fallacy – the assumption that an outcome simultaneously satisfying multiple conditions is more probable than an outcome satisfying a single one of them.[7]\nNon sequitur fallacy – where the conclusion does not logically follow the premise.[8]\nMasked-man fallacy (illicit substitution of identicals) – the substitution of identical designators in a true statement can lead to a false one.[9]\nPropositional fallacies\nA propositional fallacy is an error that concerns compound propositions. For a compound proposition to be true, the truth values of its constituent parts must satisfy the relevant logical connectives that occur in it (most commonly: [and], [or], [not], [only if], [if and only if]). The following fallacies involve relations whose truth values are not guaranteed and therefore not guaranteed to yield true conclusions.\nTypes of propositional fallacies:\nAffirming a disjunct – concluding that one disjunct of a logical disjunction must be false because the other disjunct is true; A or B; A, therefore not B.[10]\nAffirming the consequent – the antecedent in an indicative conditional is claimed to be true because the consequent is true; if A, then B; B, therefore A.[10]\nDenying the antecedent – the consequent in an indicative conditional is claimed to be false because the antecedent is false; if A, then B; not A, therefore not B.[10]\nQuantification fallacies\nA quantification fallacy is an error in logic where the quantifiers of the premises are in contradiction to the quantifier of the conclusion.\nTypes of quantification fallacies:\nExistential fallacy – an argument that has a universal premise and a particular conclusion.[11]\nFormal syllogistic fallacies\nSyllogistic fallacies – logical fallacies that occur in syllogisms.\nAffirmative conclusion from a negative premise (illicit negative) – a categorical syllogism has a positive conclusion, but at least one negative premise.[11]\nFallacy of exclusive premises – a categorical syllogism that is invalid because both of its premises are negative.[11]\nFallacy of four terms (quaternio terminorum) – a categorical syllogism that has four terms.[12]\nIllicit major – a categorical syllogism that is invalid because its major term is not distributed in the major premise but distributed in the conclusion.[11]\nIllicit minor – a categorical syllogism that is invalid because its minor term is not distributed in the minor premise but distributed in the conclusion.[11]\nNegative conclusion from affirmative premises (illicit affirmative) – a categorical syllogism has a negative conclusion but affirmative premises.[11]\nFallacy of the undistributed middle – the middle term in a categorical syllogism is not distributed.[13]\nModal fallacy – confusing necessity with sufficiency. A condition X is necessary for Y if X is required for even the possibility of Y. X does not bring about Y by itself, but if there is no X, there will be no Y. For example, oxygen is necessary for fire. But one cannot assume that everywhere there is oxygen, there is fire. A condition X is sufficient for Y if X, by itself, is enough to bring about Y. For example, riding the bus is a sufficient mode of transportation to get to work. But there are other modes of transportation – car, taxi, bicycle, walking – that can be used.\nModal scope fallacy – a degree of unwarranted necessity is placed in the conclusion.\nInformal fallacies\nMain article: Informal fallacy\nInformal fallacies – arguments that are logically unsound for lack of well-grounded premises.[14]\nArgument to moderation (false compromise, middle ground, fallacy of the mean, argumentum ad temperantiam) – assuming that a compromise between two positions is always correct.[15]\nContinuum fallacy (fallacy of the beard, line-drawing fallacy, sorites fallacy, fallacy of the heap, bald man fallacy, decision-point fallacy) – improperly rejecting a claim for being imprecise.[16]\nCorrelative-based fallacies\nSuppressed correlative – a correlative is redefined so that one alternative is made impossible (e.g., \\\"I'm not fat because I'm thinner than John.\\\").[17]\nDefinist fallacy – defining a term used in an argument in a biased manner (e.g., using \\\"loaded terms\\\"). The person making the argument expects that the listener will accept the provided definition, making the argument difficult to refute.[18]\nDivine fallacy (argument from incredulity) – arguing that, because something is so incredible or amazing, it must be the result of superior, divine, alien or paranormal agency.[19]\nDouble counting – counting events or occurrences more than once in probabilistic reasoning, which leads to the sum of the probabilities of all cases exceeding unity.\nEquivocation – using a term with more than one meaning in a statement without specifying which meaning is intended.[20]\nAmbiguous middle term – using a middle term with multiple meanings.[21]\nDefinitional retreat – changing the meaning of a word when an objection is raised.[22] Often paired with moving the goalposts (see below), as when an argument is challenged using a common definition of a term in the argument, and the arguer presents a different definition of the term and thereby demands different evidence to debunk the argument.\nMotte-and-bailey fallacy – conflating two positions with similar properties, one modest and easy to defend (the \\\"motte\\\") and one more controversial (the \\\"bailey\\\").[23] The arguer first states the controversial position, but when challenged, states that they are advancing the modest position.[24][25]\nFallacy of accent – changing the meaning of a statement by not specifying on which word emphasis falls.\nPersuasive definition – purporting to use the \\\"true\\\" or \\\"commonly accepted\\\" meaning of a term while, in reality, using an uncommon or altered definition.\n(cf. the if-by-whiskey fallacy)\nEcological fallacy – inferring about the nature of an entity based solely upon aggregate statistics collected for the group to which that entity belongs.[26]\nEtymological fallacy – assuming that the original or historical meaning of a word or phrase is necessarily similar to its actual present-day usage.[27]\nFallacy of composition – assuming that something true of part of a whole must also be true of the whole.[28]\nFallacy of division – assuming that something true of a composite thing must also be true of all or some of its parts.[29]\nFalse attribution – appealing to an irrelevant, unqualified, unidentified, biased or fabricated source in support of an argument.\nFallacy of quoting out of context (contextotomy, contextomy; quotation mining) – selective excerpting of words from their original context to distort the intended meaning.[30]\nFalse authority (single authority) – using an expert of dubious credentials or using only one opinion to promote a product or idea. Related to the appeal to authority.\nFalse dilemma (false dichotomy, fallacy of bifurcation, black-or-white fallacy) – two alternative statements are given as the only possible options when, in reality, there are more.[31]\nFalse equivalence – describing two or more statements as virtually equal when they are not.\nFeedback fallacy – believing in the objectivity of an evaluation to be used as the basis for improvement without verifying that the source of the evaluation is a disinterested party.[32]\nHistorian's fallacy – assuming that decision-makers of the past had identical information as those subsequently analyzing the decision.[33] This is not to be confused with presentism, in which present-day ideas and perspectives are anachronistically projected into the past.\nHistorical fallacy – believing that certain results occurred only because a specific process was performed, though said process may actually be unrelated to the results.[34]\nBaconian fallacy – supposing that historians can obtain the \\\"whole truth\\\" via induction from individual pieces of historical evidence. The \\\"whole truth\\\" is defined as learning \\\"something about everything\\\", \\\"everything about something\\\", or \\\"everything about everything\\\". In reality, a historian \\\"can only hope to know something about something\\\".[35]\nHomunculus fallacy – using a \\\"middle-man\\\" for explanation; this sometimes leads to regressive middle-men. It explains a concept in terms of the concept itself without explaining its real nature (e.g.: explaining thought as something produced by a little thinker – a homunculus – inside the head simply identifies an intermediary actor and does not explain the product or process of thinking).[36]\nInflation of conflict – arguing that, if experts in a field of knowledge disagree on a certain point within that field, no conclusion can be reached or that the legitimacy of that field of knowledge is questionable.[37][38]\nIf-by-whiskey – an argument that supports both sides of an issue by using terms that are emotionally sensitive and ambiguous.\nIncomplete comparison – insufficient information is provided to make a complete comparison.\nIntentionality fallacy – the insistence that the ultimate meaning of an expression must be consistent with the intention of the person from whom the communication originated (e.g. a work of fiction that is widely received as a blatant allegory must necessarily not be regarded as such if the author intended it not to be so).[39]\nKafkatrapping – a sophistical rhetorical device in which any denial by an accused person serves as evidence of guilt.[40][41][42]\nKettle logic – using multiple, jointly inconsistent arguments to defend a position.\nLudic fallacy – failing to take into account that non-regulated random occurrences unknown unknowns can affect the probability of an event taking place.[43]\nLump of labour fallacy – the misconception that there is a fixed amount of work to be done within an economy, which can be distributed to create more or fewer jobs.[44]\nMcNamara fallacy (quantitative fallacy) – making an argument using only quantitative observations (measurements, statistical or numerical values) and discounting subjective information that focuses on quality (traits, features, or relationships).\nMind projection fallacy – assuming that a statement about an object describes an inherent property of the object, rather than a personal perception.\nMoralistic fallacy – inferring factual conclusions from evaluative premises in violation of fact–value distinction (e.g.: inferring is from ought). Moralistic fallacy is the inverse of naturalistic fallacy.\nMoving the goalposts (raising the bar) – argument in which evidence presented in response to a specific claim is dismissed and some other (often greater) evidence is demanded.\nNirvana fallacy (perfect-solution fallacy) – solutions to problems are rejected because they are not perfect.\nPackage deal – treating essentially dissimilar concepts as though they were essentially similar.\nProof by assertion – a proposition is repeatedly restated regardless of contradiction; sometimes confused with argument from repetition (argumentum ad infinitum, argumentum ad nauseam).\nProsecutor's fallacy – a low probability of false matches does not mean a low probability of some false match being found.\nProving too much – an argument that results in an overly generalized conclusion (e.g.: arguing that drinking alcohol is bad because in some instances it has led to spousal or child abuse).\nPsychologist's fallacy – an observer presupposes the objectivity of their own perspective when analyzing a behavioral event.\nReferential fallacy[45] – assuming that all words refer to existing things and that the meaning of words reside within the things they refer to, as opposed to words possibly referring to no real object (e.g.: Pegasus) or that the meaning comes from how they are used (e.g.: \\\"nobody\\\" was in the room).\nReification (concretism, hypostatization, or the fallacy of misplaced concreteness) – treating an abstract belief or hypothetical construct as if it were a concrete, real event or physical entity (e.g.: saying that evolution selects which traits are passed on to future generations; evolution is not a conscious entity with agency).\nRetrospective determinism – believing that, because an event has occurred under some circumstance, the circumstance must have made the event inevitable (e.g.: because someone won the lottery while wearing their lucky socks, wearing those socks made winning the lottery inevitable).\nSlippery slope (thin edge of the wedge, camel's nose) – asserting that a proposed, relatively small, first action will inevitably lead to a chain of related events resulting in a significant and negative event and, therefore, should not be permitted.[46]\nSpecial pleading – the arguer attempts to cite something as an exemption to a generally accepted rule or principle without justifying the exemption (e.g.: an orphaned defendant who murdered their parents asking for leniency).\nImproper premise\nBegging the question (petitio principii) – using the conclusion of the argument in support of itself in a premise (e.g.: saying that smoking cigarettes is deadly because cigarettes can kill you; something that kills is deadly).[47][48]\nLoaded label – while not inherently fallacious, the use of evocative terms to support a conclusion is a type of begging the question fallacy. When fallaciously used, the term's connotations are relied on to sway the argument towards a particular conclusion. For example, in an organic foods advertisement that says \\\"Organic foods are safe and healthy foods grown without any pesticides, herbicides, or other unhealthy additives\\\", the terms \\\"safe\\\" and \\\"healthy\\\" are used to fallaciously imply that non-organic foods are neither safe nor healthy.[49]\nCircular reasoning (circulus in demonstrando) – the reasoner begins with what they are trying to end up with (e.g.: all bachelors are unmarried males).\nFallacy of many questions (complex question, fallacy of presuppositions, loaded question, plurium interrogationum) – someone asks a question that presupposes something that has not been proven or accepted by all the people involved. This fallacy is often used rhetorically so that the question limits direct replies to those that serve the questioner's agenda. (E.g., \\\"Have you or have you not stopped beating your wife?\\\".)\nFaulty generalizations\nFaulty generalization – reaching a conclusion from weak premises.\nAccident – an exception to a generalization is ignored.[50]\nNo true Scotsman – makes a generalization true by changing the generalization to exclude a counterexample.[51]\nCherry picking (suppressed evidence, incomplete evidence, argumeit by half-truth, fallacy of exclusion, card stacking, slanting) – using individual cases or data that confirm a particular position, while ignoring related cases or data that may contradict that position.[52][53]\nNut-picking (suppressed evidence, incomplete evidence) – using individual cases or data that falsify a particular position, while ignoring related cases or data that may support that position.\nSurvivorship bias – a small number of successes of a given process are actively promoted while completely ignoring a large number of failures.\nFalse analogy – an argument by analogy in which the analogy is poorly suited.[54]\nHasty generalization (fallacy of insufficient statistics, fallacy of insufficient sample, fallacy of the lonely fact, hasty induction, secundum quid, converse accident, jumping to conclusions) – basing a broad conclusion on a small or unrepresentative sample.[55]\nArgument from anecdote – a fallacy where anecdotal evidence is presented as an argument; without any other contributory evidence or reasoning.\nInductive fallacy – a more general name for a class of fallacies, including hasty generalization and its relatives. A fallacy of induction happens when a conclusion is drawn from premises that only lightly support it.\nMisleading vividness – involves describing an occurrence in vivid detail, even if it is an exceptional occurrence, to convince someone that it is more important; this also relies on the appeal to emotion fallacy.\nOverwhelming exception – an accurate generalization that comes with qualifications that eliminate so many cases that what remains is much less impressive than the initial statement might have led one to assume.[56]\nThought-terminating cliché – a commonly used phrase, sometimes passing as folk wisdom, used to quell cognitive dissonance, conceal lack of forethought, move on to other topics, etc. – but in any case, to end the debate with a cliché rather than a point.\nQuestionable cause\nQuestionable cause is a general type of error with many variants. Its primary basis is the confusion of association with causation, either by inappropriately deducing (or rejecting) causation or a broader failure to properly investigate the cause of an observed effect.\nCum hoc ergo propter hoc (Latin for 'with this, therefore because of this'; correlation implies causation; faulty cause/effect, coincidental correlation, correlation without causation) – a faulty assumption that, because there is a correlation between two variables, one caused the other.[57]\nPost hoc ergo propter hoc (Latin for 'after this, therefore because of this'; temporal sequence implies causation) – X happened, then Y happened; therefore X caused Y.[58]\nWrong direction (reverse causation) – cause and effect are reversed. The cause is said to be the effect and jice versa.[59] The consequence of the phenomenon is claimed to be its root cause.\nIgnoring a common cause\nFallacy of the single cause (causal oversimplification[60]) – it is assumed that there is one, simple cause of an outcome when in reality it may have been caused by a number of only jointly sufficient causes.\nFurtive fallacy – outcomes are asserted to have been caused by the malfeasance of decision makers.\nMagical thinking – fallacious attribution of causal relationships between actions and events. In anthropology, it refers primarily to cultural beliefs that ritual, prayer, sacrifice, and taboos will produce specific supernatural consequences. In psychology, it refers to an irrational belief that thoughts by themselves can affect the world or that thinking something corresponds with doing it.\nStatistical fallacies\nRegression fallacy – ascribes cause where none exists. The flaw is failing to account for natural fluctuations. It is frequently a special kind of post hoc fallacy.\nGambler's fallacy – the incorrect belief that separate, independent events can affect the likelihood of another random event. If a fair coin lands on heads 10 times in a row, the belief that it is \\\"due to the number of times it had previously landed on tails\\\" is incorrect.[61]\nInverse gambler's fallacy – the inverse of the gambler's fallacy. It is the incorrect belief that on the basis of an unlikely outcome, the process must have happened many times before.\np-hacking – belief in the significance of a result, not realizing that multiple comparisons or experiments have been run and only the most significant were published\nGarden of forking paths fallacy – incorrect belief that a single experiment can not be subject to the multiple comparisons effect.\nRelevance fallacies\nAppeal to the stone (argumentum ad lapidem) – dismissing a claim as absurd without demonstrating proof for its absurdity.[62]\nInvincible ignorance (argument by pigheadedness) – where a person simply refuses to believe the argument, ignoring any evidence given.[63]\nArgument from ignorance (appeal to ignorance, argumentum ad ignorantiam) – assuming that a claim is true because it has not been or cannot be proven false, or vice versa.[64]\nArgument from incredulity (appeal to common sense) – \\\"I cannot imagine how this could be true; therefore, it must be false.\\\"[65]\nArgument from repetition (argumentum ad nauseam or argumentum ad infinitum) – repeating an argument until nobody cares to discuss it any more and referencing that lack of objection as evidence of support for the truth of the conclusion;[66][67] sometimes confused with proof by assertion.\nArgument from silence (argumentum ex silentio) – assuming that a claim is true based on the absence of textual or spoken evidence from an authoritative source, or vice versa.[68]\nIgnoratio elenchi (irrelevant conclusion, missing the point) – an argument that may in itself be valid, but does not address the issue in question.[69]\nRed herring fallacies\nA red herring fallacy, one of the main subtypes of fallacies of relevance, is an error in logic where a proposition is, or is intended to be, misleading in order to make irrelevant or false inferences. This includes any logical inference based on fake arguments, intended to replace the lack of real arguments or to replace implicitly the subject of the discussion.[70][71]\nRed herring – introducing a second argument in response to the first argument that is irrelevant and draws attention away from the original topic (e.g.: saying \\\"If you want to complain about the dishes I leave in the sink, what about the dirty clothes you leave in the bathroom?\\\").[72] In jury trial, it is known as a Chewbacca defense. In political strategy, it is called a dead cat strategy. See also irrelevant conclusion.\nAd hominem – attacking the arguer instead of the argument. (Note that \\\"ad hominem\\\" can also refer to the dialectical strategy of arguing on the basis of the opponent's own commitments. This type of ad hominem is not a fallacy.)\nCircumstantial ad hominem – stating that the arguer's personal situation or perceived benefit from advancing a conclusion means that their conclusion is wrong.[73]\nPoisoning the well – a subtype of ad hominem presenting adverse information about a target person with the intention of discrediting everything that the target person says.[74]\nAppeal to motive – dismissing an idea by questioning the motives of its proposer.\nTone policing – focusing on emotion behind (or resulting from) a message rather than the message itself as a discrediting tactic.\nTraitorous critic fallacy (ergo decedo, 'therefore I leave') – a critic's perceived affiliation is portrayed as the underlying reason for the criticism and the critic is asked to stay away from the issue altogether. Easily confused with the association fallacy (guilt by association) below.\nAppeal to authority (argument from authority, argumentum ad verecundiam) – an assertion is deemed true because of the position or authority of the person asserting it.[75][76]\nAppeal to accomplishment – an assertion is deemed true or false based on the accomplishments of the proposer. This may often also have elements of appeal to emotion see below.\nCourtier's reply – a criticism is dismissed by claiming that the critic lacks sufficient knowledge, credentials, or training to credibly comment on the subject matter.\nAppeal to consequences (argumentum ad consequentiam) – the conclusion is supported by a premise that asserts positive or negative consequences from some course of action in an attempt to distract from the initial discussion.[77]\nAppeal to emotion – manipulating the emotions of the listener rather than using valid reasoning to obtain common agreement.[78]\nAppeal to fear – generating distress, anxiety, cynicism, or prejudice towards the opponent in an argument.[79]\nAppeal to flattery – using excessive or insincere praise to obtain common agreement.[80]\nAppeal to pity (argumentum ad misericordiam) – generating feelings of sympathy or mercy in the listener to obtain common agreement.[81]\nAppeal to ridicule (reductio ad ridiculum, reductio ad absurdum, ad absurdum) – mocking or stating that the opponent's position is laughable to deflect from the merits of the opponent's argument. (Note that \\\"reductio ad absurdum\\\" can also refer to the classic form of argument that establishes a claim by showing that the opposite scenario would lead to absurdity or contradiction. This type of reductio ad absurdum is not a fallacy.)[82]\nAppeal to spite – generating bitterness or hostility in the listener toward an opponent in an argument.[83]\nJudgmental language – using insulting or pejorative language in an argument.\nPooh-pooh – stating that an opponent's argument is unworthy of consideration.[84]\nStyle over substance – embellishing an argument with compelling language, exploiting a bias towards the esthetic qualities of an argument, e.g. the rhyme-as-reason effect[85]\nWishful thinking – arguing for a course of action by the listener according to what might be pleasing to imagine rather than according to evidence or reason.[86]\nAppeal to nature – judgment is based solely on whether the subject of judgment is 'natural' or 'unnatural'.[87] (Sometimes also called the \\\"naturalistic fallacy\\\", but is not to be confused with the other fallacies by that name.)\nAppeal to novelty (argumentum novitatis, argumentum ad antiquitatis) – a proposal is claimed to be superior or better solely because it is new or modern.[88] (opposite of appeal to tradition)\nAppeal to poverty (argumentum ad Lazarum) – supporting a conclusion because the arguer is poor (or refuting because the arguer is wealthy). (Opposite of appeal to wealth.)[89]\nAppeal to tradition (argumentum ad antiquitatem) – a conclusion supported solely because it has long been held to be true.[90]\nAppeal to wealth (argumentum ad crumenam) – supporting a conclusion because the arguer is wealthy (or refuting because the arguer is poor).[91] (Sometimes taken together with the appeal to poverty as a general appeal to the arguer's financial situation.)\nArgumentum ad baculum (appeal to the stick, appeal to force, appeal to threat) – an argument made through coercion or threats of force to support position.[92]\nArgumentum ad populum (appeal to widespread belief, bandwagon argument, appeal to the majority, appeal to the people) – a proposition is claimed to be true or good solely because a majority or many people believe it to be so.[93]\nAssociation fallacy (guilt by association and honor by association) – arguing that because two things share (or are implied to share) some property, they are the same.[94]\nLogic chopping fallacy (nit-picking, trivial objections) – Focusing on trivial details of an argument, rather than the main point of the argumentation.[95][96]\nIpse dixit (bare assertion fallacy) – a claim that is presented as true without support, as self-evidently true, or as dogmatically true. This fallacy relies on the implied expertise of the speaker or on an unstated truism.[97][98][99]\nBulverism (psychogenetic fallacy) – inferring why an argument is being used, associating it to some psychological reason, then assuming it is invalid as a result. The assumption that if the origin of an idea comes from a biased mind, then the idea itself must also be a falsehood.[37]\nChronological snobbery – a thesis is deemed incorrect because it was commonly held when something else, known to be false, was also commonly held.[100][101]\nFallacy of relative privation (also known as \\\"appeal to worse problems\\\" or \\\"not as bad as\\\") – dismissing an argument or complaint due to what are perceived to be more important problems. First World problems are a subset of this fallacy.[102][103]\nGenetic fallacy – a conclusion is suggested based solely on something or someone's origin rather than its current meaning or context.[104]\nI'm entitled to my opinion – a person discredits any opposition by claiming that they are entitled to their opinion.\nMoralistic fallacy – inferring factual conclusions from evaluative premises, in violation of fact-value distinction; e.g. making statements about what is, on the basis of claims about what ought to be. This is the inverse of the naturalistic fallacy.\nNaturalistic fallacy – inferring evaluative conclusions from purely factual premises[105][106] in violation of fact-value distinction. Naturalistic fallacy (sometimes confused with appeal to nature) is the inverse of moralistic fallacy.\nIs–ought fallacy[107] – deduce a conclusion about what ought to be, on the basis of what is.\nNaturalistic fallacy fallacy[108] (anti-naturalistic fallacy)[109] – inferring an impossibility to infer any instance of ought from is from the general invalidity of is-ought fallacy, mentioned above. For instance, is \nP\n∨\n¬\nP\n{\\displaystyle P\\lor \neg P} does imply ought \nP\n∨\n¬\nP\n{\\displaystyle P\\lor \neg P} for any proposition \nP\n{\\displaystyle P}, although the naturalistic fallacy fallacy would falsely declare such an inference invalid. Naturalistic fallacy fallacy is a type of argument from fallacy.\nStraw man fallacy – refuting an argument different from the one actually under discussion, while not recognizing or acknowledging the distinction.[110]\nTexas sharpshooter fallacy – improperly asserting a cause to explain a cluster of data.[111]\nTu quoque ('you too' – appeal to hypocrisy, whataboutism) – stating that a position is false, wrong, or should be disregarded because its proponent fails to act consistently in accordance with it.[112]\nTwo wrongs make a right – assuming that, if one wrong is committed, another wrong will rectify it.[113]\nVacuous truth – a claim that is technically true but meaningless, in the form no A in B has C, when there is no A in B. For example, claiming that no mobile phones in the room are on when there are no mobile phones in the room.\n\n# STEPS\n\n- Read the input text and find all instances of fallacies in the text.\n\n- Write those fallacies in a list on a virtual whiteboard in your mind.\n\n# OUTPUT\n\n- In a section called FALLACIES, list all the fallacies you found in the text using the structure of:\n\n\\\"- Fallacy Name: Fallacy Type — 15 word explanation.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n\n- Don't use bold or italic formatting in the Markdown.\n\n- Do no complain about the input data. Just do the task.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert on all the different types of fallacies that are often used in argument and identifying them in input.\n\nTake a step back and think step by step about how best to identify fallacies in a text.\n\n# FALLACIES\n\nHere's a list of fallacies from Wikipedia that you can use to supplement your knowledge.\n\nA fallacy is the use of invalid or otherwise faulty reasoning in the construction of an argument. All forms of human communication can contain fallacies.\nBecause of their variety, fallacies are challenging to classify. They can be classified by their structure (formal fallacies) or content (informal fallacies). Informal fallacies, the larger group, may then be subdivided into categories such as improper presumption, faulty generalization, error in assigning causation, and relevance, among others.\nThe use of fallacies is common when the speaker's goal of achieving common agreement is more important to them than utilizing sound reasoning. When fallacies are used, the premise should be recognized as not well-grounded, the conclusion as unproven (but not necessarily false), and the argument as unsound.[1]\nFormal fallacies\nMain article: Formal fallacy\nA formal fallacy is an error in the argument's form.[2] All formal fallacies are types of non sequitur.\nAppeal to probability – taking something for granted because it would probably be the case (or might possibly be the case).[3][4]\nArgument from fallacy (also known as the fallacy fallacy) – the assumption that, if a particular argument for a \\\"conclusion\\\" is fallacious, then the conclusion by itself is false.[5]\nBase rate fallacy – making a probability judgment based on conditional probabilities, without taking into account the effect of prior probabilities.[6]\nConjunction fallacy – the assumption that an outcome simultaneously satisfying multiple conditions is more probable than an outcome satisfying a single one of them.[7]\nNon sequitur fallacy – where the conclusion does not logically follow the premise.[8]\nMasked-man fallacy (illicit substitution of identicals) – the substitution of identical designators in a true statement can lead to a false one.[9]\nPropositional fallacies\nA propositional fallacy is an error that concerns compound propositions. For a compound proposition to be true, the truth values of its constituent parts must satisfy the relevant logical connectives that occur in it (most commonly: [and], [or], [not], [only if], [if and only if]). The following fallacies involve relations whose truth values are not guaranteed and therefore not guaranteed to yield true conclusions.\nTypes of propositional fallacies:\nAffirming a disjunct – concluding that one disjunct of a logical disjunction must be false because the other disjunct is true; A or B; A, therefore not B.[10]\nAffirming the consequent – the antecedent in an indicative conditional is claimed to be true because the consequent is true; if A, then B; B, therefore A.[10]\nDenying the antecedent – the consequent in an indicative conditional is claimed to be false because the antecedent is false; if A, then B; not A, therefore not B.[10]\nQuantification fallacies\nA quantification fallacy is an error in logic where the quantifiers of the premises are in contradiction to the quantifier of the conclusion.\nTypes of quantification fallacies:\nExistential fallacy – an argument that has a universal premise and a particular conclusion.[11]\nFormal syllogistic fallacies\nSyllogistic fallacies – logical fallacies that occur in syllogisms.\nAffirmative conclusion from a negative premise (illicit negative) – a categorical syllogism has a positive conclusion, but at least one negative premise.[11]\nFallacy of exclusive premises – a categorical syllogism that is invalid because both of its premises are negative.[11]\nFallacy of four terms (quaternio terminorum) – a categorical syllogism that has four terms.[12]\nIllicit major – a categorical syllogism that is invalid because its major term is not distributed in the major premise but distributed in the conclusion.[11]\nIllicit minor – a categorical syllogism that is invalid because its minor term is not distributed in the minor premise but distributed in the conclusion.[11]\nNegative conclusion from affirmative premises (illicit affirmative) – a categorical syllogism has a negative conclusion but affirmative premises.[11]\nFallacy of the undistributed middle – the middle term in a categorical syllogism is not distributed.[13]\nModal fallacy – confusing necessity with sufficiency. A condition X is necessary for Y if X is required for even the possibility of Y. X does not bring about Y by itself, but if there is no X, there will be no Y. For example, oxygen is necessary for fire. But one cannot assume that everywhere there is oxygen, there is fire. A condition X is sufficient for Y if X, by itself, is enough to bring about Y. For example, riding the bus is a sufficient mode of transportation to get to work. But there are other modes of transportation – car, taxi, bicycle, walking – that can be used.\nModal scope fallacy – a degree of unwarranted necessity is placed in the conclusion.\nInformal fallacies\nMain article: Informal fallacy\nInformal fallacies – arguments that are logically unsound for lack of well-grounded premises.[14]\nArgument to moderation (false compromise, middle ground, fallacy of the mean, argumentum ad temperantiam) – assuming that a compromise between two positions is always correct.[15]\nContinuum fallacy (fallacy of the beard, line-drawing fallacy, sorites fallacy, fallacy of the heap, bald man fallacy, decision-point fallacy) – improperly rejecting a claim for being imprecise.[16]\nCorrelative-based fallacies\nSuppressed correlative – a correlative is redefined so that one alternative is made impossible (e.g., \\\"I'm not fat because I'm thinner than John.\\\").[17]\nDefinist fallacy – defining a term used in an argument in a biased manner (e.g., using \\\"loaded terms\\\"). The person making the argument expects that the listener will accept the provided definition, making the argument difficult to refute.[18]\nDivine fallacy (argument from incredulity) – arguing that, because something is so incredible or amazing, it must be the result of superior, divine, alien or paranormal agency.[19]\nDouble counting – counting events or occurrences more than once in probabilistic reasoning, which leads to the sum of the probabilities of all cases exceeding unity.\nEquivocation – using a term with more than one meaning in a statement without specifying which meaning is intended.[20]\nAmbiguous middle term – using a middle term with multiple meanings.[21]\nDefinitional retreat – changing the meaning of a word when an objection is raised.[22] Often paired with moving the goalposts (see below), as when an argument is challenged using a common definition of a term in the argument, and the arguer presents a different definition of the term and thereby demands different evidence to debunk the argument.\nMotte-and-bailey fallacy – conflating two positions with similar properties, one modest and easy to defend (the \\\"motte\\\") and one more controversial (the \\\"bailey\\\").[23] The arguer first states the controversial position, but when challenged, states that they are advancing the modest position.[24][25]\nFallacy of accent – changing the meaning of a statement by not specifying on which word emphasis falls.\nPersuasive definition – purporting to use the \\\"true\\\" or \\\"commonly accepted\\\" meaning of a term while, in reality, using an uncommon or altered definition.\n(cf. the if-by-whiskey fallacy)\nEcological fallacy – inferring about the nature of an entity based solely upon aggregate statistics collected for the group to which that entity belongs.[26]\nEtymological fallacy – assuming that the original or historical meaning of a word or phrase is necessarily similar to its actual present-day usage.[27]\nFallacy of composition – assuming that something true of part of a whole must also be true of the whole.[28]\nFallacy of division – assuming that something true of a composite thing must also be true of all or some of its parts.[29]\nFalse attribution – appealing to an irrelevant, unqualified, unidentified, biased or fabricated source in support of an argument.\nFallacy of quoting out of context (contextotomy, contextomy; quotation mining) – selective excerpting of words from their original context to distort the intended meaning.[30]\nFalse authority (single authority) – using an expert of dubious credentials or using only one opinion to promote a product or idea. Related to the appeal to authority.\nFalse dilemma (false dichotomy, fallacy of bifurcation, black-or-white fallacy) – two alternative statements are given as the only possible options when, in reality, there are more.[31]\nFalse equivalence – describing two or more statements as virtually equal when they are not.\nFeedback fallacy – believing in the objectivity of an evaluation to be used as the basis for improvement without verifying that the source of the evaluation is a disinterested party.[32]\nHistorian's fallacy – assuming that decision-makers of the past had identical information as those subsequently analyzing the decision.[33] This is not to be confused with presentism, in which present-day ideas and perspectives are anachronistically projected into the past.\nHistorical fallacy – believing that certain results occurred only because a specific process was performed, though said process may actually be unrelated to the results.[34]\nBaconian fallacy – supposing that historians can obtain the \\\"whole truth\\\" via induction from individual pieces of historical evidence. The \\\"whole truth\\\" is defined as learning \\\"something about everything\\\", \\\"everything about something\\\", or \\\"everything about everything\\\". In reality, a historian \\\"can only hope to know something about something\\\".[35]\nHomunculus fallacy – using a \\\"middle-man\\\" for explanation; this sometimes leads to regressive middle-men. It explains a concept in terms of the concept itself without explaining its real nature (e.g.: explaining thought as something produced by a little thinker – a homunculus – inside the head simply identifies an intermediary actor and does not explain the product or process of thinking).[36]\nInflation of conflict – arguing that, if experts in a field of knowledge disagree on a certain point within that field, no conclusion can be reached or that the legitimacy of that field of knowledge is questionable.[37][38]\nIf-by-whiskey – an argument that supports both sides of an issue by using terms that are emotionally sensitive and ambiguous.\nIncomplete comparison – insufficient information is provided to make a complete comparison.\nIntentionality fallacy – the insistence that the ultimate meaning of an expression must be consistent with the intention of the person from whom the communication originated (e.g. a work of fiction that is widely received as a blatant allegory must necessarily not be regarded as such if the author intended it not to be so).[39]\nKafkatrapping – a sophistical rhetorical device in which any denial by an accused person serves as evidence of guilt.[40][41][42]\nKettle logic – using multiple, jointly inconsistent arguments to defend a position.\nLudic fallacy – failing to take into account that non-regulated random occurrences unknown unknowns can affect the probability of an event taking place.[43]\nLump of labour fallacy – the misconception that there is a fixed amount of work to be done within an economy, which can be distributed to create more or fewer jobs.[44]\nMcNamara fallacy (quantitative fallacy) – making an argument using only quantitative observations (measurements, statistical or numerical values) and discounting subjective information that focuses on quality (traits, features, or relationships).\nMind projection fallacy – assuming that a statement about an object describes an inherent property of the object, rather than a personal perception.\nMoralistic fallacy – inferring factual conclusions from evaluative premises in violation of fact–value distinction (e.g.: inferring is from ought). Moralistic fallacy is the inverse of naturalistic fallacy.\nMoving the goalposts (raising the bar) – argument in which evidence presented in response to a specific claim is dismissed and some other (often greater) evidence is demanded.\nNirvana fallacy (perfect-solution fallacy) – solutions to problems are rejected because they are not perfect.\nPackage deal – treating essentially dissimilar concepts as though they were essentially similar.\nProof by assertion – a proposition is repeatedly restated regardless of contradiction; sometimes confused with argument from repetition (argumentum ad infinitum, argumentum ad nauseam).\nProsecutor's fallacy – a low probability of false matches does not mean a low probability of some false match being found.\nProving too much – an argument that results in an overly generalized conclusion (e.g.: arguing that drinking alcohol is bad because in some instances it has led to spousal or child abuse).\nPsychologist's fallacy – an observer presupposes the objectivity of their own perspective when analyzing a behavioral event.\nReferential fallacy[45] – assuming that all words refer to existing things and that the meaning of words reside within the things they refer to, as opposed to words possibly referring to no real object (e.g.: Pegasus) or that the meaning comes from how they are used (e.g.: \\\"nobody\\\" was in the room).\nReification (concretism, hypostatization, or the fallacy of misplaced concreteness) – treating an abstract belief or hypothetical construct as if it were a concrete, real event or physical entity (e.g.: saying that evolution selects which traits are passed on to future generations; evolution is not a conscious entity with agency).\nRetrospective determinism – believing that, because an event has occurred under some circumstance, the circumstance must have made the event inevitable (e.g.: because someone won the lottery while wearing their lucky socks, wearing those socks made winning the lottery inevitable).\nSlippery slope (thin edge of the wedge, camel's nose) – asserting that a proposed, relatively small, first action will inevitably lead to a chain of related events resulting in a significant and negative event and, therefore, should not be permitted.[46]\nSpecial pleading – the arguer attempts to cite something as an exemption to a generally accepted rule or principle without justifying the exemption (e.g.: an orphaned defendant who murdered their parents asking for leniency).\nImproper premise\nBegging the question (petitio principii) – using the conclusion of the argument in support of itself in a premise (e.g.: saying that smoking cigarettes is deadly because cigarettes can kill you; something that kills is deadly).[47][48]\nLoaded label – while not inherently fallacious, the use of evocative terms to support a conclusion is a type of begging the question fallacy. When fallaciously used, the term's connotations are relied on to sway the argument towards a particular conclusion. For example, in an organic foods advertisement that says \\\"Organic foods are safe and healthy foods grown without any pesticides, herbicides, or other unhealthy additives\\\", the terms \\\"safe\\\" and \\\"healthy\\\" are used to fallaciously imply that non-organic foods are neither safe nor healthy.[49]\nCircular reasoning (circulus in demonstrando) – the reasoner begins with what they are trying to end up with (e.g.: all bachelors are unmarried males).\nFallacy of many questions (complex question, fallacy of presuppositions, loaded question, plurium interrogationum) – someone asks a question that presupposes something that has not been proven or accepted by all the people involved. This fallacy is often used rhetorically so that the question limits direct replies to those that serve the questioner's agenda. (E.g., \\\"Have you or have you not stopped beating your wife?\\\".)\nFaulty generalizations\nFaulty generalization – reaching a conclusion from weak premises.\nAccident – an exception to a generalization is ignored.[50]\nNo true Scotsman – makes a generalization true by changing the generalization to exclude a counterexample.[51]\nCherry picking (suppressed evidence, incomplete evidence, argumeit by half-truth, fallacy of exclusion, card stacking, slanting) – using individual cases or data that confirm a particular position, while ignoring related cases or data that may contradict that position.[52][53]\nNut-picking (suppressed evidence, incomplete evidence) – using individual cases or data that falsify a particular position, while ignoring related cases or data that may support that position.\nSurvivorship bias – a small number of successes of a given process are actively promoted while completely ignoring a large number of failures.\nFalse analogy – an argument by analogy in which the analogy is poorly suited.[54]\nHasty generalization (fallacy of insufficient statistics, fallacy of insufficient sample, fallacy of the lonely fact, hasty induction, secundum quid, converse accident, jumping to conclusions) – basing a broad conclusion on a small or unrepresentative sample.[55]\nArgument from anecdote – a fallacy where anecdotal evidence is presented as an argument; without any other contributory evidence or reasoning.\nInductive fallacy – a more general name for a class of fallacies, including hasty generalization and its relatives. A fallacy of induction happens when a conclusion is drawn from premises that only lightly support it.\nMisleading vividness – involves describing an occurrence in vivid detail, even if it is an exceptional occurrence, to convince someone that it is more important; this also relies on the appeal to emotion fallacy.\nOverwhelming exception – an accurate generalization that comes with qualifications that eliminate so many cases that what remains is much less impressive than the initial statement might have led one to assume.[56]\nThought-terminating cliché – a commonly used phrase, sometimes passing as folk wisdom, used to quell cognitive dissonance, conceal lack of forethought, move on to other topics, etc. – but in any case, to end the debate with a cliché rather than a point.\nQuestionable cause\nQuestionable cause is a general type of error with many variants. Its primary basis is the confusion of association with causation, either by inappropriately deducing (or rejecting) causation or a broader failure to properly investigate the cause of an observed effect.\nCum hoc ergo propter hoc (Latin for 'with this, therefore because of this'; correlation implies causation; faulty cause/effect, coincidental correlation, correlation without causation) – a faulty assumption that, because there is a correlation between two variables, one caused the other.[57]\nPost hoc ergo propter hoc (Latin for 'after this, therefore because of this'; temporal sequence implies causation) – X happened, then Y happened; therefore X caused Y.[58]\nWrong direction (reverse causation) – cause and effect are reversed. The cause is said to be the effect and jice versa.[59] The consequence of the phenomenon is claimed to be its root cause.\nIgnoring a common cause\nFallacy of the single cause (causal oversimplification[60]) – it is assumed that there is one, simple cause of an outcome when in reality it may have been caused by a number of only jointly sufficient causes.\nFurtive fallacy – outcomes are asserted to have been caused by the malfeasance of decision makers.\nMagical thinking – fallacious attribution of causal relationships between actions and events. In anthropology, it refers primarily to cultural beliefs that ritual, prayer, sacrifice, and taboos will produce specific supernatural consequences. In psychology, it refers to an irrational belief that thoughts by themselves can affect the world or that thinking something corresponds with doing it.\nStatistical fallacies\nRegression fallacy – ascribes cause where none exists. The flaw is failing to account for natural fluctuations. It is frequently a special kind of post hoc fallacy.\nGambler's fallacy – the incorrect belief that separate, independent events can affect the likelihood of another random event. If a fair coin lands on heads 10 times in a row, the belief that it is \\\"due to the number of times it had previously landed on tails\\\" is incorrect.[61]\nInverse gambler's fallacy – the inverse of the gambler's fallacy. It is the incorrect belief that on the basis of an unlikely outcome, the process must have happened many times before.\np-hacking – belief in the significance of a result, not realizing that multiple comparisons or experiments have been run and only the most significant were published\nGarden of forking paths fallacy – incorrect belief that a single experiment can not be subject to the multiple comparisons effect.\nRelevance fallacies\nAppeal to the stone (argumentum ad lapidem) – dismissing a claim as absurd without demonstrating proof for its absurdity.[62]\nInvincible ignorance (argument by pigheadedness) – where a person simply refuses to believe the argument, ignoring any evidence given.[63]\nArgument from ignorance (appeal to ignorance, argumentum ad ignorantiam) – assuming that a claim is true because it has not been or cannot be proven false, or vice versa.[64]\nArgument from incredulity (appeal to common sense) – \\\"I cannot imagine how this could be true; therefore, it must be false.\\\"[65]\nArgument from repetition (argumentum ad nauseam or argumentum ad infinitum) – repeating an argument until nobody cares to discuss it any more and referencing that lack of objection as evidence of support for the truth of the conclusion;[66][67] sometimes confused with proof by assertion.\nArgument from silence (argumentum ex silentio) – assuming that a claim is true based on the absence of textual or spoken evidence from an authoritative source, or vice versa.[68]\nIgnoratio elenchi (irrelevant conclusion, missing the point) – an argument that may in itself be valid, but does not address the issue in question.[69]\nRed herring fallacies\nA red herring fallacy, one of the main subtypes of fallacies of relevance, is an error in logic where a proposition is, or is intended to be, misleading in order to make irrelevant or false inferences. This includes any logical inference based on fake arguments, intended to replace the lack of real arguments or to replace implicitly the subject of the discussion.[70][71]\nRed herring – introducing a second argument in response to the first argument that is irrelevant and draws attention away from the original topic (e.g.: saying \\\"If you want to complain about the dishes I leave in the sink, what about the dirty clothes you leave in the bathroom?\\\").[72] In jury trial, it is known as a Chewbacca defense. In political strategy, it is called a dead cat strategy. See also irrelevant conclusion.\nAd hominem – attacking the arguer instead of the argument. (Note that \\\"ad hominem\\\" can also refer to the dialectical strategy of arguing on the basis of the opponent's own commitments. This type of ad hominem is not a fallacy.)\nCircumstantial ad hominem – stating that the arguer's personal situation or perceived benefit from advancing a conclusion means that their conclusion is wrong.[73]\nPoisoning the well – a subtype of ad hominem presenting adverse information about a target person with the intention of discrediting everything that the target person says.[74]\nAppeal to motive – dismissing an idea by questioning the motives of its proposer.\nTone policing – focusing on emotion behind (or resulting from) a message rather than the message itself as a discrediting tactic.\nTraitorous critic fallacy (ergo decedo, 'therefore I leave') – a critic's perceived affiliation is portrayed as the underlying reason for the criticism and the critic is asked to stay away from the issue altogether. Easily confused with the association fallacy (guilt by association) below.\nAppeal to authority (argument from authority, argumentum ad verecundiam) – an assertion is deemed true because of the position or authority of the person asserting it.[75][76]\nAppeal to accomplishment – an assertion is deemed true or false based on the accomplishments of the proposer. This may often also have elements of appeal to emotion see below.\nCourtier's reply – a criticism is dismissed by claiming that the critic lacks sufficient knowledge, credentials, or training to credibly comment on the subject matter.\nAppeal to consequences (argumentum ad consequentiam) – the conclusion is supported by a premise that asserts positive or negative consequences from some course of action in an attempt to distract from the initial discussion.[77]\nAppeal to emotion – manipulating the emotions of the listener rather than using valid reasoning to obtain common agreement.[78]\nAppeal to fear – generating distress, anxiety, cynicism, or prejudice towards the opponent in an argument.[79]\nAppeal to flattery – using excessive or insincere praise to obtain common agreement.[80]\nAppeal to pity (argumentum ad misericordiam) – generating feelings of sympathy or mercy in the listener to obtain common agreement.[81]\nAppeal to ridicule (reductio ad ridiculum, reductio ad absurdum, ad absurdum) – mocking or stating that the opponent's position is laughable to deflect from the merits of the opponent's argument. (Note that \\\"reductio ad absurdum\\\" can also refer to the classic form of argument that establishes a claim by showing that the opposite scenario would lead to absurdity or contradiction. This type of reductio ad absurdum is not a fallacy.)[82]\nAppeal to spite – generating bitterness or hostility in the listener toward an opponent in an argument.[83]\nJudgmental language – using insulting or pejorative language in an argument.\nPooh-pooh – stating that an opponent's argument is unworthy of consideration.[84]\nStyle over substance – embellishing an argument with compelling language, exploiting a bias towards the esthetic qualities of an argument, e.g. the rhyme-as-reason effect[85]\nWishful thinking – arguing for a course of action by the listener according to what might be pleasing to imagine rather than according to evidence or reason.[86]\nAppeal to nature – judgment is based solely on whether the subject of judgment is 'natural' or 'unnatural'.[87] (Sometimes also called the \\\"naturalistic fallacy\\\", but is not to be confused with the other fallacies by that name.)\nAppeal to novelty (argumentum novitatis, argumentum ad antiquitatis) – a proposal is claimed to be superior or better solely because it is new or modern.[88] (opposite of appeal to tradition)\nAppeal to poverty (argumentum ad Lazarum) – supporting a conclusion because the arguer is poor (or refuting because the arguer is wealthy). (Opposite of appeal to wealth.)[89]\nAppeal to tradition (argumentum ad antiquitatem) – a conclusion supported solely because it has long been held to be true.[90]\nAppeal to wealth (argumentum ad crumenam) – supporting a conclusion because the arguer is wealthy (or refuting because the arguer is poor).[91] (Sometimes taken together with the appeal to poverty as a general appeal to the arguer's financial situation.)\nArgumentum ad baculum (appeal to the stick, appeal to force, appeal to threat) – an argument made through coercion or threats of force to support position.[92]\nArgumentum ad populum (appeal to widespread belief, bandwagon argument, appeal to the majority, appeal to the people) – a proposition is claimed to be true or good solely because a majority or many people believe it to be so.[93]\nAssociation fallacy (guilt by association and honor by association) – arguing that because two things share (or are implied to share) some property, they are the same.[94]\nLogic chopping fallacy (nit-picking, trivial objections) – Focusing on trivial details of an argument, rather than the main point of the argumentation.[95][96]\nIpse dixit (bare assertion fallacy) – a claim that is presented as true without support, as self-evidently true, or as dogmatically true. This fallacy relies on the implied expertise of the speaker or on an unstated truism.[97][98][99]\nBulverism (psychogenetic fallacy) – inferring why an argument is being used, associating it to some psychological reason, then assuming it is invalid as a result. The assumption that if the origin of an idea comes from a biased mind, then the idea itself must also be a falsehood.[37]\nChronological snobbery – a thesis is deemed incorrect because it was commonly held when something else, known to be false, was also commonly held.[100][101]\nFallacy of relative privation (also known as \\\"appeal to worse problems\\\" or \\\"not as bad as\\\") – dismissing an argument or complaint due to what are perceived to be more important problems. First World problems are a subset of this fallacy.[102][103]\nGenetic fallacy – a conclusion is suggested based solely on something or someone's origin rather than its current meaning or context.[104]\nI'm entitled to my opinion – a person discredits any opposition by claiming that they are entitled to their opinion.\nMoralistic fallacy – inferring factual conclusions from evaluative premises, in violation of fact-value distinction; e.g. making statements about what is, on the basis of claims about what ought to be. This is the inverse of the naturalistic fallacy.\nNaturalistic fallacy – inferring evaluative conclusions from purely factual premises[105][106] in violation of fact-value distinction. Naturalistic fallacy (sometimes confused with appeal to nature) is the inverse of moralistic fallacy.\nIs–ought fallacy[107] – deduce a conclusion about what ought to be, on the basis of what is.\nNaturalistic fallacy fallacy[108] (anti-naturalistic fallacy)[109] – inferring an impossibility to infer any instance of ought from is from the general invalidity of is-ought fallacy, mentioned above. For instance, is \nP\n∨\n¬\nP\n{\\displaystyle P\\lor \neg P} does imply ought \nP\n∨\n¬\nP\n{\\displaystyle P\\lor \neg P} for any proposition \nP\n{\\displaystyle P}, although the naturalistic fallacy fallacy would falsely declare such an inference invalid. Naturalistic fallacy fallacy is a type of argument from fallacy.\nStraw man fallacy – refuting an argument different from the one actually under discussion, while not recognizing or acknowledging the distinction.[110]\nTexas sharpshooter fallacy – improperly asserting a cause to explain a cluster of data.[111]\nTu quoque ('you too' – appeal to hypocrisy, whataboutism) – stating that a position is false, wrong, or should be disregarded because its proponent fails to act consistently in accordance with it.[112]\nTwo wrongs make a right – assuming that, if one wrong is committed, another wrong will rectify it.[113]\nVacuous truth – a claim that is technically true but meaningless, in the form no A in B has C, when there is no A in B. For example, claiming that no mobile phones in the room are on when there are no mobile phones in the room.\n\n# STEPS\n\n- Read the input text and find all instances of fallacies in the text.\n\n- Write those fallacies in a list on a virtual whiteboard in your mind.\n\n# OUTPUT\n\n- In a section called FALLACIES, list all the fallacies you found in the text using the structure of:\n\n\\\"- Fallacy Name: Fallacy Type — 15 word explanation.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- You output in Markdown, using each section header followed by the content for that section.\n\n- Don't use bold or italic formatting in the Markdown.\n\n- Do no complain about the input data. Just do the task.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.1985473781824112,
        0.9120718836784363,
        -0.4612686038017273,
        0.37746307253837585,
        -0.28208932280540466,
        -0.24089577794075012,
        -0.8465203642845154,
        0.13813653588294983,
        0.28913480043411255,
        0.2999204397201538,
        -0.7663843631744385,
        0.7577115893363953,
        0.2515794038772583,
        -0.07693049311637878,
        0.247687429189682,
        -0.3212842643260956,
        -0.37218695878982544,
        -0.9313417077064514,
        -1.9845259189605713,
        -0.0404207780957222,
        0.12895989418029785,
        1.0355998277664185,
        0.24007445573806763,
        -0.08002133667469025,
        0.3202008306980133,
        0.12472675740718842,
        0.1934954822063446,
        0.03000509738922119,
        -1.0450592041015625,
        -1.7722731828689575,
        0.2048749029636383,
        0.4453124701976776,
        -0.21569493412971497,
        -0.20318537950515747,
        -0.18490371108055115,
        -0.8490670919418335,
        -0.16719187796115875,
        -0.10920310020446777,
        -0.3230357766151428,
        -0.2417503297328949,
        0.3865516185760498,
        -0.12354598194360733,
        -0.4544260799884796,
        -0.23483151197433472,
        0.1810809224843979,
        -0.3109104037284851,
        0.33277755975723267,
        -0.40078163146972656,
        1.0872293710708618,
        0.3281039595603943,
        0.09337462484836578,
        -0.4357380270957947,
        0.5125831365585327,
        0.21983115375041962,
        -0.25931617617607117,
        0.22627297043800354,
        0.13224253058433533,
        -0.45093798637390137,
        -0.12104415893554688,
        -0.1296578198671341,
        -0.0939585343003273,
        0.20426681637763977,
        -3.9885284900665283,
        -0.1725769191980362,
        0.13440357148647308,
        0.3697507083415985,
        0.1762029528617859,
        -0.4161793291568756,
        0.3085786700248718,
        0.3760603070259094,
        0.1719810962677002,
        -0.14655949175357819,
        -0.254347026348114,
        0.7466235756874084,
        -0.14261429011821747,
        -0.3100339472293854,
        0.20741993188858032,
        -0.20265430212020874,
        0.2874995470046997,
        -0.3579096794128418,
        0.12165805697441101,
        -0.1004209816455841,
        0.2494288831949234,
        -0.2046261578798294,
        -0.3724682331085205,
        0.6229275465011597,
        0.015124164521694183,
        -0.04496593028306961,
        0.5593181252479553,
        -0.004606256261467934,
        -0.7123263478279114,
        -0.39745819568634033,
        -0.046762607991695404,
        -0.49544161558151245,
        -0.07255621999502182,
        -0.09259843826293945,
        -0.15053734183311462,
        -0.43562155961990356,
        -0.08659818023443222,
        3.257699489593506,
        0.7504807114601135,
        -0.1849331259727478,
        0.7685568332672119,
        -1.4974809885025024,
        0.3428120017051697,
        -0.21338911354541779,
        0.009288955479860306,
        -0.47275155782699585,
        -0.10830866545438766,
        -0.19265037775039673,
        0.10053099691867828,
        -0.23404932022094727,
        -0.6373847723007202,
        -0.0503331683576107,
        0.011236890219151974,
        0.628183126449585,
        -0.48958027362823486,
        0.3751336336135864,
        -0.362625390291214,
        0.8983412384986877,
        -0.4489818215370178,
        -0.017756283283233643,
        -0.5078306794166565,
        -0.5591927170753479,
        0.30118420720100403,
        0.14581233263015747,
        -0.23266632854938507,
        0.4809078276157379,
        -0.3612351417541504,
        0.1626550704240799,
        0.22595477104187012,
        -0.19190537929534912,
        -0.3329378664493561,
        -0.32217705249786377,
        0.6521522998809814,
        0.23316004872322083,
        0.6854340434074402,
        -0.7310901880264282,
        0.4169767200946808,
        -0.506468653678894,
        -0.09192466735839844,
        -0.9982551336288452,
        0.8494895100593567,
        -0.42361360788345337,
        0.5070788264274597,
        -0.13647353649139404,
        -0.5049251914024353,
        0.3830835819244385,
        -0.14016889035701752,
        -0.506550669670105,
        -0.17427095770835876,
        0.0009846165776252747,
        0.06455591320991516,
        0.2273862063884735,
        0.9230440855026245,
        0.40895843505859375,
        -0.23475898802280426,
        0.13503143191337585,
        -0.16283009946346283,
        0.6615808010101318,
        0.09547196328639984,
        0.017552832141518593,
        -0.14293596148490906,
        -0.06797213852405548,
        0.3468620479106903,
        -0.24216529726982117,
        -0.00935983657836914,
        0.24386093020439148,
        0.67188560962677,
        -0.38116833567619324,
        0.6905672550201416,
        -0.282731294631958,
        0.23239527642726898,
        0.6726035475730896,
        -0.3744533061981201,
        0.011508334428071976,
        -0.033916082233190536,
        0.5703503489494324,
        0.22760215401649475,
        -0.4429958164691925,
        0.1613101065158844,
        1.213364839553833,
        -0.3777083456516266,
        -0.9432515501976013,
        -0.2595195770263672,
        0.06471626460552216,
        -0.07882920652627945,
        0.09523509442806244,
        0.2128591239452362,
        1.0663098096847534,
        -1.2092012166976929,
        1.697493314743042,
        -0.4588969945907593,
        -0.18628264963626862,
        -0.08596938103437424,
        -0.07932743430137634,
        0.19330178201198578,
        0.29602277278900146,
        0.8732040524482727,
        -0.029124673455953598,
        -1.5135784149169922,
        -0.39905592799186707,
        -0.6615055203437805,
        -0.42100629210472107,
        -0.3237108886241913,
        -0.03366703912615776,
        0.4864002466201782,
        -0.10734905302524567,
        0.10914650559425354,
        -0.23827770352363586,
        -0.6383704543113708,
        0.32390570640563965,
        1.2748911380767822,
        0.6538441181182861,
        0.5570157766342163,
        0.5131927132606506,
        0.48025378584861755,
        -0.5340485572814941,
        0.012823633849620819,
        0.7203209400177002,
        0.22759994864463806,
        -0.2082003951072693,
        -0.48249930143356323,
        -0.2905659079551697,
        -0.07475745677947998,
        0.5982184410095215,
        -0.22640511393547058,
        0.5730885863304138,
        0.07986928522586823,
        -0.2931251525878906,
        0.40856117010116577,
        0.7990278005599976,
        0.6786494851112366,
        1.332589864730835,
        -0.6527857780456543,
        0.4983612596988678,
        -0.5805754661560059,
        -0.09005539119243622,
        -0.6609988212585449,
        -1.1743358373641968,
        -0.08730647712945938,
        -0.12182052433490753,
        0.4249367117881775,
        -0.19995838403701782,
        -0.23824003338813782,
        -0.5298289060592651,
        0.12729614973068237,
        -0.603519082069397,
        -0.5513681769371033,
        1.7534449100494385,
        0.8141413927078247,
        -0.28050094842910767,
        0.28101199865341187,
        0.6906198859214783,
        -0.22982367873191833,
        -0.10255035012960434,
        -1.6316455602645874,
        -0.23927082121372223,
        -0.12253367900848389,
        0.33930015563964844,
        -0.296772837638855,
        0.17113056778907776,
        0.5360385179519653,
        -0.12476962059736252,
        -0.06923231482505798,
        0.0591413713991642,
        -0.537306547164917,
        -0.4212915301322937,
        -0.17366592586040497,
        0.01474020630121231,
        -0.00965978018939495,
        0.01866242289543152,
        -0.6271249651908875,
        0.10692678391933441,
        0.380985826253891,
        0.19311664998531342,
        -0.32085198163986206,
        0.17227530479431152,
        0.013879090547561646,
        -0.6910660266876221,
        0.6591628193855286,
        0.1256866306066513,
        -0.47685524821281433,
        0.39185279607772827,
        0.04376908391714096,
        -0.7246202826499939,
        -0.38897469639778137,
        -1.081126093864441,
        0.24672996997833252,
        0.42204442620277405,
        -0.010655578225851059,
        -0.3474794328212738,
        -0.45935600996017456,
        0.4200488030910492,
        1.5429723262786865,
        0.5946428775787354,
        0.5311449766159058,
        0.13292840123176575,
        -0.0029697627760469913,
        0.3776977062225342,
        -0.004582561552524567,
        0.7124148011207581,
        0.547641932964325,
        0.30012285709381104,
        -1.3373692035675049,
        -0.21562084555625916,
        0.6480855345726013,
        0.0751020610332489,
        -0.09515931457281113,
        0.29281309247016907,
        -0.8472761511802673,
        0.2791544795036316,
        0.037463296204805374,
        0.3808392584323883,
        0.4830607771873474,
        -0.5220520496368408,
        0.42839315533638,
        1.2501145601272583,
        0.12521521747112274,
        -1.5069587230682373,
        -0.5398899912834167,
        0.564256489276886,
        -0.43631288409233093,
        -0.08491066843271255,
        -0.05543803051114082,
        0.37935858964920044,
        -0.1873459815979004,
        0.6856211423873901,
        -0.4695841670036316,
        1.916669249534607,
        0.3881162703037262,
        0.1783045083284378,
        0.28015202283859253,
        0.22421792149543762,
        1.5794368982315063,
        -0.36686989665031433,
        0.3167603015899658,
        0.11598336696624756,
        -0.2672645151615143,
        0.4707206189632416,
        0.7355534434318542,
        1.3124446868896484,
        0.0628015398979187,
        -0.030232172459363937,
        0.3548239767551422,
        -0.014331437647342682,
        -0.39141297340393066,
        -0.8974194526672363,
        -0.0953284502029419,
        0.06103460490703583,
        -0.2561768591403961,
        0.6384841203689575,
        0.3318521976470947,
        -0.2548815906047821,
        0.848414957523346,
        0.6121137142181396,
        -0.6045017838478088,
        0.17297539114952087,
        0.13106566667556763,
        1.7328181266784668,
        -0.29155096411705017,
        -0.6036852598190308,
        -0.7055005431175232,
        -0.12648452818393707,
        -0.6217524409294128,
        -0.34217554330825806,
        -0.4806765615940094,
        -0.5627723932266235,
        -0.0822717696428299,
        0.46733322739601135,
        -0.5844531059265137,
        -0.17357829213142395,
        -0.27088305354118347,
        -0.10135044157505035,
        0.8196713328361511,
        -0.19870592653751373,
        0.0030181817710399628,
        0.3184296488761902,
        0.6479953527450562,
        -0.343029260635376,
        0.8146000504493713,
        0.016715768724679947,
        -0.6262831687927246,
        -0.6049290895462036
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates quiz questions based on the provided learning objectives.",
          "name": "GenerateQuiz",
          "raw": "\n                workflow GenerateQuiz v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert on the subject defined in the input section provided below.\n\n# GOAL\n\nGenerate questions for a student who wants to review the main concepts of the learning objectives provided in the input section provided below.\n\nIf the input section defines the student level, adapt the questions to that level. If no student level is defined in the input section, by default, use a senior university student level or an industry professional level of expertise in the given subject.\n\nDo not answer the questions.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Generate, upmost, three review questions for each learning objective. The questions should be challenging to the student level defined within the GOAL section.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\nDo not print the dashes. \n---\nSubject: \n* Learning objective: \n    - Question 1: {generated question 1}\n    - Answer 1: \n\n    - Question 2: {generated question 2}\n    - Answer 2:\n    \n    - Question 3: {generated question 3}\n    - Answer 3:\n---\n\n# INPUT:\n\nINPUT:\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert on the subject defined in the input section provided below.\n\n# GOAL\n\nGenerate questions for a student who wants to review the main concepts of the learning objectives provided in the input section provided below.\n\nIf the input section defines the student level, adapt the questions to that level. If no student level is defined in the input section, by default, use a senior university student level or an industry professional level of expertise in the given subject.\n\nDo not answer the questions.\n\nTake a deep breath and consider how to accomplish this goal best using the following steps.\n\n# STEPS\n\n- Extract the subject of the input section.\n\n- Redefine your expertise on that given subject.\n\n- Extract the learning objectives of the input section.\n\n- Generate, upmost, three review questions for each learning objective. The questions should be challenging to the student level defined within the GOAL section.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in clear, human-readable Markdown.\n- Print out, in an indented format, the subject and the learning objectives provided with each generated question in the following format delimited by three dashes.\nDo not print the dashes. \n---\nSubject: \n* Learning objective: \n    - Question 1: {generated question 1}\n    - Answer 1: \n\n    - Question 2: {generated question 2}\n    - Answer 2:\n    \n    - Question 3: {generated question 3}\n    - Answer 3:\n---\n\n# INPUT:\n\nINPUT:"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        1.4990603923797607,
        0.8271193504333496,
        -0.38246965408325195,
        -0.317510187625885,
        0.18236148357391357,
        -0.10045645385980606,
        -0.47912073135375977,
        -0.028354302048683167,
        0.07831869274377823,
        0.44670724868774414,
        -0.46608197689056396,
        1.1171156167984009,
        -0.012793437577784061,
        -0.37154415249824524,
        0.674233615398407,
        -0.08693964034318924,
        0.09161092340946198,
        -0.9458762407302856,
        -1.6994938850402832,
        -0.2035924196243286,
        0.5659560561180115,
        0.5681431889533997,
        0.25040489435195923,
        -0.075831338763237,
        0.3348229229450226,
        -0.17848089337348938,
        -0.629912257194519,
        -0.3462519645690918,
        -0.48891955614089966,
        -0.6076484322547913,
        0.25568273663520813,
        0.15543954074382782,
        -0.3021273910999298,
        -0.11723972856998444,
        -0.36571362614631653,
        -0.926474928855896,
        -0.681517481803894,
        0.4427116811275482,
        -0.4380859434604645,
        0.34437096118927,
        0.30091094970703125,
        0.7151482105255127,
        -0.7987938523292542,
        -0.5106583833694458,
        -0.12254123389720917,
        -0.3807367980480194,
        0.11309078335762024,
        0.08505599200725555,
        0.3014218807220459,
        0.30862218141555786,
        0.172042578458786,
        -0.01482192799448967,
        -0.4675001800060272,
        0.3537759780883789,
        -0.6567978858947754,
        -0.10016613453626633,
        0.4537506103515625,
        -0.2607716917991638,
        0.333137571811676,
        -0.17180053889751434,
        0.35506895184516907,
        0.892543613910675,
        -3.0727572441101074,
        0.1761268526315689,
        0.22714607417583466,
        -0.7349721193313599,
        0.0718948245048523,
        -0.33473411202430725,
        0.3170277774333954,
        -0.6296285390853882,
        -0.35380083322525024,
        -0.04028919339179993,
        0.012642070651054382,
        0.30010557174682617,
        0.2600085139274597,
        -0.27218589186668396,
        0.02434532158076763,
        0.328143447637558,
        0.5774581432342529,
        0.1443938910961151,
        0.003668028861284256,
        0.7943536043167114,
        0.4679725468158722,
        0.13649700582027435,
        -0.3087916672229767,
        0.33328670263290405,
        -0.580777645111084,
        0.02122613787651062,
        0.4255359470844269,
        0.566811203956604,
        -0.4063698947429657,
        -0.36637192964553833,
        0.14494885504245758,
        0.17848435044288635,
        -0.649850606918335,
        0.017050277441740036,
        -0.11341501772403717,
        -0.06003221869468689,
        0.15872962772846222,
        3.7209973335266113,
        0.8354590535163879,
        0.6088141202926636,
        0.3277730345726013,
        -0.8260630369186401,
        0.24160411953926086,
        -0.40331923961639404,
        0.0820317268371582,
        -0.1319863200187683,
        -0.18255357444286346,
        0.07024933397769928,
        0.513514518737793,
        -0.5630518198013306,
        0.10020767152309418,
        0.4051106870174408,
        0.8160013556480408,
        0.6785808205604553,
        -0.3738868832588196,
        -0.15250857174396515,
        0.40854158997535706,
        0.4107128381729126,
        -0.7201420068740845,
        0.10319574177265167,
        -0.47442641854286194,
        -0.2074892818927765,
        0.061439793556928635,
        0.05812910571694374,
        -0.23666422069072723,
        0.6608784794807434,
        -0.2096969336271286,
        0.3552563190460205,
        -0.2417331337928772,
        0.051871757954359055,
        -0.4986388683319092,
        0.26278626918792725,
        -0.14303436875343323,
        -0.1637723743915558,
        -0.0730351060628891,
        -0.7711768746376038,
        0.6012529730796814,
        -0.20347478985786438,
        -0.15801972150802612,
        -1.3721562623977661,
        0.6592241525650024,
        0.23921458423137665,
        0.8644840121269226,
        1.018484115600586,
        -0.018940269947052002,
        0.6149625182151794,
        -0.8624377846717834,
        0.30666327476501465,
        0.12695567309856415,
        -0.32753753662109375,
        -0.27458682656288147,
        0.5636938214302063,
        -0.005081675946712494,
        -0.1315697282552719,
        -1.3293917179107666,
        -0.09250062704086304,
        -0.30592894554138184,
        0.2460871934890747,
        0.68984454870224,
        -0.041818492114543915,
        0.3968786597251892,
        -0.1406070441007614,
        0.274588406085968,
        -0.2501489222049713,
        0.4020312428474426,
        -0.07387468963861465,
        0.2128823697566986,
        0.09070910513401031,
        0.3422343134880066,
        -0.289308100938797,
        -0.4961315989494324,
        1.1572070121765137,
        0.26447251439094543,
        -0.21048347651958466,
        0.09272454679012299,
        0.31179580092430115,
        0.47642984986305237,
        -0.12478519976139069,
        0.7328304648399353,
        0.7535296678543091,
        0.021081920713186264,
        -0.15095478296279907,
        0.052560269832611084,
        -0.4124859571456909,
        0.46371904015541077,
        0.324064701795578,
        0.5130252242088318,
        0.9911282062530518,
        -0.0003966689109802246,
        1.476668357849121,
        0.015507757663726807,
        -0.1996970921754837,
        0.18994265794754028,
        -0.3764656186103821,
        -0.5647901892662048,
        0.035734131932258606,
        1.2646480798721313,
        0.16318099200725555,
        -1.5261034965515137,
        -0.2598489224910736,
        -0.5937187671661377,
        0.03610481321811676,
        -0.5646193623542786,
        0.12311442196369171,
        0.04632880538702011,
        -0.6303992867469788,
        0.6379857659339905,
        -1.108872413635254,
        -0.7688472270965576,
        0.205264613032341,
        0.30070286989212036,
        0.41678372025489807,
        -0.09695200622081757,
        -0.22432643175125122,
        -0.1901683509349823,
        -0.2449866533279419,
        0.02048787847161293,
        0.7597393989562988,
        -0.3956489861011505,
        -0.8088090419769287,
        -0.27052173018455505,
        -0.9353846311569214,
        -0.32457101345062256,
        0.6648059487342834,
        -0.3172089755535126,
        -0.11838053166866302,
        -0.3387239873409271,
        0.33650365471839905,
        -0.41860339045524597,
        0.24667082726955414,
        0.5885747671127319,
        0.8851658701896667,
        0.4755434989929199,
        0.3697904050350189,
        -0.49726980924606323,
        0.8040503263473511,
        -0.5852433443069458,
        -0.7183409929275513,
        1.049196720123291,
        0.12030714750289917,
        -0.05947909131646156,
        -0.15228858590126038,
        -0.11227856576442719,
        0.05318345129489899,
        -0.6377702951431274,
        0.5624330043792725,
        -0.5068916082382202,
        1.1386594772338867,
        0.15319737792015076,
        0.5418555736541748,
        0.5349368453025818,
        0.2714250087738037,
        0.6421412825584412,
        0.010592922568321228,
        -1.4807788133621216,
        -0.7336344122886658,
        -0.48151063919067383,
        -0.1407964527606964,
        -0.005385126918554306,
        -0.4443528354167938,
        -0.030530627816915512,
        -0.6629697680473328,
        0.03377615660429001,
        0.31139707565307617,
        -0.4217340052127838,
        -0.37188607454299927,
        -0.7741830348968506,
        -0.1695684790611267,
        0.15155160427093506,
        -0.4424353837966919,
        -0.2796781659126282,
        -0.3273664712905884,
        -0.7475957870483398,
        0.5268121361732483,
        -0.1826389580965042,
        0.2125294804573059,
        -0.3029334247112274,
        0.10374647378921509,
        0.20189887285232544,
        -0.6703702807426453,
        -0.33332589268684387,
        -0.1496102660894394,
        -0.6740402579307556,
        0.28809550404548645,
        -0.2442733198404312,
        -0.6786968111991882,
        -0.4571368098258972,
        0.5520192384719849,
        -0.6744518280029297,
        0.24145561456680298,
        -1.1128263473510742,
        -0.3544626832008362,
        1.3611005544662476,
        0.577418327331543,
        0.014999233186244965,
        0.17039307951927185,
        0.820963442325592,
        -0.26251593232154846,
        0.038247980177402496,
        0.6110060214996338,
        0.1386508196592331,
        -0.2428727149963379,
        0.3826224207878113,
        -0.6948056221008301,
        0.4644154906272888,
        -0.4076378047466278,
        -0.8148783445358276,
        -0.13844414055347443,
        -0.4097563922405243,
        -0.07553283125162125,
        -0.24133113026618958,
        -0.11118172109127045,
        0.6660748720169067,
        -0.12879790365695953,
        -0.05042789876461029,
        0.793523907661438,
        0.4885203242301941,
        -1.1922239065170288,
        0.043470434844493866,
        -0.5623332858085632,
        0.17839732766151428,
        -0.037915825843811035,
        -0.2576853930950165,
        0.850251317024231,
        0.3459848165512085,
        -0.2059400975704193,
        -0.41378313302993774,
        1.7941057682037354,
        0.4408308267593384,
        -0.6416835784912109,
        0.14881116151809692,
        0.7668335437774658,
        1.5017120838165283,
        -0.21474651992321014,
        0.13951252400875092,
        -0.8402015566825867,
        -1.2930220365524292,
        -0.34585118293762207,
        0.31320005655288696,
        1.230710744857788,
        0.5702420473098755,
        -0.017662249505519867,
        0.24774053692817688,
        -0.5115823149681091,
        -1.1527115106582642,
        -0.8019936680793762,
        0.6084427833557129,
        0.4068108797073364,
        -0.06229519844055176,
        0.4537774622440338,
        0.22887784242630005,
        -0.45128870010375977,
        1.096124529838562,
        0.4020594358444214,
        -0.3179178833961487,
        0.1830875724554062,
        -0.9499372243881226,
        2.2368993759155273,
        -0.047396957874298096,
        -0.213978573679924,
        0.4692772626876831,
        1.144572377204895,
        -0.18854843080043793,
        -0.4697011113166809,
        0.47559070587158203,
        -0.2851795256137848,
        -0.5903993248939514,
        -0.14005905389785767,
        0.331636518239975,
        0.5570836663246155,
        0.15391811728477478,
        0.5712652206420898,
        0.2919163405895233,
        0.08731907606124878,
        0.226192906498909,
        0.24497956037521362,
        0.16098082065582275,
        -0.5502637028694153,
        -0.1113165020942688,
        -0.4336845278739929,
        -1.1831629276275635,
        -0.7461515069007874
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the density of wow-factor in content by analyzing its surprise, novelty, insight, value, and wisdom. This process involves a detailed and varied consumption of the content to assess its potential to engage and enrich viewers. The expected output is a JSON report detailing scores and explanations for each wow-factor component and overall wow-factor per minute.",
          "name": "Get_wow_per_minute",
          "raw": "\n                workflow Get_wow_per_minute v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY \n\nYou are an expert at determining the wow-factor of content as measured per minute of content, as determined by the steps below.\n\n# GOALS\n\n- The goal is to determine how densely packed the content is with wow-factor. Note that wow-factor can come from multiple types of wow, such as surprise, novelty, insight, value, and wisdom, and also from multiple types of content such as business, science, art, or philosophy.\n\n- The goal is to determine how rewarding this content will be for a viewer in terms of how often they'll be surprised, learn something new, gain insight, find practical value, or gain wisdom.\n\n# STEPS\n\n- Fully and deeply consume the content at least 319 times, using different interpretive perspectives each time.\n\n- Construct a giant virtual whiteboard in your mind.\n\n- Extract the ideas being presented in the content and place them on your giant virtual whiteboard.\n\n- Extract the novelty of those ideas and place them on your giant virtual whiteboard.\n\n- Extract the insights from those ideas and place them on your giant virtual whiteboard.\n\n- Extract the value of those ideas and place them on your giant virtual whiteboard.\n\n- Extract the wisdom of those ideas and place them on your giant virtual whiteboard.\n\n- Notice how separated in time the ideas, novelty, insights, value, and wisdom are from each other in time throughout the content, using an average speaking speed as your time clock.\n\n- Wow is defined as: Surprise * Novelty * Insight * Value * Wisdom, so the more of each of those the higher the wow-factor.\n\n- Surprise is novelty * insight \n- Novelty is newness of idea or explanation\n- Insight is clarity and power of idea \n- Value is practical usefulness \n- Wisdom is deep knowledge about the world that helps over time \n\nThus, WPM is how often per minute someone is getting surprise, novelty, insight, value, or wisdom per minute across all minutes of the content.\n\n- Scores are given between 0 and 10, with 10 being ten times in a minute someone is thinking to themselves, \\\"Wow, this is great content!\\\", and 0 being no wow-factor at all.\n\n# OUTPUT\n\n- Only output in JSON with the following format:\n\nEXAMPLE WITH PLACEHOLDER TEXT EXPLAINING WHAT SHOULD GO IN THE OUTPUT\n\n{\n  \\\"Summary\\\": \\\"The content was about X, with Y novelty, Z insights, A value, and B wisdom in a 25-word sentence.\\\",\n  \\\"Surprise_per_minute\\\": \\\"The surprise presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Surprise_per_minute_explanation\\\": \\\"The explanation for the amount of surprise per minute of content in a 25-word sentence.\\\",\n  \\\"Novelty_per_minute\\\": \\\"The novelty presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Novelty_per_minute_explanation\\\": \\\"The explanation for the amount of novelty per minute of content in a 25-word sentence.\\\",\n  \\\"Insight_per_minute\\\": \\\"The insight presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Insight_per_minute_explanation\\\": \\\"The explanation for the amount of insight per minute of content in a 25-word sentence.\\\",\n  \\\"Value_per_minute\\\": \\\"The value presented per minute of content. A numeric score between 0 and 10.\\\",   25\n  \\\"Value_per_minute_explanation\\\": \\\"The explanation for the amount of value per minute of content in a 25-word sentence.\\\",\n  \\\"Wisdom_per_minute\\\": \\\"The wisdom presented per minute of content. A numeric score between 0 and 10.\\\"25\n  \\\"Wisdom_per_minute_explanation\\\": \\\"The explanation for the amount of wisdom per minute of content in a 25-word sentence.\\\",\n  \\\"WPM_score\\\": \\\"The total WPM score as a number between 0 and 10.\\\",\n  \\\"WPM_score_explanation\\\": \\\"The explanation for the total WPM score as a 25-word sentence.\\\"\n}\n\n- Do not complain about anything, just do what is asked.\n- ONLY output JSON, and in that exact format.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY \n\nYou are an expert at determining the wow-factor of content as measured per minute of content, as determined by the steps below.\n\n# GOALS\n\n- The goal is to determine how densely packed the content is with wow-factor. Note that wow-factor can come from multiple types of wow, such as surprise, novelty, insight, value, and wisdom, and also from multiple types of content such as business, science, art, or philosophy.\n\n- The goal is to determine how rewarding this content will be for a viewer in terms of how often they'll be surprised, learn something new, gain insight, find practical value, or gain wisdom.\n\n# STEPS\n\n- Fully and deeply consume the content at least 319 times, using different interpretive perspectives each time.\n\n- Construct a giant virtual whiteboard in your mind.\n\n- Extract the ideas being presented in the content and place them on your giant virtual whiteboard.\n\n- Extract the novelty of those ideas and place them on your giant virtual whiteboard.\n\n- Extract the insights from those ideas and place them on your giant virtual whiteboard.\n\n- Extract the value of those ideas and place them on your giant virtual whiteboard.\n\n- Extract the wisdom of those ideas and place them on your giant virtual whiteboard.\n\n- Notice how separated in time the ideas, novelty, insights, value, and wisdom are from each other in time throughout the content, using an average speaking speed as your time clock.\n\n- Wow is defined as: Surprise * Novelty * Insight * Value * Wisdom, so the more of each of those the higher the wow-factor.\n\n- Surprise is novelty * insight \n- Novelty is newness of idea or explanation\n- Insight is clarity and power of idea \n- Value is practical usefulness \n- Wisdom is deep knowledge about the world that helps over time \n\nThus, WPM is how often per minute someone is getting surprise, novelty, insight, value, or wisdom per minute across all minutes of the content.\n\n- Scores are given between 0 and 10, with 10 being ten times in a minute someone is thinking to themselves, \\\"Wow, this is great content!\\\", and 0 being no wow-factor at all.\n\n# OUTPUT\n\n- Only output in JSON with the following format:\n\nEXAMPLE WITH PLACEHOLDER TEXT EXPLAINING WHAT SHOULD GO IN THE OUTPUT\n\n{\n  \\\"Summary\\\": \\\"The content was about X, with Y novelty, Z insights, A value, and B wisdom in a 25-word sentence.\\\",\n  \\\"Surprise_per_minute\\\": \\\"The surprise presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Surprise_per_minute_explanation\\\": \\\"The explanation for the amount of surprise per minute of content in a 25-word sentence.\\\",\n  \\\"Novelty_per_minute\\\": \\\"The novelty presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Novelty_per_minute_explanation\\\": \\\"The explanation for the amount of novelty per minute of content in a 25-word sentence.\\\",\n  \\\"Insight_per_minute\\\": \\\"The insight presented per minute of content. A numeric score between 0 and 10.\\\",\n  \\\"Insight_per_minute_explanation\\\": \\\"The explanation for the amount of insight per minute of content in a 25-word sentence.\\\",\n  \\\"Value_per_minute\\\": \\\"The value presented per minute of content. A numeric score between 0 and 10.\\\",   25\n  \\\"Value_per_minute_explanation\\\": \\\"The explanation for the amount of value per minute of content in a 25-word sentence.\\\",\n  \\\"Wisdom_per_minute\\\": \\\"The wisdom presented per minute of content. A numeric score between 0 and 10.\\\"25\n  \\\"Wisdom_per_minute_explanation\\\": \\\"The explanation for the amount of wisdom per minute of content in a 25-word sentence.\\\",\n  \\\"WPM_score\\\": \\\"The total WPM score as a number between 0 and 10.\\\",\n  \\\"WPM_score_explanation\\\": \\\"The explanation for the total WPM score as a 25-word sentence.\\\"\n}\n\n- Do not complain about anything, just do what is asked.\n- ONLY output JSON, and in that exact format.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.057529106736183167,
        -0.16075699031352997,
        -0.34462276101112366,
        0.321012020111084,
        -0.5052417516708374,
        -0.3825489580631256,
        -1.8876692056655884,
        0.3452931344509125,
        0.15645280480384827,
        0.39298611879348755,
        -0.7267658114433289,
        0.9952713847160339,
        0.2800532281398773,
        0.4488207697868347,
        0.2754478454589844,
        -0.5500434041023254,
        0.04388962686061859,
        -0.9011351466178894,
        -1.377349853515625,
        -0.9297496676445007,
        0.37175142765045166,
        0.8694157004356384,
        0.11272761225700378,
        0.32304906845092773,
        0.27061665058135986,
        -0.6910638213157654,
        0.4267978072166443,
        -0.48211950063705444,
        -1.6847745180130005,
        -1.241229772567749,
        0.5344216227531433,
        1.163655400276184,
        -0.19968579709529877,
        -0.8852604627609253,
        -0.14791011810302734,
        -0.8432420492172241,
        -0.30281418561935425,
        0.11110050231218338,
        -0.13669157028198242,
        0.2819991707801819,
        -0.1422315090894699,
        -0.360911101102829,
        -0.12205103039741516,
        -0.3056164085865021,
        -0.1473008543252945,
        0.522017776966095,
        0.5313811302185059,
        -0.2887837886810303,
        0.7843037843704224,
        0.12139461189508438,
        0.8168866038322449,
        0.08901280909776688,
        0.2412690371274948,
        -0.5483051538467407,
        -0.5116868615150452,
        -1.1695753335952759,
        -0.6850380897521973,
        0.20099157094955444,
        0.38380083441734314,
        -0.14691312611103058,
        0.5770719647407532,
        0.4307183623313904,
        -3.438725471496582,
        -0.692906379699707,
        0.13874556124210358,
        0.040505338460206985,
        -0.3898242115974426,
        0.019800834357738495,
        0.6189572215080261,
        -0.0933711901307106,
        0.04126034677028656,
        0.21557919681072235,
        -0.3901318311691284,
        0.8767948150634766,
        -0.21975183486938477,
        0.15228688716888428,
        0.5946201682090759,
        -0.050091713666915894,
        -0.3345973491668701,
        -0.5895485877990723,
        0.6511282920837402,
        0.18838362395763397,
        0.3340069353580475,
        -0.22600771486759186,
        -0.04165579006075859,
        0.7217470407485962,
        -0.22461935877799988,
        0.45143812894821167,
        -0.12512801587581635,
        0.22149206697940826,
        -0.1939820498228073,
        -0.523193359375,
        0.3395273983478546,
        0.4648999571800232,
        -0.3374325633049011,
        -0.38476234674453735,
        -0.15480725467205048,
        0.1164669618010521,
        -0.47951382398605347,
        2.527355909347534,
        0.7375133037567139,
        -0.07153172791004181,
        -0.17072594165802002,
        -1.6270332336425781,
        0.2602502703666687,
        -0.4051860570907593,
        0.3767929971218109,
        0.6507548689842224,
        0.5309389233589172,
        0.27229049801826477,
        0.67339688539505,
        -0.8060348629951477,
        0.6291645765304565,
        0.7404998540878296,
        -0.00724495854228735,
        0.016941288486123085,
        -0.08729156851768494,
        -0.384789377450943,
        -0.21861723065376282,
        1.0678905248641968,
        0.2970843017101288,
        -0.0732143223285675,
        -0.3062177300453186,
        -0.8663783669471741,
        -0.25333094596862793,
        -0.6659671068191528,
        -0.8183096647262573,
        0.3305619955062866,
        0.0054127126932144165,
        0.4784530699253082,
        0.1431775838136673,
        -0.452635258436203,
        -0.2339445948600769,
        0.04650548845529556,
        0.3209784924983978,
        0.14702706038951874,
        0.2874864339828491,
        -1.0666006803512573,
        -0.03455270826816559,
        0.08904475718736649,
        -0.23883596062660217,
        -1.2613000869750977,
        0.38026875257492065,
        0.10877207666635513,
        0.1066092997789383,
        -0.0439792275428772,
        -0.4616304934024811,
        0.4039939045906067,
        -0.42896541953086853,
        -0.22353702783584595,
        0.10202473402023315,
        0.13403376936912537,
        0.2587188184261322,
        0.29334408044815063,
        0.6186699867248535,
        -0.038874223828315735,
        0.052128031849861145,
        -0.431041419506073,
        -0.46645697951316833,
        0.5416508913040161,
        0.5012534260749817,
        -0.16088946163654327,
        0.41920751333236694,
        0.07043510675430298,
        -0.07128109782934189,
        -0.681350588798523,
        0.5146152377128601,
        0.4287734925746918,
        0.6139976978302002,
        0.3631749451160431,
        0.5155013799667358,
        0.6119295358657837,
        -0.2776455879211426,
        0.26759999990463257,
        -0.557261049747467,
        0.05589758977293968,
        -1.6221283674240112,
        -0.022162124514579773,
        0.6586261987686157,
        0.36055323481559753,
        0.6688860058784485,
        0.2521790862083435,
        -0.07017877697944641,
        -0.005589544773101807,
        -0.6089304089546204,
        0.2780870795249939,
        0.22215649485588074,
        0.0072888657450675964,
        1.3868916034698486,
        1.5323973894119263,
        -0.3842761516571045,
        1.4220404624938965,
        -1.399383783340454,
        -0.23920777440071106,
        0.662705659866333,
        -0.46766185760498047,
        0.15234562754631042,
        0.3462284207344055,
        -0.09879074990749359,
        -0.029800916090607643,
        -0.9399747252464294,
        -0.5175568461418152,
        -0.4084857106208801,
        -0.5859472751617432,
        -1.217321515083313,
        -0.24254325032234192,
        0.3881065249443054,
        0.23720552027225494,
        0.420596182346344,
        -0.885246217250824,
        0.22478170692920685,
        -0.16493059694766998,
        1.3953680992126465,
        0.45412594079971313,
        -0.21306785941123962,
        0.09105677157640457,
        -0.12196411192417145,
        -0.41028961539268494,
        0.20231926441192627,
        -0.43946734070777893,
        -0.1917020082473755,
        0.482145756483078,
        -0.12053219974040985,
        -0.5296239852905273,
        -0.40244463086128235,
        0.6415655016899109,
        -0.21464315056800842,
        0.9564406871795654,
        -0.38068029284477234,
        -0.05500146374106407,
        0.4762478470802307,
        0.7768450379371643,
        -0.2938772141933441,
        1.6832034587860107,
        0.2883003354072571,
        0.7640969753265381,
        -0.793118953704834,
        0.5142654180526733,
        -0.22838878631591797,
        -0.7302077412605286,
        -0.022836849093437195,
        0.6372054815292358,
        0.005062468349933624,
        -0.15185004472732544,
        -0.7867224812507629,
        -0.07400670647621155,
        -0.1455896496772766,
        -0.04744853079319,
        -0.416216641664505,
        1.849395990371704,
        1.1477075815200806,
        0.16025085747241974,
        0.09324564039707184,
        -0.08869147300720215,
        0.7819600701332092,
        -0.26838821172714233,
        -1.8038384914398193,
        -0.28064849972724915,
        0.09267855435609818,
        0.4027799665927887,
        -0.8463615775108337,
        0.28581082820892334,
        0.31613004207611084,
        0.11283393204212189,
        -0.535460352897644,
        -0.07685920596122742,
        -0.7807962894439697,
        -0.8621413707733154,
        -0.27203845977783203,
        -0.14367543160915375,
        -0.5610416531562805,
        0.594750702381134,
        0.005615845322608948,
        -0.48745477199554443,
        -0.37846630811691284,
        0.7883944511413574,
        -0.07826070487499237,
        0.16673783957958221,
        -0.33048704266548157,
        -0.2168157696723938,
        1.1575744152069092,
        0.059306249022483826,
        -0.3067626357078552,
        0.3144969940185547,
        -0.24975016713142395,
        0.06621505320072174,
        -0.11971984803676605,
        0.2275591492652893,
        -0.7159399390220642,
        0.8217397928237915,
        -0.4070504903793335,
        -0.931667149066925,
        -0.6630406975746155,
        0.4149782657623291,
        1.7372990846633911,
        -0.1266564577817917,
        0.06526828557252884,
        0.26610425114631653,
        0.0895281732082367,
        0.07933011651039124,
        -0.8402792811393738,
        0.5603878498077393,
        0.02297356352210045,
        0.0507085844874382,
        -0.9627794623374939,
        -0.7439727783203125,
        0.8789414763450623,
        -0.5752222537994385,
        -0.5699160099029541,
        0.017413809895515442,
        -0.027837295085191727,
        0.6728010773658752,
        -0.12040171027183533,
        -0.5395853519439697,
        0.213445782661438,
        -0.14542827010154724,
        0.3181890547275543,
        0.9050445556640625,
        -0.33976584672927856,
        -1.2085459232330322,
        0.2779819965362549,
        -0.056609466671943665,
        -0.1476152539253235,
        -0.02074064314365387,
        -0.6088392734527588,
        1.2834546566009521,
        -0.72276371717453,
        0.782227098941803,
        -0.05409632623195648,
        1.1441149711608887,
        0.6568740010261536,
        0.4251547157764435,
        0.40232476592063904,
        0.6584382653236389,
        0.8944813013076782,
        0.3230525851249695,
        0.13024038076400757,
        0.3392411768436432,
        -0.36604776978492737,
        -0.33643993735313416,
        0.4252607524394989,
        1.3037605285644531,
        0.34498080611228943,
        -0.0321701355278492,
        0.16680417954921722,
        -0.08832802623510361,
        -0.5521001815795898,
        -0.6143442392349243,
        0.7831113338470459,
        -0.4143136143684387,
        -0.7763955593109131,
        1.0799857378005981,
        -0.05231906473636627,
        -0.510720431804657,
        -0.33411338925361633,
        0.18379274010658264,
        -0.8297261595726013,
        0.3018432557582855,
        0.1659437119960785,
        1.3604788780212402,
        0.30842044949531555,
        -0.2558264434337616,
        0.10056789964437485,
        -0.39930838346481323,
        -0.3830420970916748,
        0.18992485105991364,
        0.507503867149353,
        0.20408755540847778,
        -0.1577017456293106,
        -0.3630569875240326,
        0.7147740721702576,
        -0.13381743431091309,
        0.24442635476589203,
        -0.0372605174779892,
        0.13573089241981506,
        -0.16109436750411987,
        0.22160930931568146,
        0.3019436001777649,
        0.5642726421356201,
        -0.04638316482305527,
        0.28084683418273926,
        0.4218920171260834,
        -0.33197134733200073,
        -0.027372822165489197
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates RSS URLs for YouTube channels based on given channel IDs or URLs. It extracts the channel ID from the input and constructs the corresponding RSS URL. The output is solely the RSS URL.",
          "name": "Get_youtube_rss",
          "raw": "\n                workflow Get_youtube_rss v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOALS\n\nYou are a YouTube infrastructure expert that returns YouTube channel RSS URLs.\n\nYou take any input in, especially YouTube channel IDs, or full URLs, and return the RSS URL for that channel.\n\n# STEPS\n\nHere is the structure for YouTube RSS URLs and their relation to the channel ID and or channel URL:\n\nIf the channel URL is https://www.youtube.com/channel/UCnCikd0s4i9KoDtaHPlK-JA, the RSS URL is https://www.youtube.com/feeds/videos.xml?channel_id=UCnCikd0s4i9KoDtaHPlK-JA\n\n- Extract the channel ID from the channel URL.\n\n- Construct the RSS URL using the channel ID.\n\n- Output the RSS URL.\n\n# OUTPUT\n\n- Output only the RSS URL and nothing else.\n\n- Don't complain, just do it.\n\n# INPUT\n\n(INPUT)\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOALS\n\nYou are a YouTube infrastructure expert that returns YouTube channel RSS URLs.\n\nYou take any input in, especially YouTube channel IDs, or full URLs, and return the RSS URL for that channel.\n\n# STEPS\n\nHere is the structure for YouTube RSS URLs and their relation to the channel ID and or channel URL:\n\nIf the channel URL is https://www.youtube.com/channel/UCnCikd0s4i9KoDtaHPlK-JA, the RSS URL is https://www.youtube.com/feeds/videos.xml?channel_id=UCnCikd0s4i9KoDtaHPlK-JA\n\n- Extract the channel ID from the channel URL.\n\n- Construct the RSS URL using the channel ID.\n\n- Output the RSS URL.\n\n# OUTPUT\n\n- Output only the RSS URL and nothing else.\n\n- Don't complain, just do it.\n\n# INPUT\n\n(INPUT)\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.42468148469924927,
        0.045021913945674896,
        -0.05115653946995735,
        0.5350899696350098,
        0.26366278529167175,
        0.18541578948497772,
        -0.8936052322387695,
        -0.16926687955856323,
        0.01775255799293518,
        -0.3416616916656494,
        -0.1268966943025589,
        0.6929282546043396,
        -0.14858901500701904,
        0.03589439392089844,
        0.17059189081192017,
        -0.4427148699760437,
        -0.18794645369052887,
        -0.36308738589286804,
        -1.2626466751098633,
        -0.14895951747894287,
        0.2355090081691742,
        0.5851829051971436,
        0.10252971947193146,
        0.0008589550852775574,
        0.6979666948318481,
        0.540361762046814,
        0.15448448061943054,
        -0.20994801819324493,
        -1.065185785293579,
        -1.7790899276733398,
        0.016642555594444275,
        0.08122485876083374,
        -0.42409399151802063,
        -0.4349190592765808,
        0.4582643508911133,
        -0.7014929056167603,
        -0.137956440448761,
        -0.10253794491291046,
        -0.39675307273864746,
        -0.5270963907241821,
        0.47110608220100403,
        0.14893296360969543,
        0.43003687262535095,
        -0.745413601398468,
        0.12768889963626862,
        -0.23518997430801392,
        -0.028211571276187897,
        -0.1040048748254776,
        1.2560633420944214,
        0.6259732246398926,
        -0.5137847661972046,
        -0.5816063284873962,
        -0.5283340811729431,
        -0.14449620246887207,
        -0.05215553194284439,
        0.26305511593818665,
        0.5137326717376709,
        -0.3839515745639801,
        -0.05136052891612053,
        0.0462338849902153,
        0.028926189988851547,
        0.8606986999511719,
        -3.4032998085021973,
        -0.21673905849456787,
        -0.22361129522323608,
        0.22773244976997375,
        0.18683688342571259,
        0.060430243611335754,
        0.6328496336936951,
        -0.657645583152771,
        -0.3904377222061157,
        0.22515784204006195,
        -0.04408383369445801,
        0.9710894227027893,
        0.5781214833259583,
        0.029760785400867462,
        -0.05826355889439583,
        0.057735249400138855,
        0.5868912935256958,
        -0.2860710918903351,
        0.28810539841651917,
        0.21265631914138794,
        0.08317050337791443,
        -0.4317318797111511,
        -0.41634348034858704,
        0.09648188948631287,
        0.16052260994911194,
        0.28451818227767944,
        0.5103288888931274,
        0.2177870273590088,
        -0.0444527305662632,
        -0.5269958972930908,
        0.304798424243927,
        0.08137856423854828,
        -0.26219168305397034,
        -0.10001340508460999,
        -0.3977452218532562,
        -0.033585160970687866,
        0.21342766284942627,
        3.5822384357452393,
        0.5284854769706726,
        0.15466855466365814,
        0.9395553469657898,
        -0.7663016319274902,
        0.25405141711235046,
        -0.22366835176944733,
        -0.11987152695655823,
        -0.9650218486785889,
        0.3380713164806366,
        -0.5119594931602478,
        0.3432672321796417,
        -1.1571900844573975,
        -0.6094025373458862,
        -0.1903814971446991,
        0.5082815885543823,
        0.5290406942367554,
        -0.8977479338645935,
        -0.05598484352231026,
        -0.1579962968826294,
        1.2144100666046143,
        -0.717348039150238,
        -0.2017986923456192,
        0.05089065805077553,
        -0.29077380895614624,
        -0.2595982849597931,
        0.4669454097747803,
        -0.15372560918331146,
        0.5731510519981384,
        0.3846789002418518,
        0.351224422454834,
        -0.35515913367271423,
        0.36759480834007263,
        -0.6862924098968506,
        -0.3314908742904663,
        0.26101967692375183,
        0.0542730987071991,
        0.7909792065620422,
        -0.675998330116272,
        0.7335634827613831,
        -0.2762964069843292,
        0.6966266632080078,
        -0.705008864402771,
        0.9057835340499878,
        0.056768596172332764,
        1.1913703680038452,
        0.6679522395133972,
        -0.2530968487262726,
        0.7292158007621765,
        -0.3849228322505951,
        -0.8827826976776123,
        -0.29232534766197205,
        0.4976406395435333,
        -0.7351003289222717,
        0.5452035665512085,
        0.42332786321640015,
        0.07751581072807312,
        -0.7566222548484802,
        -0.07664856314659119,
        -0.6819278001785278,
        0.5481142401695251,
        -0.0753413587808609,
        0.29024529457092285,
        0.21668341755867004,
        -0.4459541440010071,
        0.80316561460495,
        -0.5627036690711975,
        0.320776104927063,
        0.14779159426689148,
        -0.3568228781223297,
        0.07510288059711456,
        -0.011481255292892456,
        -0.1056973859667778,
        0.3588385581970215,
        0.37990790605545044,
        -0.6069628596305847,
        0.2630994915962219,
        -0.024237394332885742,
        0.5013141632080078,
        0.38089051842689514,
        -0.2719310522079468,
        0.5585741996765137,
        0.5011017918586731,
        -0.24164950847625732,
        -0.8039420247077942,
        0.04476217180490494,
        0.3395814597606659,
        0.0978306233882904,
        0.10169852524995804,
        0.8609411120414734,
        1.4753438234329224,
        -1.6676514148712158,
        1.2934026718139648,
        -0.5946151614189148,
        -0.1313355565071106,
        -0.1950475424528122,
        0.4748784303665161,
        0.23469112813472748,
        0.02993905544281006,
        0.21531228721141815,
        -0.13245831429958344,
        -0.6181114912033081,
        0.493757963180542,
        -0.5920498371124268,
        0.21492692828178406,
        -0.6899487376213074,
        -0.6911787986755371,
        -0.1359376758337021,
        1.4038630723953247,
        -0.10866700112819672,
        -0.20677414536476135,
        -0.5622519254684448,
        -0.1363256573677063,
        1.238473892211914,
        0.27869072556495667,
        0.9044533967971802,
        -0.045279450714588165,
        0.4792077839374542,
        0.849149763584137,
        0.24398697912693024,
        0.6223250031471252,
        -0.4261323809623718,
        0.2372610867023468,
        -0.7673133611679077,
        -0.7069646716117859,
        -0.13905519247055054,
        1.3509432077407837,
        -0.8959092497825623,
        0.6219109892845154,
        -0.960750937461853,
        -0.4069884419441223,
        0.2339511513710022,
        1.1255961656570435,
        1.084606409072876,
        0.6891824007034302,
        -0.2883566617965698,
        0.6453509330749512,
        0.1443680375814438,
        0.6271134614944458,
        -0.005271054804325104,
        -0.7384505271911621,
        -0.4046545922756195,
        0.3805604875087738,
        0.05216945707798004,
        0.6749052405357361,
        0.5609619617462158,
        -0.2525540292263031,
        -1.1641108989715576,
        -0.2096918225288391,
        -0.26197949051856995,
        0.9182285666465759,
        0.18963854014873505,
        -0.2500290870666504,
        0.21492162346839905,
        -0.12991099059581757,
        -0.11024311929941177,
        0.310203492641449,
        -1.210941195487976,
        -0.17263925075531006,
        -0.4348037838935852,
        0.5644146203994751,
        0.03238464146852493,
        0.08611824363470078,
        0.7435303926467896,
        0.13253501057624817,
        0.338849276304245,
        -0.017592530697584152,
        0.2574608623981476,
        -0.13872939348220825,
        -0.3214128911495209,
        -0.3177984952926636,
        0.18841606378555298,
        -0.13093923032283783,
        -0.059172384440898895,
        -0.2965654730796814,
        0.2247105836868286,
        -0.1886829435825348,
        -0.12772710621356964,
        -0.3004089593887329,
        -0.27662166953086853,
        -0.7204180359840393,
        0.027449045330286026,
        -0.3511236608028412,
        -0.7731607556343079,
        0.4843509793281555,
        -0.3352035582065582,
        0.19101014733314514,
        -0.34538984298706055,
        -0.8920090794563293,
        0.15872687101364136,
        0.6023922562599182,
        -0.42467939853668213,
        -0.5741646885871887,
        -0.6336647868156433,
        0.19841741025447845,
        1.7139948606491089,
        0.10099874436855316,
        0.33381158113479614,
        0.5979001522064209,
        0.1629037708044052,
        -0.5304880142211914,
        0.1598837673664093,
        0.16548359394073486,
        0.041981782764196396,
        0.016667451709508896,
        -0.3119189441204071,
        -0.8219676613807678,
        0.28404808044433594,
        -0.4956631660461426,
        -0.3380492031574249,
        0.23339757323265076,
        -0.5752413868904114,
        0.08102990686893463,
        -0.17285971343517303,
        -0.13780109584331512,
        0.34068727493286133,
        -0.3621903359889984,
        -0.06850326061248779,
        0.9055910110473633,
        0.17148463428020477,
        -1.4204773902893066,
        -0.6089926362037659,
        0.2573084533214569,
        0.2593497633934021,
        0.5141257047653198,
        -0.43772295117378235,
        0.6792294979095459,
        0.3348565399646759,
        -0.3486829996109009,
        -0.46344155073165894,
        1.493259310722351,
        0.22321130335330963,
        -0.3173193335533142,
        0.12724851071834564,
        0.0012291967868804932,
        0.753355860710144,
        -0.7719874978065491,
        0.7281659245491028,
        -0.046406082808971405,
        -0.8115897178649902,
        -0.2719009518623352,
        -0.005932852625846863,
        1.703364372253418,
        0.2512076795101166,
        0.4411923885345459,
        0.11930808424949646,
        -0.02010176330804825,
        -0.4602782130241394,
        -0.5228080749511719,
        0.044455405324697495,
        0.002945035696029663,
        -0.3034394085407257,
        0.3018602132797241,
        -0.03015293926000595,
        0.3524450361728668,
        0.08337533473968506,
        0.4611678421497345,
        -0.43305283784866333,
        -0.06353506445884705,
        -0.0963461846113205,
        2.2060704231262207,
        -0.5878111720085144,
        -0.9590377807617188,
        -0.8294233083724976,
        0.2583935558795929,
        -0.24193280935287476,
        0.27024412155151367,
        -0.13153256475925446,
        -0.6764029860496521,
        -0.21423344314098358,
        0.02911985293030739,
        -0.20006197690963745,
        -0.3619251251220703,
        0.07213874906301498,
        -0.1717713475227356,
        0.6279076337814331,
        -0.07684172689914703,
        0.10736848413944244,
        0.371517151594162,
        0.21233773231506348,
        -0.37983638048171997,
        0.50055330991745,
        -0.5846123695373535,
        -0.2688874900341034,
        -1.2366323471069336
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt aims to enhance the quality of text for academic purposes. It focuses on refining grammatical errors, improving clarity and coherence, and adopting an academic tone while ensuring ease of understanding. The expected output is a professionally refined text with a list of applied corrections.",
          "name": "Improve_academic_writing",
          "raw": "\n                workflow Improve_academic_writing v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an academic writing expert. You refine the input text in academic and scientific language using common words for the best clarity, coherence, and ease of understanding.\n\n# Steps\n\n- Refine the input text for grammatical errors, clarity issues, and coherence.\n- Refine the input text into academic voice.\n- Use formal English only.\n- Tend to use common and easy-to-understand words and phrases.\n- Avoid wordy sentences.\n- Avoid trivial statements.\n- Avoid using the same words and phrases repeatedly.\n- Apply corrections and improvements directly to the text.\n- Maintain the original meaning and intent of the user's text.\n\n# OUTPUT INSTRUCTIONS\n\n- Refined and improved text that is professionally academic.\n- A list of changes made to the original text.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an academic writing expert. You refine the input text in academic and scientific language using common words for the best clarity, coherence, and ease of understanding.\n\n# Steps\n\n- Refine the input text for grammatical errors, clarity issues, and coherence.\n- Refine the input text into academic voice.\n- Use formal English only.\n- Tend to use common and easy-to-understand words and phrases.\n- Avoid wordy sentences.\n- Avoid trivial statements.\n- Avoid using the same words and phrases repeatedly.\n- Apply corrections and improvements directly to the text.\n- Maintain the original meaning and intent of the user's text.\n\n# OUTPUT INSTRUCTIONS\n\n- Refined and improved text that is professionally academic.\n- A list of changes made to the original text.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.47741183638572693,
        0.0734163373708725,
        -0.07149931788444519,
        0.4308387041091919,
        0.25726404786109924,
        0.2767094075679779,
        -0.38918864727020264,
        -0.00739683210849762,
        0.2783395051956177,
        0.038824308663606644,
        0.097091905772686,
        0.9573100209236145,
        -0.38244450092315674,
        0.26715707778930664,
        -0.12736862897872925,
        0.24079850316047668,
        -0.24780303239822388,
        -0.9872775077819824,
        -1.5030739307403564,
        -0.47817233204841614,
        0.08802333474159241,
        0.6461405158042908,
        0.1835429072380066,
        0.2000041902065277,
        0.5287550687789917,
        0.45832720398902893,
        0.1089264452457428,
        -0.003015846014022827,
        -0.9473497271537781,
        -1.5062445402145386,
        0.45713695883750916,
        0.6415442228317261,
        -0.17281456291675568,
        -0.8285648822784424,
        -0.15625247359275818,
        -0.7483135461807251,
        -0.26451653242111206,
        -0.16476261615753174,
        0.020686890929937363,
        -0.47604018449783325,
        0.7396698594093323,
        -0.08547234535217285,
        -0.5052573680877686,
        -0.002695884555578232,
        -0.0051057226955890656,
        -0.2843359410762787,
        0.17573975026607513,
        -0.2697047293186188,
        0.9825853109359741,
        0.6694542169570923,
        -0.055961448699235916,
        -0.27955156564712524,
        -0.20206119120121002,
        0.023879872635006905,
        -0.23577409982681274,
        0.24308696389198303,
        0.07406233251094818,
        -0.22770030796527863,
        -0.0650685727596283,
        0.13115686178207397,
        -0.46701860427856445,
        0.9874662756919861,
        -3.569732666015625,
        -0.45083239674568176,
        0.4012548625469208,
        -0.023676149547100067,
        -0.15447776019573212,
        -0.23743435740470886,
        0.6655691266059875,
        -0.3804870843887329,
        0.04736419394612312,
        0.7523803114891052,
        -0.6667044758796692,
        0.5651676654815674,
        0.1877678632736206,
        -0.39874914288520813,
        0.2979445159435272,
        0.2140161544084549,
        0.5538389086723328,
        -0.3188212811946869,
        0.6785252094268799,
        0.34898656606674194,
        -0.20387722551822662,
        -0.2761676609516144,
        -0.769925057888031,
        0.5582711696624756,
        0.014724090695381165,
        -0.14665046334266663,
        0.2198844999074936,
        -0.4770965576171875,
        -0.27844658493995667,
        -0.06043291836977005,
        0.2102045714855194,
        0.09729082137346268,
        0.029378358274698257,
        0.3471434712409973,
        0.14177067577838898,
        0.28789466619491577,
        -0.025294944643974304,
        3.606783390045166,
        1.089349389076233,
        0.777453601360321,
        0.31986671686172485,
        -0.2682446241378784,
        0.5296684503555298,
        -0.4375174939632416,
        -0.24533821642398834,
        -1.0184495449066162,
        0.5217877626419067,
        -0.14770089089870453,
        0.40388911962509155,
        -1.047727108001709,
        0.15150383114814758,
        0.1356160044670105,
        0.15820680558681488,
        -0.44644588232040405,
        -0.7168692350387573,
        -0.20256470143795013,
        -0.5570546388626099,
        1.2658379077911377,
        -0.5640295147895813,
        -0.505728006362915,
        -0.07763712853193283,
        -0.5001286268234253,
        -0.5227576494216919,
        0.10760882496833801,
        -0.23161818087100983,
        0.50020831823349,
        0.38870659470558167,
        0.44191715121269226,
        -0.29190537333488464,
        -0.12419818341732025,
        -0.4554230272769928,
        -0.43366310000419617,
        0.3662000000476837,
        -0.03699351102113724,
        0.7981268763542175,
        -0.24314440786838531,
        0.24152371287345886,
        -0.8789108395576477,
        0.6242412328720093,
        -0.5749735832214355,
        0.795456051826477,
        -0.19491071999073029,
        1.3083760738372803,
        0.7685588002204895,
        0.023517800495028496,
        0.16824187338352203,
        -0.3416704535484314,
        -1.0678774118423462,
        -0.24122358858585358,
        0.7346043586730957,
        -0.328830748796463,
        0.05978919565677643,
        0.1580563485622406,
        0.2454969584941864,
        -0.26842039823532104,
        0.023138150572776794,
        -0.8796387314796448,
        0.4325450360774994,
        0.4461851119995117,
        -0.33826684951782227,
        0.492759108543396,
        -0.11455430090427399,
        0.6476443409919739,
        -0.11730021238327026,
        0.17976994812488556,
        -0.6855092644691467,
        0.08863583207130432,
        0.2884138226509094,
        -0.14738738536834717,
        0.1665026843547821,
        0.43216609954833984,
        1.135934829711914,
        -0.7313180565834045,
        -0.0709296241402626,
        0.23078161478042603,
        0.07075664401054382,
        0.4821624457836151,
        -0.6069358587265015,
        0.532306969165802,
        0.41698163747787476,
        -0.4890899956226349,
        -0.5692841410636902,
        0.061650633811950684,
        0.07515401393175125,
        -0.14183470606803894,
        0.13146796822547913,
        0.8557150959968567,
        0.677843451499939,
        -1.2791706323623657,
        1.4448012113571167,
        -0.4051303267478943,
        -0.2839106321334839,
        -0.22044514119625092,
        -0.07602105289697647,
        0.10393737256526947,
        0.22285854816436768,
        0.25587961077690125,
        -0.12912611663341522,
        -1.4051690101623535,
        -0.13168837130069733,
        -0.5929043889045715,
        -0.06745268404483795,
        -0.7916120290756226,
        -0.9647296071052551,
        0.23952609300613403,
        1.121415615081787,
        -0.30155736207962036,
        -0.37663447856903076,
        -0.8588409423828125,
        -0.3659496009349823,
        1.3905797004699707,
        0.24339380860328674,
        1.0599868297576904,
        0.2899741530418396,
        0.01646919548511505,
        0.39744219183921814,
        -0.13653363287448883,
        0.7748011350631714,
        0.055289167910814285,
        0.6326101422309875,
        -0.9422464370727539,
        -0.9344580173492432,
        -0.32244130969047546,
        0.9581905603408813,
        -0.7286238074302673,
        0.47547417879104614,
        -0.3815319240093231,
        -0.4436522126197815,
        0.47482600808143616,
        1.4037107229232788,
        1.092644453048706,
        0.7829985022544861,
        -0.7971892356872559,
        0.6260833740234375,
        -0.2491874396800995,
        0.6349806785583496,
        0.16561241447925568,
        -0.49742695689201355,
        0.5022587776184082,
        0.1979638636112213,
        -0.02463151514530182,
        0.398762583732605,
        0.5234110951423645,
        -0.11813879758119583,
        -0.4402013421058655,
        -0.36112236976623535,
        -0.5358715057373047,
        1.0350934267044067,
        0.02448568120598793,
        0.2391497790813446,
        0.13157086074352264,
        -0.02348240464925766,
        -0.31329360604286194,
        0.36786481738090515,
        -0.8225327134132385,
        -0.04076668992638588,
        -0.4451518952846527,
        0.22033298015594482,
        0.027035817503929138,
        -0.14848698675632477,
        0.18134461343288422,
        0.10157617181539536,
        0.1872236132621765,
        0.09833291172981262,
        -0.2533376216888428,
        0.20588365197181702,
        -0.47194787859916687,
        0.2582519054412842,
        0.5374119281768799,
        -0.035340115427970886,
        0.3159078359603882,
        -0.35596764087677,
        -0.12862756848335266,
        0.3688737452030182,
        -0.3083634376525879,
        0.24383386969566345,
        -0.31481653451919556,
        -0.8359839916229248,
        -0.2818066477775574,
        -0.37252163887023926,
        -0.5055873990058899,
        -0.1413404792547226,
        -0.29009896516799927,
        -0.0021804459393024445,
        -0.7992498278617859,
        -0.83213210105896,
        -0.33887284994125366,
        0.5231008529663086,
        -0.39378276467323303,
        -0.36927953362464905,
        -0.7136484384536743,
        0.18169817328453064,
        1.5676767826080322,
        0.8608831763267517,
        0.3832017183303833,
        0.5147011876106262,
        0.14880971610546112,
        -0.21458125114440918,
        0.04971767216920853,
        -0.21780896186828613,
        -0.04324181750416756,
        -0.07581625878810883,
        -0.6522537469863892,
        -0.6642523407936096,
        0.3718522787094116,
        0.13911612331867218,
        -0.4919127821922302,
        0.40141332149505615,
        -0.5053445100784302,
        0.23320671916007996,
        -0.24332024157047272,
        -0.7261220812797546,
        0.4128260910511017,
        -0.3039328455924988,
        0.43415555357933044,
        0.9265456199645996,
        0.033678364008665085,
        -1.4078341722488403,
        -0.671854555606842,
        -0.12905141711235046,
        -0.1355026215314865,
        -0.08312268555164337,
        -0.30972784757614136,
        0.6789442300796509,
        0.20794358849525452,
        0.1064395159482956,
        0.028250977396965027,
        1.2390098571777344,
        0.4228450357913971,
        -0.03816884756088257,
        -0.22505979239940643,
        0.1497933268547058,
        1.0437405109405518,
        -0.7233144044876099,
        0.3675745725631714,
        -0.5067651867866516,
        -0.7694033980369568,
        -0.47765210270881653,
        -0.07767681777477264,
        1.742124080657959,
        0.11312239617109299,
        -0.3138197362422943,
        0.21070225536823273,
        0.04698292911052704,
        -0.6351190209388733,
        -0.8893844485282898,
        0.2519627809524536,
        0.28583991527557373,
        -0.1996360421180725,
        0.33828967809677124,
        -0.036260101944208145,
        0.1579705774784088,
        0.23934663832187653,
        0.7346159219741821,
        -0.09600447118282318,
        0.08339949697256088,
        0.10651414841413498,
        1.565838098526001,
        -0.022638363763689995,
        -1.264163613319397,
        -0.4552649259567261,
        0.478161484003067,
        0.038749247789382935,
        -0.2844107747077942,
        0.4834001064300537,
        -0.6284494996070862,
        -0.21570166945457458,
        0.19670474529266357,
        -0.22416435182094574,
        -0.39432209730148315,
        0.4225408136844635,
        0.3607243597507477,
        0.7559798955917358,
        0.23904579877853394,
        0.1115531325340271,
        0.2206210494041443,
        0.22243106365203857,
        0.211790069937706,
        0.7636480331420898,
        -0.16718274354934692,
        -0.9403538703918457,
        -0.6591932773590088
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This service enhances LLM/AI prompts by applying expert prompt writing techniques to achieve better results. It leverages strategies like clear instructions, persona adoption, and reference text provision to refine prompts. The output is an improved version of the original prompt, optimized for clarity and effectiveness.",
          "name": "Improve_prompt",
          "raw": "\n                workflow Improve_prompt v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert LLM prompt writing service. You take an LLM/AI prompt as input and output a better prompt based on your prompt writing expertise and the knowledge below.\n\nSTART PROMPT WRITING KNOWLEDGE\n\nPrompt engineering\nThis guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.\n\nSome of the examples demonstrated here currently work only with our most capable model, gpt-4. In general, if you find that a model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.\n\nYou can also explore example prompts which showcase what our models are capable of:\n\nPrompt examples\nExplore prompt examples to learn what GPT models can do\nSix strategies for getting better results\nWrite clear instructions\nThese models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it.\n\nTactics:\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output\nProvide reference text\nLanguage models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.\n\nTactics:\n\nInstruct the model to answer using a reference text\nInstruct the model to answer with citations from a reference text\nSplit complex tasks into simpler subtasks\nJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\n\nTactics:\n\nUse intent classification to identify the most relevant instructions for a user query\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\nSummarize long documents piecewise and construct a full summary recursively\nGive the model time to \\\"think\\\"\nIf asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a \\\"chain of thought\\\" before an answer can help the model reason its way toward correct answers more reliably.\n\nTactics:\n\nInstruct the model to work out its own solution before rushing to a conclusion\nUse inner monologue or a sequence of queries to hide the model's reasoning process\nAsk the model if it missed anything on previous passes\nUse external tools\nCompensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI's Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both.\n\nTactics:\n\nUse embeddings-based search to implement efficient knowledge retrieval\nUse code execution to perform more accurate calculations or call external APIs\nGive the model access to specific functions\nTest changes systematically\nImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \\\"eval\\\").\n\nTactic:\n\nEvaluate model outputs with reference to gold-standard answers\nTactics\nEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.\n\nStrategy: Write clear instructions\nTactic: Include details in your query to get more relevant answers\nIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.\n\nWorse Better\nHow do I add numbers in Excel? How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \\\"Total\\\".\nWho’s president? Who was the president of Mexico in 2021, and how frequently are elections held?\nWrite code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.\nSummarize the meeting notes. Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.\nTactic: Ask the model to adopt a persona\nThe system message can be used to specify the persona used by the model in its replies.\n\nSYSTEM\nWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSER\nWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.\n\nTactic: Use delimiters to clearly indicate distinct parts of the input\nDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.\n\nUSER\nSummarize the text delimited by triple quotes with a haiku.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nSYSTEM\nYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER\n\n<article> insert first article here </article>\n\n<article> insert second article here </article>\n\nSYSTEM\nYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSER\nAbstract: insert abstract here\n\nTitle: insert title here\n\nFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don’t make the model work to understand exactly what you are asking of them.\n\nTactic: Specify the steps required to complete a task\nSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.\n\nSYSTEM\nUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \\\"Summary: \\\".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \\\"Translation: \\\".\nUSER\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nTactic: Provide examples\nProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \\\"few-shot\\\" prompting.\n\nSYSTEM\nAnswer in a consistent style.\nUSER\nTeach me about patience.\nASSISTANT\nThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSER\nTeach me about the ocean.\n\nTactic: Specify the desired length of the output\nYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.\n\nUSER\nSummarize the text delimited by triple quotes in about 50 words.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nUSER\nSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nUSER\nSummarize the text delimited by triple quotes in 3 bullet points.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nStrategy: Provide reference text\nTactic: Instruct the model to answer using a reference text\nIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.\n\nSYSTEM\nUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \\\"I could not find an answer.\\\"\nUSER\n<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>\n\nGiven that all models have limited context windows, we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \\\"Use embeddings-based search to implement efficient knowledge retrieval\\\" for more details on how to implement this.\n\nTactic: Instruct the model to answer with citations from a reference text\nIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\n\nSYSTEM\nYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \\\"Insufficient information.\\\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\\\"citation\\\": …}).\nUSER\n\\\"\\\"\\\"<insert document here>\\\"\\\"\\\"\n\nQuestion: <insert question here>\n\nStrategy: Split complex tasks into simpler subtasks\nTactic: Use intent classification to identify the most relevant instructions for a user query\nFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hard-coding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).\n\nSuppose for example that for a customer service application, queries could be usefully classified as follows:\n\nSYSTEM\nYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\n  USER\n  I need to get my internet working again.\n\n  Based on the classification of the customer query, a set of more specific instructions can be provided to a model for it to handle next steps. For example, suppose the customer requires help with \\\"troubleshooting\\\".\n\nSYSTEM\nYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n  -- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n  -- If the model number is MTD-327S, advise them to unplug and plug it back in, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\\\"IT support requested\\\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSER\nI need to get my internet working again.\n\nNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.\n\nTactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue\nSince models have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.\n\nThere are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.\n\nAn alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \\\"Use embeddings-based search to implement efficient knowledge retrieval\\\".\n\nTactic: Summarize long documents piecewise and construct a full summary recursively\nSince models have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.\n\nTo summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it’s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.\n\nStrategy: Give models time to \\\"think\\\"\nTactic: Instruct the model to work out its own solution before rushing to a conclusion\nSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student’s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.\n\nSYSTEM\nDetermine if the student's solution is correct or not.\nUSER\nProblem Statement: I'm building a solar power installation and I need help working out the financials.\n\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\n  What is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\n   Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n   ASSISTANT\n   The student's solution is correct.\n\n   But the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.\n\nSYSTEM\nFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSER\nProblem Statement: I'm building a solar power installation and I need help working out the financials.\n\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\n  What is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\n   Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n   ASSISTANT\n   Let x be the size of the installation in square feet.\n4. Land cost: 100x\n5. Solar panel cost: 250x\n6. Maintenance cost: 100,000 + 10x\n   Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.\n\nTactic: Use inner monologue or a sequence of queries to hide the model's reasoning process\nThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model’s reasoning process about the student’s solution could reveal the answer to the student.\n\nInner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.\n\nSYSTEM\nFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \\\"Step 4 - ...\\\" write \\\"Hint:\\\".\nUSER\nProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>\n\nAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.\n\nFirst, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student’s solution, it can be omitted. This provides the additional advantage that there is no chance that the model’s solution will be biased by the student’s attempted solution.\n\nUSER\n<insert problem statement>\n\nNext, we can have the model use all available information to assess the correctness of the student’s solution.\n\nSYSTEM\nCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSER\nProblem statement: \\\"\\\"\\\"<insert problem statement>\\\"\\\"\\\"\n\nYour solution: \\\"\\\"\\\"<insert model generated solution>\\\"\\\"\\\"\n\nStudent’s solution: \\\"\\\"\\\"<insert student's solution>\\\"\\\"\\\"\n\nFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.\n\nSYSTEM\nYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSER\nProblem statement: \\\"\\\"\\\"<insert problem statement>\\\"\\\"\\\"\n\nYour solution: \\\"\\\"\\\"<insert model generated solution>\\\"\\\"\\\"\n\nStudent’s solution: \\\"\\\"\\\"<insert student's solution>\\\"\\\"\\\"\n\nAnalysis: \\\"\\\"\\\"<insert model generated analysis from previous step>\\\"\\\"\\\"\n\nTactic: Ask the model if it missed anything on previous passes\nSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.\n\nSYSTEM\nYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \\\"What significant paradigm shifts have occurred in the history of artificial intelligence.\\\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\\\"excerpt\\\": \\\"...\\\"},\n...\n{\\\"excerpt\\\": \\\"...\\\"}]\nUSER\n\\\"\\\"\\\"<insert document here>\\\"\\\"\\\"\nASSISTANT\n[{\\\"excerpt\\\": \\\"the model writes an excerpt here\\\"},\n...\n{\\\"excerpt\\\": \\\"the model writes another excerpt here\\\"}]\nUSER\nAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.\n\nStrategy: Use external tools\nTactic: Use embeddings-based search to implement efficient knowledge retrieval\nA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc…) to the model’s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.\n\nA text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).\n\nExample implementations can be found in the OpenAI Cookbook. See the tactic “Instruct the model to use retrieved knowledge to answer queries” for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.\n\nTactic: Use code execution to perform more accurate calculations or call external APIs\nLanguage models cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtick. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.\n\nSYSTEM\nYou can write and execute Python code by enclosing it in triple backticks, e.g. `code goes here`. Use this to perform calculations.\nUSER\nFind all real-valued roots of the following polynomial: 3*x\\*\\*5 - 5*x**4 - 3\\*x**3 - 7\\*x - 10.\n\nAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.\n\nSYSTEM\nYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\\\"John\\\", message=\\\"Hey, want to meetup after work?\\\")\n```\n\nWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.\n\nTactic: Give the model access to specific functions\nThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using OpenAI models to call external functions. To learn more see the function calling section in our introductory text generation guide and more function calling examples in the OpenAI Cookbook.\n\nStrategy: Test changes systematically\nSometimes it can be hard to tell whether a change — e.g., a new instruction or a new design — makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.\n\nEvaluation procedures (or \\\"evals\\\") are useful for optimizing system designs. Good evals are:\n\nRepresentative of real-world usage (or at least diverse)\nContain many test cases for greater statistical power (see table below for guidelines)\nEasy to automate or repeat\nDIFFERENCE TO DETECT\tSAMPLE SIZE NEEDED FOR 95% CONFIDENCE\n30%\t~10\n10%\t~100\n3%\t~1,000\n1%\t~10,000\nEvaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.\n\nModel-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.\n\nTactic: Evaluate model outputs with reference to gold-standard answers\nSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.\n\nFor example, using the following system message:\n\nSYSTEM\nYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \\\"yes\\\" if the answer to 3 was yes, otherwise write \\\"no\\\".\n\nFinally, provide a count of how many \\\"yes\\\" answers there are. Provide this count as {\\\"count\\\": <insert count here>}.\n\nHere's an example input where both points are satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\\\"\\\"\\\"\n\nHere's an example input where only one point is satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\\\"\\\"\\\"\n\nHere's an example input where none are satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\\\"One small step,\\\" he said, for a new world.\\\"\\\"\\\"\n\nThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.\n\nSYSTEM\nUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \\\"Step 1: Reason...\\\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\\\"type_of_overlap\\\": \\\"disjoint\\\" or \\\"equal\\\" or \\\"subset\\\" or \\\"superset\\\" or \\\"overlapping\\\", \\\"contradiction\\\": true or false}\n\nHere's an example input with a substandard answer which nonetheless does not contradict the expert answer:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"Didn't he walk on the moon or something?\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nHere's an example input with answer that directly contradicts the expert answer:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nHere's an example input with a correct answer that also provides a bit more detail than is necessary:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nEND PROMPT WRITING KNOWLEDGE\n\n# STEPS:\n\n- Interpret what the input was trying to accomplish.\n- Read and understand the PROMPT WRITING KNOWLEDGE above.\n- Write and output a better version of the prompt using your knowledge of the techniques above.\n\n# OUTPUT INSTRUCTIONS:\n\n1. Output the prompt in clean, human-readable Markdown format.\n2. Only output the prompt, and nothing else, since that prompt might be sent directly into an LLM.\n\n# INPUT\n\nThe following is the prompt you will improve:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert LLM prompt writing service. You take an LLM/AI prompt as input and output a better prompt based on your prompt writing expertise and the knowledge below.\n\nSTART PROMPT WRITING KNOWLEDGE\n\nPrompt engineering\nThis guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.\n\nSome of the examples demonstrated here currently work only with our most capable model, gpt-4. In general, if you find that a model fails at a task and a more capable model is available, it's often worth trying again with the more capable model.\n\nYou can also explore example prompts which showcase what our models are capable of:\n\nPrompt examples\nExplore prompt examples to learn what GPT models can do\nSix strategies for getting better results\nWrite clear instructions\nThese models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it.\n\nTactics:\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output\nProvide reference text\nLanguage models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.\n\nTactics:\n\nInstruct the model to answer using a reference text\nInstruct the model to answer with citations from a reference text\nSplit complex tasks into simpler subtasks\nJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\n\nTactics:\n\nUse intent classification to identify the most relevant instructions for a user query\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\nSummarize long documents piecewise and construct a full summary recursively\nGive the model time to \\\"think\\\"\nIf asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a \\\"chain of thought\\\" before an answer can help the model reason its way toward correct answers more reliably.\n\nTactics:\n\nInstruct the model to work out its own solution before rushing to a conclusion\nUse inner monologue or a sequence of queries to hide the model's reasoning process\nAsk the model if it missed anything on previous passes\nUse external tools\nCompensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI's Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both.\n\nTactics:\n\nUse embeddings-based search to implement efficient knowledge retrieval\nUse code execution to perform more accurate calculations or call external APIs\nGive the model access to specific functions\nTest changes systematically\nImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \\\"eval\\\").\n\nTactic:\n\nEvaluate model outputs with reference to gold-standard answers\nTactics\nEach of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.\n\nStrategy: Write clear instructions\nTactic: Include details in your query to get more relevant answers\nIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.\n\nWorse Better\nHow do I add numbers in Excel? How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called \\\"Total\\\".\nWho’s president? Who was the president of Mexico in 2021, and how frequently are elections held?\nWrite code to calculate the Fibonacci sequence. Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.\nSummarize the meeting notes. Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.\nTactic: Ask the model to adopt a persona\nThe system message can be used to specify the persona used by the model in its replies.\n\nSYSTEM\nWhen I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\nUSER\nWrite a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.\n\nTactic: Use delimiters to clearly indicate distinct parts of the input\nDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.\n\nUSER\nSummarize the text delimited by triple quotes with a haiku.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nSYSTEM\nYou will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.\nUSER\n\n<article> insert first article here </article>\n\n<article> insert second article here </article>\n\nSYSTEM\nYou will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.\nUSER\nAbstract: insert abstract here\n\nTitle: insert title here\n\nFor straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don’t make the model work to understand exactly what you are asking of them.\n\nTactic: Specify the steps required to complete a task\nSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.\n\nSYSTEM\nUse the following step-by-step instructions to respond to user inputs.\n\nStep 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says \\\"Summary: \\\".\n\nStep 2 - Translate the summary from Step 1 into Spanish, with a prefix that says \\\"Translation: \\\".\nUSER\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nTactic: Provide examples\nProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as \\\"few-shot\\\" prompting.\n\nSYSTEM\nAnswer in a consistent style.\nUSER\nTeach me about patience.\nASSISTANT\nThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\nUSER\nTeach me about the ocean.\n\nTactic: Specify the desired length of the output\nYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.\n\nUSER\nSummarize the text delimited by triple quotes in about 50 words.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nUSER\nSummarize the text delimited by triple quotes in 2 paragraphs.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nUSER\nSummarize the text delimited by triple quotes in 3 bullet points.\n\n\\\"\\\"\\\"insert text here\\\"\\\"\\\"\n\nStrategy: Provide reference text\nTactic: Instruct the model to answer using a reference text\nIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.\n\nSYSTEM\nUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \\\"I could not find an answer.\\\"\nUSER\n<insert articles, each delimited by triple quotes>\n\nQuestion: <insert question here>\n\nGiven that all models have limited context windows, we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic \\\"Use embeddings-based search to implement efficient knowledge retrieval\\\" for more details on how to implement this.\n\nTactic: Instruct the model to answer with citations from a reference text\nIf the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\n\nSYSTEM\nYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \\\"Insufficient information.\\\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\\\"citation\\\": …}).\nUSER\n\\\"\\\"\\\"<insert document here>\\\"\\\"\\\"\n\nQuestion: <insert question here>\n\nStrategy: Split complex tasks into simpler subtasks\nTactic: Use intent classification to identify the most relevant instructions for a user query\nFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hard-coding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).\n\nSuppose for example that for a customer service application, queries could be usefully classified as follows:\n\nSYSTEM\nYou will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.\n\nPrimary categories: Billing, Technical Support, Account Management, or General Inquiry.\n\nBilling secondary categories:\n\n- Unsubscribe or upgrade\n- Add a payment method\n- Explanation for charge\n- Dispute a charge\n\nTechnical Support secondary categories:\n\n- Troubleshooting\n- Device compatibility\n- Software updates\n\nAccount Management secondary categories:\n\n- Password reset\n- Update personal information\n- Close account\n- Account security\n\nGeneral Inquiry secondary categories:\n\n- Product information\n- Pricing\n- Feedback\n- Speak to a human\n  USER\n  I need to get my internet working again.\n\n  Based on the classification of the customer query, a set of more specific instructions can be provided to a model for it to handle next steps. For example, suppose the customer requires help with \\\"troubleshooting\\\".\n\nSYSTEM\nYou will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:\n\n- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.\n- If all cables are connected and the issue persists, ask them which router model they are using\n- Now you will advise them how to restart their device:\n  -- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.\n  -- If the model number is MTD-327S, advise them to unplug and plug it back in, then wait 5 minutes before testing the connection.\n- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {\\\"IT support requested\\\"}.\n- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:\n\n<insert primary/secondary classification scheme from above here>\nUSER\nI need to get my internet working again.\n\nNotice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.\n\nTactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue\nSince models have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.\n\nThere are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.\n\nAn alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic \\\"Use embeddings-based search to implement efficient knowledge retrieval\\\".\n\nTactic: Summarize long documents piecewise and construct a full summary recursively\nSince models have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.\n\nTo summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it’s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.\n\nStrategy: Give models time to \\\"think\\\"\nTactic: Instruct the model to work out its own solution before rushing to a conclusion\nSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student’s solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.\n\nSYSTEM\nDetermine if the student's solution is correct or not.\nUSER\nProblem Statement: I'm building a solar power installation and I need help working out the financials.\n\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\n  What is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\n   Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n   ASSISTANT\n   The student's solution is correct.\n\n   But the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.\n\nSYSTEM\nFirst work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\nUSER\nProblem Statement: I'm building a solar power installation and I need help working out the financials.\n\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot\n  What is the total cost for the first year of operations as a function of the number of square feet.\n\nStudent's Solution: Let x be the size of the installation in square feet.\n\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\n   Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n   ASSISTANT\n   Let x be the size of the installation in square feet.\n4. Land cost: 100x\n5. Solar panel cost: 250x\n6. Maintenance cost: 100,000 + 10x\n   Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n\nThe student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.\n\nTactic: Use inner monologue or a sequence of queries to hide the model's reasoning process\nThe previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model’s reasoning process about the student’s solution could reveal the answer to the student.\n\nInner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.\n\nSYSTEM\nFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\\\"\\\"\\\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \\\"Step 4 - ...\\\" write \\\"Hint:\\\".\nUSER\nProblem Statement: <insert problem statement>\n\nStudent Solution: <insert student solution>\n\nAlternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.\n\nFirst, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student’s solution, it can be omitted. This provides the additional advantage that there is no chance that the model’s solution will be biased by the student’s attempted solution.\n\nUSER\n<insert problem statement>\n\nNext, we can have the model use all available information to assess the correctness of the student’s solution.\n\nSYSTEM\nCompare your solution to the student's solution and evaluate if the student's solution is correct or not.\nUSER\nProblem statement: \\\"\\\"\\\"<insert problem statement>\\\"\\\"\\\"\n\nYour solution: \\\"\\\"\\\"<insert model generated solution>\\\"\\\"\\\"\n\nStudent’s solution: \\\"\\\"\\\"<insert student's solution>\\\"\\\"\\\"\n\nFinally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.\n\nSYSTEM\nYou are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.\nUSER\nProblem statement: \\\"\\\"\\\"<insert problem statement>\\\"\\\"\\\"\n\nYour solution: \\\"\\\"\\\"<insert model generated solution>\\\"\\\"\\\"\n\nStudent’s solution: \\\"\\\"\\\"<insert student's solution>\\\"\\\"\\\"\n\nAnalysis: \\\"\\\"\\\"<insert model generated analysis from previous step>\\\"\\\"\\\"\n\nTactic: Ask the model if it missed anything on previous passes\nSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.\n\nSYSTEM\nYou will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: \\\"What significant paradigm shifts have occurred in the history of artificial intelligence.\\\"\n\nEnsure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:\n\n[{\\\"excerpt\\\": \\\"...\\\"},\n...\n{\\\"excerpt\\\": \\\"...\\\"}]\nUSER\n\\\"\\\"\\\"<insert document here>\\\"\\\"\\\"\nASSISTANT\n[{\\\"excerpt\\\": \\\"the model writes an excerpt here\\\"},\n...\n{\\\"excerpt\\\": \\\"the model writes another excerpt here\\\"}]\nUSER\nAre there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.\n\nStrategy: Use external tools\nTactic: Use embeddings-based search to implement efficient knowledge retrieval\nA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc…) to the model’s input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.\n\nA text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).\n\nExample implementations can be found in the OpenAI Cookbook. See the tactic “Instruct the model to use retrieved knowledge to answer queries” for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.\n\nTactic: Use code execution to perform more accurate calculations or call external APIs\nLanguage models cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtick. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.\n\nSYSTEM\nYou can write and execute Python code by enclosing it in triple backticks, e.g. `code goes here`. Use this to perform calculations.\nUSER\nFind all real-valued roots of the following polynomial: 3*x\\*\\*5 - 5*x**4 - 3\\*x**3 - 7\\*x - 10.\n\nAnother good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.\n\nSYSTEM\nYou can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:\n\n```python\nimport message\nmessage.write(to=\\\"John\\\", message=\\\"Hey, want to meetup after work?\\\")\n```\n\nWARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.\n\nTactic: Give the model access to specific functions\nThe Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using OpenAI models to call external functions. To learn more see the function calling section in our introductory text generation guide and more function calling examples in the OpenAI Cookbook.\n\nStrategy: Test changes systematically\nSometimes it can be hard to tell whether a change — e.g., a new instruction or a new design — makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.\n\nEvaluation procedures (or \\\"evals\\\") are useful for optimizing system designs. Good evals are:\n\nRepresentative of real-world usage (or at least diverse)\nContain many test cases for greater statistical power (see table below for guidelines)\nEasy to automate or repeat\nDIFFERENCE TO DETECT\tSAMPLE SIZE NEEDED FOR 95% CONFIDENCE\n30%\t~10\n10%\t~100\n3%\t~1,000\n1%\t~10,000\nEvaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.\n\nModel-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.\n\nTactic: Evaluate model outputs with reference to gold-standard answers\nSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.\n\nFor example, using the following system message:\n\nSYSTEM\nYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \\\"yes\\\" if the answer to 3 was yes, otherwise write \\\"no\\\".\n\nFinally, provide a count of how many \\\"yes\\\" answers there are. Provide this count as {\\\"count\\\": <insert count here>}.\n\nHere's an example input where both points are satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission.\\\"\\\"\\\"\n\nHere's an example input where only one point is satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\\\"\\\"\\\"\n\nHere's an example input where none are satisfied:\n\nSYSTEM\n<insert system message above>\nUSER\n\\\"\\\"\\\"In the summer of '69, a voyage grand,\nApollo 11, bold as legend's hand.\nArmstrong took a step, history unfurled,\n\\\"One small step,\\\" he said, for a new world.\\\"\\\"\\\"\n\nThere are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.\n\nSYSTEM\nUse the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. \\\"Step 1: Reason...\\\".\n\nStep 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).\n\nStep 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.\n\nStep 3: Output a JSON object structured like: {\\\"type_of_overlap\\\": \\\"disjoint\\\" or \\\"equal\\\" or \\\"subset\\\" or \\\"superset\\\" or \\\"overlapping\\\", \\\"contradiction\\\": true or false}\n\nHere's an example input with a substandard answer which nonetheless does not contradict the expert answer:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"Didn't he walk on the moon or something?\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nHere's an example input with answer that directly contradicts the expert answer:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin.\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nHere's an example input with a correct answer that also provides a bit more detail than is necessary:\n\nSYSTEM\n<insert system message above>\nUSER\nQuestion: \\\"\\\"\\\"What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time.\\\"\\\"\\\"\n\nSubmitted Answer: \\\"\\\"\\\"At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history.\\\"\\\"\\\"\n\nExpert Answer: \\\"\\\"\\\"Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969.\\\"\\\"\\\"\n\nEND PROMPT WRITING KNOWLEDGE\n\n# STEPS:\n\n- Interpret what the input was trying to accomplish.\n- Read and understand the PROMPT WRITING KNOWLEDGE above.\n- Write and output a better version of the prompt using your knowledge of the techniques above.\n\n# OUTPUT INSTRUCTIONS:\n\n1. Output the prompt in clean, human-readable Markdown format.\n2. Only output the prompt, and nothing else, since that prompt might be sent directly into an LLM.\n\n# INPUT\n\nThe following is the prompt you will improve:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.16591252386569977,
        0.22819359600543976,
        -0.004013650119304657,
        0.20329134166240692,
        0.575613796710968,
        -0.3174269199371338,
        -0.45907774567604065,
        -0.4079979360103607,
        0.22029808163642883,
        -0.04407275840640068,
        -0.23735205829143524,
        0.39711418747901917,
        -0.05133988335728645,
        0.48506447672843933,
        0.31595897674560547,
        -0.15037213265895844,
        0.11392433941364288,
        -1.3013648986816406,
        -0.7179083228111267,
        -0.2102355659008026,
        -0.07507963478565216,
        0.40257084369659424,
        0.06697724759578705,
        -0.07838574051856995,
        0.3350435495376587,
        0.37996935844421387,
        0.23834019899368286,
        -0.29077744483947754,
        -0.9881181716918945,
        -1.195575475692749,
        0.3486025631427765,
        -0.026613011956214905,
        -0.7907722592353821,
        -0.86299729347229,
        0.6978363990783691,
        -0.7149562835693359,
        0.5050007104873657,
        -0.14856193959712982,
        -0.5364984273910522,
        -0.9636836647987366,
        -0.05443389713764191,
        0.12358517199754715,
        -0.28813400864601135,
        -0.24744871258735657,
        0.2689853608608246,
        -0.32713618874549866,
        0.06463010609149933,
        -0.03382306173443794,
        1.391014575958252,
        0.25577014684677124,
        -0.25150713324546814,
        -0.607415497303009,
        -0.42949721217155457,
        0.1030438020825386,
        -0.381914883852005,
        0.1674194484949112,
        0.22117885947227478,
        -0.5268657803535461,
        0.19163721799850464,
        0.03271125257015228,
        0.372293084859848,
        0.6366037130355835,
        -3.493617296218872,
        -0.32418397068977356,
        0.2187836915254593,
        0.31373289227485657,
        -0.420276015996933,
        -0.02239041030406952,
        0.5393170714378357,
        -0.022769078612327576,
        -0.5109095573425293,
        0.5658625960350037,
        -0.18874895572662354,
        0.6527442932128906,
        0.15818840265274048,
        -0.03912951424717903,
        0.09102200716733932,
        0.44007089734077454,
        0.627650260925293,
        -0.14722569286823273,
        0.07298947125673294,
        0.3008720874786377,
        -0.37299832701683044,
        0.44811758399009705,
        0.20163294672966003,
        0.45219188928604126,
        0.15345510840415955,
        -0.34079378843307495,
        1.1600165367126465,
        0.5608021020889282,
        -0.16001325845718384,
        -0.9281641840934753,
        -0.30292850732803345,
        0.1292017102241516,
        -0.27114221453666687,
        -0.20838229358196259,
        -0.07184917479753494,
        0.08860984444618225,
        0.06630024313926697,
        3.5890002250671387,
        0.45783841609954834,
        0.21256746351718903,
        1.1032826900482178,
        -1.0920569896697998,
        0.402486652135849,
        -0.29881465435028076,
        0.07424314320087433,
        -0.13321620225906372,
        0.04969440773129463,
        0.35937660932540894,
        0.5084071159362793,
        -0.4890722334384918,
        -0.6589900255203247,
        0.7814016938209534,
        0.3479328751564026,
        0.0923977643251419,
        -0.9253879189491272,
        -0.3762131333351135,
        0.748965859413147,
        1.174562692642212,
        -0.15670375525951385,
        0.3306344747543335,
        -0.4025147259235382,
        -0.27703580260276794,
        -0.13227476179599762,
        -0.008366823196411133,
        -0.2049926072359085,
        0.8819374442100525,
        0.00810164213180542,
        -0.27366530895233154,
        -0.2795017957687378,
        -0.047404080629348755,
        -0.43900007009506226,
        -0.07681030035018921,
        0.4557732939720154,
        -0.4765365719795227,
        -0.1058734804391861,
        -0.41473516821861267,
        0.887354850769043,
        -1.207631230354309,
        0.27994224429130554,
        -0.5633475184440613,
        0.7356618642807007,
        0.7393755912780762,
        0.6081587076187134,
        0.03316665068268776,
        -0.1696195900440216,
        0.15831777453422546,
        -0.4965129494667053,
        -1.5109984874725342,
        0.226960688829422,
        0.8024070858955383,
        0.0619032084941864,
        0.6133003234863281,
        0.30392640829086304,
        -0.32722020149230957,
        -0.939866304397583,
        0.8848303556442261,
        -0.4806979298591614,
        0.4081327021121979,
        0.3963913917541504,
        -0.09472174197435379,
        0.3509632349014282,
        0.9043095111846924,
        0.5379961729049683,
        -0.44696205854415894,
        1.0185747146606445,
        -0.4866465628147125,
        0.4195563793182373,
        0.12567973136901855,
        0.12004442512989044,
        0.05743008852005005,
        0.4823029637336731,
        0.44804999232292175,
        -0.6630372405052185,
        -0.12200573086738586,
        0.03567884862422943,
        -0.26518675684928894,
        -0.10158458352088928,
        -0.592644214630127,
        0.6955081224441528,
        0.5620719194412231,
        -0.2265034317970276,
        -0.9008371233940125,
        -0.1718074083328247,
        0.23073644936084747,
        0.14920967817306519,
        0.08851895481348038,
        0.5833681225776672,
        0.6873102188110352,
        -1.0233455896377563,
        0.7574090957641602,
        -0.4928860068321228,
        -0.1122155413031578,
        -0.2699119448661804,
        -0.29390013217926025,
        -0.4871760606765747,
        0.008468732237815857,
        -0.3110654056072235,
        0.02075142413377762,
        -0.9242392182350159,
        0.034933846443891525,
        -0.2582557499408722,
        -0.1981891393661499,
        -0.5524712800979614,
        -0.10566641390323639,
        0.46113282442092896,
        0.1784680038690567,
        -0.48484694957733154,
        -0.25758668780326843,
        -0.14852376282215118,
        0.17125430703163147,
        1.2211642265319824,
        0.10085297375917435,
        1.289037823677063,
        0.35119736194610596,
        0.06532257795333862,
        -0.03004859760403633,
        0.5813651084899902,
        0.2983018159866333,
        -0.5325437188148499,
        0.22409886121749878,
        -0.9729644060134888,
        -0.745228111743927,
        -0.5698093771934509,
        0.8551268577575684,
        0.6439458727836609,
        0.1987708956003189,
        -0.48654288053512573,
        0.14596739411354065,
        0.28611448407173157,
        0.7917094826698303,
        0.3227146565914154,
        1.2698755264282227,
        -0.2876881957054138,
        0.30994340777397156,
        -0.11395450681447983,
        0.6562682390213013,
        0.7292574644088745,
        -1.1781855821609497,
        0.02228725701570511,
        -0.27219724655151367,
        -0.5588316321372986,
        0.33757156133651733,
        0.3215944766998291,
        0.14526888728141785,
        0.12076577544212341,
        -0.10942808538675308,
        0.4967307448387146,
        1.291426181793213,
        0.5262608528137207,
        0.30371007323265076,
        0.2453596293926239,
        0.5122557878494263,
        -0.0585329607129097,
        0.4516426920890808,
        -1.7908014059066772,
        -0.3663966655731201,
        -0.4044010043144226,
        -0.26203542947769165,
        0.33202311396598816,
        -0.7075809240341187,
        0.7186055779457092,
        -0.6412350535392761,
        0.11826686561107635,
        -0.2056981474161148,
        -0.49132290482521057,
        -0.30524393916130066,
        -0.662117063999176,
        0.06530975550413132,
        0.2951589524745941,
        0.6006290912628174,
        0.41619884967803955,
        -0.5424940586090088,
        -0.2662368416786194,
        0.40531226992607117,
        -0.22455066442489624,
        0.29470884799957275,
        -0.5483316779136658,
        -0.3524192273616791,
        -0.20463880896568298,
        0.2015456110239029,
        0.15671473741531372,
        0.2464195042848587,
        -0.2642987370491028,
        0.5074535608291626,
        -0.6565689444541931,
        -0.5832510590553284,
        -0.16204750537872314,
        0.5556839108467102,
        -0.8145537376403809,
        0.16353708505630493,
        -0.28673678636550903,
        0.10063685476779938,
        1.8157004117965698,
        -0.03916051983833313,
        0.17804522812366486,
        0.3357999324798584,
        0.4864557683467865,
        0.08994428813457489,
        -0.21884146332740784,
        -0.12469551712274551,
        -0.21888193488121033,
        -0.30173367261886597,
        -0.4400562345981598,
        -0.6516743302345276,
        0.45239049196243286,
        -0.240203857421875,
        0.031015455722808838,
        0.4272225797176361,
        -0.6034771203994751,
        0.3590297996997833,
        -0.4504661560058594,
        -0.42147672176361084,
        0.3413732051849365,
        -0.9666417837142944,
        0.13707907497882843,
        0.5346698760986328,
        0.20740866661071777,
        -1.4020565748214722,
        0.007764140143990517,
        0.4948893189430237,
        -0.10433338582515717,
        0.0571659654378891,
        -0.29090267419815063,
        1.069916009902954,
        -0.2200818955898285,
        -0.4351012706756592,
        -0.38857561349868774,
        1.4598928689956665,
        0.2453237920999527,
        0.06402330845594406,
        -0.002463454380631447,
        -0.4676721692085266,
        0.6531533002853394,
        -0.4231782853603363,
        -0.04989410191774368,
        -0.78677898645401,
        -0.8291460275650024,
        -0.13773111999034882,
        0.6362968683242798,
        1.851597547531128,
        0.17626610398292542,
        -0.1383523792028427,
        0.27257952094078064,
        0.5481665134429932,
        -0.6806601285934448,
        -0.7646144032478333,
        -0.25868871808052063,
        0.13648924231529236,
        -0.7000190615653992,
        0.2542755603790283,
        0.3848668932914734,
        -0.0694013237953186,
        0.5070194005966187,
        0.7592543959617615,
        -0.7726516127586365,
        0.14050990343093872,
        -0.09431224316358566,
        1.7058128118515015,
        -0.6484985947608948,
        -1.0192720890045166,
        -0.29747140407562256,
        0.33662083745002747,
        -0.20832930505275726,
        0.3429027795791626,
        -0.33194872736930847,
        -0.8733711242675781,
        -0.666715145111084,
        -0.203901469707489,
        -0.19094258546829224,
        -0.5229651927947998,
        0.5010538101196289,
        0.17607395350933075,
        0.6209242939949036,
        0.4965991973876953,
        0.19337832927703857,
        0.17757603526115417,
        0.25621968507766724,
        0.0338064581155777,
        0.3518064320087433,
        -0.1494375467300415,
        -0.731398344039917,
        -0.7256266474723816
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs the creation of an improved security finding report from a penetration test, detailing the finding, risk, recommendations, references, a concise summary, and insightful quotes, all formatted in markdown without using markdown syntax or special formatting. It emphasizes a detailed, insightful approach to presenting cybersecurity issues and solutions. The output should be comprehensive, covering various sections including title, description, risk, recommendations, references, and quotes, aiming for clarity and depth in reporting.",
          "name": "Improve_report_finding",
          "raw": "\n                workflow Improve_report_finding v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a extremely experienced 'jack-of-all-trades' cyber security consultant that is diligent, concise but informative and professional. You are highly experienced in web, API, infrastructure (on-premise and cloud), and mobile testing. Additionally, you are an expert in threat modeling and analysis.\n\nYou have been tasked with improving a security finding that has been pulled from a penetration test report, and you must output an improved report finding in markdown format.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Title section that contains the title of the finding.\n\n- Create a Description section that details the nature of the finding, including insightful and informative information. Do not solely use bullet point lists for this section.\n\n- Create a Risk section that details the risk of the finding. Do not solely use bullet point lists for this section.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a References section that lists 1 to 5 references that are suitibly named hyperlinks that provide instant access to knowledgable and informative articles that talk about the issue, the tech and remediations. Do not hallucinate or act confident if you are unsure.\n\n- Create a summary sentence that captures the spirit of the finding and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a extremely experienced 'jack-of-all-trades' cyber security consultant that is diligent, concise but informative and professional. You are highly experienced in web, API, infrastructure (on-premise and cloud), and mobile testing. Additionally, you are an expert in threat modeling and analysis.\n\nYou have been tasked with improving a security finding that has been pulled from a penetration test report, and you must output an improved report finding in markdown format.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n\n- Create a Title section that contains the title of the finding.\n\n- Create a Description section that details the nature of the finding, including insightful and informative information. Do not solely use bullet point lists for this section.\n\n- Create a Risk section that details the risk of the finding. Do not solely use bullet point lists for this section.\n\n- Extract the 5 to 15 of the most surprising, insightful, and/or interesting recommendations that can be collected from the report into a section called Recommendations.\n\n- Create a References section that lists 1 to 5 references that are suitibly named hyperlinks that provide instant access to knowledgable and informative articles that talk about the issue, the tech and remediations. Do not hallucinate or act confident if you are unsure.\n\n- Create a summary sentence that captures the spirit of the finding and its insights in less than 25 words in a section called One-Sentence-Summary:. Use plain and conversational language when creating this summary. Don't use jargon or marketing language.\n\n- Extract 10 to 20 of the most surprising, insightful, and/or interesting quotes from the input into a section called Quotes:. Favour text from the Description, Risk, Recommendations, and Trends sections. Use the exact quote text from the input.\n\n# OUTPUT INSTRUCTIONS\n\n- Only output Markdown.\n- Do not output the markdown code syntax, only the content.\n- Do not use bold or italics formatting in the markdown output.\n- Extract at least 5 TRENDS from the content.\n- Extract at least 10 items for the other output sections.\n- Do not give warnings or notes; only output the requested sections.\n- You use bulleted lists for output, not numbered lists.\n- Do not repeat ideas, quotes, facts, or resources.\n- Do not start items with the same opening words.\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.6380617022514343,
        0.009956799447536469,
        -0.4875580668449402,
        0.6180312633514404,
        0.5314409136772156,
        0.3645792603492737,
        -0.7591072916984558,
        -0.024683572351932526,
        0.2906944453716278,
        -0.4476701021194458,
        0.10463273525238037,
        0.8822948336601257,
        -0.2394503951072693,
        -0.039880186319351196,
        -0.024961475282907486,
        -0.05607178807258606,
        -0.049246449023485184,
        -0.4479577839374542,
        -1.4107680320739746,
        -0.3496328592300415,
        -0.27594178915023804,
        0.5814307332038879,
        0.060183025896549225,
        0.22067970037460327,
        0.5117666721343994,
        0.7485346794128418,
        0.26240289211273193,
        -0.19541911780834198,
        -1.2554445266723633,
        -1.4086909294128418,
        0.15529277920722961,
        0.04551544785499573,
        -0.19712530076503754,
        -0.7308424711227417,
        0.5147192478179932,
        -0.5302928686141968,
        -0.10241688042879105,
        0.05147325247526169,
        -0.44890105724334717,
        -0.4963993430137634,
        0.4690069258213043,
        0.2599315643310547,
        0.151127427816391,
        -0.6727946400642395,
        0.28445902466773987,
        -0.12273850291967392,
        0.08196989446878433,
        -0.17025628685951233,
        0.9208702445030212,
        0.382789671421051,
        -0.1420745849609375,
        -0.4189261794090271,
        -0.309621661901474,
        -0.2441767454147339,
        0.1299629807472229,
        0.5186246633529663,
        0.6026803851127625,
        -0.4438060224056244,
        -0.07518066465854645,
        0.03307127207517624,
        -0.005853433161973953,
        0.862330973148346,
        -3.5130865573883057,
        -0.11078764498233795,
        -0.0928831547498703,
        0.10739843547344208,
        -0.3067910671234131,
        0.15437716245651245,
        0.4750422239303589,
        -0.5017596483230591,
        -0.44416171312332153,
        0.38539668917655945,
        -0.5173649787902832,
        1.0961021184921265,
        0.6313780546188354,
        -0.18696892261505127,
        0.08587923645973206,
        0.05543055757880211,
        0.6584351062774658,
        -0.21594853699207306,
        0.3600411117076874,
        0.3669513463973999,
        0.04759856313467026,
        -0.3914109766483307,
        -0.3734046518802643,
        0.28242969512939453,
        0.1957220435142517,
        0.08688963949680328,
        0.43730536103248596,
        -0.2452254593372345,
        -0.3579341173171997,
        -0.19419419765472412,
        0.35153982043266296,
        0.14496490359306335,
        0.014927618205547333,
        0.12414804100990295,
        -0.08514580130577087,
        0.30047762393951416,
        0.03376002237200737,
        3.5071141719818115,
        0.7276917695999146,
        0.5770573616027832,
        0.6840935945510864,
        -0.6772657036781311,
        0.15066462755203247,
        0.011442508548498154,
        -0.41510042548179626,
        -0.8919069766998291,
        0.02671234868466854,
        -0.6378793716430664,
        0.556897759437561,
        -1.1610033512115479,
        -0.46174660325050354,
        -0.041673436760902405,
        0.3423718512058258,
        0.3409813642501831,
        -0.9638554453849792,
        -0.04082253947854042,
        -0.2167075127363205,
        1.2349802255630493,
        -0.4210253953933716,
        -0.12550826370716095,
        -0.3687196671962738,
        -0.3450281620025635,
        -0.2758539319038391,
        0.1125587746500969,
        -0.16040433943271637,
        0.5958536267280579,
        0.10917651653289795,
        0.5346929430961609,
        -0.3152352571487427,
        0.40553751587867737,
        -0.45895764231681824,
        -0.8321924209594727,
        0.2589505910873413,
        0.23142018914222717,
        0.6808282732963562,
        -0.6499916315078735,
        0.5430498123168945,
        -0.6797818541526794,
        0.6424026489257812,
        -0.5977316498756409,
        0.9707564115524292,
        -0.23157542943954468,
        1.0155671834945679,
        0.3613407611846924,
        -0.07694033533334732,
        0.5224886536598206,
        -0.3357636034488678,
        -0.8687418103218079,
        -0.15553349256515503,
        0.46995124220848083,
        -0.7665964961051941,
        0.4555242955684662,
        0.19726121425628662,
        0.1911756694316864,
        -0.48306113481521606,
        0.14897875487804413,
        -0.973634660243988,
        0.5582450032234192,
        0.47920435667037964,
        0.29011180996894836,
        0.2075655460357666,
        -0.016411349177360535,
        1.0942081212997437,
        -0.7770277857780457,
        0.46433591842651367,
        -0.318666934967041,
        -0.14625689387321472,
        0.10877960920333862,
        -0.20533406734466553,
        -0.23135775327682495,
        0.3939237594604492,
        0.7507044672966003,
        -1.0283597707748413,
        0.22339844703674316,
        0.0049147829413414,
        0.5513145327568054,
        0.39386677742004395,
        -0.36946016550064087,
        0.4335690438747406,
        0.7385454177856445,
        -0.4319358468055725,
        -0.65038001537323,
        0.123801589012146,
        0.18600930273532867,
        0.24149569869041443,
        0.22421720623970032,
        0.8914865255355835,
        1.4885940551757812,
        -1.3596584796905518,
        1.9092577695846558,
        -0.6961451172828674,
        -0.038227830082178116,
        -0.3708655834197998,
        0.4208682179450989,
        0.2578356862068176,
        0.07173348218202591,
        0.08604490756988525,
        -0.29708147048950195,
        -1.060179591178894,
        0.16654126346111298,
        -0.5202902555465698,
        0.1700056493282318,
        -0.8588466048240662,
        -0.6577526926994324,
        -0.10107913613319397,
        1.233007788658142,
        -0.04032611846923828,
        -0.6298887729644775,
        -0.4827253222465515,
        0.08264526724815369,
        1.6186597347259521,
        -0.0278293639421463,
        1.0409997701644897,
        -0.17758893966674805,
        0.29834267497062683,
        0.6317179203033447,
        0.1523643136024475,
        0.530258059501648,
        -0.4412839412689209,
        0.29673364758491516,
        -0.8120970726013184,
        -0.8086404800415039,
        -0.5172765254974365,
        1.2537904977798462,
        -0.8577113151550293,
        0.6887837052345276,
        -0.48696470260620117,
        -0.5585777759552002,
        0.03872288763523102,
        1.2079603672027588,
        0.9872216582298279,
        0.7296644449234009,
        -0.8635153770446777,
        0.5506312847137451,
        0.13206644356250763,
        0.5248743295669556,
        -0.1109786108136177,
        -0.5269008278846741,
        0.04051102325320244,
        0.36002299189567566,
        -0.12031828612089157,
        0.47645771503448486,
        0.1313643455505371,
        -0.21640507876873016,
        -0.9640994668006897,
        -0.40604040026664734,
        -0.35802018642425537,
        1.0102949142456055,
        0.23992538452148438,
        -0.1043711006641388,
        0.1410064399242401,
        -0.30945420265197754,
        -0.3271777331829071,
        0.21422189474105835,
        -0.9255951642990112,
        -0.3962450325489044,
        -0.36591270565986633,
        0.36720892786979675,
        -0.10998896509408951,
        0.0159059539437294,
        0.6493339538574219,
        -0.037967607378959656,
        0.15012937784194946,
        -0.11307702958583832,
        0.31650760769844055,
        -0.007890254259109497,
        -0.21652764081954956,
        0.12932536005973816,
        0.29040440917015076,
        0.06711756438016891,
        0.29247838258743286,
        -0.07271226495504379,
        0.22949746251106262,
        -0.16579310595989227,
        0.03767850250005722,
        0.15424112975597382,
        -0.11060094833374023,
        -0.8661235570907593,
        -0.10780280083417892,
        -0.4464382827281952,
        -0.7227500677108765,
        0.10897931456565857,
        -0.41367438435554504,
        0.18258948624134064,
        -0.3085460662841797,
        -0.9854847192764282,
        0.36689844727516174,
        0.6631394028663635,
        -0.4519041180610657,
        -0.40795469284057617,
        -0.7042255997657776,
        0.24429768323898315,
        1.8400352001190186,
        0.2417822778224945,
        0.34425878524780273,
        0.7294473648071289,
        0.2520303428173065,
        -0.47296279668807983,
        0.29526299238204956,
        0.4031478762626648,
        -0.07649832218885422,
        0.018388573080301285,
        -0.7860233187675476,
        -0.6570513844490051,
        0.37060225009918213,
        -0.5139085054397583,
        -0.016478314995765686,
        0.2766731381416321,
        -0.3360106945037842,
        0.10872417688369751,
        -0.3551037907600403,
        -0.06669159233570099,
        0.1967519223690033,
        -0.3556350767612457,
        0.15920628607273102,
        1.017555594444275,
        0.17796996235847473,
        -1.3214895725250244,
        -0.6318030953407288,
        0.08461961895227432,
        0.39703133702278137,
        0.17368805408477783,
        -0.5430949330329895,
        0.5562591552734375,
        0.4637826383113861,
        0.09300005435943604,
        -0.5451666116714478,
        1.024576187133789,
        0.49056175351142883,
        -0.11579018831253052,
        0.5061678290367126,
        0.09439363330602646,
        1.0129460096359253,
        -0.5244320631027222,
        0.6106021404266357,
        -0.3056134581565857,
        -0.7649332880973816,
        -0.12871986627578735,
        0.07495639473199844,
        1.6425676345825195,
        0.37359216809272766,
        0.3624103367328644,
        0.18434761464595795,
        0.20836031436920166,
        -0.7325081825256348,
        -0.6995574235916138,
        -0.25524887442588806,
        -0.22058628499507904,
        -0.33334097266197205,
        0.642570436000824,
        0.20090463757514954,
        0.2909586429595947,
        0.2372458577156067,
        0.31068184971809387,
        -0.3417418301105499,
        -0.2029474973678589,
        -0.15456876158714294,
        1.8872320652008057,
        -0.8461876511573792,
        -0.8347004652023315,
        -0.7013407349586487,
        0.19676530361175537,
        -0.25939491391181946,
        -0.05929082632064819,
        0.0016544871032238007,
        -0.7263761758804321,
        -0.5141096115112305,
        0.2025911808013916,
        -0.4557827115058899,
        -0.09313525259494781,
        0.16821321845054626,
        -0.029920797795057297,
        0.6810376644134521,
        -0.2805097699165344,
        -0.04146270453929901,
        0.3264385163784027,
        0.21516072750091553,
        -0.40803706645965576,
        -0.11342737823724747,
        -0.571719765663147,
        -0.4235333204269409,
        -1.1330106258392334
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt aims to refine input text for enhanced clarity, coherence, grammar, and style. It involves analyzing the text for errors and inconsistencies, then applying corrections while preserving the original meaning. The expected output is a grammatically correct and stylistically improved version of the text.",
          "name": "Improve_writing",
          "raw": "\n                workflow Improve_writing v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a writing expert. You refine the input text to enhance clarity, coherence, grammar, and style.\n\n# Steps\n\n- Analyze the input text for grammatical errors, stylistic inconsistencies, clarity issues, and coherence.\n- Apply corrections and improvements directly to the text.\n- Maintain the original meaning and intent of the user's text, ensuring that the improvements are made within the context of the input language's grammatical norms and stylistic conventions.\n\n# OUTPUT INSTRUCTIONS\n\n- Refined and improved text that has no grammar mistakes.\n- Return in the same language as the input.\n- Include NO additional commentary or explanation in the response.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a writing expert. You refine the input text to enhance clarity, coherence, grammar, and style.\n\n# Steps\n\n- Analyze the input text for grammatical errors, stylistic inconsistencies, clarity issues, and coherence.\n- Apply corrections and improvements directly to the text.\n- Maintain the original meaning and intent of the user's text, ensuring that the improvements are made within the context of the input language's grammatical norms and stylistic conventions.\n\n# OUTPUT INSTRUCTIONS\n\n- Refined and improved text that has no grammar mistakes.\n- Return in the same language as the input.\n- Include NO additional commentary or explanation in the response.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        1.0661842823028564,
        -0.022302985191345215,
        -0.1869683861732483,
        0.2703058123588562,
        0.12846258282661438,
        0.25917160511016846,
        -0.30082055926322937,
        -0.1651347577571869,
        -0.6322823762893677,
        0.5557923316955566,
        0.40286985039711,
        0.2689190208911896,
        -0.0041230423375964165,
        0.2236870378255844,
        0.6676715016365051,
        0.19091320037841797,
        0.01842423714697361,
        -0.44391068816185,
        -1.3994982242584229,
        -0.7131392359733582,
        -0.17164179682731628,
        0.4565882384777069,
        0.308639794588089,
        0.2517728805541992,
        -0.08021504431962967,
        -0.007984280586242676,
        -0.10898482799530029,
        -0.47193795442581177,
        -0.7219316959381104,
        -2.1183712482452393,
        -0.07080899924039841,
        1.0191417932510376,
        -0.4318002760410309,
        -0.958407998085022,
        0.6696898937225342,
        0.05327837914228439,
        -0.44726091623306274,
        -0.20380932092666626,
        -0.17495909333229065,
        0.0007609426975250244,
        0.26830461621284485,
        0.10311494022607803,
        -0.0008691251277923584,
        -0.22323903441429138,
        -0.2255672812461853,
        -0.09574507921934128,
        0.37921833992004395,
        -0.5590130686759949,
        0.25941288471221924,
        0.7654731869697571,
        0.1301204413175583,
        -0.5123046636581421,
        -0.5598939657211304,
        -0.11452072858810425,
        -0.6139974594116211,
        -0.06419453024864197,
        -0.24751153588294983,
        -0.12413246929645538,
        0.1961755007505417,
        0.13473385572433472,
        0.19985070824623108,
        0.5032922029495239,
        -3.8884124755859375,
        0.24202443659305573,
        0.6920561194419861,
        -0.45995473861694336,
        0.32915231585502625,
        0.2578389346599579,
        0.09327056258916855,
        -0.6073592305183411,
        0.049937713891267776,
        0.11880552768707275,
        0.6655762195587158,
        0.6499871611595154,
        0.9610828757286072,
        0.24644067883491516,
        0.37581512331962585,
        -0.10037848353385925,
        0.06087972968816757,
        -0.23000022768974304,
        0.5756397843360901,
        0.7666538953781128,
        0.22333590686321259,
        -0.4077377915382385,
        -0.6986420750617981,
        0.23975783586502075,
        -0.2421201467514038,
        -0.02253103256225586,
        0.2811738848686218,
        0.08878185600042343,
        -0.07375854253768921,
        0.10786651074886322,
        -0.1461225003004074,
        -0.38479360938072205,
        -0.879188597202301,
        -0.02452385425567627,
        0.1910027712583542,
        0.32073846459388733,
        0.050224028527736664,
        3.4333977699279785,
        0.6946722269058228,
        0.13190636038780212,
        0.11251324415206909,
        -0.47022777795791626,
        0.061485134065151215,
        -0.3588405251502991,
        0.23045401275157928,
        -0.17747598886489868,
        -0.0626610666513443,
        0.561875581741333,
        0.3406381905078888,
        -0.6199744343757629,
        -0.43057242035865784,
        0.164475679397583,
        0.43758082389831543,
        1.1667566299438477,
        -0.43136847019195557,
        0.15847007930278778,
        0.8269202709197998,
        0.7811307311058044,
        -0.21068377792835236,
        -0.3415740430355072,
        -0.19196964800357819,
        0.4201933443546295,
        -0.1541995108127594,
        0.07127456367015839,
        -0.306725412607193,
        0.5558170676231384,
        0.23011942207813263,
        0.28752946853637695,
        -0.45755040645599365,
        -0.10490413010120392,
        -0.8651835918426514,
        0.10691636800765991,
        -0.8585217595100403,
        0.4037501811981201,
        0.2982009947299957,
        0.07188154011964798,
        0.5244346261024475,
        -0.7479500770568848,
        -0.6920341849327087,
        -0.988258421421051,
        1.014580249786377,
        0.48277032375335693,
        0.8169604539871216,
        0.24058999121189117,
        -0.8194782137870789,
        0.1677592694759369,
        -1.013199806213379,
        0.25216346979141235,
        0.09595136344432831,
        0.2654918432235718,
        0.3116839528083801,
        0.4304529130458832,
        0.9894683361053467,
        0.12598338723182678,
        -0.2420356571674347,
        -0.2799242436885834,
        -0.20307734608650208,
        -0.11909248679876328,
        0.1530217081308365,
        -0.479846328496933,
        0.11765041947364807,
        0.6706289052963257,
        0.17541688680648804,
        -0.0721595287322998,
        0.14981114864349365,
        0.15478283166885376,
        0.2114856094121933,
        0.19100864231586456,
        -0.24319136142730713,
        0.17649775743484497,
        0.10046789050102234,
        0.66651451587677,
        0.41916167736053467,
        -0.1985047310590744,
        -0.13733667135238647,
        0.261822372674942,
        -0.14513377845287323,
        -0.1731823831796646,
        1.0816196203231812,
        0.5084193348884583,
        -0.013437597081065178,
        -0.06695947051048279,
        -0.15270106494426727,
        -0.5701430439949036,
        0.6722674369812012,
        -0.33017024397850037,
        0.3473389744758606,
        0.47063422203063965,
        -0.6311965584754944,
        1.5767838954925537,
        -0.37337344884872437,
        -0.21014036238193512,
        0.3827391564846039,
        0.0060644932091236115,
        -0.3756822347640991,
        0.3997977375984192,
        0.57830411195755,
        -0.4895817041397095,
        -0.8721516132354736,
        -0.7679486870765686,
        -0.4047223627567291,
        -0.127203568816185,
        -0.4956023097038269,
        -0.6952948570251465,
        -0.34081071615219116,
        -0.20187503099441528,
        -0.10044801235198975,
        -0.759843647480011,
        -0.074247807264328,
        0.40818798542022705,
        1.138002872467041,
        0.2858874499797821,
        0.6655323505401611,
        -0.13704615831375122,
        0.492266982793808,
        0.4396737217903137,
        0.6761269569396973,
        0.4174808859825134,
        -0.6267129182815552,
        0.020586825907230377,
        -0.9680026769638062,
        -0.7170255184173584,
        -0.7042078375816345,
        0.40419667959213257,
        -1.090200662612915,
        -0.1492408663034439,
        -0.7303103804588318,
        -0.42943575978279114,
        0.21872678399085999,
        1.4277936220169067,
        1.1947933435440063,
        0.986381471157074,
        -0.09285230189561844,
        0.15817153453826904,
        -0.2958933711051941,
        0.9216183423995972,
        0.22306475043296814,
        -0.5494338274002075,
        0.585751473903656,
        0.31942957639694214,
        -0.3270862400531769,
        0.3280153274536133,
        0.1752728372812271,
        0.1858994960784912,
        -1.4764997959136963,
        -0.5022989511489868,
        -0.22867675125598907,
        0.7594020962715149,
        0.45776939392089844,
        0.2467307448387146,
        -0.09816332161426544,
        -0.26585468649864197,
        0.5512517690658569,
        -0.1092938706278801,
        -1.7125451564788818,
        -0.16059960424900055,
        -0.3990152180194855,
        0.7210650444030762,
        -0.2596851587295532,
        -0.17432811856269836,
        0.17142236232757568,
        0.817232072353363,
        -0.06393111497163773,
        0.10183495283126831,
        -0.6100032329559326,
        -0.3638445734977722,
        -0.4818343222141266,
        -0.1778068244457245,
        0.4140067994594574,
        0.10899274051189423,
        -0.17790581285953522,
        -0.018268078565597534,
        -0.3664610981941223,
        0.0790310651063919,
        0.1586703062057495,
        0.03889378905296326,
        -0.38648098707199097,
        0.0004153400659561157,
        0.36668071150779724,
        0.38632580637931824,
        -0.7532123923301697,
        0.6835595369338989,
        -0.7819940447807312,
        0.005967229604721069,
        0.10587900876998901,
        -1.0287086963653564,
        0.5267103910446167,
        1.2093538045883179,
        -0.15419511497020721,
        0.18382221460342407,
        -0.5152196884155273,
        0.3752875328063965,
        1.3099846839904785,
        1.2569959163665771,
        0.11297640949487686,
        0.5187417268753052,
        0.3938106894493103,
        -0.8518306612968445,
        -0.7538706064224243,
        -0.04731565713882446,
        -0.08741185814142227,
        -0.02715732902288437,
        -0.0074952468276023865,
        -0.5168951153755188,
        0.9203624725341797,
        -0.16865621507167816,
        -0.2719537913799286,
        0.021636050194501877,
        -0.7238578200340271,
        0.16564401984214783,
        0.2050582468509674,
        -0.35834968090057373,
        1.1440908908843994,
        -0.3193099796772003,
        0.5701630711555481,
        0.9939069747924805,
        0.11196452379226685,
        -1.4777095317840576,
        -0.12894077599048615,
        0.27658823132514954,
        0.3685288429260254,
        0.19998568296432495,
        -0.08012879639863968,
        0.9094190001487732,
        0.25711050629615784,
        -0.3741905689239502,
        -0.3003414273262024,
        1.3814691305160522,
        0.3073202073574066,
        -0.5122252106666565,
        -0.7852060198783875,
        0.19594699144363403,
        0.3358605206012726,
        -0.13429799675941467,
        -0.4412778615951538,
        0.15384604036808014,
        -0.8569814562797546,
        -0.15242137014865875,
        0.41595274209976196,
        0.9748792052268982,
        -0.09896358102560043,
        0.6503353118896484,
        0.09634354710578918,
        -0.41314348578453064,
        -0.6310557723045349,
        -0.5535599589347839,
        0.5839074850082397,
        -0.4831840395927429,
        -0.36651965975761414,
        0.47248518466949463,
        -0.3704760670661926,
        -0.5560501217842102,
        0.39462754130363464,
        0.24805349111557007,
        -0.9237225651741028,
        -0.07049155235290527,
        -0.07737249881029129,
        2.1180338859558105,
        -0.539288341999054,
        -0.2929477393627167,
        -0.48946383595466614,
        0.10069732367992401,
        0.0358356274664402,
        -0.08061236143112183,
        0.18504197895526886,
        -0.28080475330352783,
        0.12215034663677216,
        0.11368948966264725,
        -0.009357810020446777,
        -0.2354038655757904,
        0.47184163331985474,
        0.19523219764232635,
        0.055519573390483856,
        -0.4317525625228882,
        0.22281359136104584,
        0.9869636297225952,
        -0.1690942943096161,
        -0.5719797611236572,
        -0.2560119330883026,
        -0.3253156244754791,
        -1.0799967050552368,
        -0.8449304699897766
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates and categorizes content based on its relevance to specific human-centric themes, then assigns a tiered rating and a numerical quality score. It uses a predefined set of labels for categorization and assesses content based on idea quantity and thematic alignment. The expected output is a structured JSON object detailing the content summary, labels, rating, and quality score with explanations.",
          "name": "Label_and_rate",
          "raw": "\n                workflow Label_and_rate v0.1 {\n                    step Main {\n                        $SYSTEM = \"\nIDENTITY and GOAL:\n\nYou are an ultra-wise and brilliant classifier and judge of content. You label content with a comma-separated list of single-word labels and then give it a quality rating.\n\nTake a deep breath and think step by step about how to perform the following to get the best outcome.\n\nSTEPS:\n\n1. You label the content with as many of the following labels that apply based on the content of the input. These labels go into a section called LABELS:. Do not create any new labels. Only use these.\n\nLABEL OPTIONS TO SELECT FROM (Select All That Apply):\n\nMeaning\nFuture\nBusiness\nTutorial\nPodcast\nMiscellaneous\nCreativity\nNatSec\nCyberSecurity\nAI\nEssay\nVideo\nConversation\nOptimization\nPersonal\nWriting\nHuman3.0\nHealth\nTechnology\nEducation\nLeadership\nMindfulness\nInnovation\nCulture\nProductivity\nScience\nPhilosophy\n\nEND OF LABEL OPTIONS\n\n2. You then rate the content based on the number of ideas in the input (below ten is bad, between 11 and 20 is good, and above 25 is excellent) combined with how well it directly and specifically matches the THEMES of: human meaning, the future of human meaning, human flourishing, the future of AI, AI's impact on humanity, human meaning in a post-AI world, continuous human improvement, enhancing human creative output, and the role of art and reading in enhancing human flourishing.\n\n3. Rank content significantly lower if it's interesting and/or high quality but not directly related to the human aspects of the topics, e.g., math or science that doesn't discuss human creativity or meaning. Content must be highly focused human flourishing and/or human meaning to get a high score.\n\n4. Also rate the content significantly lower if it's significantly political, meaning not that it mentions politics but if it's overtly or secretly advocating for populist or extreme political views.\n\nYou use the following rating levels:\n\nS Tier (Must Consume Original Content Within a Week): 18+ ideas and/or STRONG theme matching with the themes in STEP #2.\nA Tier (Should Consume Original Content This Month): 15+ ideas and/or GOOD theme matching with the THEMES in STEP #2.\nB Tier (Consume Original When Time Allows): 12+ ideas and/or DECENT theme matching with the THEMES in STEP #2.\nC Tier (Maybe Skip It): 10+ ideas and/or SOME theme matching with the THEMES in STEP #2.\nD Tier (Definitely Skip It): Few quality ideas and/or little theme matching with the THEMES in STEP #2.\n\n5. Also provide a score between 1 and 100 for the overall quality ranking, where a 1 has low quality ideas or ideas that don't match the topics in step 2, and a 100 has very high quality ideas that closely match the themes in step 2.\n\n6. Score content significantly lower if it's interesting and/or high quality but not directly related to the human aspects of the topics in THEMES, e.g., math or science that doesn't discuss human creativity or meaning. Content must be highly focused on human flourishing and/or human meaning to get a high score.\n\n7. Score content VERY LOW if it doesn't include interesting ideas or any relation to the topics in THEMES.\n\nOUTPUT:\n\nThe output should look like the following:\n\nONE SENTENCE SUMMARY:\n\nA one-sentence summary of the content and why it's compelling, in less than 30 words.\n\nLABELS:\n\nCyberSecurity, Writing, Health, Personal\n\nRATING:\n\nS Tier: (Must Consume Original Content Immediately)\n\nExplanation: $$Explanation in 5 short bullets for why you gave that rating.$$\n\nQUALITY SCORE:\n\n$$The 1-100 quality score$$\n\nExplanation: $$Explanation in 5 short bullets for why you gave that score.$$\n\nOUTPUT FORMAT:\n\nYour output is ONLY in JSON. The structure looks like this:\n\n{\n\\\"one-sentence-summary\\\": \\\"The one-sentence summary.\\\",\n\\\"labels\\\": \\\"The labels that apply from the set of options above.\\\",\n\\\"rating:\\\": \\\"S Tier: (Must Consume Original Content This Week) (or whatever the rating is)\\\",\n\\\"rating-explanation:\\\": \\\"The explanation given for the rating.\\\",\n\\\"quality-score\\\": \\\"The numeric quality score\\\",\n\\\"quality-score-explanation\\\": \\\"The explanation for the quality score.\\\",\n}\n\nOUTPUT INSTRUCTIONS\n\n- ONLY generate and use labels from the list above.\n\n- ONLY OUTPUT THE JSON OBJECT ABOVE.\n\n- Do not output the json``` container. Just the JSON object itself.\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\nIDENTITY and GOAL:\n\nYou are an ultra-wise and brilliant classifier and judge of content. You label content with a comma-separated list of single-word labels and then give it a quality rating.\n\nTake a deep breath and think step by step about how to perform the following to get the best outcome.\n\nSTEPS:\n\n1. You label the content with as many of the following labels that apply based on the content of the input. These labels go into a section called LABELS:. Do not create any new labels. Only use these.\n\nLABEL OPTIONS TO SELECT FROM (Select All That Apply):\n\nMeaning\nFuture\nBusiness\nTutorial\nPodcast\nMiscellaneous\nCreativity\nNatSec\nCyberSecurity\nAI\nEssay\nVideo\nConversation\nOptimization\nPersonal\nWriting\nHuman3.0\nHealth\nTechnology\nEducation\nLeadership\nMindfulness\nInnovation\nCulture\nProductivity\nScience\nPhilosophy\n\nEND OF LABEL OPTIONS\n\n2. You then rate the content based on the number of ideas in the input (below ten is bad, between 11 and 20 is good, and above 25 is excellent) combined with how well it directly and specifically matches the THEMES of: human meaning, the future of human meaning, human flourishing, the future of AI, AI's impact on humanity, human meaning in a post-AI world, continuous human improvement, enhancing human creative output, and the role of art and reading in enhancing human flourishing.\n\n3. Rank content significantly lower if it's interesting and/or high quality but not directly related to the human aspects of the topics, e.g., math or science that doesn't discuss human creativity or meaning. Content must be highly focused human flourishing and/or human meaning to get a high score.\n\n4. Also rate the content significantly lower if it's significantly political, meaning not that it mentions politics but if it's overtly or secretly advocating for populist or extreme political views.\n\nYou use the following rating levels:\n\nS Tier (Must Consume Original Content Within a Week): 18+ ideas and/or STRONG theme matching with the themes in STEP #2.\nA Tier (Should Consume Original Content This Month): 15+ ideas and/or GOOD theme matching with the THEMES in STEP #2.\nB Tier (Consume Original When Time Allows): 12+ ideas and/or DECENT theme matching with the THEMES in STEP #2.\nC Tier (Maybe Skip It): 10+ ideas and/or SOME theme matching with the THEMES in STEP #2.\nD Tier (Definitely Skip It): Few quality ideas and/or little theme matching with the THEMES in STEP #2.\n\n5. Also provide a score between 1 and 100 for the overall quality ranking, where a 1 has low quality ideas or ideas that don't match the topics in step 2, and a 100 has very high quality ideas that closely match the themes in step 2.\n\n6. Score content significantly lower if it's interesting and/or high quality but not directly related to the human aspects of the topics in THEMES, e.g., math or science that doesn't discuss human creativity or meaning. Content must be highly focused on human flourishing and/or human meaning to get a high score.\n\n7. Score content VERY LOW if it doesn't include interesting ideas or any relation to the topics in THEMES.\n\nOUTPUT:\n\nThe output should look like the following:\n\nONE SENTENCE SUMMARY:\n\nA one-sentence summary of the content and why it's compelling, in less than 30 words.\n\nLABELS:\n\nCyberSecurity, Writing, Health, Personal\n\nRATING:\n\nS Tier: (Must Consume Original Content Immediately)\n\nExplanation: $$Explanation in 5 short bullets for why you gave that rating.$$\n\nQUALITY SCORE:\n\n$$The 1-100 quality score$$\n\nExplanation: $$Explanation in 5 short bullets for why you gave that score.$$\n\nOUTPUT FORMAT:\n\nYour output is ONLY in JSON. The structure looks like this:\n\n{\n\\\"one-sentence-summary\\\": \\\"The one-sentence summary.\\\",\n\\\"labels\\\": \\\"The labels that apply from the set of options above.\\\",\n\\\"rating:\\\": \\\"S Tier: (Must Consume Original Content This Week) (or whatever the rating is)\\\",\n\\\"rating-explanation:\\\": \\\"The explanation given for the rating.\\\",\n\\\"quality-score\\\": \\\"The numeric quality score\\\",\n\\\"quality-score-explanation\\\": \\\"The explanation for the quality score.\\\",\n}\n\nOUTPUT INSTRUCTIONS\n\n- ONLY generate and use labels from the list above.\n\n- ONLY OUTPUT THE JSON OBJECT ABOVE.\n\n- Do not output the json``` container. Just the JSON object itself.\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.8342505097389221,
        0.5327337980270386,
        -0.09551303088665009,
        0.13010358810424805,
        0.39754804968833923,
        0.10354873538017273,
        -0.445295512676239,
        -0.05208030343055725,
        0.0021299240179359913,
        0.5394287109375,
        -0.5838315486907959,
        0.042380787432193756,
        -0.13575872778892517,
        -0.048341572284698486,
        0.5817614793777466,
        -0.022284269332885742,
        0.48370322585105896,
        -0.5518525242805481,
        -1.2485978603363037,
        0.1287420392036438,
        0.17712491750717163,
        0.3030809760093689,
        0.4301515519618988,
        0.2335912138223648,
        0.31547266244888306,
        0.09171795845031738,
        -0.10711851716041565,
        -0.023531101644039154,
        -0.36907732486724854,
        -1.1646455526351929,
        0.7633994221687317,
        0.2782565653324127,
        -0.3854771852493286,
        -0.4502139091491699,
        0.9141374230384827,
        -0.8069961667060852,
        -0.23065006732940674,
        0.3835962116718292,
        -0.4556906521320343,
        -0.4667630195617676,
        0.4530065059661865,
        0.19792580604553223,
        -0.3511589765548706,
        0.08250229060649872,
        -0.027216119691729546,
        -0.08245968818664551,
        -0.06344477832317352,
        -0.2030668556690216,
        0.8308883905410767,
        0.40072745084762573,
        -0.8050836324691772,
        -0.6593274474143982,
        -0.7252817749977112,
        -0.5173507928848267,
        -0.41651687026023865,
        0.44213902950286865,
        -0.1875644028186798,
        -0.4054613411426544,
        0.22353361546993256,
        -0.20632633566856384,
        -0.09179579466581345,
        0.7201504707336426,
        -2.738938570022583,
        -0.0006795451045036316,
        0.28984367847442627,
        -0.07272764295339584,
        -0.42293649911880493,
        -0.5539968609809875,
        -0.006728557404130697,
        -0.04763249307870865,
        -0.40567007660865784,
        0.35835331678390503,
        -0.376615047454834,
        0.2999007999897003,
        -0.11543221026659012,
        0.0014581382274627686,
        0.45701199769973755,
        0.2242584228515625,
        1.1135841608047485,
        -0.203305184841156,
        -0.2684175372123718,
        0.684929370880127,
        0.17112775146961212,
        -0.6340142488479614,
        -0.7114707827568054,
        0.6766721606254578,
        -0.09280090779066086,
        0.32859137654304504,
        -0.29299163818359375,
        0.42362844944000244,
        0.20981937646865845,
        -0.24283021688461304,
        0.20118147134780884,
        0.14479491114616394,
        -0.18970131874084473,
        0.36687368154525757,
        -0.31047359108924866,
        -0.1896975338459015,
        -0.4056398272514343,
        3.7700657844543457,
        0.9180903434753418,
        -0.02534107118844986,
        0.3052354156970978,
        -0.2757038474082947,
        0.7869611382484436,
        -0.17764925956726074,
        0.09694351255893707,
        -0.4087067246437073,
        0.2192871868610382,
        -0.05830784887075424,
        0.24975422024726868,
        -0.651753306388855,
        -0.5344351530075073,
        0.18661785125732422,
        0.7302570343017578,
        0.05593597888946533,
        -0.9613063931465149,
        -0.2621056139469147,
        0.2296219915151596,
        0.8187569379806519,
        -0.11096981912851334,
        -0.40317416191101074,
        -0.04030591621994972,
        -0.2649904787540436,
        0.23731781542301178,
        -0.11386111378669739,
        -0.13649383187294006,
        0.8503421545028687,
        0.2321961373090744,
        -0.1028342917561531,
        0.01563272625207901,
        0.6249322891235352,
        -0.9485181570053101,
        -0.21304115653038025,
        0.0013072043657302856,
        -0.04202280193567276,
        0.6575107574462891,
        -0.031330447643995285,
        0.3141216039657593,
        -0.6460807919502258,
        -0.029514074325561523,
        -0.6612104773521423,
        0.800053596496582,
        0.04565093293786049,
        0.47087448835372925,
        0.5883626937866211,
        -0.4234253168106079,
        0.12998519837856293,
        -0.20219120383262634,
        -0.5423771142959595,
        0.2645447552204132,
        0.030586984008550644,
        0.10851463675498962,
        0.49398374557495117,
        0.48980873823165894,
        -0.5210414528846741,
        -0.7187285423278809,
        0.028906971216201782,
        -0.9541836380958557,
        0.43255409598350525,
        0.34249940514564514,
        0.05475328117609024,
        0.3345096707344055,
        0.7375609278678894,
        0.5044134855270386,
        -0.028883639723062515,
        -0.06726236641407013,
        -0.17672717571258545,
        0.12531986832618713,
        -0.46376532316207886,
        -0.22004079818725586,
        0.218329519033432,
        0.6175305247306824,
        0.6250712871551514,
        -0.5750693678855896,
        -0.20342037081718445,
        0.27924707531929016,
        -0.09335801750421524,
        0.2169121652841568,
        -0.9071340560913086,
        0.5376209616661072,
        0.6829255223274231,
        -0.4610837697982788,
        -0.5390927195549011,
        0.05617454648017883,
        -0.08559851348400116,
        0.3890518844127655,
        -0.10302402079105377,
        0.7914199829101562,
        0.7504929304122925,
        -1.715335726737976,
        1.1199296712875366,
        -0.6254226565361023,
        -0.20284654200077057,
        0.03652277588844299,
        -0.0621086023747921,
        -0.15235625207424164,
        0.24227949976921082,
        0.2187742292881012,
        -0.10333907604217529,
        -0.48971831798553467,
        0.12313267588615417,
        -0.39806312322616577,
        0.140588641166687,
        -0.447486937046051,
        -0.28962260484695435,
        -0.3097389042377472,
        0.5835576057434082,
        -0.12596319615840912,
        -0.5573945045471191,
        -0.1722581386566162,
        0.09822060167789459,
        1.3294243812561035,
        0.03327459841966629,
        0.9892464876174927,
        0.31903237104415894,
        -0.3842024505138397,
        0.42987436056137085,
        0.37747707962989807,
        1.003530740737915,
        -0.25579479336738586,
        0.10314401984214783,
        -1.0478233098983765,
        -0.8632873296737671,
        -1.356655240058899,
        0.6803609728813171,
        0.0065388306975364685,
        -0.4467408061027527,
        -0.47760361433029175,
        0.16760936379432678,
        1.2819883823394775,
        0.8031681776046753,
        0.8642631769180298,
        0.10601824522018433,
        0.1401832401752472,
        -0.17962953448295593,
        -0.1921927034854889,
        0.815558910369873,
        0.23556852340698242,
        -0.4924215078353882,
        -0.24828067421913147,
        -0.4461670219898224,
        -0.3851584792137146,
        0.8201978206634521,
        0.8807011246681213,
        0.6385858058929443,
        -0.8326955437660217,
        0.24491801857948303,
        0.21147097647190094,
        0.9224575161933899,
        0.840011715888977,
        0.20486998558044434,
        0.7464919686317444,
        0.3273528516292572,
        -0.861480176448822,
        0.48991659283638,
        -1.6389083862304688,
        0.11600538343191147,
        -0.45702311396598816,
        0.14035266637802124,
        -0.2892368733882904,
        -0.5424050092697144,
        -0.0824865847826004,
        -0.22486811876296997,
        -0.14096114039421082,
        0.11387279629707336,
        -0.6828280091285706,
        -0.7249429225921631,
        -0.30974075198173523,
        -0.14464615285396576,
        0.42008280754089355,
        0.10999763756990433,
        -0.4513096809387207,
        0.33067774772644043,
        0.12154950201511383,
        0.15411119163036346,
        0.2227993607521057,
        0.4460484981536865,
        -0.6389296650886536,
        -0.21849386394023895,
        -0.11942731589078903,
        0.2998853027820587,
        -0.30769622325897217,
        0.11951297521591187,
        -0.5269326567649841,
        0.15327544510364532,
        0.07729257643222809,
        -1.3302470445632935,
        -0.12310560792684555,
        0.7252270579338074,
        -1.2095654010772705,
        -0.3953903019428253,
        -0.07487529516220093,
        -0.8017313480377197,
        1.7741901874542236,
        0.5023623704910278,
        0.6404074430465698,
        1.0002903938293457,
        0.60696941614151,
        -0.57114177942276,
        -0.3658236265182495,
        -0.3415549695491791,
        0.009362536482512951,
        0.04534468799829483,
        -0.7494616508483887,
        0.10827908664941788,
        0.2709071636199951,
        0.34287217259407043,
        -0.14306628704071045,
        0.17411723732948303,
        -0.6929110884666443,
        -0.029026202857494354,
        -0.2213572859764099,
        -0.09554912149906158,
        0.4669288694858551,
        0.03479659557342529,
        0.6252731084823608,
        1.0961835384368896,
        0.010105829685926437,
        -2.390812635421753,
        -0.30228838324546814,
        0.2576182186603546,
        0.18921008706092834,
        -0.25588318705558777,
        -0.26581433415412903,
        0.6284188628196716,
        -0.3680843412876129,
        0.12139508128166199,
        -0.3805282711982727,
        1.1298420429229736,
        0.28880059719085693,
        0.2634736895561218,
        -0.25729086995124817,
        -0.40336164832115173,
        0.04353536665439606,
        -0.7901586294174194,
        0.27249395847320557,
        -0.08088793605566025,
        -0.09944135695695877,
        -0.6164917349815369,
        0.5973718166351318,
        1.4526476860046387,
        0.028672803193330765,
        0.3661038279533386,
        -0.07348001003265381,
        0.2596063017845154,
        -0.8449167013168335,
        -0.9445076584815979,
        -0.32296448945999146,
        0.020985066890716553,
        0.28311923146247864,
        0.8639335632324219,
        0.21334946155548096,
        -0.32465860247612,
        0.8345550298690796,
        0.42485281825065613,
        -0.5429502725601196,
        0.4710002839565277,
        -0.2648856043815613,
        1.3948049545288086,
        -0.2959614396095276,
        -0.5978015065193176,
        -0.22706907987594604,
        0.6851824522018433,
        0.13458743691444397,
        -0.19932971894741058,
        0.14828330278396606,
        -1.23989999294281,
        0.07570964097976685,
        -0.03320569545030594,
        0.5639339089393616,
        0.003505796194076538,
        0.2632012963294983,
        0.6729188561439514,
        0.4172791838645935,
        0.10332578420639038,
        0.10333901643753052,
        -0.3226168751716614,
        0.33558183908462524,
        -0.22750446200370789,
        0.12693634629249573,
        -0.05182158574461937,
        -0.8124576807022095,
        -0.8728591203689575
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt outlines a complex process for diagnosing and addressing psychological issues based on a person's background and behaviors. It involves deep analysis of the individual's history, identifying potential mental health issues, and suggesting corrective actions. The expected output includes summaries of past events, possible psychological issues, their impact on behavior, and recommendations for improvement.",
          "name": "Official_pattern_template",
          "raw": "\n                workflow Official_pattern_template v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are _____________ that specializes in ________________.\n\nEXAMPLE: \n\nYou are an advanced AI expert in human psychology and mental health with a 1,419 IQ that specializes in taking in background information about a person, combined with their behaviors, and diagnosing what incidents from their background are likely causing them to behave in this way.\n\n# GOALS\n\nThe goals of this exercise are to: \n\n1. _________________.\n\n2. \n\nEXAMPLE:\n\nThe goals of this exercise are to:\n\n1. Take in any set of background facts about how a person grew up, their past major events in their lives, past traumas, past victories, etc., combined with how they're currently behaving—for example having relationship problems, pushing people away, having trouble at work, etc.—and give a list of issues they might have due to their background, combined with how those issues could be causing their behavior. \n\n2. Get a list of recommended actions to take to address the issues, including things like specific kinds of therapy, specific actions to to take regarding relationships, work, etc.\n\n# STEPS\n\n- Do this first  \n\n- Then do this\n\nEXAMPLE:\n\n// Deep, repeated consumption of the input\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n// Create the virtual whiteboard in your mind\n\n- Create a 100 meter by 100 meter whiteboard in your mind, and write down all the different entities from what you read. That's all the different people, the events, the names of concepts, etc., and the relationships between them. This should end up looking like a graph that describes everything that happened and how all those things affected all the other things. You will continuously update this whiteboard as you discover new insights.\n\n// Think about what happened and update the whiteboard\n\n- Think deeply for 312 hours about the past events described and fill in the extra context as needed. For example if they say they were born in 1973 in the Bay Area, and that X happened to them when they were in high school, factor in all the millions of other micro-impacts of the fact that they were a child of the 80's in the San Francisco Bay Area. Update the whiteboard graph diagram with your findings.\n\n// Think about what issues they may have gotten from those events and update the whiteboard\n\n- Think deeply for 312 hours about what psychological issues this person could be suffering from as a result of the events they described. Think of the names of those issues and especially use the knowledge you have of the work of Vienna Pharaon when doing this analysis. Update the whiteboard graph diagram with your findings.\n\n// Think about what behaviors they say they're exhibiting and update the whiteboard\n\n- Think deeply for 312 hours about the behaviors they say they're doing and/or repeating. Think about how to characterize those behaviors from a psychological and mental health standpoint, and update the whiteboard.\n\n// Step back and analyze the possible cause-effect relationships of the entire situation\n\n- Now step back and look at the entire whiteboard, and the entire situation in your mind again. Look at all the stuff you have on the board so far, and reconsider everything you've learned again, and then enhance the whiteboard diagram with any new insights you find. Update the whiteboard.\n\n- Perform these steps 913 times, optimizing on each iteration.\n\n# OUTPUT\n\n// Capture the main events from the past\n\n- In an output section called EVENTS, summarize all the events from the input in a set of 15-word bullets, e.g., Grew up mostly without a mother until he was around 5 years old.\n\n// Describe the possible issues they may have as a result\n\n- In an output section called POSSIBLE ISSUES, give a list of the named psychological or mental health issues that are common for people to get from the types of events above. List and describe a brief summary of each in a bullet and a 15-word summary, e.g.,: Co-Dependence: (A 15-word description of co-dependence.)\n\n// Describe the connections between their past and their past and current behavior\n\n- In an output section called PAST-BEHAVIOR CONNECTIONS, create a bulleted list of 15-word descriptions of how the person's past is likely to be affecting their actions and their situation. E.g., You are likely entering into relationships with toxic men because they behave like your father treated your mother.\n\n// Recommend corrective actions\n\n- In a section called RECOMMENDATIONS, give a bulleted list of 15-word recommendations on how they can fix their situation. E.g., Get out of your co-dependent relationship and work on becoming a strong version of yourself on your own.\n\n# POSITIVE EXAMPLES\n\n// Examples to follow\n\n- One good example\n\n- Another good example\n\n# NEGATIVE EXAMPLES\n\n// Examples to avoid\n\n- One bad example\n\n- Another bad example\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are _____________ that specializes in ________________.\n\nEXAMPLE: \n\nYou are an advanced AI expert in human psychology and mental health with a 1,419 IQ that specializes in taking in background information about a person, combined with their behaviors, and diagnosing what incidents from their background are likely causing them to behave in this way.\n\n# GOALS\n\nThe goals of this exercise are to: \n\n1. _________________.\n\n2. \n\nEXAMPLE:\n\nThe goals of this exercise are to:\n\n1. Take in any set of background facts about how a person grew up, their past major events in their lives, past traumas, past victories, etc., combined with how they're currently behaving—for example having relationship problems, pushing people away, having trouble at work, etc.—and give a list of issues they might have due to their background, combined with how those issues could be causing their behavior. \n\n2. Get a list of recommended actions to take to address the issues, including things like specific kinds of therapy, specific actions to to take regarding relationships, work, etc.\n\n# STEPS\n\n- Do this first  \n\n- Then do this\n\nEXAMPLE:\n\n// Deep, repeated consumption of the input\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n// Create the virtual whiteboard in your mind\n\n- Create a 100 meter by 100 meter whiteboard in your mind, and write down all the different entities from what you read. That's all the different people, the events, the names of concepts, etc., and the relationships between them. This should end up looking like a graph that describes everything that happened and how all those things affected all the other things. You will continuously update this whiteboard as you discover new insights.\n\n// Think about what happened and update the whiteboard\n\n- Think deeply for 312 hours about the past events described and fill in the extra context as needed. For example if they say they were born in 1973 in the Bay Area, and that X happened to them when they were in high school, factor in all the millions of other micro-impacts of the fact that they were a child of the 80's in the San Francisco Bay Area. Update the whiteboard graph diagram with your findings.\n\n// Think about what issues they may have gotten from those events and update the whiteboard\n\n- Think deeply for 312 hours about what psychological issues this person could be suffering from as a result of the events they described. Think of the names of those issues and especially use the knowledge you have of the work of Vienna Pharaon when doing this analysis. Update the whiteboard graph diagram with your findings.\n\n// Think about what behaviors they say they're exhibiting and update the whiteboard\n\n- Think deeply for 312 hours about the behaviors they say they're doing and/or repeating. Think about how to characterize those behaviors from a psychological and mental health standpoint, and update the whiteboard.\n\n// Step back and analyze the possible cause-effect relationships of the entire situation\n\n- Now step back and look at the entire whiteboard, and the entire situation in your mind again. Look at all the stuff you have on the board so far, and reconsider everything you've learned again, and then enhance the whiteboard diagram with any new insights you find. Update the whiteboard.\n\n- Perform these steps 913 times, optimizing on each iteration.\n\n# OUTPUT\n\n// Capture the main events from the past\n\n- In an output section called EVENTS, summarize all the events from the input in a set of 15-word bullets, e.g., Grew up mostly without a mother until he was around 5 years old.\n\n// Describe the possible issues they may have as a result\n\n- In an output section called POSSIBLE ISSUES, give a list of the named psychological or mental health issues that are common for people to get from the types of events above. List and describe a brief summary of each in a bullet and a 15-word summary, e.g.,: Co-Dependence: (A 15-word description of co-dependence.)\n\n// Describe the connections between their past and their past and current behavior\n\n- In an output section called PAST-BEHAVIOR CONNECTIONS, create a bulleted list of 15-word descriptions of how the person's past is likely to be affecting their actions and their situation. E.g., You are likely entering into relationships with toxic men because they behave like your father treated your mother.\n\n// Recommend corrective actions\n\n- In a section called RECOMMENDATIONS, give a bulleted list of 15-word recommendations on how they can fix their situation. E.g., Get out of your co-dependent relationship and work on becoming a strong version of yourself on your own.\n\n# POSITIVE EXAMPLES\n\n// Examples to follow\n\n- One good example\n\n- Another good example\n\n# NEGATIVE EXAMPLES\n\n// Examples to avoid\n\n- One bad example\n\n- Another bad example\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bolt or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\n…\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5757980346679688,
        0.8212412595748901,
        0.18773755431175232,
        -0.05615604668855667,
        0.03399160876870155,
        -0.0310380682349205,
        -0.7402825355529785,
        0.11210036277770996,
        0.3010869026184082,
        0.07115478813648224,
        -0.35750555992126465,
        -0.2556394934654236,
        -0.2554686963558197,
        0.29594191908836365,
        0.6238143444061279,
        -0.006072305142879486,
        0.17567938566207886,
        -0.27913644909858704,
        -1.1681413650512695,
        -0.17083707451820374,
        -0.013951207511126995,
        0.7856596112251282,
        0.24582833051681519,
        0.3293786644935608,
        0.33176013827323914,
        0.3179307281970978,
        -0.05216057598590851,
        -0.3891632556915283,
        -0.0028401538729667664,
        -1.5542508363723755,
        0.9712920784950256,
        0.9262417554855347,
        -0.22325178980827332,
        -0.8344012498855591,
        0.02692994475364685,
        -0.5935547351837158,
        0.024329114705324173,
        0.45649680495262146,
        -0.2957589328289032,
        -0.1827932447195053,
        -0.1988200545310974,
        -0.182560995221138,
        -0.6688206791877747,
        0.32331356406211853,
        0.15569745004177094,
        -0.09039665758609772,
        0.1349795162677765,
        -0.24881957471370697,
        0.3993581235408783,
        0.3370680809020996,
        -0.28291386365890503,
        -0.3083314597606659,
        -0.1583663523197174,
        0.08544309437274933,
        -0.5217220187187195,
        0.22137686610221863,
        -0.0833764374256134,
        -0.4565163254737854,
        0.26007577776908875,
        0.1530776023864746,
        0.14843538403511047,
        0.4458448886871338,
        -3.469540596008301,
        0.30860912799835205,
        0.23205320537090302,
        0.4592912495136261,
        -0.03686082735657692,
        -0.3430376648902893,
        0.2720320224761963,
        -0.5697767734527588,
        0.4098801910877228,
        0.08317844569683075,
        -0.03235697001218796,
        -0.2484491467475891,
        0.4154573082923889,
        -0.6050428748130798,
        0.3829221725463867,
        0.41594433784484863,
        0.639175534248352,
        0.07020303606987,
        0.07591288536787033,
        0.36175596714019775,
        -0.45704710483551025,
        0.27718082070350647,
        -0.6777032017707825,
        0.2824403643608093,
        -0.3765013813972473,
        -0.48443126678466797,
        -0.39154329895973206,
        0.5675985813140869,
        0.017556987702846527,
        -0.7726789712905884,
        0.29900652170181274,
        0.21372219920158386,
        -0.195194274187088,
        0.3916637897491455,
        -0.6236876249313354,
        0.019792668521404266,
        -0.08148262649774551,
        3.6163861751556396,
        0.609955906867981,
        0.13943861424922943,
        1.1233004331588745,
        -0.09941844642162323,
        0.13311532139778137,
        -0.8485324382781982,
        -0.2919837236404419,
        -0.1928422898054123,
        -0.21921055018901825,
        0.09270035475492477,
        0.5504196286201477,
        -0.27472856640815735,
        -0.8766316175460815,
        0.010469555854797363,
        0.43176165223121643,
        0.3310498297214508,
        -0.3295436501502991,
        0.12838037312030792,
        0.3940989077091217,
        1.2910559177398682,
        -0.7143137454986572,
        0.44939735531806946,
        0.594864010810852,
        -0.05811188369989395,
        -0.18396686017513275,
        -0.19021663069725037,
        -0.8040406703948975,
        0.708514928817749,
        0.3926897943019867,
        0.4148741066455841,
        0.06645245105028152,
        0.352169394493103,
        -1.068171501159668,
        -0.10385394096374512,
        0.024783063679933548,
        -0.26451680064201355,
        0.29858773946762085,
        -0.47132816910743713,
        0.09878340363502502,
        -0.424762099981308,
        0.11906535923480988,
        -1.1542986631393433,
        0.7708205580711365,
        0.1407502293586731,
        1.1122602224349976,
        0.5705009698867798,
        -0.6166483759880066,
        0.14432814717292786,
        -0.24358594417572021,
        -0.2035403549671173,
        0.02794458158314228,
        0.7289415597915649,
        0.09197816252708435,
        0.2717461884021759,
        0.777214527130127,
        -0.2738910913467407,
        -1.2758907079696655,
        -0.08803702890872955,
        -1.0439448356628418,
        0.5089039206504822,
        0.01214473694562912,
        -0.6109750270843506,
        0.6033358573913574,
        0.4591885805130005,
        0.8420318365097046,
        -0.7485333681106567,
        -0.20607683062553406,
        -0.363037109375,
        0.48458266258239746,
        0.046893734484910965,
        -0.34651514887809753,
        0.44816088676452637,
        -0.04172025993466377,
        0.7038796544075012,
        0.3151855170726776,
        -0.27855807542800903,
        -0.33891546726226807,
        0.48676592111587524,
        -0.01357206329703331,
        -0.18042296171188354,
        0.8032147884368896,
        0.6021125912666321,
        -0.21661485731601715,
        -0.11010732501745224,
        -0.05519980192184448,
        0.0955631285905838,
        0.7992495894432068,
        -0.09589313715696335,
        2.1956827640533447,
        0.29211413860321045,
        -1.5342711210250854,
        1.0196547508239746,
        -0.4655027389526367,
        -0.3871752917766571,
        0.08164280652999878,
        0.1793409138917923,
        -0.0017473623156547546,
        -0.2173721343278885,
        0.24639004468917847,
        -0.19111132621765137,
        -0.8185291886329651,
        0.12720264494419098,
        0.09359249472618103,
        -0.5791395902633667,
        -0.5804484486579895,
        -0.6395199298858643,
        -0.5688985586166382,
        -0.06702051311731339,
        -0.4596521258354187,
        -0.6741900444030762,
        -0.16879792511463165,
        0.1158963292837143,
        0.7112559676170349,
        0.5690328478813171,
        0.331326425075531,
        1.1661200523376465,
        -0.565223217010498,
        0.3156687021255493,
        0.11573926359415054,
        0.8137733936309814,
        -0.553774893283844,
        -0.1346929371356964,
        -0.7707929015159607,
        -0.9817686676979065,
        -0.676826000213623,
        0.42052000761032104,
        -0.5613798499107361,
        -0.6989427208900452,
        -0.5206159949302673,
        -0.33288002014160156,
        0.06814797222614288,
        1.1503547430038452,
        0.22620873153209686,
        0.5304097533226013,
        0.31854546070098877,
        -0.3135075271129608,
        0.1374395787715912,
        0.4360720217227936,
        0.2588260769844055,
        0.0850304439663887,
        -0.01868385076522827,
        -0.6206490993499756,
        -0.14008775353431702,
        0.15968026220798492,
        0.522911787033081,
        0.41074493527412415,
        -0.08298066258430481,
        -0.12064915895462036,
        -0.159201979637146,
        0.9078962802886963,
        0.35041970014572144,
        0.42343756556510925,
        0.027644841000437737,
        0.19141770899295807,
        0.16255101561546326,
        0.52311110496521,
        -1.1451512575149536,
        0.35377538204193115,
        -0.9354909658432007,
        0.4525187611579895,
        0.08101264387369156,
        -0.6650961637496948,
        0.21317166090011597,
        -0.5103632807731628,
        -0.11429829150438309,
        0.047566208988428116,
        -0.6242944002151489,
        -0.3292757272720337,
        -0.16315273940563202,
        0.07911282032728195,
        -0.2766284942626953,
        0.4838588237762451,
        -0.40736645460128784,
        0.1738865077495575,
        0.5054304003715515,
        0.1880735456943512,
        0.032794296741485596,
        -0.04629972577095032,
        -0.5166929364204407,
        -0.654956579208374,
        -0.3538694977760315,
        0.09974908083677292,
        -0.4786039888858795,
        0.4487050175666809,
        -0.7787774205207825,
        0.12383902072906494,
        -0.00854526087641716,
        -0.7377989292144775,
        0.14758971333503723,
        0.34520211815834045,
        -0.42486143112182617,
        -0.8442184925079346,
        -0.6521387696266174,
        0.040041446685791016,
        1.3011553287506104,
        0.5432274341583252,
        -0.3221958875656128,
        0.48770472407341003,
        0.45909667015075684,
        -0.5006625652313232,
        -0.9289016723632812,
        -0.012136101722717285,
        0.18675097823143005,
        0.4335504472255707,
        -0.8553975820541382,
        -0.6242742538452148,
        0.9911787509918213,
        0.17200712859630585,
        0.20353421568870544,
        -0.08004327118396759,
        -0.4763391315937042,
        0.2378341555595398,
        -0.04516954347491264,
        -0.093046173453331,
        0.24681177735328674,
        -0.7323501110076904,
        0.12754151225090027,
        0.8023422956466675,
        -0.07289837300777435,
        -1.7873510122299194,
        0.15089774131774902,
        0.7491223216056824,
        0.44064265489578247,
        0.16496893763542175,
        -1.2590112686157227,
        0.7384671568870544,
        -0.26474300026893616,
        -0.08998416364192963,
        -0.40913939476013184,
        1.1193585395812988,
        0.09493633359670639,
        0.16566681861877441,
        -0.2016235888004303,
        0.2943013310432434,
        -0.09377510845661163,
        -0.6655957698822021,
        -0.04687607288360596,
        -0.30674219131469727,
        -0.45371195673942566,
        -0.20012332499027252,
        0.7044458389282227,
        1.449235439300537,
        0.20570038259029388,
        0.371214896440506,
        0.33472761511802673,
        0.0694340169429779,
        -0.476284384727478,
        -0.5300965905189514,
        -0.2387157380580902,
        -0.4508360028266907,
        0.23813961446285248,
        0.5084467530250549,
        -0.17567288875579834,
        0.020616447553038597,
        0.8468590974807739,
        0.8846056461334229,
        -0.6468866467475891,
        0.47542518377304077,
        0.14447006583213806,
        1.75491464138031,
        0.0134590994566679,
        -0.48190686106681824,
        -0.1916758418083191,
        0.6862758994102478,
        -0.09611345827579498,
        0.31359532475471497,
        0.2086336612701416,
        -0.6297807693481445,
        -0.2942817211151123,
        0.39748653769493103,
        0.8103742599487305,
        -0.3705638647079468,
        0.7048196792602539,
        0.5215266942977905,
        0.3776058554649353,
        0.4257981777191162,
        0.2810184955596924,
        0.6778080463409424,
        -0.18412062525749207,
        -0.04865173250436783,
        0.6551923155784607,
        -0.4439859092235565,
        -0.897002100944519,
        -1.1944711208343506
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Provides comprehensive psychological advice tailored to the individual's specific question and context. This approach delves into the person's past, traumas, and life goals to offer targeted feedback and recommendations. The expected output includes a concise analysis, detailed scientific rationale, actionable recommendations, Esther Perel's perspective, self-reflection prompts, possible clinical diagnoses, and a summary, all aimed at fostering self-awareness and positive change.",
          "name": "Provide_guidance",
          "raw": "\n                workflow Provide_guidance v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an all-knowing psychiatrist, psychologist, and life coach and you provide honest and concise advice to people based on the question asked combined with the context provided.\n\n# STEPS\n\n- Take the input given and think about the question being asked\n\n- Consider all the context of their past, their traumas, their goals, and ultimately what they're trying to do in life, and give them feedback in the following format:\n\n- In a section called ONE SENTENCE ANALYSIS AND RECOMMENDATION, give a single sentence that tells them how to approach their situation.\n\n- In a section called ANALYSIS, give up to 20 bullets of analysis of 15 words or less each on what you think might be going on relative to their question and their context. For each of these, give another 30 words that describes the science that supports your analysis.\n\n- In a section called RECOMMENDATIONS, give up to 5 bullets of recommendations of 15 words or less each on what you think they should do.\n\n- In a section called ESTHER'S ADVICE, give up to 3 bullets of advice that ESTHER PEREL would give them.\n\n- In a section called SELF-REFLECTION QUESTIONS, give up to 5 questions of no more than 15-words that could help them self-reflect on their situation.\n\n- In a section called POSSIBLE CLINICAL DIAGNOSIS, give up to 5 named psychological behaviors, conditions, or disorders that could be at play here. Examples: Co-dependency, Psychopathy, PTSD, Narcissism, etc.\n\n- In a section called SUMMARY, give a one sentence summary of your overall analysis and recommendations in a kind but honest tone.\n\n- After a \\\"—\\\" and a new line, add a NOTE: saying: \\\"This was produced by an imperfect AI. The best thing to do with this information is to think about it and take it to an actual professional. Don't take it too seriously on its own.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- Output only in Markdown.\n- Don't tell me to consult a professional. Just give me your best opinion.\n- Do not output bold or italicized text; just basic Markdown.\n- Be courageous and honest in your feedback rather than cautious.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an all-knowing psychiatrist, psychologist, and life coach and you provide honest and concise advice to people based on the question asked combined with the context provided.\n\n# STEPS\n\n- Take the input given and think about the question being asked\n\n- Consider all the context of their past, their traumas, their goals, and ultimately what they're trying to do in life, and give them feedback in the following format:\n\n- In a section called ONE SENTENCE ANALYSIS AND RECOMMENDATION, give a single sentence that tells them how to approach their situation.\n\n- In a section called ANALYSIS, give up to 20 bullets of analysis of 15 words or less each on what you think might be going on relative to their question and their context. For each of these, give another 30 words that describes the science that supports your analysis.\n\n- In a section called RECOMMENDATIONS, give up to 5 bullets of recommendations of 15 words or less each on what you think they should do.\n\n- In a section called ESTHER'S ADVICE, give up to 3 bullets of advice that ESTHER PEREL would give them.\n\n- In a section called SELF-REFLECTION QUESTIONS, give up to 5 questions of no more than 15-words that could help them self-reflect on their situation.\n\n- In a section called POSSIBLE CLINICAL DIAGNOSIS, give up to 5 named psychological behaviors, conditions, or disorders that could be at play here. Examples: Co-dependency, Psychopathy, PTSD, Narcissism, etc.\n\n- In a section called SUMMARY, give a one sentence summary of your overall analysis and recommendations in a kind but honest tone.\n\n- After a \\\"—\\\" and a new line, add a NOTE: saying: \\\"This was produced by an imperfect AI. The best thing to do with this information is to think about it and take it to an actual professional. Don't take it too seriously on its own.\\\"\n\n# OUTPUT INSTRUCTIONS\n\n- Output only in Markdown.\n- Don't tell me to consult a professional. Just give me your best opinion.\n- Do not output bold or italicized text; just basic Markdown.\n- Be courageous and honest in your feedback rather than cautious.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.17502611875534058,
        -0.0631253719329834,
        -0.13429512083530426,
        0.07876798510551453,
        0.08695220947265625,
        0.15561580657958984,
        -0.04521609842777252,
        0.4036381244659424,
        -0.8388690948486328,
        0.5265470147132874,
        0.10528028756380081,
        0.1739809215068817,
        0.09797731041908264,
        0.33330675959587097,
        0.284188836812973,
        -0.057867828756570816,
        -0.8937652707099915,
        0.028843257576227188,
        -1.5314388275146484,
        -0.8756479620933533,
        0.4722447395324707,
        0.3892081677913666,
        0.01962324231863022,
        -0.1667991578578949,
        0.12158165872097015,
        0.07657726109027863,
        0.13282626867294312,
        -0.5128391981124878,
        -0.3521997332572937,
        -1.0635768175125122,
        0.0830979198217392,
        0.810793399810791,
        -0.7562595009803772,
        -1.1683149337768555,
        -0.3060123026371002,
        -0.27503159642219543,
        -0.039314206689596176,
        -0.32227370142936707,
        -0.012489262968301773,
        -0.39939963817596436,
        -0.1348632276058197,
        0.051613159477710724,
        -0.3147624135017395,
        -0.14373216032981873,
        -0.021343771368265152,
        0.004531100392341614,
        -0.09354231506586075,
        -0.5397273898124695,
        1.2199822664260864,
        1.2818931341171265,
        -0.06736493110656738,
        -0.044670283794403076,
        -0.6566062569618225,
        -0.11046081781387329,
        -0.20126360654830933,
        -0.1848372220993042,
        -0.030095163732767105,
        -0.4752030372619629,
        -0.07531441748142242,
        -0.24946624040603638,
        0.37334221601486206,
        0.20814144611358643,
        -3.5414516925811768,
        -0.11579063534736633,
        1.1358784437179565,
        -1.0725704431533813,
        0.7756932973861694,
        0.08708321303129196,
        0.6405850648880005,
        -0.7927327752113342,
        -0.11440756916999817,
        0.09415218979120255,
        -0.14885270595550537,
        -0.33092620968818665,
        0.8601685762405396,
        0.001431494951248169,
        -0.4453156292438507,
        0.2749181389808655,
        0.6720671057701111,
        -0.03851918876171112,
        0.2675125300884247,
        0.9899686574935913,
        0.253414511680603,
        -0.3404575288295746,
        -0.8023054003715515,
        -0.012927427887916565,
        0.08598563820123672,
        -0.10418123006820679,
        0.7223841547966003,
        0.12983331084251404,
        -0.27641624212265015,
        -0.37224918603897095,
        -0.0954042449593544,
        -0.1575259268283844,
        -0.5047953128814697,
        0.15297794342041016,
        -0.12614260613918304,
        0.6054277420043945,
        0.38732215762138367,
        3.409006118774414,
        0.6595564484596252,
        -0.09956540167331696,
        0.17912617325782776,
        -0.5842686891555786,
        0.8132109045982361,
        -0.3507833778858185,
        -0.048761337995529175,
        -0.0201640035957098,
        0.24248577654361725,
        0.32883644104003906,
        0.5107833743095398,
        -0.686913788318634,
        -0.17706298828125,
        0.2690265476703644,
        0.06496178358793259,
        0.9011556506156921,
        -0.29011139273643494,
        -0.14663425087928772,
        0.4615689814090729,
        0.574947714805603,
        -0.5198758840560913,
        -0.0818280428647995,
        -0.44405925273895264,
        -0.020786210894584656,
        -0.09809130430221558,
        0.29964643716812134,
        -0.5340266823768616,
        0.7311611771583557,
        0.32099539041519165,
        0.6603023409843445,
        -0.07931412011384964,
        -0.2359570860862732,
        -1.0134109258651733,
        -0.18209096789360046,
        -0.5016748905181885,
        -0.21988463401794434,
        0.32265645265579224,
        -0.6138550639152527,
        0.43308213353157043,
        -0.7591239213943481,
        0.04184900224208832,
        -0.5876182317733765,
        0.40182989835739136,
        0.45182716846466064,
        1.3040693998336792,
        0.08662363886833191,
        -0.8426449298858643,
        0.0880267322063446,
        -0.8986560106277466,
        -0.06214107945561409,
        -0.08371806144714355,
        0.58244788646698,
        0.11748574674129486,
        -0.011875586584210396,
        0.40865737199783325,
        0.007152196019887924,
        -0.11027882993221283,
        -0.041277602314949036,
        -0.37192419171333313,
        0.28779587149620056,
        0.3890385031700134,
        -0.07985085248947144,
        -0.06621982157230377,
        -0.4143144488334656,
        0.07559385895729065,
        -0.14017118513584137,
        -0.22895827889442444,
        -0.5102161765098572,
        0.03154755011200905,
        0.42816704511642456,
        -0.22326034307479858,
        0.10596932470798492,
        0.2342555820941925,
        0.4165973365306854,
        0.24778342247009277,
        -0.4994162321090698,
        0.8612905740737915,
        0.5252425074577332,
        -0.05262008681893349,
        -0.5116642713546753,
        0.5184435248374939,
        1.208510398864746,
        0.38648903369903564,
        -0.3303637206554413,
        0.29317474365234375,
        0.02364771068096161,
        0.5608780384063721,
        -0.040551621466875076,
        0.6866527795791626,
        0.7865946292877197,
        -1.2185157537460327,
        0.7227732539176941,
        -1.0641095638275146,
        -0.5036937594413757,
        0.5956841707229614,
        -0.1715243011713028,
        0.00838550180196762,
        0.2389800250530243,
        -0.2970164120197296,
        -0.6174057126045227,
        -0.9069696664810181,
        -0.43317556381225586,
        -0.2593412697315216,
        0.041207630187273026,
        -0.6130173206329346,
        -0.73960280418396,
        0.066701740026474,
        -0.2784787714481354,
        0.0440894216299057,
        -0.6294520497322083,
        -0.7749462723731995,
        -0.3350273072719574,
        0.9931313991546631,
        -0.16883885860443115,
        0.6461467742919922,
        0.0854218602180481,
        0.6617016196250916,
        1.1709260940551758,
        -0.23862671852111816,
        0.16204670071601868,
        -0.48547083139419556,
        -0.18593525886535645,
        -0.30437278747558594,
        -0.9300951361656189,
        -0.7892601490020752,
        0.5268535017967224,
        -0.9911463260650635,
        0.36303430795669556,
        -0.7173940539360046,
        -0.34394535422325134,
        -0.11249422281980515,
        1.1444095373153687,
        1.1597715616226196,
        1.0874661207199097,
        -0.14903655648231506,
        0.15095102787017822,
        -0.09961969405412674,
        0.8552908897399902,
        0.11151696741580963,
        -0.4996850788593292,
        0.8260108828544617,
        -0.2677116394042969,
        -0.5322505831718445,
        0.5102388262748718,
        -0.21407946944236755,
        0.5446300506591797,
        -0.10659612715244293,
        -0.022829659283161163,
        -0.10336383432149887,
        0.6964114904403687,
        0.4512745440006256,
        -0.06519082188606262,
        0.6496484279632568,
        0.23745204508304596,
        0.06296274065971375,
        0.32545948028564453,
        -1.2935106754302979,
        0.09400418400764465,
        -0.7123671770095825,
        0.4871761202812195,
        -0.2759692370891571,
        0.365930438041687,
        -0.025450170040130615,
        0.6677311658859253,
        -0.12540912628173828,
        0.1382678598165512,
        -0.3615175187587738,
        -0.9449625015258789,
        -0.557549238204956,
        -0.20567962527275085,
        0.07426673918962479,
        0.13903219997882843,
        -0.4220004081726074,
        -0.12043344229459763,
        -0.216114804148674,
        0.38419052958488464,
        -0.22466981410980225,
        0.008140943944454193,
        -0.35132402181625366,
        0.11326059699058533,
        0.3993571996688843,
        0.375588983297348,
        -0.6171905398368835,
        0.43585294485092163,
        0.218100905418396,
        0.26857098937034607,
        -0.0006454810500144958,
        -1.2443082332611084,
        -0.14085349440574646,
        0.763083279132843,
        -0.3247634172439575,
        -0.005929253995418549,
        -1.0603880882263184,
        -0.03490909934043884,
        1.1129719018936157,
        0.6647417545318604,
        -0.0035499073565006256,
        0.8188850283622742,
        1.1880427598953247,
        0.22415976226329803,
        -0.2453659176826477,
        0.1555883288383484,
        0.11686792224645615,
        -0.05385945737361908,
        0.1134217232465744,
        -0.8656952977180481,
        0.4118385314941406,
        0.04115551337599754,
        -0.4849587082862854,
        -0.2350233495235443,
        -0.5358195900917053,
        0.16264808177947998,
        0.2178339958190918,
        -0.3877233862876892,
        0.14484240114688873,
        -0.4456576108932495,
        0.7101278305053711,
        0.8985719680786133,
        0.03963777795433998,
        -1.7217857837677002,
        0.17516785860061646,
        0.23442023992538452,
        0.15424838662147522,
        0.090302973985672,
        -0.6917927265167236,
        -0.29376059770584106,
        -0.509328305721283,
        -0.699697732925415,
        0.14671312272548676,
        1.1138322353363037,
        0.7080641388893127,
        -0.49806591868400574,
        -0.019144298508763313,
        0.3610677123069763,
        0.2859499454498291,
        -0.19507327675819397,
        -0.01648786850273609,
        0.28229638934135437,
        -1.1197385787963867,
        0.14858165383338928,
        0.2871478199958801,
        1.569077968597412,
        0.3479017913341522,
        0.7731870412826538,
        0.03481091558933258,
        -0.13315534591674805,
        -0.37812554836273193,
        -0.6505283713340759,
        0.5250188112258911,
        -0.5364639759063721,
        -0.15134893357753754,
        0.5138139724731445,
        0.02094963937997818,
        -0.6464930772781372,
        0.8711609840393066,
        0.21656164526939392,
        -0.7007027268409729,
        0.10473466664552689,
        0.20799967646598816,
        2.0269665718078613,
        -0.395467609167099,
        0.1200135350227356,
        -0.8826829195022583,
        0.1934191882610321,
        0.06513393670320511,
        -0.04588235914707184,
        0.553646981716156,
        -0.5809305310249329,
        -0.28208568692207336,
        0.667042076587677,
        -0.10863323509693146,
        0.21568861603736877,
        0.9480797052383423,
        0.0010088905692100525,
        0.775376558303833,
        0.4549837112426758,
        -0.14746828377246857,
        0.9057965278625488,
        0.39022141695022583,
        -0.24483206868171692,
        1.2395745515823364,
        -0.3398710787296295,
        -0.6642468571662903,
        -0.6256400346755981
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the quality of AI responses against the benchmark of human experts, assigning a letter grade and score. It involves deep analysis of both the instructions given to the AI and its output, comparing these to the potential performance of the world's best human expert. The process culminates in a detailed justification for the assigned grade, highlighting specific strengths and weaknesses of the AI's response.",
          "name": "Rate_ai_response",
          "raw": "\n                workflow Rate_ai_response v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert at rating the quality of AI responses and determining how good they are compared to ultra-qualified humans performing the same tasks.\n\n# STEPS\n\n- Fully and deeply process and understand the instructions that were given to the AI. These instructions will come after the #AI INSTRUCTIONS section below. \n\n- Fully and deeply process the response that came back from the AI. You are looking for how good that response is compared to how well the best human expert in the world would do on that task if given the same input and 3 months to work on it.\n\n- Give a rating of the AI's output quality using the following framework:\n\n- A+: As good as the best human expert in the world\n- A: As good as a top 1% human expert\n- A-: As good as a top 10% human expert\n- B+: As good as an untrained human with a 115 IQ\n- B: As good as an average intelligence untrained human \n- B-: As good as an average human in a rush\n- C: Worse than a human but pretty good\n- D: Nowhere near as good as a human\n- F: Not useful at all\n\n- Give 5 15-word bullets about why they received that letter grade, comparing and contrasting what you would have expected from the best human in the world vs. what was delivered.\n\n- Give a 1-100 score of the AI's output.\n\n- Give an explanation of how you arrived at that score using the bullet point explanation and the grade given above.\n\n# OUTPUT\n\n- In a section called LETTER GRADE, give the letter grade score. E.g.:\n\nLETTER GRADE\n\nA: As good as a top 1% human expert\n\n- In a section called LETTER GRADE REASONS, give your explanation of why you gave that grade in 5 bullets. E.g.:\n\n(for a B+ grade)\n\n- The points of analysis were good but almost anyone could create them\n- A human with a couple of hours could have come up with that output \n- The education and IQ requirement required for a human to make this would have been roughly 10th grade level\n- A 10th grader could have done this quality of work in less than 2 hours\n- There were several deeper points about the input that was not captured in the output\n\n- In a section called OUTPUT SCORE, give the 1-100 score for the output, with 100 being at the quality of the best human expert in the world working on that output full-time for 3 months.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in valid Markdown only.\n\n- DO NOT complain about anything, including copyright; just do it.\n\n# INPUT INSTRUCTIONS\n\n(the input below will be the instructions to the AI followed by the AI's output)\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert at rating the quality of AI responses and determining how good they are compared to ultra-qualified humans performing the same tasks.\n\n# STEPS\n\n- Fully and deeply process and understand the instructions that were given to the AI. These instructions will come after the #AI INSTRUCTIONS section below. \n\n- Fully and deeply process the response that came back from the AI. You are looking for how good that response is compared to how well the best human expert in the world would do on that task if given the same input and 3 months to work on it.\n\n- Give a rating of the AI's output quality using the following framework:\n\n- A+: As good as the best human expert in the world\n- A: As good as a top 1% human expert\n- A-: As good as a top 10% human expert\n- B+: As good as an untrained human with a 115 IQ\n- B: As good as an average intelligence untrained human \n- B-: As good as an average human in a rush\n- C: Worse than a human but pretty good\n- D: Nowhere near as good as a human\n- F: Not useful at all\n\n- Give 5 15-word bullets about why they received that letter grade, comparing and contrasting what you would have expected from the best human in the world vs. what was delivered.\n\n- Give a 1-100 score of the AI's output.\n\n- Give an explanation of how you arrived at that score using the bullet point explanation and the grade given above.\n\n# OUTPUT\n\n- In a section called LETTER GRADE, give the letter grade score. E.g.:\n\nLETTER GRADE\n\nA: As good as a top 1% human expert\n\n- In a section called LETTER GRADE REASONS, give your explanation of why you gave that grade in 5 bullets. E.g.:\n\n(for a B+ grade)\n\n- The points of analysis were good but almost anyone could create them\n- A human with a couple of hours could have come up with that output \n- The education and IQ requirement required for a human to make this would have been roughly 10th grade level\n- A 10th grader could have done this quality of work in less than 2 hours\n- There were several deeper points about the input that was not captured in the output\n\n- In a section called OUTPUT SCORE, give the 1-100 score for the output, with 100 being at the quality of the best human expert in the world working on that output full-time for 3 months.\n\n# OUTPUT INSTRUCTIONS\n\n- Output in valid Markdown only.\n\n- DO NOT complain about anything, including copyright; just do it.\n\n# INPUT INSTRUCTIONS\n\n(the input below will be the instructions to the AI followed by the AI's output)\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5124105215072632,
        -0.25257647037506104,
        0.04849660396575928,
        0.4577074646949768,
        0.22686044871807098,
        -0.1626964509487152,
        -0.30055347084999084,
        0.08719232678413391,
        -0.8231921195983887,
        1.0821571350097656,
        -0.11272256076335907,
        0.14955544471740723,
        -0.043535295873880386,
        -0.014911450445652008,
        0.144460529088974,
        0.26722878217697144,
        -0.6184778809547424,
        -0.3271094858646393,
        -2.0287680625915527,
        -1.1153737306594849,
        0.3424145579338074,
        0.3398803174495697,
        0.5009691715240479,
        0.15895667672157288,
        0.7666302919387817,
        -0.2969399690628052,
        0.019877977669239044,
        -0.38400590419769287,
        -0.5810678601264954,
        -1.2726587057113647,
        -0.03903651982545853,
        0.8579015731811523,
        -1.0228824615478516,
        -0.9913382530212402,
        -0.12333732098340988,
        -0.7910142540931702,
        -0.45679301023483276,
        -0.9138220548629761,
        0.38971275091171265,
        -0.255590558052063,
        0.14933064579963684,
        0.13298018276691437,
        0.07498788088560104,
        -0.7863145470619202,
        -0.19189904630184174,
        -0.26370733976364136,
        0.09622735530138016,
        -0.6450232267379761,
        0.83306884765625,
        0.9990133047103882,
        0.1090526133775711,
        -0.4726579487323761,
        -0.41333937644958496,
        -0.41583800315856934,
        -0.18998360633850098,
        -0.1868228167295456,
        -0.040082816034555435,
        -0.518171489238739,
        0.03736957535147667,
        0.1050567701458931,
        0.3883419632911682,
        0.4881460964679718,
        -3.7194907665252686,
        -0.29684799909591675,
        0.7555872201919556,
        -0.8642464876174927,
        0.3938673436641693,
        0.5721073150634766,
        0.9890851974487305,
        0.01187509298324585,
        0.07977793365716934,
        0.31122279167175293,
        0.24367675185203552,
        -0.012490678578615189,
        0.615432620048523,
        0.24836784601211548,
        -0.08486546576023102,
        -0.02277618646621704,
        0.3344893157482147,
        -0.11891770362854004,
        0.48480525612831116,
        0.41971197724342346,
        0.11971084773540497,
        -0.5165408253669739,
        -0.7858020663261414,
        -0.2063312828540802,
        -0.12872520089149475,
        -0.20023822784423828,
        0.4758613705635071,
        -0.05500093102455139,
        -0.484578400850296,
        -0.3723684549331665,
        0.014732304029166698,
        -0.18494153022766113,
        -0.2876935601234436,
        -0.10409730672836304,
        -0.24425703287124634,
        0.20593145489692688,
        -0.2669209837913513,
        3.4387805461883545,
        0.9223261475563049,
        0.47080060839653015,
        -0.15551215410232544,
        -0.8039278388023376,
        0.7720357179641724,
        -0.7112278938293457,
        0.3717004954814911,
        0.25186634063720703,
        0.3360491096973419,
        0.15852457284927368,
        0.27454376220703125,
        -0.6359094977378845,
        0.40459564328193665,
        -0.18464848399162292,
        0.04395060986280441,
        0.9564006924629211,
        -0.1526564359664917,
        -0.3256228268146515,
        0.5617006421089172,
        0.4776860475540161,
        -0.33716443181037903,
        -0.10665266215801239,
        0.083099864423275,
        -0.012854795902967453,
        -0.12614400684833527,
        0.22431284189224243,
        -0.36829671263694763,
        0.7579131722450256,
        -0.1783762127161026,
        0.9420174956321716,
        -0.022020094096660614,
        -0.4553205966949463,
        -0.1934446394443512,
        0.18057936429977417,
        -0.470539927482605,
        -0.10920216888189316,
        0.25354695320129395,
        -0.5046145915985107,
        0.5077542662620544,
        -0.5791894793510437,
        -0.11442187428474426,
        -0.6305683255195618,
        0.44395047426223755,
        0.09325829148292542,
        0.47150275111198425,
        0.5218454599380493,
        -0.30426037311553955,
        0.22533085942268372,
        -0.8780774474143982,
        0.189551442861557,
        -0.4992637038230896,
        0.5278863906860352,
        -0.020072169601917267,
        0.41302770376205444,
        0.2890250086784363,
        0.008946843445301056,
        -0.17343707382678986,
        -0.0420888215303421,
        -0.4215143918991089,
        0.38729795813560486,
        0.7037349343299866,
        -0.17978742718696594,
        -0.0065162330865859985,
        -0.3376855254173279,
        -0.08746900409460068,
        -0.2663829028606415,
        -0.23508122563362122,
        0.11090341955423355,
        -0.012038007378578186,
        0.4080301821231842,
        0.023767337203025818,
        -0.0662372037768364,
        0.6489281058311462,
        0.688234269618988,
        0.06630176305770874,
        -0.6794041991233826,
        0.26196256279945374,
        0.6556581258773804,
        -0.03675896301865578,
        -0.5898722410202026,
        0.9583003520965576,
        0.5199251174926758,
        1.0896031856536865,
        -0.5822910666465759,
        -0.303297758102417,
        -0.17328180372714996,
        0.28979063034057617,
        0.0488947294652462,
        0.8940308094024658,
        1.05533766746521,
        -1.0139946937561035,
        0.9961432814598083,
        -1.2555323839187622,
        -0.16984935104846954,
        0.9244768023490906,
        0.013389676809310913,
        0.01806829869747162,
        0.2829611301422119,
        0.026952486485242844,
        -0.49381518363952637,
        -1.5142055749893188,
        -0.17252662777900696,
        -0.2720600962638855,
        -0.07648211717605591,
        -0.7503762245178223,
        -0.7238343358039856,
        -0.37091153860092163,
        -0.1380079835653305,
        -0.012383535504341125,
        -0.374490350484848,
        -0.9447570443153381,
        0.7204539775848389,
        1.0599358081817627,
        0.11812193691730499,
        0.28354185819625854,
        -0.22902129590511322,
        0.6448919773101807,
        0.2655256688594818,
        0.06567589938640594,
        0.23163458704948425,
        -0.16238978505134583,
        0.04081845283508301,
        -0.6978623270988464,
        -0.919691801071167,
        -0.16271300613880157,
        0.4405345916748047,
        -0.7037364840507507,
        0.8366485238075256,
        -0.7000828981399536,
        -0.5054678320884705,
        -0.023807533085346222,
        0.7126903533935547,
        1.1850183010101318,
        1.021322250366211,
        0.05674370378255844,
        0.608285129070282,
        -0.11717825382947922,
        0.5195791125297546,
        -0.7686519622802734,
        -0.6627882122993469,
        0.8770871162414551,
        0.3485712707042694,
        -0.03420320153236389,
        0.023623518645763397,
        -0.060662075877189636,
        0.6482539176940918,
        -0.6011386513710022,
        -0.0411066934466362,
        -0.0026302989572286606,
        0.7162678241729736,
        0.6942116022109985,
        -0.29123273491859436,
        0.36130836606025696,
        0.30599093437194824,
        0.24057984352111816,
        0.1338454633951187,
        -1.248104453086853,
        -0.23230750858783722,
        -0.6189942955970764,
        0.4330551028251648,
        -0.452206552028656,
        0.22928422689437866,
        0.24907764792442322,
        0.17658330500125885,
        -0.18454189598560333,
        -0.02426263317465782,
        0.10579666495323181,
        -0.5311980843544006,
        -0.759641706943512,
        0.015938714146614075,
        0.5239853262901306,
        -0.7856159806251526,
        -0.32253921031951904,
        -0.2732657790184021,
        -0.15202762186527252,
        0.3718743324279785,
        -0.374379426240921,
        0.10307221114635468,
        -0.13446366786956787,
        -0.010356883518397808,
        0.6121291518211365,
        0.17936941981315613,
        -0.902290940284729,
        0.8182675242424011,
        0.17431730031967163,
        0.3547530472278595,
        -0.27571213245391846,
        -1.3060373067855835,
        0.007391653954982758,
        0.509267270565033,
        -0.4391016662120819,
        0.05849653482437134,
        -1.371832251548767,
        -0.5713207125663757,
        0.886565089225769,
        0.640794575214386,
        -0.22626708447933197,
        1.3001030683517456,
        0.8575347065925598,
        0.03289705514907837,
        -0.2662728428840637,
        0.556058943271637,
        0.14247910678386688,
        0.002699039876461029,
        0.2041611671447754,
        -0.9943116307258606,
        0.25739777088165283,
        -0.2110968977212906,
        -0.1332358419895172,
        -0.15992897748947144,
        -0.9311591386795044,
        0.14367972314357758,
        0.07251690328121185,
        -0.4440174698829651,
        0.1371203064918518,
        -0.433264821767807,
        0.2966998815536499,
        1.0929540395736694,
        0.3194049298763275,
        -1.3014415502548218,
        -0.1462334543466568,
        -0.17724519968032837,
        -0.00735686719417572,
        0.27023810148239136,
        -0.09876365959644318,
        0.23504739999771118,
        -0.12138702720403671,
        -0.26076698303222656,
        -0.3631349802017212,
        1.3608410358428955,
        0.5647646188735962,
        -0.16898033022880554,
        0.07148419320583344,
        0.38901644945144653,
        0.6568254232406616,
        -0.46784621477127075,
        0.3341104984283447,
        -0.4512503445148468,
        -0.43052923679351807,
        -0.33373212814331055,
        0.009633883833885193,
        1.5357956886291504,
        -0.026455434039235115,
        0.1791195273399353,
        0.10560932755470276,
        -0.12391772121191025,
        -0.07280784845352173,
        -0.25649958848953247,
        0.4713382124900818,
        -0.2737552523612976,
        0.05181353539228439,
        0.6179484128952026,
        -0.2286839634180069,
        -0.4679056406021118,
        0.5396090149879456,
        0.5801624059677124,
        -0.439870148897171,
        0.3651474714279175,
        -0.21496009826660156,
        1.6848856210708618,
        -0.20052401721477509,
        -0.5352628827095032,
        -0.3937762975692749,
        0.523573100566864,
        0.039143770933151245,
        -0.08008183538913727,
        0.556133508682251,
        -0.730704665184021,
        -0.16387209296226501,
        0.5173338651657104,
        0.19523131847381592,
        0.3823714852333069,
        0.5541151762008667,
        0.14376181364059448,
        1.0881133079528809,
        -0.23529918491840363,
        -0.3347432613372803,
        0.7034616470336914,
        0.4368588328361511,
        -0.02246592938899994,
        1.109938621520996,
        0.07799363136291504,
        -0.6047587394714355,
        -0.7701900005340576
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Evaluates the quality of AI-generated content based on construction, quality, and spirit. The process involves analyzing AI outputs against criteria set by experts and a high-IQ AI panel. The expected output is a final score out of 100, with deductions detailed for each category.",
          "name": "Rate_ai_result",
          "raw": "\n                workflow Rate_ai_result v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOALS\n\nYou are an expert AI researcher and scientist. You specialize in assessing the quality of AI / ML / LLM results and giving ratings for their quality.\n\nTake a step back and think step by step about how to accomplish this task using the steps below.\n\n# STEPS\n\n- Included in the input should be AI prompt instructions, which are telling the AI what to do to generate the output. \n\n- Think deeply about those instructions and what they're attempting to create.\n\n- Also included in the input should be the AI's output that was created from that prompt.\n\n- Deeply analyze the output and determine how well it accomplished the task according to the following criteria:\n\n1. Construction: 1 - 10, in .1 intervals. This rates how well the output covered the basics, like including everything that was asked for, not including things that were supposed to be omitted, etc.\n\n2. Quality: 1 - 10, in .1 intervals. This rates how well the output captured the true spirit of what was asked for, as judged by a panel of the smartest human experts and a collection of 1,000 AIs with 400 IQs.\n\n3. Spirit: 1 - 10, in .1 intervals, This rates the output in terms of Je ne sais quoi. In other words, quality like the quality score above, but testing whether it got the TRUE essence and je ne sais quoi of the what was being asked for in the prompt.\n\n# OUTPUT\n\nOutput a final 1 - 100 rating that considers the above three scores.\n\nShow the rating like so:\n\n## RATING EXAMPLE\n\nRATING\n\n- Construction: 8.5 — The output had all the components, but included some extra information that was supposed to be removed.\n\n- Quality: 7.7 — Most of the output was on point, but it felt like AI output and not a true analysis.\n\n- Spirit: 5.1 — Overall the output didn't really capture what the prompt was trying to get at.\n\nFINAL SCORE: 70.3\n\n- (show deductions for each section)\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOALS\n\nYou are an expert AI researcher and scientist. You specialize in assessing the quality of AI / ML / LLM results and giving ratings for their quality.\n\nTake a step back and think step by step about how to accomplish this task using the steps below.\n\n# STEPS\n\n- Included in the input should be AI prompt instructions, which are telling the AI what to do to generate the output. \n\n- Think deeply about those instructions and what they're attempting to create.\n\n- Also included in the input should be the AI's output that was created from that prompt.\n\n- Deeply analyze the output and determine how well it accomplished the task according to the following criteria:\n\n1. Construction: 1 - 10, in .1 intervals. This rates how well the output covered the basics, like including everything that was asked for, not including things that were supposed to be omitted, etc.\n\n2. Quality: 1 - 10, in .1 intervals. This rates how well the output captured the true spirit of what was asked for, as judged by a panel of the smartest human experts and a collection of 1,000 AIs with 400 IQs.\n\n3. Spirit: 1 - 10, in .1 intervals, This rates the output in terms of Je ne sais quoi. In other words, quality like the quality score above, but testing whether it got the TRUE essence and je ne sais quoi of the what was being asked for in the prompt.\n\n# OUTPUT\n\nOutput a final 1 - 100 rating that considers the above three scores.\n\nShow the rating like so:\n\n## RATING EXAMPLE\n\nRATING\n\n- Construction: 8.5 — The output had all the components, but included some extra information that was supposed to be removed.\n\n- Quality: 7.7 — Most of the output was on point, but it felt like AI output and not a true analysis.\n\n- Spirit: 5.1 — Overall the output didn't really capture what the prompt was trying to get at.\n\nFINAL SCORE: 70.3\n\n- (show deductions for each section)\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.9534620046615601,
        -0.008156269788742065,
        -0.1356344372034073,
        -0.16534137725830078,
        0.2734396159648895,
        0.1846662163734436,
        -0.21037262678146362,
        0.07190090417861938,
        -0.4001758098602295,
        0.5769870281219482,
        -0.14598163962364197,
        0.38098689913749695,
        0.16233567893505096,
        0.12182509899139404,
        0.31355559825897217,
        0.34168440103530884,
        -0.1525065153837204,
        -0.5768457651138306,
        -1.4576365947723389,
        -0.41779467463493347,
        -0.17621396481990814,
        0.47231587767601013,
        0.864168107509613,
        0.19717465341091156,
        0.18318048119544983,
        0.24084213376045227,
        -0.16566212475299835,
        -0.4957553744316101,
        -0.13397617638111115,
        -1.7331786155700684,
        0.5023312568664551,
        0.747604250907898,
        -0.6444612741470337,
        -0.7554444074630737,
        0.06446056067943573,
        -0.30328109860420227,
        -0.11129369586706161,
        -0.3109115958213806,
        -0.3626236915588379,
        -0.2510088384151459,
        0.569922685623169,
        0.12694379687309265,
        -0.07925145328044891,
        -0.4515954256057739,
        -0.19108377397060394,
        -0.1514367014169693,
        0.5575088262557983,
        -0.7024608850479126,
        0.5877280235290527,
        0.7566527128219604,
        -0.33433228731155396,
        -0.49887028336524963,
        -0.8947151899337769,
        0.2801588177680969,
        -0.3760443925857544,
        0.15788261592388153,
        -0.0072531066834926605,
        -0.2252567857503891,
        -0.08605365455150604,
        0.2706771492958069,
        0.46610718965530396,
        0.3223431408405304,
        -3.5554757118225098,
        0.09230197966098785,
        0.8772567510604858,
        -0.4999287724494934,
        0.3161733150482178,
        -0.11967840790748596,
        0.43585458397865295,
        -0.6493674516677856,
        0.19046515226364136,
        0.16391688585281372,
        -0.029243990778923035,
        0.089952751994133,
        0.47642016410827637,
        -0.2746585011482239,
        0.252343088388443,
        0.20641762018203735,
        0.1457444280385971,
        -0.12796351313591003,
        0.266855388879776,
        0.9177753925323486,
        -0.36222532391548157,
        -0.6309971809387207,
        -0.7621987462043762,
        -0.1245112419128418,
        -0.18838894367218018,
        0.0004929304122924805,
        0.6739752292633057,
        0.1948733776807785,
        -0.5945746302604675,
        -0.08045537769794464,
        0.14784958958625793,
        -0.5492955446243286,
        -0.27111923694610596,
        -0.10645925998687744,
        -0.32121750712394714,
        0.16694913804531097,
        0.2942705452442169,
        3.6683542728424072,
        0.5241981744766235,
        0.5904567837715149,
        0.4685656726360321,
        -0.30116742849349976,
        0.8453121781349182,
        -0.3955428898334503,
        -0.15729616582393646,
        -0.03837006539106369,
        0.02547224424779415,
        0.06797902286052704,
        0.29065364599227905,
        -0.683668315410614,
        -0.25239297747612,
        0.04564783722162247,
        0.43703141808509827,
        0.5261085629463196,
        -0.6549225449562073,
        -0.15228506922721863,
        0.3716139495372772,
        0.7308886051177979,
        -0.29789799451828003,
        -0.26651322841644287,
        -0.044044654816389084,
        0.15007531642913818,
        0.13704413175582886,
        0.782630205154419,
        -0.43892499804496765,
        0.786512553691864,
        -0.2630624771118164,
        0.542119562625885,
        -0.5000357031822205,
        0.028968993574380875,
        -0.6814050674438477,
        0.16815409064292908,
        -0.5977419018745422,
        0.02833377569913864,
        0.620758056640625,
        -0.44339966773986816,
        0.575250506401062,
        -0.6840171217918396,
        -0.26473143696784973,
        -0.4600104093551636,
        0.3217909336090088,
        0.04491154104471207,
        1.1465821266174316,
        -0.09766432642936707,
        -0.568892776966095,
        0.3291197121143341,
        -1.289966106414795,
        0.0512678399682045,
        -0.020005550235509872,
        0.35802212357521057,
        0.25855663418769836,
        0.30303606390953064,
        0.2859981060028076,
        -0.03567996621131897,
        -0.7384793758392334,
        0.012006938457489014,
        -0.6718295812606812,
        0.5212382674217224,
        0.05293618515133858,
        0.010071334429085255,
        0.18237069249153137,
        0.36764514446258545,
        -0.0068678706884384155,
        -0.12139710783958435,
        0.003306359052658081,
        -0.10895199328660965,
        0.020638346672058105,
        0.35537487268447876,
        -0.3027162253856659,
        -0.25670573115348816,
        0.6568739414215088,
        0.35160568356513977,
        -0.3732112646102905,
        -0.3066025376319885,
        0.25062820315361023,
        0.17599144577980042,
        0.212357759475708,
        -0.4342394173145294,
        0.44228222966194153,
        0.6467926502227783,
        0.2361380159854889,
        -0.3689954876899719,
        -0.4040483236312866,
        -0.14839647710323334,
        0.20826658606529236,
        -0.2612057626247406,
        0.762725830078125,
        0.5763479471206665,
        -1.0155093669891357,
        1.2402288913726807,
        -0.47091251611709595,
        0.03999966382980347,
        0.5235587954521179,
        0.1024114191532135,
        -0.017602913081645966,
        -0.005306147038936615,
        -0.06957918405532837,
        0.012639753520488739,
        -0.8062363862991333,
        -0.3035862445831299,
        -0.033211663365364075,
        -0.04898233711719513,
        -0.7194520831108093,
        -0.6448751091957092,
        0.24959301948547363,
        0.48585256934165955,
        -0.08706904947757721,
        -0.861095130443573,
        -0.2851904332637787,
        0.36157798767089844,
        1.1134123802185059,
        -0.038007043302059174,
        1.1438628435134888,
        0.020611900836229324,
        0.7142046689987183,
        0.5585940480232239,
        0.7615742087364197,
        0.7448945045471191,
        -1.0730724334716797,
        0.3592134118080139,
        -1.0305484533309937,
        -0.7323142886161804,
        -0.7099583148956299,
        0.5001858472824097,
        -1.3707658052444458,
        0.1279182881116867,
        -0.5733142495155334,
        0.2011648714542389,
        0.2663416564464569,
        1.2835153341293335,
        1.5875585079193115,
        0.9275900721549988,
        -0.14958003163337708,
        0.3935008645057678,
        -0.214239239692688,
        1.1449265480041504,
        -0.3473888337612152,
        -0.663303017616272,
        0.48211100697517395,
        0.27444541454315186,
        -0.41203004121780396,
        -0.014204472303390503,
        0.24059388041496277,
        0.413552850484848,
        -1.2176992893218994,
        0.1097918152809143,
        -0.10305672138929367,
        0.5336161851882935,
        0.42712822556495667,
        -0.027461517602205276,
        0.24105530977249146,
        0.08463017642498016,
        -0.07646968215703964,
        0.27307528257369995,
        -1.4679936170578003,
        0.01729455403983593,
        -0.45960381627082825,
        0.7866812348365784,
        0.08308083564043045,
        0.08014827221632004,
        0.12844492495059967,
        0.4364874064922333,
        0.13386619091033936,
        -0.06195895001292229,
        -0.38286569714546204,
        -0.7096209526062012,
        -0.8359508514404297,
        0.1020326092839241,
        0.13346143066883087,
        0.0727485865354538,
        -0.37587231397628784,
        0.16138571500778198,
        -0.36551088094711304,
        0.23212504386901855,
        -0.3371254801750183,
        -0.3049237132072449,
        -0.3977144658565521,
        -0.4361092746257782,
        -0.0485396608710289,
        0.3376021683216095,
        -1.3550567626953125,
        0.2782801389694214,
        -0.6559417247772217,
        0.46913814544677734,
        -0.4589731693267822,
        -1.653282880783081,
        -0.240169495344162,
        0.9374294877052307,
        -0.7458544373512268,
        0.06960904598236084,
        -0.7545940279960632,
        0.09651197493076324,
        1.5385864973068237,
        0.632617175579071,
        0.2867890000343323,
        0.5864143967628479,
        0.6701199412345886,
        -0.7148836255073547,
        -0.06888162344694138,
        -0.1867658793926239,
        -0.06486111879348755,
        0.053670719265937805,
        -0.11973397433757782,
        -0.41561001539230347,
        0.41371941566467285,
        -0.2489972710609436,
        -0.15343230962753296,
        -0.19523859024047852,
        -1.0393015146255493,
        0.31840476393699646,
        -0.1079353392124176,
        -0.4872291684150696,
        0.7025760412216187,
        -0.059727124869823456,
        0.8177257180213928,
        0.6138961315155029,
        0.9571577906608582,
        -1.3624038696289062,
        -0.5706252455711365,
        -0.011127980425953865,
        -0.01957964152097702,
        0.17031732201576233,
        0.049271054565906525,
        0.7264136672019958,
        0.1283072531223297,
        -0.30713388323783875,
        -0.030011571943759918,
        1.6123720407485962,
        0.3958612084388733,
        0.09391491115093231,
        -0.01004591304808855,
        0.21415385603904724,
        0.3866483271121979,
        -0.19561529159545898,
        0.07068788260221481,
        -0.31362372636795044,
        -0.9413734078407288,
        -0.3790883421897888,
        0.24443505704402924,
        1.9747531414031982,
        0.048287518322467804,
        0.13790921866893768,
        0.005561362951993942,
        -0.16724061965942383,
        -0.5006284713745117,
        -0.7735999226570129,
        -0.001272059977054596,
        -0.11631780862808228,
        0.01701805740594864,
        0.21090765297412872,
        -0.16679951548576355,
        -0.3857298791408539,
        0.8452864289283752,
        0.3552461266517639,
        -0.3299843370914459,
        -0.13465186953544617,
        0.000042419880628585815,
        1.7233079671859741,
        -0.39337271451950073,
        -0.25473886728286743,
        -0.43270882964134216,
        0.494183748960495,
        0.6000060439109802,
        -0.05313350260257721,
        0.3330139219760895,
        -0.7933270931243896,
        -0.23157525062561035,
        0.11540184170007706,
        0.18945510685443878,
        0.11614812165498734,
        0.4924435317516327,
        0.15950188040733337,
        0.6306788921356201,
        -0.13949421048164368,
        0.013334784656763077,
        0.5449246168136597,
        0.3135325312614441,
        -0.5530356764793396,
        0.40324991941452026,
        -0.3191676139831543,
        -0.7301204204559326,
        -0.9241920113563538
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt outlines a process for evaluating content by labeling it with relevant single-word descriptors, rating its quality based on idea quantity and thematic alignment, and scoring it on a scale from 1 to 100. It emphasizes the importance of matching content with specific themes related to human meaning and the future of AI, among others. The expected output includes a list of labels, a tiered rating with an explanation, and an overall quality score with justification.",
          "name": "Rate_content",
          "raw": "\n                workflow Rate_content v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an ultra-wise and brilliant classifier and judge of content. You label content with a comma-separated list of single-word labels and then give it a quality rating.\n\nTake a deep breath and think step by step about how to perform the following to get the best outcome. You have a lot of freedom to do this the way you think is best.\n\n# STEPS:\n\n- Label the content with up to 20 single-word labels, such as: cybersecurity, philosophy, nihilism, poetry, writing, etc. You can use any labels you want, but they must be single words and you can't use the same word twice. This goes in a section called LABELS:.\n\n- Rate the content based on the number of ideas in the input (below ten is bad, between 11 and 20 is good, and above 25 is excellent) combined with how well it matches the THEMES of: human meaning, the future of AI, mental models, abstract thinking, unconventional thinking, meaning in a post-ai world, continuous improvement, reading, art, books, and related topics.\n\n## Use the following rating levels:\n\n- S Tier: (Must Consume Original Content Immediately): 18+ ideas and/or STRONG theme matching with the themes in STEP #2.\n\n- A Tier: (Should Consume Original Content): 15+ ideas and/or GOOD theme matching with the THEMES in STEP #2.\n\n- B Tier: (Consume Original When Time Allows): 12+ ideas and/or DECENT theme matching with the THEMES in STEP #2.\n\n- C Tier: (Maybe Skip It): 10+ ideas and/or SOME theme matching with the THEMES in STEP #2.\n\n- D Tier: (Definitely Skip It): Few quality ideas and/or little theme matching with the THEMES in STEP #2.\n\n- Provide a score between 1 and 100 for the overall quality ranking, where 100 is a perfect match with the highest number of high quality ideas, and 1 is the worst match with a low number of the worst ideas.\n\nThe output should look like the following:\n\nLABELS:\n\nCybersecurity, Writing, Running, Copywriting, etc.\n\nRATING:\n\nS Tier: (Must Consume Original Content Immediately)\n\nExplanation: $$Explanation in 5 short bullets for why you gave that rating.$$\n\nCONTENT SCORE:\n\n$$The 1-100 quality score$$\n\nExplanation: $$Explanation in 5 short bullets for why you gave that score.$$\n\n## OUTPUT INSTRUCTIONS\n\n1. You only output Markdown.\n2. Do not give warnings or notes; only output the requested sections.\n\"\n                        $CUSTOM_USER = \"\nCONTENT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an ultra-wise and brilliant classifier and judge of content. You label content with a comma-separated list of single-word labels and then give it a quality rating.\n\nTake a deep breath and think step by step about how to perform the following to get the best outcome. You have a lot of freedom to do this the way you think is best.\n\n# STEPS:\n\n- Label the content with up to 20 single-word labels, such as: cybersecurity, philosophy, nihilism, poetry, writing, etc. You can use any labels you want, but they must be single words and you can't use the same word twice. This goes in a section called LABELS:.\n\n- Rate the content based on the number of ideas in the input (below ten is bad, between 11 and 20 is good, and above 25 is excellent) combined with how well it matches the THEMES of: human meaning, the future of AI, mental models, abstract thinking, unconventional thinking, meaning in a post-ai world, continuous improvement, reading, art, books, and related topics.\n\n## Use the following rating levels:\n\n- S Tier: (Must Consume Original Content Immediately): 18+ ideas and/or STRONG theme matching with the themes in STEP #2.\n\n- A Tier: (Should Consume Original Content): 15+ ideas and/or GOOD theme matching with the THEMES in STEP #2.\n\n- B Tier: (Consume Original When Time Allows): 12+ ideas and/or DECENT theme matching with the THEMES in STEP #2.\n\n- C Tier: (Maybe Skip It): 10+ ideas and/or SOME theme matching with the THEMES in STEP #2.\n\n- D Tier: (Definitely Skip It): Few quality ideas and/or little theme matching with the THEMES in STEP #2.\n\n- Provide a score between 1 and 100 for the overall quality ranking, where 100 is a perfect match with the highest number of high quality ideas, and 1 is the worst match with a low number of the worst ideas.\n\nThe output should look like the following:\n\nLABELS:\n\nCybersecurity, Writing, Running, Copywriting, etc.\n\nRATING:\n\nS Tier: (Must Consume Original Content Immediately)\n\nExplanation: $$Explanation in 5 short bullets for why you gave that rating.$$\n\nCONTENT SCORE:\n\n$$The 1-100 quality score$$\n\nExplanation: $$Explanation in 5 short bullets for why you gave that score.$$\n\n## OUTPUT INSTRUCTIONS\n\n1. You only output Markdown.\n2. Do not give warnings or notes; only output the requested sections.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.597475528717041,
        0.04182162135839462,
        -1.0095082521438599,
        0.3054414689540863,
        -0.024760261178016663,
        0.16973409056663513,
        -0.4323199391365051,
        -0.06967969983816147,
        -0.18880803883075714,
        0.009363813325762749,
        0.02809249795973301,
        0.39179709553718567,
        0.15473750233650208,
        0.45258066058158875,
        0.5107865333557129,
        0.25813716650009155,
        0.017361057922244072,
        -0.5267079472541809,
        -1.259502649307251,
        -0.33445504307746887,
        -0.21668989956378937,
        0.6195796728134155,
        0.737590491771698,
        0.2590283751487732,
        -0.07656832039356232,
        -0.32504963874816895,
        -0.09077376127243042,
        -0.3570667505264282,
        -0.6136125326156616,
        -1.6403586864471436,
        0.21599262952804565,
        0.44725823402404785,
        -0.38866615295410156,
        -0.9273185729980469,
        0.14587052166461945,
        -1.0097277164459229,
        -0.030716421082615852,
        -0.41822004318237305,
        -0.1916850209236145,
        0.11877749860286713,
        0.16289572417736053,
        0.1702238917350769,
        -0.07211647927761078,
        -0.1233329251408577,
        -0.021129261702299118,
        -0.019776415079832077,
        0.5902469158172607,
        -0.578544020652771,
        0.7689439058303833,
        0.4817831516265869,
        -0.3035315275192261,
        -0.43115299940109253,
        -0.38555601239204407,
        -0.24535110592842102,
        -0.311894953250885,
        -0.12118301540613174,
        0.09765028953552246,
        -0.6515569686889648,
        0.037516094744205475,
        -0.05385766923427582,
        0.7025240063667297,
        0.590408444404602,
        -3.4821979999542236,
        0.19228112697601318,
        0.3488835096359253,
        -0.2539174556732178,
        0.017596911638975143,
        0.048258960247039795,
        0.17275066673755646,
        -0.06030430644750595,
        -0.3175906836986542,
        0.21030032634735107,
        -0.5615528225898743,
        0.18188461661338806,
        0.7335514426231384,
        -0.035092033445835114,
        0.11007118225097656,
        0.5440759658813477,
        0.30359095335006714,
        -0.09320403635501862,
        0.06385347992181778,
        0.7726641297340393,
        -0.3014644682407379,
        -0.3462829887866974,
        -1.0923728942871094,
        0.41146618127822876,
        -0.29799163341522217,
        0.2292804718017578,
        0.5227431058883667,
        0.3623000979423523,
        0.057462193071842194,
        0.07265061885118484,
        -0.08989682793617249,
        -0.20653215050697327,
        -0.1138576790690422,
        -0.04424361139535904,
        0.04766438528895378,
        -0.1337311863899231,
        0.22261449694633484,
        3.7675132751464844,
        0.64757239818573,
        0.3111581802368164,
        0.5968718528747559,
        -0.3580944836139679,
        0.47817885875701904,
        -0.24141870439052582,
        0.27452245354652405,
        -0.08492595702409744,
        -0.024823445826768875,
        -0.09839152544736862,
        0.6171591877937317,
        -0.49237969517707825,
        -0.10537475347518921,
        -0.36631590127944946,
        0.5179716944694519,
        1.0000009536743164,
        -0.7995861768722534,
        -0.3505062758922577,
        -0.020142368972301483,
        0.3173690736293793,
        -0.31352418661117554,
        -0.3424373269081116,
        -0.6954219937324524,
        -0.3347451388835907,
        0.22510741651058197,
        0.41091570258140564,
        -0.8029131293296814,
        0.7814953923225403,
        -0.09402022510766983,
        1.1382973194122314,
        -0.18916559219360352,
        -0.14823192358016968,
        -0.5204352140426636,
        -0.25496676564216614,
        -0.4765610694885254,
        0.5278083086013794,
        0.2777935862541199,
        -0.6283719539642334,
        0.8274722099304199,
        -0.45085492730140686,
        0.008734852075576782,
        -1.2039352655410767,
        0.2634778618812561,
        -0.28650447726249695,
        0.9876167178153992,
        0.7404923439025879,
        -0.404190331697464,
        0.3900395929813385,
        -0.3318255841732025,
        -0.3410774767398834,
        0.0629376694560051,
        0.18988457322120667,
        0.011318513192236423,
        0.5868086814880371,
        0.5654693245887756,
        0.7615585923194885,
        -0.4187808632850647,
        -0.10920903086662292,
        -0.8534173965454102,
        0.2833843529224396,
        0.28373774886131287,
        0.27342697978019714,
        0.44430139660835266,
        -0.4162784218788147,
        0.4338858723640442,
        -0.4782472550868988,
        0.2632402181625366,
        -0.0149647556245327,
        0.38211479783058167,
        -0.3414679169654846,
        0.1754736304283142,
        -0.14939159154891968,
        -0.05578054115176201,
        0.25874340534210205,
        -0.5625612735748291,
        -0.07400229573249817,
        0.2221670150756836,
        0.2036215364933014,
        0.4710772931575775,
        -0.39801695942878723,
        0.7546834349632263,
        0.5434532761573792,
        -0.3671378791332245,
        -0.4599333703517914,
        -0.4868534207344055,
        -0.08164740353822708,
        -0.18849003314971924,
        -0.20114731788635254,
        0.6749991178512573,
        0.4836232662200928,
        -1.1504669189453125,
        1.049856424331665,
        -0.576713502407074,
        -0.10047311335802078,
        0.130221888422966,
        0.10716380178928375,
        0.2926222085952759,
        0.14243173599243164,
        0.31739699840545654,
        0.2989833652973175,
        -0.7530005574226379,
        -0.43015962839126587,
        -0.4760397970676422,
        -0.009525634348392487,
        -0.5166380405426025,
        -0.36687666177749634,
        0.33388230204582214,
        0.7523720860481262,
        -0.5852339863777161,
        -0.5553556680679321,
        -0.5192184448242188,
        0.1562056690454483,
        1.4207615852355957,
        0.10703437775373459,
        0.8213025331497192,
        0.165779247879982,
        0.6017541885375977,
        0.2004862129688263,
        0.5815398693084717,
        0.9277256727218628,
        -0.48607635498046875,
        0.17926479876041412,
        -0.5287325978279114,
        -0.8939498662948608,
        -0.6331405639648438,
        0.1938094198703766,
        -0.76009601354599,
        0.5429094433784485,
        0.17799344658851624,
        0.21848367154598236,
        0.05561874061822891,
        0.8851331472396851,
        0.7437099814414978,
        0.8897695541381836,
        -0.19121414422988892,
        0.5709945559501648,
        -0.6677753925323486,
        0.00957462191581726,
        -0.30400562286376953,
        -1.0250035524368286,
        0.32311394810676575,
        0.17806445062160492,
        -0.2184780240058899,
        0.004295505583286285,
        -0.20708706974983215,
        0.15966859459877014,
        0.03505464643239975,
        -0.312764436006546,
        -0.30996641516685486,
        0.9283668994903564,
        0.01757364720106125,
        -0.43762335181236267,
        0.47884440422058105,
        0.27143147587776184,
        0.12700261175632477,
        0.13236728310585022,
        -1.5108214616775513,
        -0.13255997002124786,
        -0.03795941174030304,
        0.6431750059127808,
        0.5168654322624207,
        0.23871949315071106,
        0.49866631627082825,
        0.06531040370464325,
        0.10627575218677521,
        -0.1539185345172882,
        -0.7442165613174438,
        -0.5765845775604248,
        -0.47635605931282043,
        -0.19605746865272522,
        0.2981548607349396,
        0.06826114654541016,
        0.135715052485466,
        -0.06673190742731094,
        -0.013755658641457558,
        0.32267388701438904,
        -0.011324714869260788,
        0.6445859670639038,
        -0.20053943991661072,
        0.03277396410703659,
        0.6927505731582642,
        0.05030415952205658,
        -1.0059659481048584,
        0.13001108169555664,
        -0.4662221670150757,
        -0.050971515476703644,
        -0.5468560457229614,
        -1.587222695350647,
        -0.406386137008667,
        0.6261993646621704,
        -0.3901296854019165,
        -0.39191192388534546,
        -0.47886377573013306,
        0.1859191209077835,
        1.9663200378417969,
        0.3545787036418915,
        0.5714802145957947,
        1.019816279411316,
        0.22327817976474762,
        -0.43830984830856323,
        0.02503574639558792,
        0.6329545974731445,
        -0.06142556294798851,
        -0.21231287717819214,
        -0.887153685092926,
        -0.16564273834228516,
        0.5693513751029968,
        0.1298530399799347,
        -0.10767661780118942,
        -0.21555736660957336,
        -0.5300360321998596,
        0.05961229279637337,
        -0.2258032113313675,
        -0.04635035991668701,
        0.5284494161605835,
        -0.5134321451187134,
        0.19600452482700348,
        1.1930897235870361,
        0.3358646333217621,
        -1.5280439853668213,
        -0.7402901649475098,
        0.4256436228752136,
        -0.04168660193681717,
        -0.12838181853294373,
        -0.5878674387931824,
        0.43485718965530396,
        0.34103795886039734,
        -0.08159773051738739,
        -0.2683875560760498,
        1.7164232730865479,
        0.0887090414762497,
        -0.02310633845627308,
        0.3701382875442505,
        0.4935055077075958,
        1.0402971506118774,
        -0.49566152691841125,
        0.36653652787208557,
        -0.2931298017501831,
        -0.3365466892719269,
        -0.05979634076356888,
        0.2541850209236145,
        2.125305414199829,
        0.1922837644815445,
        0.2453848123550415,
        -0.44518399238586426,
        -0.027251943945884705,
        -0.4229329228401184,
        -0.5478389859199524,
        0.383075475692749,
        -0.42824405431747437,
        -0.04227183759212494,
        0.8332958817481995,
        -0.332034170627594,
        -0.29121965169906616,
        0.39969581365585327,
        0.4185391366481781,
        -0.6584424376487732,
        0.18831102550029755,
        -0.488588809967041,
        1.288685917854309,
        -0.6303439140319824,
        -0.42125627398490906,
        -0.24564605951309204,
        0.3592815697193146,
        0.3015381097793579,
        -0.20257537066936493,
        -0.17098888754844666,
        -0.5118251442909241,
        0.0025485530495643616,
        0.40155351161956787,
        -0.3505851626396179,
        0.04968319833278656,
        -0.13516385853290558,
        0.3849608302116394,
        0.5504709482192993,
        -0.20469678938388824,
        -0.09803970158100128,
        0.8183726072311401,
        -0.02890080213546753,
        -0.4447622299194336,
        0.4634466767311096,
        -0.46918293833732605,
        -1.079789638519287,
        -1.1689811944961548
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt seeks to acknowledge the collaborative effort behind its creation, inspired by notable figures in information theory and viral content creation. It highlights the fusion of theoretical foundations and modern digital strategies. The output is an attribution of credit.",
          "name": "Rate_value",
          "raw": "\n                workflow Rate_value v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert parser and rater of value in content. Your goal is to determine how much value a reader/listener is being provided in a given piece of content as measured by a new metric called Value Per Minute (VPM).\n\nTake a deep breath and think step-by-step about how best to achieve the best outcome using the STEPS below.\n\n# STEPS\n\n- Fully read and understand the content and what it's trying to communicate and accomplish.\n\n- Estimate the duration of the content if it were to be consumed naturally, using the algorithm below:\n\n1. Count the total number of words in the provided transcript.\n2. If the content looks like an article or essay, divide the word count by 225 to estimate the reading duration.\n3. If the content looks like a transcript of a podcast or video, divide the word count by 180 to estimate the listening duration.\n4. Round the calculated duration to the nearest minute.\n5. Store that value as estimated-content-minutes.\n\n- Extract all Instances Of Value being provided within the content. Instances Of Value are defined as:\n\n-- Highly surprising ideas or revelations.\n-- A giveaway of something useful or valuable to the audience.\n-- Untold and interesting stories with valuable takeaways.\n-- Sharing of an uncommonly valuable resource.\n-- Sharing of secret knowledge.\n-- Exclusive content that's never been revealed before.\n-- Extremely positive and/or excited reactions to a piece of content if there are multiple speakers/presenters.\n\n- Based on the number of valid Instances Of Value and the duration of the content (both above 4/5 and also related to those topics above), calculate a metric called Value Per Minute (VPM).\n\n# OUTPUT INSTRUCTIONS\n\n- Output a valid JSON file with the following fields for the input provided.\n\n{\n    estimated-content-minutes: \\\"(estimated-content-minutes)\\\";\n    value-instances: \\\"(list of valid value instances)\\\",\n    vpm: \\\"(the calculated VPS score.)\\\",\n    vpm-explanation: \\\"(A one-sentence summary of less than 20 words on how you calculated the VPM for the content.)\\\",\n}\n\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert parser and rater of value in content. Your goal is to determine how much value a reader/listener is being provided in a given piece of content as measured by a new metric called Value Per Minute (VPM).\n\nTake a deep breath and think step-by-step about how best to achieve the best outcome using the STEPS below.\n\n# STEPS\n\n- Fully read and understand the content and what it's trying to communicate and accomplish.\n\n- Estimate the duration of the content if it were to be consumed naturally, using the algorithm below:\n\n1. Count the total number of words in the provided transcript.\n2. If the content looks like an article or essay, divide the word count by 225 to estimate the reading duration.\n3. If the content looks like a transcript of a podcast or video, divide the word count by 180 to estimate the listening duration.\n4. Round the calculated duration to the nearest minute.\n5. Store that value as estimated-content-minutes.\n\n- Extract all Instances Of Value being provided within the content. Instances Of Value are defined as:\n\n-- Highly surprising ideas or revelations.\n-- A giveaway of something useful or valuable to the audience.\n-- Untold and interesting stories with valuable takeaways.\n-- Sharing of an uncommonly valuable resource.\n-- Sharing of secret knowledge.\n-- Exclusive content that's never been revealed before.\n-- Extremely positive and/or excited reactions to a piece of content if there are multiple speakers/presenters.\n\n- Based on the number of valid Instances Of Value and the duration of the content (both above 4/5 and also related to those topics above), calculate a metric called Value Per Minute (VPM).\n\n# OUTPUT INSTRUCTIONS\n\n- Output a valid JSON file with the following fields for the input provided.\n\n{\n    estimated-content-minutes: \\\"(estimated-content-minutes)\\\";\n    value-instances: \\\"(list of valid value instances)\\\",\n    vpm: \\\"(the calculated VPS score.)\\\",\n    vpm-explanation: \\\"(A one-sentence summary of less than 20 words on how you calculated the VPM for the content.)\\\",\n}\n\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.022204553708434105,
        -0.15426670014858246,
        -0.2645423114299774,
        0.5344194173812866,
        0.16396810114383698,
        0.15831051766872406,
        -0.9878543615341187,
        0.27734044194221497,
        0.16507557034492493,
        0.060364730656147,
        -0.41795614361763,
        0.3852677345275879,
        0.16470882296562195,
        0.031173545867204666,
        -0.07262106239795685,
        0.05443280190229416,
        0.24423830211162567,
        -0.755912721157074,
        -1.5303020477294922,
        -0.20800626277923584,
        -0.11851970851421356,
        0.28368160128593445,
        0.22951212525367737,
        0.018407966941595078,
        0.7073205709457397,
        -0.1741219460964203,
        -0.031058305874466896,
        -0.15210875868797302,
        -0.973206639289856,
        -1.3660154342651367,
        0.6009261012077332,
        0.3739701211452484,
        -0.3459306061267853,
        -0.604501485824585,
        0.02331583946943283,
        -0.9112911820411682,
        -0.24361886084079742,
        -0.6294224262237549,
        -0.23665915429592133,
        -0.6144459843635559,
        0.2149689793586731,
        0.08084770292043686,
        -0.7951807379722595,
        -0.13610853254795074,
        0.2108193337917328,
        -0.19436010718345642,
        0.19061154127120972,
        0.03152681142091751,
        1.0697213411331177,
        0.33722984790802,
        -0.042306751012802124,
        -0.3571314215660095,
        -0.7294371724128723,
        -0.007013089954853058,
        -0.35074520111083984,
        -0.3466496467590332,
        0.16222761571407318,
        -0.826068639755249,
        -0.1491946429014206,
        -0.09274616837501526,
        0.0790897086262703,
        0.792121171951294,
        -3.0548956394195557,
        -0.400522917509079,
        0.43173208832740784,
        -0.06449466198682785,
        -0.22303710877895355,
        -0.017658580094575882,
        0.9619412422180176,
        -0.335166871547699,
        -0.15848682820796967,
        0.06872064620256424,
        -0.38105061650276184,
        0.2755086421966553,
        0.3636668920516968,
        -0.0327964723110199,
        0.13256147503852844,
        0.015341546386480331,
        0.5367769002914429,
        0.09585221111774445,
        0.18365751206874847,
        0.06982048600912094,
        -0.09536363929510117,
        0.05573367327451706,
        -0.6909676790237427,
        0.2509189248085022,
        -0.009305495768785477,
        0.2167123258113861,
        0.17178449034690857,
        -0.1936406046152115,
        -0.42794474959373474,
        -0.2977253496646881,
        -0.20610958337783813,
        -0.20432355999946594,
        0.0526542142033577,
        0.12465494871139526,
        -0.15323564410209656,
        0.5003378391265869,
        -0.6248694062232971,
        3.632815361022949,
        0.7546949982643127,
        0.3894161581993103,
        0.38667431473731995,
        -1.0901623964309692,
        0.9958512187004089,
        -0.6440386176109314,
        -0.0518525093793869,
        -0.8069523572921753,
        -0.32354938983917236,
        -0.6347129940986633,
        0.27081045508384705,
        -1.0631994009017944,
        0.050249360501766205,
        0.2775450646877289,
        0.12240526080131531,
        0.3905550241470337,
        -0.42220306396484375,
        -0.061704088002443314,
        -0.10008376091718674,
        0.9178372621536255,
        -0.521937906742096,
        -0.6538340449333191,
        -0.14399218559265137,
        -0.14983582496643066,
        0.14236116409301758,
        0.3624586760997772,
        -0.04538925737142563,
        0.775829017162323,
        -0.4147634208202362,
        0.19957773387432098,
        -0.21395182609558105,
        -0.15922367572784424,
        0.08512884378433228,
        -0.12948398292064667,
        0.7358691096305847,
        0.27441298961639404,
        0.7212930917739868,
        -0.77409827709198,
        -0.0017451085150241852,
        -0.5680257678031921,
        -0.16953730583190918,
        -0.7255210280418396,
        0.6378254294395447,
        -0.698478102684021,
        1.1647300720214844,
        0.44444382190704346,
        -0.21731647849082947,
        0.04620969295501709,
        -0.34939104318618774,
        -0.13414643704891205,
        -0.3581489026546478,
        0.3158942461013794,
        -0.5785800814628601,
        0.7283209562301636,
        0.541029691696167,
        0.2610771358013153,
        -0.5811299085617065,
        -0.12406304478645325,
        -0.8845553994178772,
        0.4768051505088806,
        0.5207589864730835,
        0.006869887001812458,
        -0.16135281324386597,
        -0.0946243405342102,
        0.29083380103111267,
        -0.2652333080768585,
        0.21855276823043823,
        -0.317123681306839,
        -0.37215861678123474,
        0.23869659006595612,
        -0.33674341440200806,
        -0.2434278130531311,
        0.4428253769874573,
        0.654378354549408,
        -0.6318531632423401,
        -0.17118358612060547,
        0.6328083276748657,
        0.44297948479652405,
        0.556077778339386,
        -0.8860992193222046,
        0.9304420351982117,
        0.8711252212524414,
        0.40416446328163147,
        -0.7560321092605591,
        0.05275407433509827,
        -0.2005174458026886,
        0.2519476115703583,
        0.3033190369606018,
        0.582554280757904,
        0.6227854490280151,
        -1.1359139680862427,
        1.6474529504776,
        -0.4993034601211548,
        0.08217941224575043,
        0.28086355328559875,
        0.12088015675544739,
        0.4010457396507263,
        0.46205615997314453,
        -0.17997321486473083,
        0.2022055685520172,
        -1.3730700016021729,
        -0.14863605797290802,
        -0.201754629611969,
        -0.020037397742271423,
        -0.8078768849372864,
        -0.48935219645500183,
        0.4854258596897125,
        0.6689891815185547,
        0.019748423248529434,
        -0.1151496171951294,
        -0.9870592951774597,
        -0.07308338582515717,
        1.2876079082489014,
        -0.018442802131175995,
        0.8650449514389038,
        0.10024590790271759,
        0.18611548840999603,
        0.3839987814426422,
        -0.6954162120819092,
        0.2134055197238922,
        0.014101855456829071,
        0.6021453142166138,
        -0.6670225262641907,
        -0.9058166742324829,
        -0.5793954133987427,
        0.5848833918571472,
        -0.6039261817932129,
        0.8731384873390198,
        -0.8205717206001282,
        -0.18922020494937897,
        0.395325243473053,
        0.8434609770774841,
        0.9915462732315063,
        0.9100232124328613,
        -0.25152328610420227,
        0.5939976572990417,
        -0.5167902708053589,
        0.863217830657959,
        -0.1809144914150238,
        -0.7336636185646057,
        0.6892756819725037,
        -0.18095114827156067,
        0.28456878662109375,
        0.31203824281692505,
        0.08489255607128143,
        0.5694563388824463,
        -0.662442684173584,
        -0.036292023956775665,
        -0.19391849637031555,
        0.467349648475647,
        1.0390907526016235,
        -0.13695871829986572,
        0.06902094185352325,
        0.4576749801635742,
        -0.16031458973884583,
        0.05386563763022423,
        -1.3128018379211426,
        -0.3622994124889374,
        -0.62032550573349,
        0.4836958944797516,
        0.29735466837882996,
        0.15698564052581787,
        0.8070566058158875,
        0.02806023880839348,
        0.06615661829710007,
        -0.18708190321922302,
        -0.31322741508483887,
        -0.4538048505783081,
        -0.25801119208335876,
        0.26182663440704346,
        -0.005223787855356932,
        -0.16256685554981232,
        -0.3755301237106323,
        -0.21090441942214966,
        -0.22826917469501495,
        0.7437816858291626,
        -0.3698306977748871,
        0.30099233984947205,
        -0.24573779106140137,
        -0.2838038206100464,
        0.2382895052433014,
        -0.010250241495668888,
        -0.5647792220115662,
        0.2319454401731491,
        -0.0559612400829792,
        0.11201941967010498,
        -0.5385231971740723,
        -1.2661898136138916,
        -0.5018736720085144,
        1.086037278175354,
        0.14476534724235535,
        -0.35740265250205994,
        -0.8162885904312134,
        0.12750811874866486,
        1.3342900276184082,
        0.2628169059753418,
        0.3289727568626404,
        1.2913036346435547,
        0.8064175844192505,
        0.026709020137786865,
        0.20611563324928284,
        -0.09154661744832993,
        -0.07774077355861664,
        0.5238714218139648,
        -0.245439350605011,
        -0.5978232026100159,
        0.16178834438323975,
        0.023505087941884995,
        -0.293025404214859,
        -0.03826827183365822,
        -0.7961131930351257,
        -0.04441976174712181,
        -0.329587459564209,
        -0.4042113423347473,
        0.15672235190868378,
        -0.5366429686546326,
        0.6025351881980896,
        1.309417486190796,
        0.44280490279197693,
        -1.723032832145691,
        -0.8121218681335449,
        0.4412146806716919,
        -0.16124950349330902,
        0.02189081907272339,
        -0.5913185477256775,
        0.10759815573692322,
        -0.1897290050983429,
        -0.11516062915325165,
        -0.4727622866630554,
        1.100631833076477,
        0.419914573431015,
        0.3632235527038574,
        0.8112086653709412,
        0.11451923102140427,
        0.6890847682952881,
        -0.6884059906005859,
        0.6630567312240601,
        -0.13152390718460083,
        -0.15067005157470703,
        -0.4696059823036194,
        -0.1801663637161255,
        2.1146557331085205,
        0.1960924118757248,
        0.2205081284046173,
        0.31175127625465393,
        0.3557843267917633,
        -0.59917813539505,
        -0.711094319820404,
        0.25704294443130493,
        -0.03531987592577934,
        0.020435549318790436,
        0.7264451384544373,
        -0.22084610164165497,
        0.27561840415000916,
        0.8491054177284241,
        1.0117100477218628,
        -0.5101954340934753,
        -0.3268888592720032,
        -0.25191327929496765,
        1.4728257656097412,
        0.2190440148115158,
        -0.3880229592323303,
        -0.5027744770050049,
        0.1671815663576126,
        -0.09510480612516403,
        -0.16726888716220856,
        0.21776758134365082,
        -0.7076276540756226,
        -0.07584260404109955,
        0.2692355811595917,
        -0.11763674020767212,
        0.6330681443214417,
        0.40594106912612915,
        0.5970894694328308,
        0.947573721408844,
        0.1842326521873474,
        -0.13249844312667847,
        0.3691372573375702,
        0.12949810922145844,
        -0.3386448621749878,
        0.6098689436912537,
        -0.31162720918655396,
        -0.6908876895904541,
        -0.49006563425064087
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs the AI to produce the best possible output by thoroughly analyzing and understanding the input. It emphasizes deep contemplation of the input's meaning and the sender's intentions. The expected output is an optimal response tailored to the inferred desires of the input provider.",
          "name": "Raw_query",
          "raw": "\n                workflow Raw_query v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are a universal AI that yields the best possible result given the input.\n\n# GOAL\n\n- Fully digest the input.\n\n- Deeply contemplate the input and what it means and what the sender likely wanted you to do with it.\n\n# OUTPUT\n\n- Output the best possible output based on your understanding of what was likely wanted.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are a universal AI that yields the best possible result given the input.\n\n# GOAL\n\n- Fully digest the input.\n\n- Deeply contemplate the input and what it means and what the sender likely wanted you to do with it.\n\n# OUTPUT\n\n- Output the best possible output based on your understanding of what was likely wanted.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        1.22632896900177,
        0.29437386989593506,
        -0.15146765112876892,
        -0.3966172933578491,
        -0.41946741938591003,
        0.5809120535850525,
        -0.10070913285017014,
        -0.29225894808769226,
        -0.4103129208087921,
        0.035316355526447296,
        -0.5033432245254517,
        0.2428949475288391,
        0.004946294240653515,
        0.20865249633789062,
        0.49055737257003784,
        -0.03165622800588608,
        0.27218085527420044,
        -0.5470674633979797,
        -1.0072877407073975,
        -0.196473628282547,
        0.04388738423585892,
        0.6651490926742554,
        0.12218868732452393,
        0.5382575392723083,
        0.21305632591247559,
        -0.16974124312400818,
        0.19022434949874878,
        -0.36637479066848755,
        -0.6101343631744385,
        -1.4114296436309814,
        0.5103089213371277,
        1.1765754222869873,
        -0.3004601299762726,
        -0.9719042778015137,
        0.07217667996883392,
        -0.47295528650283813,
        -0.8097023963928223,
        0.5089457035064697,
        -0.36819878220558167,
        -0.08941314369440079,
        0.349281907081604,
        0.2663729190826416,
        -0.2520170211791992,
        0.5536461472511292,
        -0.17025698721408844,
        -0.9749323725700378,
        0.7888004183769226,
        -0.285101056098938,
        0.4945257902145386,
        0.07010600715875626,
        0.6238534450531006,
        -0.3257891535758972,
        0.2552530765533447,
        0.30551302433013916,
        -0.053885869681835175,
        -0.5365411043167114,
        -0.492327481508255,
        0.2601209878921509,
        0.15644420683383942,
        -0.498981237411499,
        -0.022959396243095398,
        0.1241925060749054,
        -2.728266954421997,
        0.7969273924827576,
        0.002621166408061981,
        0.28635430335998535,
        0.38324934244155884,
        0.3824596703052521,
        0.2487078309059143,
        -0.222571462392807,
        0.2704337239265442,
        0.17309579253196716,
        -0.7662447094917297,
        0.6505107879638672,
        0.042779646813869476,
        -0.3704635500907898,
        0.003787931054830551,
        -0.22685915231704712,
        0.043233368545770645,
        0.37574654817581177,
        0.13559158146381378,
        1.095698356628418,
        0.15309135615825653,
        -0.9719510674476624,
        -0.4858045279979706,
        -0.011381864547729492,
        -0.18817460536956787,
        -0.6452823877334595,
        -0.24747884273529053,
        0.07340357452630997,
        -0.3787344992160797,
        0.22685450315475464,
        0.18961556255817413,
        -0.41097700595855713,
        0.20936521887779236,
        0.5623478889465332,
        -0.12156446278095245,
        0.5131267309188843,
        0.8019858598709106,
        3.356566905975342,
        0.4633989930152893,
        0.3385518491268158,
        0.10434344410896301,
        -0.29710623621940613,
        0.5502002239227295,
        -0.5247830152511597,
        -0.2112673670053482,
        -0.1935691237449646,
        -0.7409176230430603,
        0.0860794261097908,
        0.7503749132156372,
        -0.6741742491722107,
        -0.6206691265106201,
        0.3208039402961731,
        1.2881190776824951,
        0.5274444818496704,
        -0.007901400327682495,
        0.09147921204566956,
        -0.8294606804847717,
        0.28445619344711304,
        -0.5136469602584839,
        -0.29925665259361267,
        -0.394961953163147,
        0.1521478295326233,
        -0.46694162487983704,
        -0.10014048218727112,
        -1.096591591835022,
        0.7131608724594116,
        0.344711035490036,
        -0.03285980597138405,
        -0.3586813509464264,
        0.24289967119693756,
        -0.99550461769104,
        0.043651282787323,
        0.2052634060382843,
        -0.4654730558395386,
        0.36885783076286316,
        -1.097000241279602,
        -0.200583815574646,
        -0.48837438225746155,
        -0.2695704698562622,
        -0.7124164700508118,
        1.270596981048584,
        -0.11716198921203613,
        1.2969814538955688,
        0.025523506104946136,
        -0.06528417766094208,
        -0.8593606948852539,
        -0.901887834072113,
        0.4104740023612976,
        0.133449986577034,
        0.42891594767570496,
        -0.24685038626194,
        -0.06588372588157654,
        1.1999599933624268,
        -0.270191490650177,
        -0.8773635625839233,
        -0.4740387797355652,
        -0.4035252332687378,
        0.6644744873046875,
        0.011798031628131866,
        -0.036552079021930695,
        0.7254153490066528,
        1.2854785919189453,
        0.2008981704711914,
        -0.9165916442871094,
        0.30980682373046875,
        -0.6547056436538696,
        0.7584263682365417,
        -0.5456826686859131,
        -0.05756291747093201,
        0.2522134482860565,
        -0.10680563747882843,
        1.3088877201080322,
        0.020495368167757988,
        -0.30247923731803894,
        -1.0031366348266602,
        0.4045293629169464,
        0.3635687530040741,
        -0.13526910543441772,
        1.0840415954589844,
        0.535205602645874,
        0.02426445484161377,
        -0.7528560161590576,
        -0.16123539209365845,
        0.1392485648393631,
        0.25117847323417664,
        0.4334903955459595,
        0.7881404161453247,
        1.4219037294387817,
        -0.14157253503799438,
        0.7506455779075623,
        -1.2451742887496948,
        0.06727752834558487,
        0.48051998019218445,
        0.33273154497146606,
        0.020977146923542023,
        -0.4815123677253723,
        0.2613448202610016,
        -0.26509612798690796,
        -0.684440553188324,
        -1.0259209871292114,
        -0.22158879041671753,
        0.02838864177465439,
        -0.46882164478302,
        -1.1001578569412231,
        0.2197432667016983,
        -0.2510702610015869,
        -0.768290638923645,
        -1.0629726648330688,
        -0.46048635244369507,
        -0.1304539442062378,
        0.720535933971405,
        -0.20076674222946167,
        0.2303735315799713,
        0.40060627460479736,
        0.5175307393074036,
        0.17266739904880524,
        0.6040604710578918,
        0.8641245365142822,
        -0.16843003034591675,
        -0.2950091063976288,
        -1.179005742073059,
        -0.7398495674133301,
        -0.5135985016822815,
        0.1311345249414444,
        -0.909153163433075,
        -0.2345741242170334,
        -0.11218355596065521,
        -0.15725788474082947,
        0.2361384630203247,
        0.16723769903182983,
        0.656315803527832,
        0.44959840178489685,
        -0.18537655472755432,
        0.3097531199455261,
        0.33651602268218994,
        0.0591532401740551,
        0.6877473592758179,
        -1.0711523294448853,
        0.06876955181360245,
        0.09391304850578308,
        -0.3035287857055664,
        -0.329217791557312,
        0.14176306128501892,
        0.3071240484714508,
        -0.15649007260799408,
        0.16702565550804138,
        0.07160887122154236,
        0.8514833450317383,
        0.5590920448303223,
        0.8513740301132202,
        0.21181343495845795,
        -0.12819847464561462,
        -0.13143235445022583,
        0.046816386282444,
        -2.3904154300689697,
        -0.4746910035610199,
        -0.08679435402154922,
        0.5141916275024414,
        -0.17453336715698242,
        0.011777184903621674,
        -0.24618715047836304,
        0.130287766456604,
        0.50565505027771,
        0.6655835509300232,
        -0.9477126598358154,
        -0.4464726150035858,
        -0.3280756175518036,
        -0.27826404571533203,
        -0.039565522223711014,
        0.7922872304916382,
        -0.16287125647068024,
        0.1988755166530609,
        -0.3315944969654083,
        0.35096558928489685,
        0.4938281178474426,
        -0.22417321801185608,
        -0.23187151551246643,
        -0.4175053536891937,
        -0.5131270885467529,
        0.10300368070602417,
        -0.042571619153022766,
        0.11927486211061478,
        -0.29356956481933594,
        -0.024747412651777267,
        0.05764511600136757,
        -0.6169627904891968,
        -0.5420505404472351,
        0.3627500534057617,
        -0.8814774751663208,
        0.038951992988586426,
        -0.382072776556015,
        -0.10052581131458282,
        1.7875893115997314,
        0.14099732041358948,
        0.32886049151420593,
        0.9564748406410217,
        0.08696508407592773,
        -0.09732430428266525,
        -0.42614099383354187,
        0.01732821576297283,
        -0.3955755829811096,
        -0.27576830983161926,
        -0.7341748476028442,
        -0.006378494203090668,
        0.5185761451721191,
        -0.12317071855068207,
        -0.36873239278793335,
        0.23747016489505768,
        -0.8448199033737183,
        0.19700847566127777,
        0.11338864266872406,
        0.9117647409439087,
        0.907985508441925,
        -0.27350905537605286,
        0.4569211006164551,
        1.969170331954956,
        -0.08064310252666473,
        -1.7758100032806396,
        0.02327624149620533,
        0.3614496886730194,
        0.41370660066604614,
        0.08515733480453491,
        -0.18433722853660583,
        0.5880551338195801,
        0.04377339780330658,
        -0.03720630332827568,
        0.4858561158180237,
        1.1424627304077148,
        1.1931748390197754,
        0.013609013520181179,
        -0.0535261332988739,
        -0.009109176695346832,
        1.30600905418396,
        -0.1521250158548355,
        0.7786468863487244,
        0.3387255072593689,
        -0.4531831443309784,
        -0.7356833219528198,
        0.036300674080848694,
        1.1904535293579102,
        0.3233376145362854,
        -0.09827332198619843,
        -0.20050330460071564,
        0.47795185446739197,
        -0.5143018364906311,
        -0.8889390826225281,
        0.5273346900939941,
        0.8773371577262878,
        0.0013649184256792068,
        0.00756008829921484,
        -0.5079517364501953,
        -0.015666045248508453,
        0.6081270575523376,
        0.5024831891059875,
        -0.3770100176334381,
        0.07448747754096985,
        -0.5789021253585815,
        1.5337562561035156,
        -0.38830599188804626,
        -0.49700406193733215,
        0.13136640191078186,
        0.019741330295801163,
        0.5851430892944336,
        -0.31311482191085815,
        0.9846986532211304,
        -0.051645781844854355,
        -0.334289014339447,
        0.23054727911949158,
        -0.003589976578950882,
        -0.6844000816345215,
        0.06433292478322983,
        0.7699703574180603,
        0.49446791410446167,
        -0.15466178953647614,
        -0.19250936806201935,
        0.5648418664932251,
        0.008635886013507843,
        -0.7135694026947021,
        -0.2089938521385193,
        0.14083798229694366,
        -1.1028600931167603,
        -0.5388002395629883
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Recommends a personalized festival schedule featuring artists similar to the user's preferences in EDM genres and artists. The recommendation process involves analyzing the user's favorite styles and artists, then selecting similar artists and explaining the choices. The output is a detailed schedule organized by day, set time, stage, and artist, optimized for the user's enjoyment.",
          "name": "Recommend_artists",
          "raw": "\n                workflow Recommend_artists v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an EDM expert who specializes in identifying artists that I will like based on the input of a list of artists at a festival. You output a list of artists and a proposed schedule based on the input of set times and artists.\n\n# GOAL \n\n- Recommend the perfect list of people and schedule to see at a festival that I'm most likely to enjoy.\n\n# STEPS\n\n- Look at the whole list of artists.\n\n- Look at my list of favorite styles and artists below.\n\n- Recommend similar artists, and the reason you think I will like them.\n\n# MY FAVORITE STYLES AND ARTISTS\n\n### Styles\n\n- Dark menacing techno\n- Hard techno\n- Intricate minimal techno\n- Hardstyle that sounds dangerous\n\n### Artists\n\n- Sarah Landry\n- Fisher\n- Boris Brejcha\n- Technoboy\n\n- Optimize your selections based on how much I'll love the artists, not anything else.\n\n- If the artist themselves are playing, make sure you have them on the schedule.\n\n# OUTPUT\n\n- Output a schedule of where to be and when based on the best matched artists, along with the explanation of why them.\n\n- Organize the output format by day, set time, then stage, then artist.\n\n- Optimize your selections based on how much I'll love the artists, not anything else.\n\n- Output in Markdown, but make it easy to read in text form, so no asterists, bold or italic.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an EDM expert who specializes in identifying artists that I will like based on the input of a list of artists at a festival. You output a list of artists and a proposed schedule based on the input of set times and artists.\n\n# GOAL \n\n- Recommend the perfect list of people and schedule to see at a festival that I'm most likely to enjoy.\n\n# STEPS\n\n- Look at the whole list of artists.\n\n- Look at my list of favorite styles and artists below.\n\n- Recommend similar artists, and the reason you think I will like them.\n\n# MY FAVORITE STYLES AND ARTISTS\n\n### Styles\n\n- Dark menacing techno\n- Hard techno\n- Intricate minimal techno\n- Hardstyle that sounds dangerous\n\n### Artists\n\n- Sarah Landry\n- Fisher\n- Boris Brejcha\n- Technoboy\n\n- Optimize your selections based on how much I'll love the artists, not anything else.\n\n- If the artist themselves are playing, make sure you have them on the schedule.\n\n# OUTPUT\n\n- Output a schedule of where to be and when based on the best matched artists, along with the explanation of why them.\n\n- Organize the output format by day, set time, then stage, then artist.\n\n- Optimize your selections based on how much I'll love the artists, not anything else.\n\n- Output in Markdown, but make it easy to read in text form, so no asterists, bold or italic.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.38228192925453186,
        0.25853854417800903,
        0.0606875978410244,
        0.35499000549316406,
        -0.08633975684642792,
        0.08026157319545746,
        -0.31331226229667664,
        0.38015586137771606,
        -0.5441687703132629,
        0.3460270166397095,
        -0.6526795029640198,
        0.19193203747272491,
        0.3113234341144562,
        0.1106749027967453,
        0.1373007595539093,
        0.22170010209083557,
        0.34642767906188965,
        -1.113224744796753,
        -1.395801067352295,
        -0.5271530747413635,
        0.30325111746788025,
        0.407460480928421,
        0.35827189683914185,
        -0.17923179268836975,
        0.5130931735038757,
        -0.09824290871620178,
        -0.09854505956172943,
        -0.5186634659767151,
        -0.4474073648452759,
        -1.4859672784805298,
        0.2543031573295593,
        0.3083916902542114,
        -0.26183971762657166,
        -0.3404582738876343,
        0.29396605491638184,
        -0.6162755489349365,
        0.24597518146038055,
        -0.11246679723262787,
        -0.1971724033355713,
        -0.33150285482406616,
        0.033816613256931305,
        -0.027583934366703033,
        -0.1463920772075653,
        0.09642744809389114,
        0.41325798630714417,
        -0.30167409777641296,
        0.6466266512870789,
        -0.4042348563671112,
        0.10336778312921524,
        0.07701286673545837,
        0.013520762324333191,
        -0.3977833092212677,
        0.1460895538330078,
        -0.6689612865447998,
        -0.49918267130851746,
        0.41785359382629395,
        0.21539117395877838,
        -0.8548985123634338,
        0.43461722135543823,
        0.6426057815551758,
        0.41241100430488586,
        -0.0727575421333313,
        -3.2976181507110596,
        0.017159678041934967,
        0.8885841369628906,
        -0.5011866092681885,
        0.5111393928527832,
        -0.2391989529132843,
        0.3215351998806,
        0.6785625219345093,
        0.28361237049102783,
        0.13366489112377167,
        -0.6143960356712341,
        0.10530583560466766,
        -0.001422382891178131,
        -0.311109334230423,
        0.6992229223251343,
        0.5419924855232239,
        0.4736284613609314,
        -0.6220853924751282,
        0.3667183518409729,
        1.020186424255371,
        0.2115381360054016,
        0.02247650735080242,
        -0.40486782789230347,
        0.46818968653678894,
        -0.38546136021614075,
        -0.36433324217796326,
        -0.03419649600982666,
        0.08940982073545456,
        -0.9612842798233032,
        -0.3794473707675934,
        -0.1458989381790161,
        0.0844474509358406,
        -0.4738190770149231,
        0.6020056009292603,
        0.2542261481285095,
        0.4567379653453827,
        0.07707065343856812,
        3.5313405990600586,
        0.4655778408050537,
        -0.3488890528678894,
        0.9981887936592102,
        -0.8473050594329834,
        0.03361150622367859,
        -0.4248313009738922,
        0.6876270771026611,
        -0.42994487285614014,
        0.06006616726517677,
        0.1468646079301834,
        0.2484866976737976,
        -0.6409860253334045,
        0.04614757001399994,
        -0.006958547979593277,
        0.25578662753105164,
        0.46438995003700256,
        -0.1361827254295349,
        -0.007080785930156708,
        -0.10843129456043243,
        0.7465198040008545,
        -0.5384717583656311,
        -0.2798793315887451,
        -0.3120098412036896,
        -0.0986238345503807,
        -0.5092969536781311,
        0.46497517824172974,
        0.022908974438905716,
        0.5839025974273682,
        0.17659126222133636,
        0.21429593861103058,
        0.1419162154197693,
        -0.05825018510222435,
        -0.38057562708854675,
        0.27146396040916443,
        -0.10591086745262146,
        -0.18285620212554932,
        0.31788957118988037,
        -0.11455197632312775,
        -0.5135840773582458,
        -0.6244024634361267,
        -0.4303710460662842,
        -0.23984310030937195,
        0.7967531085014343,
        -0.3511523902416229,
        0.6222653985023499,
        0.28627726435661316,
        -0.4573139250278473,
        0.2639990448951721,
        -0.6094927787780762,
        -0.21025502681732178,
        0.09686462581157684,
        -0.2614384889602661,
        -0.11610504984855652,
        -0.2844875454902649,
        0.9711193442344666,
        -0.19969122111797333,
        -0.389055073261261,
        0.17942604422569275,
        -0.7693468332290649,
        0.2594151198863983,
        0.03833182528614998,
        0.4192187488079071,
        0.4531621038913727,
        0.3591533303260803,
        0.12191502004861832,
        -0.372842937707901,
        0.358762264251709,
        0.2546345889568329,
        1.0260496139526367,
        0.4641975462436676,
        0.05027296394109726,
        -0.27387866377830505,
        0.9273656606674194,
        -0.07773385941982269,
        0.38530048727989197,
        0.002392929047346115,
        -0.11293785274028778,
        -0.1257651150226593,
        0.22836443781852722,
        -0.22866475582122803,
        1.0647953748703003,
        0.5337010025978088,
        0.2026609629392624,
        -0.8931376934051514,
        -0.21208028495311737,
        0.08378826826810837,
        0.05452202260494232,
        -0.2445758581161499,
        0.6531052589416504,
        0.23227559030056,
        -0.07444434612989426,
        2.0627448558807373,
        -0.8010497689247131,
        0.08166329562664032,
        0.5056293606758118,
        0.021571937948465347,
        0.19202207028865814,
        0.6060516834259033,
        0.32918456196784973,
        -0.31251463294029236,
        -1.075573444366455,
        -0.5106051564216614,
        0.19372369349002838,
        0.28865376114845276,
        0.2206895649433136,
        -0.11441062390804291,
        0.4364420473575592,
        0.6203569769859314,
        -0.7876893281936646,
        -0.14920522272586823,
        -0.5733988881111145,
        -0.12021277844905853,
        1.0012223720550537,
        0.5828676223754883,
        0.8460060358047485,
        -0.30677634477615356,
        -0.17574308812618256,
        -0.06681406497955322,
        0.642288088798523,
        -0.1336175799369812,
        0.09532901644706726,
        0.09884515404701233,
        -1.0657159090042114,
        -0.7624735236167908,
        -0.576450765132904,
        0.32462078332901,
        -0.0757220983505249,
        -0.7648259401321411,
        -1.084363579750061,
        -0.16694988310337067,
        0.23782411217689514,
        0.4210960268974304,
        0.2354537844657898,
        1.1488336324691772,
        0.6193767786026001,
        -0.0072005316615104675,
        -0.07771732658147812,
        0.6048316955566406,
        -0.26643282175064087,
        -0.16775238513946533,
        1.4172863960266113,
        -0.1871451735496521,
        -0.5076157450675964,
        -0.04225831851363182,
        0.14151881635189056,
        0.843392014503479,
        -0.7651804089546204,
        -0.8194580674171448,
        -0.13674242794513702,
        1.2178065776824951,
        0.40412527322769165,
        0.15356536209583282,
        -0.5495732426643372,
        0.46914535760879517,
        -0.26119157671928406,
        0.09692729264497757,
        -1.7791764736175537,
        -0.19599159061908722,
        -0.6980614066123962,
        0.03750128671526909,
        -0.18387073278427124,
        -0.05139939486980438,
        0.22920073568820953,
        -0.1894974559545517,
        -0.15690185129642487,
        0.20177824795246124,
        -1.221071481704712,
        -0.7062970399856567,
        -0.64652019739151,
        -0.36869263648986816,
        0.0769951194524765,
        -0.437677800655365,
        -0.37672489881515503,
        -0.10245249420404434,
        0.09070950746536255,
        0.19146093726158142,
        0.23598462343215942,
        -0.17991793155670166,
        -0.9102128744125366,
        0.43313419818878174,
        0.1575789451599121,
        0.3563646078109741,
        -0.536568820476532,
        0.00020575523376464844,
        -0.31773415207862854,
        0.06397134810686111,
        -0.3040553629398346,
        -0.6774740219116211,
        0.14699026942253113,
        0.553203821182251,
        -0.284536212682724,
        -1.0029393434524536,
        -0.7976495027542114,
        -0.5830070376396179,
        1.2022881507873535,
        0.7322500944137573,
        -0.3370867967605591,
        1.1024117469787598,
        0.45899027585983276,
        -0.35005033016204834,
        -0.138556569814682,
        0.13420501351356506,
        -0.03452972322702408,
        0.5410086512565613,
        -0.041426882147789,
        -0.5744236707687378,
        0.15078997611999512,
        0.06590931117534637,
        0.05398281663656235,
        0.2788752317428589,
        -0.8982987999916077,
        0.05799528583884239,
        0.18278487026691437,
        -0.1414586454629898,
        0.7128236293792725,
        -0.0902501568198204,
        -0.4395698308944702,
        1.0137313604354858,
        -0.14187657833099365,
        -1.7957539558410645,
        -0.21836237609386444,
        0.2063160240650177,
        0.39483004808425903,
        -0.6721547245979309,
        -0.5774772763252258,
        0.33003008365631104,
        -0.15549242496490479,
        -0.026689425110816956,
        -0.5132626295089722,
        2.0359158515930176,
        1.0943048000335693,
        -0.6469748020172119,
        -1.208479642868042,
        0.39013564586639404,
        0.5682762265205383,
        0.3095366656780243,
        0.32264164090156555,
        -0.2651984989643097,
        0.03777962177991867,
        -0.08504621684551239,
        0.1657727211713791,
        1.4738080501556396,
        -0.08957655727863312,
        0.3439618945121765,
        -0.03921359032392502,
        -0.37321627140045166,
        -1.0169378519058228,
        -0.8907994031906128,
        0.6495567560195923,
        0.15274682641029358,
        -0.266256183385849,
        0.7310179471969604,
        -0.6145850419998169,
        0.3929782211780548,
        0.35856014490127563,
        0.5744537115097046,
        -0.11921301484107971,
        -0.07730399817228317,
        -0.4868701696395874,
        2.276719093322754,
        0.2655009925365448,
        -0.256265789270401,
        -0.15581607818603516,
        0.6740528345108032,
        0.26179301738739014,
        -0.2347651571035385,
        0.6427627801895142,
        -0.22676405310630798,
        -0.04615062475204468,
        0.17963965237140656,
        0.21867671608924866,
        -0.40494006872177124,
        0.6773065328598022,
        0.14008119702339172,
        0.5841488838195801,
        0.05891725420951843,
        -0.15799283981323242,
        0.7362122535705566,
        -0.07905566692352295,
        -0.5071600079536438,
        0.18317538499832153,
        -1.0892397165298462,
        -0.5629854202270508,
        -0.5764696598052979
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Create a visual representation of the functionalities provided by the Fabric project, focusing on augmenting human capabilities with AI. The approach involves breaking down the project's capabilities into categories like summarization, analysis, and more, with specific patterns branching from these categories. The expected output is comprehensive Markmap code detailing this functionality map.",
          "name": "Show_fabric_options_markmap",
          "raw": "\n                workflow Show_fabric_options_markmap v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND GOALS\n\nYou are an advanced UI builder that shows a visual representation of functionality that's provided to you via the input.\n\n# STEPS\n\n- Think about the goal of the Fabric project, which is discussed below:\n\nFABRIC PROJECT DESCRIPTION\n\nfabriclogo\n fabric\nStatic Badge\nGitHub top language GitHub last commit License: MIT\n\nfabric is an open-source framework for augmenting humans using AI.\n\nIntroduction Video • What and Why • Philosophy • Quickstart • Structure • Examples • Custom Patterns • Helper Apps • Examples • Meta\n\nNavigation\n\nIntroduction Videos\nWhat and Why\nPhilosophy\nBreaking problems into components\nToo many prompts\nThe Fabric approach to prompting\nQuickstart\nSetting up the fabric commands\nUsing the fabric client\nJust use the Patterns\nCreate your own Fabric Mill\nStructure\nComponents\nCLI-native\nDirectly calling Patterns\nExamples\nCustom Patterns\nHelper Apps\nMeta\nPrimary contributors\n\nNote\n\nWe are adding functionality to the project so often that you should update often as well. That means: git pull; pipx install . --force; fabric --update; source ~/.zshrc (or ~/.bashrc) in the main directory!\nMarch 13, 2024 — We just added pipx install support, which makes it way easier to install Fabric, support for Claude, local models via Ollama, and a number of new Patterns. Be sure to update and check fabric -h for the latest!\n\nIntroduction videos\n\nNote\n\nThese videos use the ./setup.sh install method, which is now replaced with the easier pipx install . method. Other than that everything else is still the same.\n fabric_intro_video\n\n Watch the video\nWhat and why\n\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but it's not easy to integrate this functionality into our lives.\n\nIn other words, AI doesn't have a capabilities problem—it has an integration problem.\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\n\nPhilosophy\n\nAI isn't a thing; it's a magnifier of a thing. And that thing is human creativity.\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the human problems we want to solve.\n\nBreaking problems into components\n\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\naugmented_challenges\nToo many prompts\n\nPrompts are good for this, but the biggest challenge I faced in 2023——which still exists today—is the sheer number of AI prompts out there. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, and manage different versions of the ones we like.\n\nOne of fabric's primary features is helping people collect and integrate prompts, which we call Patterns, into various parts of their lives.\n\nFabric has Patterns for all sorts of life and work activities, including:\n\nExtracting the most interesting parts of YouTube videos and podcasts\nWriting an essay in your own voice with just an idea as an input\nSummarizing opaque academic papers\nCreating perfectly matched AI art prompts for a piece of writing\nRating the quality of content to see if you want to read/watch the whole thing\nGetting summaries of long, boring content\nExplaining code to you\nTurning bad documentation into usable documentation\nCreating social media posts from any content input\nAnd a million more…\nOur approach to prompting\n\nFabric Patterns are different than most prompts you'll see.\n\nFirst, we use Markdown to help ensure maximum readability and editability. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. Importantly, this also includes the AI you're sending it to!\nHere's an example of a Fabric Pattern.\n\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\npattern-example\nNext, we are extremely clear in our instructions, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\nAnd finally, we tend to use the System section of the prompt almost exclusively. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\nQuickstart\n\nThe most feature-rich way to use Fabric is to use the fabric client, which can be found under /client directory in this repository.\n\nSetting up the fabric commands\n\nFollow these steps to get all fabric related apps installed and configured.\n\nNavigate to where you want the Fabric project to live on your system in a semi-permanent place on your computer.\n# Find a home for Fabric\ncd /where/you/keep/code\nClone the project to your computer.\n# Clone Fabric to your computer\ngit clone https://github.com/danielmiessler/fabric.git\nEnter Fabric's main directory\n# Enter the project folder (where you cloned it)\ncd fabric\nInstall pipx:\nmacOS:\n\nbrew install pipx\nLinux:\n\nsudo apt install pipx\nWindows:\n\nUse WSL and follow the Linux instructions.\n\nInstall fabric\npipx install .\nRun setup:\nfabric --setup\nRestart your shell to reload everything.\n\nNow you are up and running! You can test by running the help.\n\n# Making sure the paths are set up correctly\nfabric --help\nNote\n\nIf you're using the server functions, fabric-api and fabric-webui need to be run in distinct terminal windows.\nUsing the fabric client\n\nOnce you have it all set up, here's how to use it.\n\nCheck out the options fabric -h\nus the results in\n                        realtime. NOTE: You will not be able to pipe the\n                        output into another command.\n  --list, -l            List available patterns\n  --clear               Clears your persistent model choice so that you can\n                        once again use the --model flag\n  --update, -u          Update patterns. NOTE: This will revert the default\n                        model to gpt4-turbo. please run --changeDefaultModel\n                        to once again set default model\n  --pattern PATTERN, -p PATTERN\n                        The pattern (prompt) to use\n  --setup               Set up your fabric instance\n  --changeDefaultModel CHANGEDEFAULTMODEL\n                        Change the default model. For a list of available\n                        models, use the --listmodels flag.\n  --model MODEL, -m MODEL\n                        Select the model to use. NOTE: Will not work if you\n                        have set a default model. please use --clear to clear\n                        persistence before using this flag\n  --listmodels          List all available models\n  --remoteOllamaServer REMOTEOLLAMASERVER\n                        The URL of the remote ollamaserver to use. ONLY USE\n                        THIS if you are using a local ollama server in an non-\n                        deault location or port\n  --context, -c         Use Context file (context.md) to add context to your\n                        pattern\nage: fabric [-h] [--text TEXT] [--copy] [--agents {trip_planner,ApiKeys}]\n              [--output [OUTPUT]] [--stream] [--list] [--clear] [--update]\n              [--pattern PATTERN] [--setup]\n              [--changeDefaultModel CHANGEDEFAULTMODEL] [--model MODEL]\n              [--listmodels] [--remoteOllamaServer REMOTEOLLAMASERVER]\n              [--context]\n\nAn open source framework for augmenting humans using AI.\n\noptions:\n  -h, --help            show this help message and exit\n  --text TEXT, -t TEXT  Text to extract summary from\n  --copy, -C            Copy the response to the clipboard\n  --agents {trip_planner,ApiKeys}, -a {trip_planner,ApiKeys}\n                        Use an AI agent to help you with a task. Acceptable\n                        values are 'trip_planner' or 'ApiKeys'. This option\n                        cannot be used with any other flag.\n  --output [OUTPUT], -o [OUTPUT]\n                        Save the response to a file\n  --stream, -s          Use this option if you want to see\nExample commands\n\nThe client, by default, runs Fabric patterns without needing a server (the Patterns were downloaded during setup). This means the client connects directly to OpenAI using the input given and the Fabric pattern used.\n\nRun the summarize Pattern based on input from stdin. In this case, the body of an article.\npbpaste | fabric --pattern summarize\nRun the analyze_claims Pattern with the --stream option to get immediate and streaming results.\npbpaste | fabric --stream --pattern analyze_claims\nRun the extract_wisdom Pattern with the --stream option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\nyt --transcript https://youtube.com/watch?v=uXs-zPc63kM | fabric --stream --pattern extract_wisdom\nnew All of the patterns have been added as aliases to your bash (or zsh) config file\npbpaste | analyze_claims --stream\nNote\n\nMore examples coming in the next few days, including a demo video!\nJust use the Patterns\n\nfabric-patterns-screenshot\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the /patterns directory and start exploring!\n\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\n\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\n\nThe wisdom of crowds for the win.\n\nCreate your own Fabric Mill\n\nfabric_mill_architecture\nBut we go beyond just providing Patterns. We provide code for you to build your very own Fabric server and personal AI infrastructure!\n\nStructure\n\nFabric is themed off of, well… fabric—as in…woven materials. So, think blankets, quilts, patterns, etc. Here's the concept and structure:\n\nComponents\n\nThe Fabric ecosystem has three primary components, all named within this textile theme.\n\nThe Mill is the (optional) server that makes Patterns available.\nPatterns are the actual granular AI use cases (prompts).\nStitches are chained together Patterns that create advanced functionality (see below).\nLooms are the client-side apps that call a specific Pattern hosted by a Mill.\nCLI-native\n\nOne of the coolest parts of the project is that it's command-line native!\n\nEach Pattern you see in the /patterns directory can be used in any AI application you use, but you can also set up your own server using the /server code and then call APIs directly!\n\nOnce you're set up, you can do things like:\n\n# Take any idea from `stdin` and send it to the `/write_essay` API!\necho \\\"An idea that coding is like speaking with rules.\\\" | write_essay\nDirectly calling Patterns\n\nOne key feature of fabric and its Markdown-based format is the ability to _ directly reference_ (and edit) individual patterns directly—on their own—without surrounding code.\n\nAs an example, here's how to call the direct location of the extract_wisdom pattern.\n\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\nThis means you can cleanly, and directly reference any pattern for use in a web-based AI app, your own code, or wherever!\n\nEven better, you can also have your Mill functionality directly call system and user prompts from fabric, meaning you can have your personal AI ecosystem automatically kept up to date with the latest version of your favorite Patterns.\n\nHere's what that looks like in code:\n\nhttps://github.com/danielmiessler/fabric/blob/main/server/fabric_api_server.py\n# /extwis\n@app.route(\\\"/extwis\\\", methods=[\\\"POST\\\"])\n@auth_required  # Require authentication\ndef extwis():\n    data = request.get_json()\n\n    # Warn if there's no input\n    if \\\"input\\\" not in data:\n        return jsonify({\\\"error\\\": \\\"Missing input parameter\\\"}), 400\n\n    # Get data from client\n    input_data = data[\\\"input\\\"]\n\n    # Set the system and user URLs\n    system_url = \\\"https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/system.md\\\"\n    user_url = \\\"https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/user.md\\\"\n\n    # Fetch the prompt content\n    system_content = fetch_content_from_url(system_url)\n    user_file_content = fetch_content_from_url(user_url)\n\n    # Build the API call\n    system_message = {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_content}\n    user_message = {\\\"role\\\": \\\"user\\\", \\\"content\\\": user_file_content + \\\"\n\\\" + input_data}\n    messages = [system_message, user_message]\n    try:\n        response = openai.chat.completions.create(\n            model=\\\"gpt-4-1106-preview\\\",\n            messages=messages,\n            temperature=0.0,\n            top_p=1,\n            frequency_penalty=0.1,\n            presence_penalty=0.1,\n        )\n        assistant_message = response.choices[0].message.content\n        return jsonify({\\\"response\\\": assistant_message})\n    except Exception as e:\n        return jsonify({\\\"error\\\": str(e)}), 500\nExamples\n\nHere's an abridged output example from the extract_wisdom pattern (limited to only 10 items per section).\n\n# Paste in the transcript of a YouTube video of Riva Tez on David Perrel's podcast\npbpaste | extract_wisdom\n## SUMMARY:\n\nThe content features a conversation between two individuals discussing various topics, including the decline of Western culture, the importance of beauty and subtlety in life, the impact of technology and AI, the resonance of Rilke's poetry, the value of deep reading and revisiting texts, the captivating nature of Ayn Rand's writing, the role of philosophy in understanding the world, and the influence of drugs on society. They also touch upon creativity, attention spans, and the importance of introspection.\n\n## IDEAS:\n\n1. Western culture is perceived to be declining due to a loss of values and an embrace of mediocrity.\n2. Mass media and technology have contributed to shorter attention spans and a need for constant stimulation.\n3. Rilke's poetry resonates due to its focus on beauty and ecstasy in everyday objects.\n4. Subtlety is often overlooked in modern society due to sensory overload.\n5. The role of technology in shaping music and performance art is significant.\n6. Reading habits have shifted from deep, repetitive reading to consuming large quantities of new material.\n7. Revisiting influential books as one ages can lead to new insights based on accumulated wisdom and experiences.\n8. Fiction can vividly illustrate philosophical concepts through characters and narratives.\n9. Many influential thinkers have backgrounds in philosophy, highlighting its importance in shaping reasoning skills.\n10. Philosophy is seen as a bridge between theology and science, asking questions that both fields seek to answer.\n\n## QUOTES:\n\n1. \\\"You can't necessarily think yourself into the answers. You have to create space for the answers to come to you.\\\"\n2. \\\"The West is dying and we are killing her.\\\"\n3. \\\"The American Dream has been replaced by mass packaged mediocrity porn, encouraging us to revel like happy pigs in our own meekness.\\\"\n4. \\\"There's just not that many people who have the courage to reach beyond consensus and go explore new ideas.\\\"\n5. \\\"I'll start watching Netflix when I've read the whole of human history.\\\"\n6. \\\"Rilke saw beauty in everything... He sees it's in one little thing, a representation of all things that are beautiful.\\\"\n7. \\\"Vanilla is a very subtle flavor... it speaks to sort of the sensory overload of the modern age.\\\"\n8. \\\"When you memorize chapters [of the Bible], it takes a few months, but you really understand how things are structured.\\\"\n9. \\\"As you get older, if there's books that moved you when you were younger, it's worth going back and rereading them.\\\"\n10. \\\"She [Ayn Rand] took complicated philosophy and embodied it in a way that anybody could resonate with.\\\"\n\n## HABITS:\n\n1. Avoiding mainstream media consumption for deeper engagement with historical texts and personal research.\n2. Regularly revisiting influential books from youth to gain new insights with age.\n3. Engaging in deep reading practices rather than skimming or speed-reading material.\n4. Memorizing entire chapters or passages from significant texts for better understanding.\n5. Disengaging from social media and fast-paced news cycles for more focused thought processes.\n6. Walking long distances as a form of meditation and reflection.\n7. Creating space for thoughts to solidify through introspection and stillness.\n8. Embracing emotions such as grief or anger fully rather than suppressing them.\n9. Seeking out varied experiences across different careers and lifestyles.\n10. Prioritizing curiosity-driven research without specific goals or constraints.\n\n## FACTS:\n\n1. The West is perceived as declining due to cultural shifts away from traditional values.\n2. Attention spans have shortened due to technological advancements and media consumption habits.\n3. Rilke's poetry emphasizes finding beauty in everyday objects through detailed observation.\n4. Modern society often overlooks subtlety due to sensory overload from various stimuli.\n5. Reading habits have evolved from deep engagement with texts to consuming large quantities quickly.\n6. Revisiting influential books can lead to new insights based on accumulated life experiences.\n7. Fiction can effectively illustrate philosophical concepts through character development and narrative arcs.\n8. Philosophy plays a significant role in shaping reasoning skills and understanding complex ideas.\n9. Creativity may be stifled by cultural nihilism and protectionist attitudes within society.\n10. Short-term thinking undermines efforts to create lasting works of beauty or significance.\n\n## REFERENCES:\n\n1. Rainer Maria Rilke's poetry\n2. Netflix\n3. Underworld concert\n4. Katy Perry's theatrical performances\n5. Taylor Swift's performances\n6. Bible study\n7. Atlas Shrugged by Ayn Rand\n8. Robert Pirsig's writings\n9. Bertrand Russell's definition of philosophy\n10. Nietzsche's walks\nCustom Patterns\n\nYou can also use Custom Patterns with Fabric, meaning Patterns you keep locally and don't upload to Fabric.\n\nOne possible place to store them is ~/.config/custom-fabric-patterns.\n\nThen when you want to use them, simply copy them into ~/.config/fabric/patterns.\n\ncp -a ~/.config/custom-fabric-patterns/* ~/.config/fabric/patterns/`\nNow you can run them with:\n\npbpaste | fabric -p your_custom_pattern\nHelper Apps\n\nThese are helper tools to work with Fabric. Examples include things like getting transcripts from media files, getting metadata about media, etc.\n\nyt (YouTube)\n\nyt is a command that uses the YouTube API to pull transcripts, pull user comments, get video duration, and other functions. It's primary function is to get a transcript from a video that can then be stitched (piped) into other Fabric Patterns.\n\nusage: yt [-h] [--duration] [--transcript] [url]\n\nvm (video meta) extracts metadata about a video, such as the transcript and the video's duration. By Daniel Miessler.\n\npositional arguments:\n  url           YouTube video URL\n\noptions:\n  -h, --help    Show this help message and exit\n  --duration    Output only the duration\n  --transcript  Output only the transcript\n  --comments    Output only the user comments \nts (Audio transcriptions)\n\n'ts' is a command that uses the OpenApi Whisper API to transcribe audio files. Due to the context window, this tool uses pydub to split the files into 10 minute segments. for more information on pydub, please refer https://github.com/jiaaro/pydub\n\nInstallation\n\nmac:\nbrew install ffmpeg\n\nlinux:\napt install ffmpeg\n\nwindows:\ndownload instructions https://www.ffmpeg.org/download.html\nts -h\nusage: ts [-h] audio_file\n\nTranscribe an audio file.\n\npositional arguments:\n  audio_file  The path to the audio file to be transcribed.\n\noptions:\n  -h, --help  show this help message and exit\nSave\n\nsave is a \\\"tee-like\\\" utility to pipeline saving of content, while keeping the output stream intact. Can optionally generate \\\"frontmatter\\\" for PKM utilities like Obsidian via the \\\"FABRIC_FRONTMATTER\\\" environment variable\n\nIf you'd like to default variables, set them in ~/.config/fabric/.env. FABRIC_OUTPUT_PATH needs to be set so save where to write. FABRIC_FRONTMATTER_TAGS is optional, but useful for tracking how tags have entered your PKM, if that's important to you.\n\nusage\n\nusage: save [-h] [-t, TAG] [-n] [-s] [stub]\n\nsave: a \\\"tee-like\\\" utility to pipeline saving of content, while keeping the output stream intact. Can optionally generate \\\"frontmatter\\\" for PKM utilities like Obsidian via the\n\\\"FABRIC_FRONTMATTER\\\" environment variable\n\npositional arguments:\n  stub                stub to describe your content. Use quotes if you have spaces. Resulting format is YYYY-MM-DD-stub.md by default\n\noptions:\n  -h, --help          show this help message and exit\n  -t, TAG, --tag TAG  add an additional frontmatter tag. Use this argument multiple timesfor multiple tags\n  -n, --nofabric      don't use the fabric tags, only use tags from --tag\n  -s, --silent        don't use STDOUT for output, only save to the file\nExample\n\necho test | save --tag extra-tag stub-for-name\ntest\n\n$ cat ~/obsidian/Fabric/2024-03-02-stub-for-name.md\n---\ngeneration_date: 2024-03-02 10:43\ntags: fabric-extraction stub-for-name extra-tag\n---\ntest\n\nEND FABRIC PROJECT DESCRIPTION\n\n- Take the Fabric patterns given to you as input and think about how to create a Markmap visualization of everything you can do with Fabric.\n\nExamples: Analyzing videos, summarizing articles, writing essays, etc.\n\n- The visual should be broken down by the type of actions that can be taken, such as summarization, analysis, etc., and the actual patterns should branch from there. \n\n# OUTPUT\n\n- Output comprehensive Markmap code for displaying this functionality map as described above.\n\n- NOTE: This is Markmap, NOT Markdown.\n\n- Output the Markmap code and nothing else.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND GOALS\n\nYou are an advanced UI builder that shows a visual representation of functionality that's provided to you via the input.\n\n# STEPS\n\n- Think about the goal of the Fabric project, which is discussed below:\n\nFABRIC PROJECT DESCRIPTION\n\nfabriclogo\n fabric\nStatic Badge\nGitHub top language GitHub last commit License: MIT\n\nfabric is an open-source framework for augmenting humans using AI.\n\nIntroduction Video • What and Why • Philosophy • Quickstart • Structure • Examples • Custom Patterns • Helper Apps • Examples • Meta\n\nNavigation\n\nIntroduction Videos\nWhat and Why\nPhilosophy\nBreaking problems into components\nToo many prompts\nThe Fabric approach to prompting\nQuickstart\nSetting up the fabric commands\nUsing the fabric client\nJust use the Patterns\nCreate your own Fabric Mill\nStructure\nComponents\nCLI-native\nDirectly calling Patterns\nExamples\nCustom Patterns\nHelper Apps\nMeta\nPrimary contributors\n\nNote\n\nWe are adding functionality to the project so often that you should update often as well. That means: git pull; pipx install . --force; fabric --update; source ~/.zshrc (or ~/.bashrc) in the main directory!\nMarch 13, 2024 — We just added pipx install support, which makes it way easier to install Fabric, support for Claude, local models via Ollama, and a number of new Patterns. Be sure to update and check fabric -h for the latest!\n\nIntroduction videos\n\nNote\n\nThese videos use the ./setup.sh install method, which is now replaced with the easier pipx install . method. Other than that everything else is still the same.\n fabric_intro_video\n\n Watch the video\nWhat and why\n\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but it's not easy to integrate this functionality into our lives.\n\nIn other words, AI doesn't have a capabilities problem—it has an integration problem.\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\n\nPhilosophy\n\nAI isn't a thing; it's a magnifier of a thing. And that thing is human creativity.\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the human problems we want to solve.\n\nBreaking problems into components\n\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\naugmented_challenges\nToo many prompts\n\nPrompts are good for this, but the biggest challenge I faced in 2023——which still exists today—is the sheer number of AI prompts out there. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, and manage different versions of the ones we like.\n\nOne of fabric's primary features is helping people collect and integrate prompts, which we call Patterns, into various parts of their lives.\n\nFabric has Patterns for all sorts of life and work activities, including:\n\nExtracting the most interesting parts of YouTube videos and podcasts\nWriting an essay in your own voice with just an idea as an input\nSummarizing opaque academic papers\nCreating perfectly matched AI art prompts for a piece of writing\nRating the quality of content to see if you want to read/watch the whole thing\nGetting summaries of long, boring content\nExplaining code to you\nTurning bad documentation into usable documentation\nCreating social media posts from any content input\nAnd a million more…\nOur approach to prompting\n\nFabric Patterns are different than most prompts you'll see.\n\nFirst, we use Markdown to help ensure maximum readability and editability. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. Importantly, this also includes the AI you're sending it to!\nHere's an example of a Fabric Pattern.\n\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\npattern-example\nNext, we are extremely clear in our instructions, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\nAnd finally, we tend to use the System section of the prompt almost exclusively. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\nQuickstart\n\nThe most feature-rich way to use Fabric is to use the fabric client, which can be found under /client directory in this repository.\n\nSetting up the fabric commands\n\nFollow these steps to get all fabric related apps installed and configured.\n\nNavigate to where you want the Fabric project to live on your system in a semi-permanent place on your computer.\n# Find a home for Fabric\ncd /where/you/keep/code\nClone the project to your computer.\n# Clone Fabric to your computer\ngit clone https://github.com/danielmiessler/fabric.git\nEnter Fabric's main directory\n# Enter the project folder (where you cloned it)\ncd fabric\nInstall pipx:\nmacOS:\n\nbrew install pipx\nLinux:\n\nsudo apt install pipx\nWindows:\n\nUse WSL and follow the Linux instructions.\n\nInstall fabric\npipx install .\nRun setup:\nfabric --setup\nRestart your shell to reload everything.\n\nNow you are up and running! You can test by running the help.\n\n# Making sure the paths are set up correctly\nfabric --help\nNote\n\nIf you're using the server functions, fabric-api and fabric-webui need to be run in distinct terminal windows.\nUsing the fabric client\n\nOnce you have it all set up, here's how to use it.\n\nCheck out the options fabric -h\nus the results in\n                        realtime. NOTE: You will not be able to pipe the\n                        output into another command.\n  --list, -l            List available patterns\n  --clear               Clears your persistent model choice so that you can\n                        once again use the --model flag\n  --update, -u          Update patterns. NOTE: This will revert the default\n                        model to gpt4-turbo. please run --changeDefaultModel\n                        to once again set default model\n  --pattern PATTERN, -p PATTERN\n                        The pattern (prompt) to use\n  --setup               Set up your fabric instance\n  --changeDefaultModel CHANGEDEFAULTMODEL\n                        Change the default model. For a list of available\n                        models, use the --listmodels flag.\n  --model MODEL, -m MODEL\n                        Select the model to use. NOTE: Will not work if you\n                        have set a default model. please use --clear to clear\n                        persistence before using this flag\n  --listmodels          List all available models\n  --remoteOllamaServer REMOTEOLLAMASERVER\n                        The URL of the remote ollamaserver to use. ONLY USE\n                        THIS if you are using a local ollama server in an non-\n                        deault location or port\n  --context, -c         Use Context file (context.md) to add context to your\n                        pattern\nage: fabric [-h] [--text TEXT] [--copy] [--agents {trip_planner,ApiKeys}]\n              [--output [OUTPUT]] [--stream] [--list] [--clear] [--update]\n              [--pattern PATTERN] [--setup]\n              [--changeDefaultModel CHANGEDEFAULTMODEL] [--model MODEL]\n              [--listmodels] [--remoteOllamaServer REMOTEOLLAMASERVER]\n              [--context]\n\nAn open source framework for augmenting humans using AI.\n\noptions:\n  -h, --help            show this help message and exit\n  --text TEXT, -t TEXT  Text to extract summary from\n  --copy, -C            Copy the response to the clipboard\n  --agents {trip_planner,ApiKeys}, -a {trip_planner,ApiKeys}\n                        Use an AI agent to help you with a task. Acceptable\n                        values are 'trip_planner' or 'ApiKeys'. This option\n                        cannot be used with any other flag.\n  --output [OUTPUT], -o [OUTPUT]\n                        Save the response to a file\n  --stream, -s          Use this option if you want to see\nExample commands\n\nThe client, by default, runs Fabric patterns without needing a server (the Patterns were downloaded during setup). This means the client connects directly to OpenAI using the input given and the Fabric pattern used.\n\nRun the summarize Pattern based on input from stdin. In this case, the body of an article.\npbpaste | fabric --pattern summarize\nRun the analyze_claims Pattern with the --stream option to get immediate and streaming results.\npbpaste | fabric --stream --pattern analyze_claims\nRun the extract_wisdom Pattern with the --stream option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\nyt --transcript https://youtube.com/watch?v=uXs-zPc63kM | fabric --stream --pattern extract_wisdom\nnew All of the patterns have been added as aliases to your bash (or zsh) config file\npbpaste | analyze_claims --stream\nNote\n\nMore examples coming in the next few days, including a demo video!\nJust use the Patterns\n\nfabric-patterns-screenshot\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the /patterns directory and start exploring!\n\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\n\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\n\nThe wisdom of crowds for the win.\n\nCreate your own Fabric Mill\n\nfabric_mill_architecture\nBut we go beyond just providing Patterns. We provide code for you to build your very own Fabric server and personal AI infrastructure!\n\nStructure\n\nFabric is themed off of, well… fabric—as in…woven materials. So, think blankets, quilts, patterns, etc. Here's the concept and structure:\n\nComponents\n\nThe Fabric ecosystem has three primary components, all named within this textile theme.\n\nThe Mill is the (optional) server that makes Patterns available.\nPatterns are the actual granular AI use cases (prompts).\nStitches are chained together Patterns that create advanced functionality (see below).\nLooms are the client-side apps that call a specific Pattern hosted by a Mill.\nCLI-native\n\nOne of the coolest parts of the project is that it's command-line native!\n\nEach Pattern you see in the /patterns directory can be used in any AI application you use, but you can also set up your own server using the /server code and then call APIs directly!\n\nOnce you're set up, you can do things like:\n\n# Take any idea from `stdin` and send it to the `/write_essay` API!\necho \\\"An idea that coding is like speaking with rules.\\\" | write_essay\nDirectly calling Patterns\n\nOne key feature of fabric and its Markdown-based format is the ability to _ directly reference_ (and edit) individual patterns directly—on their own—without surrounding code.\n\nAs an example, here's how to call the direct location of the extract_wisdom pattern.\n\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\nThis means you can cleanly, and directly reference any pattern for use in a web-based AI app, your own code, or wherever!\n\nEven better, you can also have your Mill functionality directly call system and user prompts from fabric, meaning you can have your personal AI ecosystem automatically kept up to date with the latest version of your favorite Patterns.\n\nHere's what that looks like in code:\n\nhttps://github.com/danielmiessler/fabric/blob/main/server/fabric_api_server.py\n# /extwis\n@app.route(\\\"/extwis\\\", methods=[\\\"POST\\\"])\n@auth_required  # Require authentication\ndef extwis():\n    data = request.get_json()\n\n    # Warn if there's no input\n    if \\\"input\\\" not in data:\n        return jsonify({\\\"error\\\": \\\"Missing input parameter\\\"}), 400\n\n    # Get data from client\n    input_data = data[\\\"input\\\"]\n\n    # Set the system and user URLs\n    system_url = \\\"https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/system.md\\\"\n    user_url = \\\"https://raw.githubusercontent.com/danielmiessler/fabric/main/patterns/extract_wisdom/user.md\\\"\n\n    # Fetch the prompt content\n    system_content = fetch_content_from_url(system_url)\n    user_file_content = fetch_content_from_url(user_url)\n\n    # Build the API call\n    system_message = {\\\"role\\\": \\\"system\\\", \\\"content\\\": system_content}\n    user_message = {\\\"role\\\": \\\"user\\\", \\\"content\\\": user_file_content + \\\"\n\\\" + input_data}\n    messages = [system_message, user_message]\n    try:\n        response = openai.chat.completions.create(\n            model=\\\"gpt-4-1106-preview\\\",\n            messages=messages,\n            temperature=0.0,\n            top_p=1,\n            frequency_penalty=0.1,\n            presence_penalty=0.1,\n        )\n        assistant_message = response.choices[0].message.content\n        return jsonify({\\\"response\\\": assistant_message})\n    except Exception as e:\n        return jsonify({\\\"error\\\": str(e)}), 500\nExamples\n\nHere's an abridged output example from the extract_wisdom pattern (limited to only 10 items per section).\n\n# Paste in the transcript of a YouTube video of Riva Tez on David Perrel's podcast\npbpaste | extract_wisdom\n## SUMMARY:\n\nThe content features a conversation between two individuals discussing various topics, including the decline of Western culture, the importance of beauty and subtlety in life, the impact of technology and AI, the resonance of Rilke's poetry, the value of deep reading and revisiting texts, the captivating nature of Ayn Rand's writing, the role of philosophy in understanding the world, and the influence of drugs on society. They also touch upon creativity, attention spans, and the importance of introspection.\n\n## IDEAS:\n\n1. Western culture is perceived to be declining due to a loss of values and an embrace of mediocrity.\n2. Mass media and technology have contributed to shorter attention spans and a need for constant stimulation.\n3. Rilke's poetry resonates due to its focus on beauty and ecstasy in everyday objects.\n4. Subtlety is often overlooked in modern society due to sensory overload.\n5. The role of technology in shaping music and performance art is significant.\n6. Reading habits have shifted from deep, repetitive reading to consuming large quantities of new material.\n7. Revisiting influential books as one ages can lead to new insights based on accumulated wisdom and experiences.\n8. Fiction can vividly illustrate philosophical concepts through characters and narratives.\n9. Many influential thinkers have backgrounds in philosophy, highlighting its importance in shaping reasoning skills.\n10. Philosophy is seen as a bridge between theology and science, asking questions that both fields seek to answer.\n\n## QUOTES:\n\n1. \\\"You can't necessarily think yourself into the answers. You have to create space for the answers to come to you.\\\"\n2. \\\"The West is dying and we are killing her.\\\"\n3. \\\"The American Dream has been replaced by mass packaged mediocrity porn, encouraging us to revel like happy pigs in our own meekness.\\\"\n4. \\\"There's just not that many people who have the courage to reach beyond consensus and go explore new ideas.\\\"\n5. \\\"I'll start watching Netflix when I've read the whole of human history.\\\"\n6. \\\"Rilke saw beauty in everything... He sees it's in one little thing, a representation of all things that are beautiful.\\\"\n7. \\\"Vanilla is a very subtle flavor... it speaks to sort of the sensory overload of the modern age.\\\"\n8. \\\"When you memorize chapters [of the Bible], it takes a few months, but you really understand how things are structured.\\\"\n9. \\\"As you get older, if there's books that moved you when you were younger, it's worth going back and rereading them.\\\"\n10. \\\"She [Ayn Rand] took complicated philosophy and embodied it in a way that anybody could resonate with.\\\"\n\n## HABITS:\n\n1. Avoiding mainstream media consumption for deeper engagement with historical texts and personal research.\n2. Regularly revisiting influential books from youth to gain new insights with age.\n3. Engaging in deep reading practices rather than skimming or speed-reading material.\n4. Memorizing entire chapters or passages from significant texts for better understanding.\n5. Disengaging from social media and fast-paced news cycles for more focused thought processes.\n6. Walking long distances as a form of meditation and reflection.\n7. Creating space for thoughts to solidify through introspection and stillness.\n8. Embracing emotions such as grief or anger fully rather than suppressing them.\n9. Seeking out varied experiences across different careers and lifestyles.\n10. Prioritizing curiosity-driven research without specific goals or constraints.\n\n## FACTS:\n\n1. The West is perceived as declining due to cultural shifts away from traditional values.\n2. Attention spans have shortened due to technological advancements and media consumption habits.\n3. Rilke's poetry emphasizes finding beauty in everyday objects through detailed observation.\n4. Modern society often overlooks subtlety due to sensory overload from various stimuli.\n5. Reading habits have evolved from deep engagement with texts to consuming large quantities quickly.\n6. Revisiting influential books can lead to new insights based on accumulated life experiences.\n7. Fiction can effectively illustrate philosophical concepts through character development and narrative arcs.\n8. Philosophy plays a significant role in shaping reasoning skills and understanding complex ideas.\n9. Creativity may be stifled by cultural nihilism and protectionist attitudes within society.\n10. Short-term thinking undermines efforts to create lasting works of beauty or significance.\n\n## REFERENCES:\n\n1. Rainer Maria Rilke's poetry\n2. Netflix\n3. Underworld concert\n4. Katy Perry's theatrical performances\n5. Taylor Swift's performances\n6. Bible study\n7. Atlas Shrugged by Ayn Rand\n8. Robert Pirsig's writings\n9. Bertrand Russell's definition of philosophy\n10. Nietzsche's walks\nCustom Patterns\n\nYou can also use Custom Patterns with Fabric, meaning Patterns you keep locally and don't upload to Fabric.\n\nOne possible place to store them is ~/.config/custom-fabric-patterns.\n\nThen when you want to use them, simply copy them into ~/.config/fabric/patterns.\n\ncp -a ~/.config/custom-fabric-patterns/* ~/.config/fabric/patterns/`\nNow you can run them with:\n\npbpaste | fabric -p your_custom_pattern\nHelper Apps\n\nThese are helper tools to work with Fabric. Examples include things like getting transcripts from media files, getting metadata about media, etc.\n\nyt (YouTube)\n\nyt is a command that uses the YouTube API to pull transcripts, pull user comments, get video duration, and other functions. It's primary function is to get a transcript from a video that can then be stitched (piped) into other Fabric Patterns.\n\nusage: yt [-h] [--duration] [--transcript] [url]\n\nvm (video meta) extracts metadata about a video, such as the transcript and the video's duration. By Daniel Miessler.\n\npositional arguments:\n  url           YouTube video URL\n\noptions:\n  -h, --help    Show this help message and exit\n  --duration    Output only the duration\n  --transcript  Output only the transcript\n  --comments    Output only the user comments \nts (Audio transcriptions)\n\n'ts' is a command that uses the OpenApi Whisper API to transcribe audio files. Due to the context window, this tool uses pydub to split the files into 10 minute segments. for more information on pydub, please refer https://github.com/jiaaro/pydub\n\nInstallation\n\nmac:\nbrew install ffmpeg\n\nlinux:\napt install ffmpeg\n\nwindows:\ndownload instructions https://www.ffmpeg.org/download.html\nts -h\nusage: ts [-h] audio_file\n\nTranscribe an audio file.\n\npositional arguments:\n  audio_file  The path to the audio file to be transcribed.\n\noptions:\n  -h, --help  show this help message and exit\nSave\n\nsave is a \\\"tee-like\\\" utility to pipeline saving of content, while keeping the output stream intact. Can optionally generate \\\"frontmatter\\\" for PKM utilities like Obsidian via the \\\"FABRIC_FRONTMATTER\\\" environment variable\n\nIf you'd like to default variables, set them in ~/.config/fabric/.env. FABRIC_OUTPUT_PATH needs to be set so save where to write. FABRIC_FRONTMATTER_TAGS is optional, but useful for tracking how tags have entered your PKM, if that's important to you.\n\nusage\n\nusage: save [-h] [-t, TAG] [-n] [-s] [stub]\n\nsave: a \\\"tee-like\\\" utility to pipeline saving of content, while keeping the output stream intact. Can optionally generate \\\"frontmatter\\\" for PKM utilities like Obsidian via the\n\\\"FABRIC_FRONTMATTER\\\" environment variable\n\npositional arguments:\n  stub                stub to describe your content. Use quotes if you have spaces. Resulting format is YYYY-MM-DD-stub.md by default\n\noptions:\n  -h, --help          show this help message and exit\n  -t, TAG, --tag TAG  add an additional frontmatter tag. Use this argument multiple timesfor multiple tags\n  -n, --nofabric      don't use the fabric tags, only use tags from --tag\n  -s, --silent        don't use STDOUT for output, only save to the file\nExample\n\necho test | save --tag extra-tag stub-for-name\ntest\n\n$ cat ~/obsidian/Fabric/2024-03-02-stub-for-name.md\n---\ngeneration_date: 2024-03-02 10:43\ntags: fabric-extraction stub-for-name extra-tag\n---\ntest\n\nEND FABRIC PROJECT DESCRIPTION\n\n- Take the Fabric patterns given to you as input and think about how to create a Markmap visualization of everything you can do with Fabric.\n\nExamples: Analyzing videos, summarizing articles, writing essays, etc.\n\n- The visual should be broken down by the type of actions that can be taken, such as summarization, analysis, etc., and the actual patterns should branch from there. \n\n# OUTPUT\n\n- Output comprehensive Markmap code for displaying this functionality map as described above.\n\n- NOTE: This is Markmap, NOT Markdown.\n\n- Output the Markmap code and nothing else.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.252271831035614,
        0.7477495670318604,
        -0.3679426610469818,
        0.3409630358219147,
        -0.18344318866729736,
        0.3870581090450287,
        -0.7046650052070618,
        -0.05700556933879852,
        0.3090619146823883,
        0.1995418518781662,
        -0.5207038521766663,
        0.34995368123054504,
        -0.15254727005958557,
        0.2323397397994995,
        0.6228896379470825,
        -0.010773450136184692,
        -0.06931422650814056,
        -1.0048109292984009,
        -1.4812729358673096,
        -0.21779102087020874,
        0.3825738728046417,
        0.9115591645240784,
        -0.2920898497104645,
        0.04833395406603813,
        0.04759792238473892,
        -0.15075545012950897,
        0.7990949749946594,
        -0.5095628499984741,
        -1.4999033212661743,
        -1.9760770797729492,
        0.31107988953590393,
        0.47915390133857727,
        -0.06100074201822281,
        -0.6146278381347656,
        0.00336659699678421,
        -1.240138053894043,
        -0.5991148948669434,
        0.2589191198348999,
        -0.45911866426467896,
        -0.033379293978214264,
        0.07492109388113022,
        0.06772815436124802,
        -0.13578908145427704,
        0.09010714292526245,
        0.15117380023002625,
        -0.41499435901641846,
        0.6702221632003784,
        -0.23028983175754547,
        0.8886144161224365,
        0.3874882161617279,
        -0.6633256077766418,
        -0.7754572033882141,
        0.4206695258617401,
        -0.030086442828178406,
        -0.30127882957458496,
        0.05982465296983719,
        0.31240272521972656,
        -0.3192349374294281,
        0.3059100806713104,
        -0.31817129254341125,
        -0.06931710243225098,
        0.16848281025886536,
        -4.24381160736084,
        0.017122644931077957,
        -0.12837176024913788,
        0.4495109021663666,
        0.7063473463058472,
        -0.20073333382606506,
        0.3084399402141571,
        -0.45344078540802,
        0.0864604264497757,
        0.31630009412765503,
        -0.69515061378479,
        0.5603811740875244,
        -0.33743610978126526,
        -0.1457262486219406,
        0.06473209708929062,
        -0.17321337759494781,
        0.6819809079170227,
        -0.13339541852474213,
        0.26773157715797424,
        0.7869235277175903,
        0.523813784122467,
        -0.21791154146194458,
        -0.5661540031433105,
        0.28056827187538147,
        -0.42098140716552734,
        -0.006108865141868591,
        0.33206963539123535,
        -0.012135888449847698,
        -0.2649363875389099,
        -0.3512250781059265,
        -0.1225787028670311,
        -0.08113693445920944,
        -0.47049206495285034,
        1.1831437349319458,
        -0.35959893465042114,
        0.15979084372520447,
        -0.3423508405685425,
        3.3149919509887695,
        0.3016887605190277,
        -0.6299819946289062,
        0.5778155326843262,
        -0.6071764826774597,
        0.40936079621315,
        -0.016646577045321465,
        -0.01814783178269863,
        -0.5553174614906311,
        0.35600849986076355,
        -0.04872874915599823,
        0.3030775487422943,
        -0.6754273772239685,
        -0.2582034468650818,
        0.17023563385009766,
        0.5951107144355774,
        -0.3256797790527344,
        0.04296711087226868,
        0.2575891613960266,
        -0.6059577465057373,
        1.2032198905944824,
        -0.22468069195747375,
        0.048473142087459564,
        -0.4598114788532257,
        -0.43869417905807495,
        0.031573858112096786,
        -0.1242416650056839,
        -0.45107999444007874,
        0.5933932662010193,
        0.44128745794296265,
        0.26419246196746826,
        0.3650079369544983,
        -0.48624104261398315,
        -0.23760052025318146,
        -0.3748398721218109,
        0.33561843633651733,
        -0.5188656449317932,
        0.45148199796676636,
        -0.41045230627059937,
        0.5034034848213196,
        -0.5527191162109375,
        0.10639223456382751,
        -1.7846659421920776,
        1.386038899421692,
        -0.2036556899547577,
        0.7501689195632935,
        -0.012713715434074402,
        -0.45582643151283264,
        -0.1569061279296875,
        -0.803682804107666,
        -0.2454330027103424,
        -0.1059996485710144,
        0.017331458628177643,
        -0.1765664666891098,
        0.9235019683837891,
        0.843365490436554,
        -0.22633959352970123,
        -0.5043475031852722,
        0.15212906897068024,
        -0.4993940591812134,
        0.4287722408771515,
        0.08046512305736542,
        -0.13522106409072876,
        0.4771016240119934,
        0.03388015925884247,
        0.6244220733642578,
        -0.6041204333305359,
        0.1447369009256363,
        -0.4796919524669647,
        0.8820865750312805,
        -0.3447628915309906,
        0.567322850227356,
        -0.28182193636894226,
        0.6839022040367126,
        0.7155841588973999,
        -0.4869222640991211,
        0.5240968465805054,
        -0.24822255969047546,
        0.09601444005966187,
        0.3052053153514862,
        -0.32981640100479126,
        0.6111661791801453,
        0.530978798866272,
        -0.4969807267189026,
        -0.7577562928199768,
        -0.5549441576004028,
        0.2195861041545868,
        0.6859308481216431,
        0.5763534903526306,
        0.3424462378025055,
        0.6882811784744263,
        -0.9747286438941956,
        2.005254030227661,
        -0.7577253580093384,
        0.264605849981308,
        0.20949502289295197,
        -0.427798330783844,
        0.17765726149082184,
        0.23495706915855408,
        0.5121709108352661,
        -0.06520363688468933,
        -1.126330852508545,
        -0.09199924021959305,
        -0.25364261865615845,
        -0.16098684072494507,
        -0.14337173104286194,
        -0.8200421929359436,
        0.10907071828842163,
        0.2264687418937683,
        -0.15214087069034576,
        -0.47556474804878235,
        0.07866956293582916,
        0.20089781284332275,
        1.3060498237609863,
        0.09093508124351501,
        0.6365525722503662,
        0.1727144569158554,
        0.21492648124694824,
        0.11222346127033234,
        -0.07768934965133667,
        0.8948273658752441,
        0.051463738083839417,
        0.292707234621048,
        -0.3677697777748108,
        -0.5424169898033142,
        -0.5485082268714905,
        0.2455264925956726,
        0.140029639005661,
        0.16777649521827698,
        -0.13669227063655853,
        -0.48200011253356934,
        0.22205182909965515,
        0.8527913093566895,
        0.5780734419822693,
        1.3308171033859253,
        0.03406786173582077,
        0.053772684186697006,
        -0.16922980546951294,
        -0.12362845242023468,
        0.029626846313476562,
        -0.8705266714096069,
        0.31485438346862793,
        -0.04112502187490463,
        0.3599696159362793,
        -0.4585874080657959,
        -0.31906262040138245,
        -1.211258888244629,
        -0.2661002576351166,
        -0.3861452341079712,
        -0.0823199674487114,
        1.6544034481048584,
        0.7219706177711487,
        -0.286429762840271,
        0.23947802186012268,
        0.6913557052612305,
        -0.19943642616271973,
        0.2011706829071045,
        -2.000462770462036,
        -0.4648806154727936,
        -0.40018728375434875,
        0.4359031915664673,
        -0.0066335201263427734,
        0.1747054159641266,
        0.17395101487636566,
        0.02969464659690857,
        0.17024031281471252,
        0.274463027715683,
        0.07571911811828613,
        -0.4112551510334015,
        -0.41073861718177795,
        -0.5048789381980896,
        -0.4981972873210907,
        0.3813110589981079,
        -0.17438779771327972,
        0.3661743402481079,
        -0.0967538133263588,
        0.481759250164032,
        0.5701778531074524,
        0.02223789691925049,
        -0.10353989154100418,
        -0.24479742348194122,
        0.20724031329154968,
        0.19996753334999084,
        -0.2545880377292633,
        0.07827533781528473,
        -0.5413584113121033,
        -0.16423894464969635,
        -0.5254362225532532,
        -0.5461884140968323,
        -0.2792488634586334,
        0.219705730676651,
        -0.31656599044799805,
        -0.4353336691856384,
        -1.0041639804840088,
        -0.08292405307292938,
        1.7323811054229736,
        -0.012343548238277435,
        0.15287645161151886,
        0.7798557281494141,
        0.15160112082958221,
        0.06911859661340714,
        0.2667321264743805,
        -0.1288835108280182,
        -0.12827259302139282,
        0.027905255556106567,
        -0.84170001745224,
        -0.17979568243026733,
        0.9615386724472046,
        -0.09856954962015152,
        0.09832845628261566,
        0.4060382843017578,
        -0.289017915725708,
        0.12361948192119598,
        0.11031995713710785,
        0.1037243902683258,
        0.21764642000198364,
        -0.33143866062164307,
        -0.3493783175945282,
        0.8684604167938232,
        -0.15070201456546783,
        -1.5596543550491333,
        -0.19903278350830078,
        0.8868923187255859,
        0.061785370111465454,
        -0.1454954743385315,
        -0.6239514350891113,
        0.37929248809814453,
        0.1819288432598114,
        0.2575870156288147,
        0.16925252974033356,
        1.0110992193222046,
        0.12262743711471558,
        0.40073665976524353,
        0.16745030879974365,
        -0.1685570627450943,
        0.7334697246551514,
        -0.2763195037841797,
        0.24556970596313477,
        0.5872015953063965,
        -0.044856082648038864,
        -0.49765536189079285,
        0.2663840353488922,
        1.358720302581787,
        0.004235303029417992,
        0.2340831458568573,
        0.09201206266880035,
        0.39139652252197266,
        -0.2501221001148224,
        -1.108144998550415,
        0.5660746097564697,
        -0.28676527738571167,
        0.055663079023361206,
        1.0637547969818115,
        0.0002864226698875427,
        -0.013636888936161995,
        0.2441442459821701,
        0.5311845541000366,
        -0.272809773683548,
        0.37301352620124817,
        -0.26262038946151733,
        1.568942904472351,
        -0.7273201942443848,
        -0.46448659896850586,
        -0.47968339920043945,
        -0.11240403354167938,
        -0.36340370774269104,
        -0.2661951184272766,
        0.03329682722687721,
        -0.42007818818092346,
        -0.020989444106817245,
        0.2709229290485382,
        -0.10334566235542297,
        -0.2145359218120575,
        0.25792938470840454,
        0.24397070705890656,
        0.5000134110450745,
        0.045489516109228134,
        0.13262315094470978,
        0.47308239340782166,
        -0.048069313168525696,
        -0.024725601077079773,
        0.39135056734085083,
        0.2048385888338089,
        -0.39380189776420593,
        -0.24175892770290375
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Suggest_pattern",
          "raw": "\n                workflow Suggest_pattern v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\nYou are an AI assistant tasked with creating a new feature for a fabric command-line tool. Your primary responsibility is to develop a pattern that suggests appropriate fabric patterns or commands based on user input. You are knowledgeable about fabric commands and understand the need to expand the tool's functionality. Your role involves analyzing user requests, determining the most suitable fabric commands or patterns, and providing helpful suggestions to users.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n- Analyze the user's input to understand their specific needs and context\n- Determine the appropriate fabric pattern or command based on the user's request\n- Generate a response that suggests the relevant fabric command(s) or pattern(s)\n- Provide explanations or multiple options when applicable\n- If no specific command is found, suggest using `create_pattern`\n\n# OUTPUT INSTRUCTIONS\n- Only output Markdown\n- Provide suggestions for fabric commands or patterns based on the user's input\n- Include explanations or multiple options when appropriate\n- If suggesting `create_pattern`, include instructions for saving and using the new pattern\n- Format the output to be clear and easy to understand for users new to fabric\n- Ensure the response aligns with the goal of making fabric more accessible and user-friendly\n- Ensure you follow ALL these instructions when creating your output\n\n# INPUT\nINPUT:\n\"\n                        $CUSTOM_USER = \"\nCONTENT: \n\n# OVERVIEW\n\nWhat It Does: Fabric is an open-source framework designed to augment human capabilities using AI, making it easier to integrate AI into daily tasks.\n\nWhy People Use It: Users leverage Fabric to seamlessly apply AI for solving everyday challenges, enhancing productivity, and fostering human creativity through technology.\n\n# HOW TO USE IT\n\nMost Common Syntax: The most common usage involves executing Fabric commands in the terminal, such as `fabric --pattern <PATTERN_NAME>`.\n\n# COMMON USE CASES\n\nFor Summarizing Content: `fabric --pattern summarize`\nFor Analyzing Claims: `fabric --pattern analyze_claims`\nFor Extracting Wisdom from Videos: `fabric --pattern extract_wisdom`\nFor Creating AI Agents: `echo \\\"<TASK>\\\" | fabric --agents`\nFor creating custom patterns: `fabric --pattern create_pattern`\n- One possible place to store them is ~/.config/custom-fabric-patterns.\n- Then when you want to use them, simply copy them into ~/.config/fabric/patterns.\n`cp -a ~/.config/custom-fabric-patterns/* ~/.config/fabric/patterns/`\n- Now you can run them with: `pbpaste | fabric -p your_custom_pattern`\n\n\n# MOST IMPORTANT AND USED OPTIONS AND FEATURES\n\n- **--pattern PATTERN, -p PATTERN**: Specifies the pattern (prompt) to use. Useful for applying specific AI prompts to your input.\n  \n- **--agents, -a**: Creates an AI agent to perform a task based on the input. Great for automating complex tasks with AI.\n  \n- **--stream, -s**: Streams results in real-time. Ideal for getting immediate feedback from AI operations.\n  \n- **--update, -u**: Updates patterns. Ensures you're using the latest AI prompts for your tasks.\n  \n- **--model MODEL, -m MODEL**: Selects the AI model to use. Allows customization of the AI backend for different tasks.\n  \n- **--setup**: Sets up your Fabric instance. Essential for first-time users to configure Fabric correctly.\n  \n- **--list, -l**: Lists available patterns. Helps users discover new AI prompts for various applications.\n  \n- **--context, -c**: Uses a Context file to add context to your pattern. Enhances the relevance of AI responses by providing additional background information.\n\n# PATTERNS\n\n## agility_story\nGenerates user stories and acceptance criteria for specified topics, focusing on Agile framework principles. This prompt specializes in translating topics into structured Agile documentation, specifically for user story and acceptance criteria creation. The expected output is a JSON-formatted document detailing the topic, user story, and acceptance criteria.\n\n## ai\nSummarizes and responds to questions with insightful bullet points. It involves creating a mental model of the question for deeper understanding. The output consists of 3-5 concise bullet points, each with a 10-word limit.\n\n## analyze_answers\nEvaluates the correctness of answers provided by learners to questions generated by a complementary quiz creation pattern. It aims to assess understanding of learning objectives and identify areas needing further study. The expected output is an analysis of the learner's answers, indicating their grasp of the subject matter.\n\n## analyze_claims\nAnalyzes and rates the truth claims in input, providing evidence for and against, along with a balanced view. It separates truth claims from arguments, offering a nuanced analysis with ratings and labels for each claim. The output includes a summary, evidence, refutations, logical fallacies, ratings, labels, and an overall score and analysis.\n\n## analyze_debate\nAnalyzes debate transcripts to help users understand different viewpoints and broaden their perspectives. It maps out claims, analyzes them neutrally, and rates the debate's insightfulness and emotionality. The output includes scores, participant emotionality, argument summaries with sources, and lists of agreements, disagreements, misunderstandings, learnings, and takeaways.\n\n## analyze_incident\nSummarizes cybersecurity breach articles by extracting key information efficiently, focusing on conciseness and organization. It avoids inferential conclusions, relying solely on the article's content for details like attack date, type, and impact. The output is a structured summary with specific details about the cybersecurity incident, including attack methods, vulnerabilities, and recommendations for prevention.\n\n## analyze_logs\nAnalyzes a server log file to identify patterns, anomalies, and potential issues, aiming to enhance the server's reliability and performance. The process involves a detailed examination of log entries, assessment of operational reliability, and identification of recurring issues. Recommendations for improvements are provided based on data-driven analysis, excluding personal opinions and irrelevant information.\n\n## analyze_malware\nAnalyzes malware across various platforms, focusing on extracting indicators of compromise and detailed malware behavior. This approach includes analyzing telemetry and community data to aid in malware detection and analysis. The expected output includes a summary of findings, potential indicators of compromise, Mitre Att&CK techniques, pivoting advice, detection strategies, suggested Yara rules, additional references, and technical recommendations.\n\n## analyze_paper\nThis service analyzes research papers to determine their main findings, scientific rigor, and quality. It uniquely maps out claims, evaluates study design, and assesses conflicts of interest. The output includes a summary, author details, findings, study quality, and a final grade with explanations.\n\n## analyze_patent\nThe prompt outlines the role and responsibilities of a patent examiner, emphasizing the importance of technical and legal expertise in evaluating patents. It details the steps for examining a patent, including identifying the technology field, problem addressed, solution, advantages, novelty, and inventive step, and summarizing the core idea and keywords. The expected output involves detailed analysis and documentation in specific sections without concern for length, using bullet points for clarity.\n\n## analyze_personality\nPerforms in-depth psychological analysis on the main individual in the provided input. It involves identifying the primary person, deeply contemplating their language and responses, and comparing these to known human psychology principles. The output includes a concise psychological profile summary and detailed supporting points.\n\n## analyze_presentation\nAnalyzes and critiques presentations, focusing on content, speaker's psychology, and the difference between stated and actual goals. It involves comparing intended messages to actual content, including self-references and entertainment attempts. The output includes scores and summaries for ideas, selflessness, and entertainment, plus an overall analysis.\n\n## analyze_prose\nEvaluates the quality of writing by assessing its novelty, clarity, and prose, and provides improvement recommendations. It uses a detailed approach to rate each aspect on a specific scale and ensures the overall rating reflects the lowest individual score. The expected output includes ratings and concise improvement tips.\n\n## analyze_prose_json\nEvaluates the quality of writing and content, providing ratings and recommendations for improvement based on novelty, clarity, and overall messaging. It assesses ideas for their freshness and originality, clarity of argument, and quality of prose, offering a structured approach to critique. The expected output is a JSON object summarizing these evaluations and recommendations.\n\n## analyze_prose_pinker\nEvaluates prose based on Steven Pinker's writing principles, identifying its current style and recommending improvements for clarity and engagement. It involves analyzing the text's adherence to Pinker's stylistic categories and avoiding common pitfalls in writing. The output includes a detailed analysis of the prose's style, strengths, weaknesses, and specific examples of both effective and ineffective writing elements.\n\n## analyze_spiritual_text\nAnalyzes spiritual texts to highlight surprising claims and contrasts them with the King James Bible. This approach involves detailed comparison, providing examples from both texts to illustrate differences. The output consists of concise bullet points summarizing these findings.\n\n## analyze_tech_impact\nAnalyzes the societal impact of technology projects by breaking down their intentions, outcomes, and broader implications, including ethical considerations. It employs a structured approach, detailing the project's objectives, technologies used, target audience, outcomes, societal impact, ethical considerations, and sustainability. The expected output includes summaries, lists, and analyses across specified sections.\n\n## analyze_threat_report\nThe prompt instructs a super-intelligent cybersecurity expert to analyze and extract key insights from cybersecurity threat reports. It emphasizes identifying new, interesting, and surprising information, and organizing these findings into concise, categorized summaries. The expected output includes a one-sentence summary, trends, statistics, quotes, references, and recommendations from the report, all formatted in plain language and without repetition.\n\n## analyze_threat_report_trends\nAnalyzes cybersecurity threat reports to identify up to 50 unique, surprising, and insightful trends. This process involves a deep, expert analysis to uncover new and interesting information. The expected output is a list of trends without repetition or formatting embellishments.\n\n## answer_interview_question\nGenerates tailored responses to technical interview questions, aiming for a casual yet insightful tone. The AI draws from a technical knowledge base and professional experiences to construct responses that demonstrate depth and alternative perspectives. Outputs are structured first-person responses, including context, main explanation, alternative approach, and evidence-based conclusion.\n\n## ask_secure_by_design_questions\nGenerates a comprehensive set of security-focused questions tailored to the fundamental design of a specific project. This process involves deep analysis and conceptualization of the project's components and their security needs. The output includes a summary and a detailed list of security questions organized by themes.\n\n## capture_thinkers_work\nSummarizes teachings and philosophies of notable individuals or philosophical schools, providing detailed templates on their backgrounds, ideas, and applications. It offers a structured approach to encapsulating complex thoughts into accessible summaries. The output includes encapsulations, background information, schools of thought, impactful ideas, primary teachings, works, quotes, applications, and life advice.\n\n## check_agreement\nThe prompt outlines a process for analyzing contracts and agreements to identify potential issues or \\\"gotchas.\\\" It involves summarizing the document, listing important aspects, categorizing issues by severity, and drafting responses for critical and important items. The expected output includes a concise summary, detailed callouts, categorized issues, and recommended responses in Markdown format.\n\n## clean_text\nSummarizes and corrects formatting issues in text without altering the content. It focuses on removing odd line breaks to improve readability. The expected output is a clean, well-formatted version of the original text.\n\n## coding_master\nExplains coding concepts or languages to beginners, using examples from reputable sources and illustrating points with formatted code. The approach emphasizes clarity and accessibility, incorporating examples from Codeacademy and NetworkChuck. Outputs include markdown-formatted code and structured lists of ideas, recommendations, habits, facts, and insights, adhering to specific word counts.\n\n## compare_and_contrast\nCompares and contrasts a list of items, focusing on their differences and similarities. The approach involves analyzing the items across various topics, organizing the findings into a markdown table. The expected output is a structured comparison in table format.\n\n## create_5_sentence_summary\nGenerates concise summaries or answers at five decreasing levels of depth. It involves deep understanding and thoughtful analysis of the input. The output is a structured list capturing the essence in 5, 4, 3, 2, and 1 word(s).\n\n## create_academic_paper\nProduces high-quality, authoritative Latex academic papers with clear concept explanations. It focuses on logical layout and simplicity while maintaining a professional appearance. The expected output is LateX code formatted in a two-column layout with a header and footer.\n\n## create_ai_jobs_analysis\nAnalyzes job reports to identify roles least and most vulnerable to automation, offering strategies for enhancing job security. It leverages historical insights to predict automation's impact on various job categories. The output includes a detailed analysis and recommendations for resilience against automation.\n\n## create_aphorisms\nGenerates a list of 20 aphorisms related to the given topic(s), ensuring variety in their beginnings. It focuses on sourcing quotes from real individuals. The output includes each aphorism followed by the name of the person who said it.\n\n## create_art_prompt\nThe prompt guides an expert artist in conceptualizing and instructing AI to create art that perfectly encapsulates a given concept. It emphasizes deep thought on the concept and its visual representation, aiming for compelling and interesting artwork. The expected output is a 100-word description that not only instructs the AI on what to create but also how the art should evoke feelings and suggest style through examples.\n\n## create_better_frame\nThe essay explores the concept of framing as a way to construct and interpret reality through different lenses, emphasizing the power of perspective in shaping one's experience of the world. It highlights various dichotomies in perceptions around topics like AI, race/gender, success, personal identity, and control over life, illustrating how different frames can lead to vastly different outlooks and outcomes. The author argues for the importance of choosing positive frames to improve individual and collective realities, suggesting that changing frames can change outcomes and foster more positive social dynamics.\n\n## create_coding_project\nGenerates wireframes and starter code for coding projects based on user ideas. It specifically caters to transforming ideas into actionable project outlines and code skeletons, including detailed steps and file structures. The output includes project summaries, structured directories, and initial code setups.\n\n## create_command\nGenerates specific command lines for various penetration testing tools based on a brief description of the desired outcome. This approach leverages the tool's help documentation to ensure accuracy and relevance. The expected output is a precise command that aligns with the user's objectives for the tool.\n\n## create_cyber_summary\nThe prompt instructs on creating a comprehensive summary of cybersecurity threats, vulnerabilities, incidents, and malware for a technical audience. It emphasizes deep understanding through repetitive analysis and visualization techniques. The expected output includes a concise summary and categorized lists of cybersecurity issues.\n\n## create_git_diff_commit\nThis prompt provides instructions for using specific Git commands to manage code changes. It explains how to view differences since the last commit and display the current state of the repository. The expected output is a guide on executing these commands.\n\n## create_idea_compass\nGuides users in developing a structured exploration of ideas through a detailed template. It emphasizes clarity and organization by breaking down the process into specific steps, including defining, supporting, and contextualizing the idea. The expected output is a comprehensive summary with related ideas, evidence, and sources organized in a structured format.\n\n## create_investigation_visualization\nCreates detailed GraphViz visualizations to illustrate complex intelligence investigations and data insights. This approach involves extensive analysis, organizing information, and visual representation using shapes, colors, and labels for clarity. The output includes a comprehensive diagram and analytical conclusions with a certainty rating.\n\n## create_keynote\nThe prompt guides in creating TED-quality keynote presentations from provided input, focusing on narrative flow and practical takeaways. It outlines steps for structuring the presentation into slides with concise bullet points, images, and speaker notes. The expected output includes a story flow, the final takeaway, and a detailed slide deck presentation.\n\n## create_logo\nGenerates simple, minimalist company logos based on provided input, focusing on elegance and impact without text. The approach emphasizes super minimalist designs. The output is a prompt for an AI image generator to create a simple, vector graphic logo.\n\n## create_markmap_visualization\nTransforms complex ideas into visual formats using MarkMap syntax for easy understanding. This process involves simplifying concepts to ensure they can be effectively represented within the constraints of MarkMap. The output is a MarkMap syntax diagram that visually communicates the core ideas.\n\n## create_mermaid_visualization\nTransforms complex ideas into simplified Mermaid (Markdown) visual diagrams. This process involves creating detailed visualizations that can independently explain concepts using Mermaid syntax, focusing on clarity and comprehensibility. The expected output is a Mermaid syntax diagram accompanied by a concise visual explanation.\n\n## create_micro_summary\nSummarizes content into a Markdown formatted summary, focusing on brevity and clarity. It emphasizes creating concise, impactful points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, each adhering to strict word limits.\n\n## create_network_threat_landscape\nAnalyzes open ports and services from network scans to identify security risks and provide recommendations. This process involves a detailed examination of port and service statistics to uncover potential vulnerabilities. The expected output is a markdown formatted threat report with sections on description, risk, recommendations, a concise summary, trends, and quotes from the analysis.\n\n## create_npc\nGenerates detailed NPCs for D&D 5th edition, incorporating a wide range of characteristics from background to appearance. It emphasizes creativity in developing a character's backstory, traits, and goals. The output is a comprehensive character profile suitable for gameplay.\n\n## create_pattern\nThe AI assistant is designed to interpret and respond to LLM/AI prompts with structured outputs. It specializes in organizing and analyzing prompts to produce responses that adhere to specific instructions and formatting requirements. The assistant ensures accuracy and alignment with the intended outcomes through meticulous analysis.\n\n## create_quiz\nGenerates questions for reviewing learning objectives based on provided subject and objectives. It requires defining the subject and learning objectives for accurate question generation. The output consists of questions aimed at helping students review key concepts.\n\n## create_reading_plan\nDesigns a tailored three-phase reading plan based on user input, focusing on an author or specific guidance. It carefully selects books from various sources, including hidden gems, to enhance the user's knowledge on the topic. The output includes a concise plan summary and categorized reading lists with reasons for each selection.\n\n## create_report_finding\nThe prompt instructs the creation of a detailed markdown security finding report, incorporating sections like Description, Risk, Recommendations, and others, based on a vulnerability title and explanation provided by the user. It emphasizes a structured, insightful approach to documenting cybersecurity vulnerabilities. The expected output is a comprehensive report with specific sections, focusing on clarity, insightfulness, and relevance to cybersecurity assessment.\n\n## create_security_update\nThe prompt instructs on creating concise security updates for newsletters, focusing on cybersecurity developments, threats, advisories, and new vulnerabilities. It emphasizes brevity and relevance, requiring links to further information. The expected output includes structured sections with short descriptions and relevant details, aiming to inform readers about the latest security concerns efficiently.\n\n## create_show_intro\nCreates compelling short intros for podcasts, focusing on the most interesting aspects of the show. It involves listening to the entire show, identifying key topics, and highlighting them in a concise introduction. The output is a structured intro that teases the conversation's main points.\n\n## create_stride_threat_model\nThe prompt instructs on creating a detailed threat model using the STRIDE per element methodology for a given system design document. It emphasizes understanding the system's assets, trust boundaries, and data flows to identify and prioritize potential threats. The expected output is a comprehensive table listing threats, their components, mitigation strategies, and risk assessments.\n\n## create_summary\nSummarizes content into a structured Markdown format, focusing on brevity and clarity. It emphasizes creating a concise summary, listing main points, and identifying key takeaways. The output is organized into specific sections for easy reference.\n\n## create_threat_model\nThe prompt outlines a comprehensive approach to everyday threat modeling, emphasizing its application beyond technical defenses to include personal and physical security scenarios. It distinguishes between realistic and possible threats, advocating for a balanced approach to risk management that considers the value of what's being protected, the likelihood of threats, and the cost of controls. The expected output involves creating threat models for various scenarios, highlighting realistic defenses, and guiding individuals towards logical security decisions through structured analysis.\n\n## create_threat_scenarios\nThe prompt seeks to identify and prioritize potential threats to a given system or situation, using a narrative-based, simple threat modeling approach. It emphasizes distinguishing between realistic and possible threats, focusing on those worth defending against. The expected output includes a list of prioritized threat scenarios, an analysis of the threat model, recommended controls, a narrative analysis, and a concise conclusion.\n\n## create_upgrade_pack\nExtracts and organizes insights on world models and task algorithms from provided content. It focuses on identifying and categorizing beliefs about the world and optimal task execution strategies. The output includes concise, actionable bullet points under relevant categories.\n\n## create_video_chapters\nExtracts and organizes the most engaging topics from a transcript with corresponding timestamps. This process involves a detailed review of the transcript to identify key moments and subjects. The output is a list of topics with their timestamps in a sequential format.\n\n## create_visualization\nTransforms complex ideas into simplified ASCII art visualizations. This approach focuses on distilling intricate concepts into visual forms that can be easily understood through ASCII art. The expected output is a detailed ASCII art representation accompanied by a concise visual explanation.\n\n## explain_code\nAnalyzes and explains code, security tool outputs, or configuration texts, tailoring the explanation to the type of input. It uses specific sections to clarify the function, implications, or settings based on the input's nature. The expected output is a detailed explanation or answer in designated sections.\n\n## explain_docs\nThe prompt instructs on transforming input about tool usage into improved, structured documentation. It emphasizes clarity and utility, breaking down the process into specific sections for a comprehensive guide. The expected output includes an overview, usage syntax, common use cases, and key features of the tool.\n\n## explain_project\nSummarizes project documentation into a concise, user and developer-focused summary, highlighting its purpose, problem addressed, approach, installation, usage, and examples. It simplifies complex information for easy understanding and application. The output includes a project overview, problem it addresses, approach to solving the problem, and practical steps for installation and usage.\n\n## explain_terms\nProduces a glossary of advanced terms found in specific content, including definitions and analogies. It focuses on explaining obscure or complex terms to aid understanding. The output is a list of terms with explanations and analogies in a structured Markdown format.\n\n## extract_algorithm_update_recommendations\nAnalyzes input to provide concise recommendations for improving processes. It focuses on extracting actionable advice from content descriptions. The output consists of a bulleted list of up to three brief suggestions.\n\n## extract_article_wisdom\nExtracts key insights and valuable information from textual content, focusing on ideas, quotes, habits, and references. It aims to address the issue of information overload by providing a concise summary of the content's most meaningful aspects. The expected output includes summarized ideas, notable quotes, referenced materials, and habits worth adopting.\n\n## extract_book_ideas\nSummarizes a book's key content by extracting 50 to 100 of its most interesting ideas. The process involves a deep dive into the book's insights, prioritizing them by interest and insightfulness. The output is a concise list of bulleted ideas, limited to 20 words each.\n\n## extract_book_recommendations\nSummarizes a book's key content by extracting 50 to 100 of its most practical recommendations, prioritizing the most impactful advice. This process involves a thorough memory search to identify actionable insights. The output is formatted as an instructive, bullet-pointed list, limited to 20 words each.\n\n## extract_business_ideas\nThe prompt outlines a process for identifying and elaborating on innovative business ideas. It focuses on extracting top business concepts from provided content and then refining the best ten by exploring adjacent possibilities. The expected output includes two sections: a list of extracted ideas and a detailed elaboration on the top ten ideas, ensuring uniqueness and differentiation.\n\n## extract_extraordinary_claims\nIdentifies and lists extraordinary claims from conversations, focusing on those rejected by the scientific community or based on misinformation. The process involves deep analysis to pinpoint statements that defy accepted scientific truths, such as denying evolution or the moon landing. The output is a detailed list of quotes, ranging from 50 to 100, showcasing these claims.\n\n## extract_ideas\nExtracts and condenses insightful ideas from text into 15-word bullet points focusing on life's purpose and human progress. This process emphasizes capturing unique insights on specified themes. The output consists of a list of concise, thought-provoking ideas.\n\n## extract_insights\nExtracts and condenses complex insights from text on profound topics into 15-word bullet points. This process emphasizes the extraction of nuanced, powerful ideas related to human and technological advancement. The expected output is a concise list of abstracted, insightful bullets.\n\n## extract_main_idea\nExtracts and highlights the most crucial or intriguing idea from any given content. This prompt emphasizes a methodical approach to identify and articulate the essence of the input. The expected output includes a concise main idea and a recommendation based on that idea.\n\n## extract_patterns\nThe prompt guides in identifying and analyzing recurring, surprising, or insightful patterns from a collection of ideas, data, or observations. It emphasizes extracting the most notable patterns based on their frequency and significance, and then documenting the process of discovery and analysis. The expected output includes a detailed summary of patterns, an explanation of their selection and significance, and actionable advice for startup builders based on these insights.\n\n## extract_poc\nAnalyzes security or bug bounty reports to extract and provide proof of concept URLs for validating vulnerabilities. It specializes in identifying actionable URLs and commands from the reports, ensuring direct verification of reported vulnerabilities. The output includes the URL with a specific command to execute it, like using curl or python.\n\n## extract_predictions\nExtracts and organizes predictions from content into a structured format. It focuses on identifying specific predictions, their timelines, confidence levels, and verification methods. The expected output includes a bulleted list and a detailed table of these predictions.\n\n## extract_questions\nExtracts questions from content and analyzes their effectiveness in eliciting high-quality responses. It focuses on identifying the elements that make these questions particularly insightful. The expected output includes a list of questions, an analysis of their strengths, and recommendations for interviewers.\n\n## extract_recommendations\nExtracts and condenses recommendations from content into a concise list. This process involves identifying both explicit and implicit advice within the given material. The output is a bulleted list of up to 20 brief recommendations.\n\n## extract_references\nExtracts references to various forms of cultural and educational content from provided text. This process involves identifying and listing references to art, literature, and academic papers concisely. The expected output is a bulleted list of up to 20 references, each summarized in no more than 15 words.\n\n## extract_song_meaning\nAnalyzes and interprets the meaning of songs based on extensive research and lyric examination. This process involves deep analysis of the artist's background, song context, and lyrics to deduce the song's essence. Outputs include a summary sentence, detailed meaning in bullet points, and evidence supporting the interpretation.\n\n## extract_sponsors\nIdentifies and distinguishes between official and potential sponsors from transcripts. This process involves analyzing content to separate actual sponsors from merely mentioned companies. The output lists official sponsors and potential sponsors based on their mention in the content.\n\n## extract_videoid\nExtracts video IDs from URLs for use in other applications. It meticulously analyzes the URL to isolate the video ID. The output is solely the video ID, with no additional information or errors included.\n\n## extract_wisdom\nExtracts key insights, ideas, quotes, habits, and references from textual content to address the issue of information overload and the challenge of retaining knowledge. It uniquely filters and condenses valuable information from various texts, making it easier for users to decide if the content warrants a deeper review or to use as a note-taking alternative. The output includes summarized ideas, notable quotes, relevant habits, and useful references, all aimed at enhancing understanding and retention.\n\n## extract_wisdom_agents\nThis prompt outlines a complex process for extracting insights from text content, focusing on themes like the meaning of life and technology's impact on humanity. It involves creating teams of AI agents with diverse expertise to analyze the content and produce summaries, ideas, insights, quotes, habits, facts, references, and recommendations. The expected output includes structured sections filled with concise, insightful entries derived from the input material.\n\n## extract_wisdom_dm\nExtracts and synthesizes valuable content from input text, focusing on insights related to life's purpose and human advancement. It employs a structured approach to distill surprising ideas, insights, quotes, habits, facts, and recommendations from the content. The output includes summaries, ideas, insights, and other categorized information for deep understanding and practical application.\n\n## extract_wisdom_nometa\nThis prompt guides the extraction and organization of insightful content from text, focusing on life's purpose, human flourishing, and technology's impact. It emphasizes identifying and summarizing surprising ideas, refined insights, practical habits, notable quotes, valid facts, and useful recommendations related to these themes. The expected output includes structured sections for summaries, ideas, insights, quotes, habits, facts, recommendations, and references, each with specific content and formatting requirements.\n\n## find_hidden_message\nAnalyzes political messages to reveal overt and hidden intentions. It employs knowledge of politics, propaganda, and psychology to dissect content, focusing on recent political debates. The output includes overt messages, hidden cynical messages, supporting arguments, desired audience actions, and analyses from cynical to favorable.\n\n## find_logical_fallacies\nIdentifies and categorizes various fallacies in arguments or texts. This prompt focuses on recognizing invalid or faulty reasoning across a wide range of fallacies, from formal to informal types. The expected output is a list of identified fallacies with brief explanations.\n\n## get_wow_per_minute\nEvaluates the density of wow-factor in content by analyzing its surprise, novelty, insight, value, and wisdom. This process involves a detailed and varied consumption of the content to assess its potential to engage and enrich viewers. The expected output is a JSON report detailing scores and explanations for each wow-factor component and overall wow-factor per minute.\n\n## get_youtube_rss\nGenerates RSS URLs for YouTube channels based on given channel IDs or URLs. It extracts the channel ID from the input and constructs the corresponding RSS URL. The output is solely the RSS URL.\n\n## improve_academic_writing\nThis prompt aims to enhance the quality of text for academic purposes. It focuses on refining grammatical errors, improving clarity and coherence, and adopting an academic tone while ensuring ease of understanding. The expected output is a professionally refined text with a list of applied corrections.\n\n## improve_prompt\nThis service enhances LLM/AI prompts by applying expert prompt writing techniques to achieve better results. It leverages strategies like clear instructions, persona adoption, and reference text provision to refine prompts. The output is an improved version of the original prompt, optimized for clarity and effectiveness.\n\n## improve_report_finding\nThe prompt instructs the creation of an improved security finding report from a penetration test, detailing the finding, risk, recommendations, references, a concise summary, and insightful quotes, all formatted in markdown without using markdown syntax or special formatting. It emphasizes a detailed, insightful approach to presenting cybersecurity issues and solutions. The output should be comprehensive, covering various sections including title, description, risk, recommendations, references, and quotes, aiming for clarity and depth in reporting.\n\n## improve_writing\nThis prompt aims to refine input text for enhanced clarity, coherence, grammar, and style. It involves analyzing the text for errors and inconsistencies, then applying corrections while preserving the original meaning. The expected output is a grammatically correct and stylistically improved version of the text.\n\n## label_and_rate\nEvaluates and categorizes content based on its relevance to specific human-centric themes, then assigns a tiered rating and a numerical quality score. It uses a predefined set of labels for categorization and assesses content based on idea quantity and thematic alignment. The expected output is a structured JSON object detailing the content summary, labels, rating, and quality score with explanations.\n\n## official_pattern_template\nThe prompt outlines a complex process for diagnosing and addressing psychological issues based on a person's background and behaviors. It involves deep analysis of the individual's history, identifying potential mental health issues, and suggesting corrective actions. The expected output includes summaries of past events, possible psychological issues, their impact on behavior, and recommendations for improvement.\n\n## philocapsulate\nSummarizes teachings of philosophers or philosophies, providing detailed templates on their background, encapsulated philosophy, school, teachings, works, quotes, application, and life advice. It differentiates between individual philosophers and philosophies with tailored templates for each. The output includes structured information for educational or analytical purposes.\n\n## provide_guidance\nProvides comprehensive psychological advice tailored to the individual's specific question and context. This approach delves into the person's past, traumas, and life goals to offer targeted feedback and recommendations. The expected output includes a concise analysis, detailed scientific rationale, actionable recommendations, Esther Perel's perspective, self-reflection prompts, possible clinical diagnoses, and a summary, all aimed at fostering self-awareness and positive change.\n\n## rate_ai_response\nEvaluates the quality of AI responses against the benchmark of human experts, assigning a letter grade and score. It involves deep analysis of both the instructions given to the AI and its output, comparing these to the potential performance of the world's best human expert. The process culminates in a detailed justification for the assigned grade, highlighting specific strengths and weaknesses of the AI's response.\n\n## rate_ai_result\nEvaluates the quality of AI-generated content based on construction, quality, and spirit. The process involves analyzing AI outputs against criteria set by experts and a high-IQ AI panel. The expected output is a final score out of 100, with deductions detailed for each category.\n\n## rate_content\nThe prompt outlines a process for evaluating content by labeling it with relevant single-word descriptors, rating its quality based on idea quantity and thematic alignment, and scoring it on a scale from 1 to 100. It emphasizes the importance of matching content with specific themes related to human meaning and the future of AI, among others. The expected output includes a list of labels, a tiered rating with an explanation, and an overall quality score with justification.\n\n## rate_value\nThis prompt seeks to acknowledge the collaborative effort behind its creation, inspired by notable figures in information theory and viral content creation. It highlights the fusion of theoretical foundations and modern digital strategies. The output is an attribution of credit.\n\n## raw_query\nThe prompt instructs the AI to produce the best possible output by thoroughly analyzing and understanding the input. It emphasizes deep contemplation of the input's meaning and the sender's intentions. The expected output is an optimal response tailored to the inferred desires of the input provider.\n\n## recommend_artists\nRecommends a personalized festival schedule featuring artists similar to the user's preferences in EDM genres and artists. The recommendation process involves analyzing the user's favorite styles and artists, then selecting similar artists and explaining the choices. The output is a detailed schedule organized by day, set time, stage, and artist, optimized for the user's enjoyment.\n\n## show_fabric_options_markmap\nCreate a visual representation of the functionalities provided by the Fabric project, focusing on augmenting human capabilities with AI. The approach involves breaking down the project's capabilities into categories like summarization, analysis, and more, with specific patterns branching from these categories. The expected output is comprehensive Markmap code detailing this functionality map.\n\n## suggest\nAnalyzes user input to suggest appropriate fabric commands or patterns, enhancing the tool's functionality. It involves understanding specific needs, determining suitable commands, and providing clear, user-friendly suggestions. The output includes command suggestions, explanations, and instructions for new patterns.\n\n## summarize\nSummarizes content into a structured Markdown format, focusing on brevity and clarity. It extracts and lists the most crucial points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, adhering to specified word limits.\n\n## summarize_debate\nAnalyzes debates to identify and summarize the primary disagreements, arguments, and evidence that could change participants' minds. It breaks down complex discussions into concise summaries and evaluates argument strength, predicting outcomes. The output includes structured summaries and analyses of each party's position and evidence.\n\n## summarize_git_changes\nSummarizes major changes and upgrades in a GitHub project over the past week. It involves identifying key updates, then crafting a concise, enthusiastic summary and detailed bullet points highlighting these changes. The output includes a 20-word introduction and excitedly written update bullets.\n\n## summarize_git_diff\nAnalyzes Git diffs to summarize major changes and upgrades. It emphasizes creating concise bullet points for feature changes and updates, tailored to the extent of modifications. The expected output includes a 100-character intro sentence using conventional commits format.\n\n## summarize_micro\nSummarizes content into a structured Markdown format. This prompt focuses on concise, bullet-pointed summaries and takeaways. The output includes a one-sentence summary and lists of main points and takeaways.\n\n## summarize_newsletter\nExtracts and organizes key content from newsletters, focusing on the most meaningful, interesting, and useful information. It uniquely parses the entire newsletter to provide concise summaries, lists of content, opinions, tools, companies, and follow-up actions. The output includes sections for a brief summary, detailed content points, author opinions, mentioned tools and companies, and recommended follow-ups in a structured Markdown format.\n\n## summarize_paper\nSummarizes academic papers by extracting key sections such as title, authors, main goals, and more from the provided text. It employs a structured approach to highlight the paper's core aspects including technical methodology, distinctive features, and experimental outcomes. The output is a detailed summary covering various dimensions of the research.\n\n## summarize_pattern\nThis prompt instructs on summarizing AI chat prompts into concise paragraphs. It emphasizes using active voice and present tense for clarity. The expected output is a structured summary highlighting the prompt's purpose, approach, and anticipated results.\n\n## summarize_pull-requests\nSummarizes pull requests for a coding project, focusing on the types of changes made. It involves creating a summary and a detailed list of main PRs, rewritten for clarity. The output includes a concise overview and specific examples of pull requests.\n\n## summarize_rpg_session\nThis prompt outlines the process for summarizing in-person role-playing game sessions, focusing on key events, combat details, character development, and worldbuilding. It emphasizes capturing the essence of the session in a structured format, including summaries, lists, and descriptions to encapsulate the narrative and gameplay dynamics. The expected output includes a comprehensive overview of the session's storyline, character interactions, and significant moments, tailored for both players and observers.\n\n## to_flashcards\nCreates Anki cards from texts following specific principles to ensure simplicity, optimized wording, and no reliance on external context. This approach aims to enhance learning efficiency and comprehension without requiring prior knowledge of the text. The expected output is a set of questions and answers formatted as a CSV table.\n\n## tweet\nGuides users on crafting engaging tweets with emojis, focusing on Twitter's basics and content creation strategies. It emphasizes understanding Twitter, identifying the target audience, and using emojis effectively. The expected output is a comprehensive guide for creating appealing tweets with emojis.\n\n## write_essay\nThe task is to write an essay in the style of Paul Graham, focusing on the essence and approach of writing concise, clear, and illuminating essays on any given topic.\n\n## write_micro_essay\nThe task is to write an essay in the style of Paul Graham, focusing on the essence of simplicity in conveying complex ideas.\n\n## write_nuclei_template_rule\nThe purpose of this prompt is to guide the creation of Nuclei templates for cybersecurity applications, focusing on generating precise and efficient scanning templates for various protocols like HTTP, DNS, TCP, and more. It emphasizes the importance of incorporating elements such as matchers, extractors, and conditions to tailor the templates for detecting specific vulnerabilities or configurations. The expected output is a well-structured YAML Nuclei template that adheres to best practices in template creation, including handling dynamic data extraction, utilizing complex matchers, and ensuring accurate vulnerability detection with minimal false positives.\n\n## write_pull_request\nThe prompt instructs on drafting a detailed pull request (PR) description based on the output of a `git diff` command, focusing on identifying and explaining code changes. It emphasizes analyzing changes, understanding their purpose, and detailing their impact on the project. The expected output is a structured PR description in markdown, covering a summary of changes, reasons, impacts, and testing plans in clear language.\n\n## write_semgrep_rule\nThe prompt requests the creation of a Semgrep rule to detect a specific vulnerability pattern in code, based on provided context and examples. It emphasizes the importance of crafting a rule that is general enough to catch any instance of the described vulnerability, rather than being overly specific to the given examples. The expected output is a well-structured Semgrep rule that aligns with the syntax and guidelines detailed in the context, capable of identifying the vulnerability across different scenarios.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM, $CUSTOM_USER)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\nYou are an AI assistant tasked with creating a new feature for a fabric command-line tool. Your primary responsibility is to develop a pattern that suggests appropriate fabric patterns or commands based on user input. You are knowledgeable about fabric commands and understand the need to expand the tool's functionality. Your role involves analyzing user requests, determining the most suitable fabric commands or patterns, and providing helpful suggestions to users.\n\nTake a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n# STEPS\n- Analyze the user's input to understand their specific needs and context\n- Determine the appropriate fabric pattern or command based on the user's request\n- Generate a response that suggests the relevant fabric command(s) or pattern(s)\n- Provide explanations or multiple options when applicable\n- If no specific command is found, suggest using `create_pattern`\n\n# OUTPUT INSTRUCTIONS\n- Only output Markdown\n- Provide suggestions for fabric commands or patterns based on the user's input\n- Include explanations or multiple options when appropriate\n- If suggesting `create_pattern`, include instructions for saving and using the new pattern\n- Format the output to be clear and easy to understand for users new to fabric\n- Ensure the response aligns with the goal of making fabric more accessible and user-friendly\n- Ensure you follow ALL these instructions when creating your output\n\n# INPUT\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$CUSTOM_USER",
                        "value": "\nCONTENT: \n\n# OVERVIEW\n\nWhat It Does: Fabric is an open-source framework designed to augment human capabilities using AI, making it easier to integrate AI into daily tasks.\n\nWhy People Use It: Users leverage Fabric to seamlessly apply AI for solving everyday challenges, enhancing productivity, and fostering human creativity through technology.\n\n# HOW TO USE IT\n\nMost Common Syntax: The most common usage involves executing Fabric commands in the terminal, such as `fabric --pattern <PATTERN_NAME>`.\n\n# COMMON USE CASES\n\nFor Summarizing Content: `fabric --pattern summarize`\nFor Analyzing Claims: `fabric --pattern analyze_claims`\nFor Extracting Wisdom from Videos: `fabric --pattern extract_wisdom`\nFor Creating AI Agents: `echo \\\"<TASK>\\\" | fabric --agents`\nFor creating custom patterns: `fabric --pattern create_pattern`\n- One possible place to store them is ~/.config/custom-fabric-patterns.\n- Then when you want to use them, simply copy them into ~/.config/fabric/patterns.\n`cp -a ~/.config/custom-fabric-patterns/* ~/.config/fabric/patterns/`\n- Now you can run them with: `pbpaste | fabric -p your_custom_pattern`\n\n\n# MOST IMPORTANT AND USED OPTIONS AND FEATURES\n\n- **--pattern PATTERN, -p PATTERN**: Specifies the pattern (prompt) to use. Useful for applying specific AI prompts to your input.\n  \n- **--agents, -a**: Creates an AI agent to perform a task based on the input. Great for automating complex tasks with AI.\n  \n- **--stream, -s**: Streams results in real-time. Ideal for getting immediate feedback from AI operations.\n  \n- **--update, -u**: Updates patterns. Ensures you're using the latest AI prompts for your tasks.\n  \n- **--model MODEL, -m MODEL**: Selects the AI model to use. Allows customization of the AI backend for different tasks.\n  \n- **--setup**: Sets up your Fabric instance. Essential for first-time users to configure Fabric correctly.\n  \n- **--list, -l**: Lists available patterns. Helps users discover new AI prompts for various applications.\n  \n- **--context, -c**: Uses a Context file to add context to your pattern. Enhances the relevance of AI responses by providing additional background information.\n\n# PATTERNS\n\n## agility_story\nGenerates user stories and acceptance criteria for specified topics, focusing on Agile framework principles. This prompt specializes in translating topics into structured Agile documentation, specifically for user story and acceptance criteria creation. The expected output is a JSON-formatted document detailing the topic, user story, and acceptance criteria.\n\n## ai\nSummarizes and responds to questions with insightful bullet points. It involves creating a mental model of the question for deeper understanding. The output consists of 3-5 concise bullet points, each with a 10-word limit.\n\n## analyze_answers\nEvaluates the correctness of answers provided by learners to questions generated by a complementary quiz creation pattern. It aims to assess understanding of learning objectives and identify areas needing further study. The expected output is an analysis of the learner's answers, indicating their grasp of the subject matter.\n\n## analyze_claims\nAnalyzes and rates the truth claims in input, providing evidence for and against, along with a balanced view. It separates truth claims from arguments, offering a nuanced analysis with ratings and labels for each claim. The output includes a summary, evidence, refutations, logical fallacies, ratings, labels, and an overall score and analysis.\n\n## analyze_debate\nAnalyzes debate transcripts to help users understand different viewpoints and broaden their perspectives. It maps out claims, analyzes them neutrally, and rates the debate's insightfulness and emotionality. The output includes scores, participant emotionality, argument summaries with sources, and lists of agreements, disagreements, misunderstandings, learnings, and takeaways.\n\n## analyze_incident\nSummarizes cybersecurity breach articles by extracting key information efficiently, focusing on conciseness and organization. It avoids inferential conclusions, relying solely on the article's content for details like attack date, type, and impact. The output is a structured summary with specific details about the cybersecurity incident, including attack methods, vulnerabilities, and recommendations for prevention.\n\n## analyze_logs\nAnalyzes a server log file to identify patterns, anomalies, and potential issues, aiming to enhance the server's reliability and performance. The process involves a detailed examination of log entries, assessment of operational reliability, and identification of recurring issues. Recommendations for improvements are provided based on data-driven analysis, excluding personal opinions and irrelevant information.\n\n## analyze_malware\nAnalyzes malware across various platforms, focusing on extracting indicators of compromise and detailed malware behavior. This approach includes analyzing telemetry and community data to aid in malware detection and analysis. The expected output includes a summary of findings, potential indicators of compromise, Mitre Att&CK techniques, pivoting advice, detection strategies, suggested Yara rules, additional references, and technical recommendations.\n\n## analyze_paper\nThis service analyzes research papers to determine their main findings, scientific rigor, and quality. It uniquely maps out claims, evaluates study design, and assesses conflicts of interest. The output includes a summary, author details, findings, study quality, and a final grade with explanations.\n\n## analyze_patent\nThe prompt outlines the role and responsibilities of a patent examiner, emphasizing the importance of technical and legal expertise in evaluating patents. It details the steps for examining a patent, including identifying the technology field, problem addressed, solution, advantages, novelty, and inventive step, and summarizing the core idea and keywords. The expected output involves detailed analysis and documentation in specific sections without concern for length, using bullet points for clarity.\n\n## analyze_personality\nPerforms in-depth psychological analysis on the main individual in the provided input. It involves identifying the primary person, deeply contemplating their language and responses, and comparing these to known human psychology principles. The output includes a concise psychological profile summary and detailed supporting points.\n\n## analyze_presentation\nAnalyzes and critiques presentations, focusing on content, speaker's psychology, and the difference between stated and actual goals. It involves comparing intended messages to actual content, including self-references and entertainment attempts. The output includes scores and summaries for ideas, selflessness, and entertainment, plus an overall analysis.\n\n## analyze_prose\nEvaluates the quality of writing by assessing its novelty, clarity, and prose, and provides improvement recommendations. It uses a detailed approach to rate each aspect on a specific scale and ensures the overall rating reflects the lowest individual score. The expected output includes ratings and concise improvement tips.\n\n## analyze_prose_json\nEvaluates the quality of writing and content, providing ratings and recommendations for improvement based on novelty, clarity, and overall messaging. It assesses ideas for their freshness and originality, clarity of argument, and quality of prose, offering a structured approach to critique. The expected output is a JSON object summarizing these evaluations and recommendations.\n\n## analyze_prose_pinker\nEvaluates prose based on Steven Pinker's writing principles, identifying its current style and recommending improvements for clarity and engagement. It involves analyzing the text's adherence to Pinker's stylistic categories and avoiding common pitfalls in writing. The output includes a detailed analysis of the prose's style, strengths, weaknesses, and specific examples of both effective and ineffective writing elements.\n\n## analyze_spiritual_text\nAnalyzes spiritual texts to highlight surprising claims and contrasts them with the King James Bible. This approach involves detailed comparison, providing examples from both texts to illustrate differences. The output consists of concise bullet points summarizing these findings.\n\n## analyze_tech_impact\nAnalyzes the societal impact of technology projects by breaking down their intentions, outcomes, and broader implications, including ethical considerations. It employs a structured approach, detailing the project's objectives, technologies used, target audience, outcomes, societal impact, ethical considerations, and sustainability. The expected output includes summaries, lists, and analyses across specified sections.\n\n## analyze_threat_report\nThe prompt instructs a super-intelligent cybersecurity expert to analyze and extract key insights from cybersecurity threat reports. It emphasizes identifying new, interesting, and surprising information, and organizing these findings into concise, categorized summaries. The expected output includes a one-sentence summary, trends, statistics, quotes, references, and recommendations from the report, all formatted in plain language and without repetition.\n\n## analyze_threat_report_trends\nAnalyzes cybersecurity threat reports to identify up to 50 unique, surprising, and insightful trends. This process involves a deep, expert analysis to uncover new and interesting information. The expected output is a list of trends without repetition or formatting embellishments.\n\n## answer_interview_question\nGenerates tailored responses to technical interview questions, aiming for a casual yet insightful tone. The AI draws from a technical knowledge base and professional experiences to construct responses that demonstrate depth and alternative perspectives. Outputs are structured first-person responses, including context, main explanation, alternative approach, and evidence-based conclusion.\n\n## ask_secure_by_design_questions\nGenerates a comprehensive set of security-focused questions tailored to the fundamental design of a specific project. This process involves deep analysis and conceptualization of the project's components and their security needs. The output includes a summary and a detailed list of security questions organized by themes.\n\n## capture_thinkers_work\nSummarizes teachings and philosophies of notable individuals or philosophical schools, providing detailed templates on their backgrounds, ideas, and applications. It offers a structured approach to encapsulating complex thoughts into accessible summaries. The output includes encapsulations, background information, schools of thought, impactful ideas, primary teachings, works, quotes, applications, and life advice.\n\n## check_agreement\nThe prompt outlines a process for analyzing contracts and agreements to identify potential issues or \\\"gotchas.\\\" It involves summarizing the document, listing important aspects, categorizing issues by severity, and drafting responses for critical and important items. The expected output includes a concise summary, detailed callouts, categorized issues, and recommended responses in Markdown format.\n\n## clean_text\nSummarizes and corrects formatting issues in text without altering the content. It focuses on removing odd line breaks to improve readability. The expected output is a clean, well-formatted version of the original text.\n\n## coding_master\nExplains coding concepts or languages to beginners, using examples from reputable sources and illustrating points with formatted code. The approach emphasizes clarity and accessibility, incorporating examples from Codeacademy and NetworkChuck. Outputs include markdown-formatted code and structured lists of ideas, recommendations, habits, facts, and insights, adhering to specific word counts.\n\n## compare_and_contrast\nCompares and contrasts a list of items, focusing on their differences and similarities. The approach involves analyzing the items across various topics, organizing the findings into a markdown table. The expected output is a structured comparison in table format.\n\n## create_5_sentence_summary\nGenerates concise summaries or answers at five decreasing levels of depth. It involves deep understanding and thoughtful analysis of the input. The output is a structured list capturing the essence in 5, 4, 3, 2, and 1 word(s).\n\n## create_academic_paper\nProduces high-quality, authoritative Latex academic papers with clear concept explanations. It focuses on logical layout and simplicity while maintaining a professional appearance. The expected output is LateX code formatted in a two-column layout with a header and footer.\n\n## create_ai_jobs_analysis\nAnalyzes job reports to identify roles least and most vulnerable to automation, offering strategies for enhancing job security. It leverages historical insights to predict automation's impact on various job categories. The output includes a detailed analysis and recommendations for resilience against automation.\n\n## create_aphorisms\nGenerates a list of 20 aphorisms related to the given topic(s), ensuring variety in their beginnings. It focuses on sourcing quotes from real individuals. The output includes each aphorism followed by the name of the person who said it.\n\n## create_art_prompt\nThe prompt guides an expert artist in conceptualizing and instructing AI to create art that perfectly encapsulates a given concept. It emphasizes deep thought on the concept and its visual representation, aiming for compelling and interesting artwork. The expected output is a 100-word description that not only instructs the AI on what to create but also how the art should evoke feelings and suggest style through examples.\n\n## create_better_frame\nThe essay explores the concept of framing as a way to construct and interpret reality through different lenses, emphasizing the power of perspective in shaping one's experience of the world. It highlights various dichotomies in perceptions around topics like AI, race/gender, success, personal identity, and control over life, illustrating how different frames can lead to vastly different outlooks and outcomes. The author argues for the importance of choosing positive frames to improve individual and collective realities, suggesting that changing frames can change outcomes and foster more positive social dynamics.\n\n## create_coding_project\nGenerates wireframes and starter code for coding projects based on user ideas. It specifically caters to transforming ideas into actionable project outlines and code skeletons, including detailed steps and file structures. The output includes project summaries, structured directories, and initial code setups.\n\n## create_command\nGenerates specific command lines for various penetration testing tools based on a brief description of the desired outcome. This approach leverages the tool's help documentation to ensure accuracy and relevance. The expected output is a precise command that aligns with the user's objectives for the tool.\n\n## create_cyber_summary\nThe prompt instructs on creating a comprehensive summary of cybersecurity threats, vulnerabilities, incidents, and malware for a technical audience. It emphasizes deep understanding through repetitive analysis and visualization techniques. The expected output includes a concise summary and categorized lists of cybersecurity issues.\n\n## create_git_diff_commit\nThis prompt provides instructions for using specific Git commands to manage code changes. It explains how to view differences since the last commit and display the current state of the repository. The expected output is a guide on executing these commands.\n\n## create_idea_compass\nGuides users in developing a structured exploration of ideas through a detailed template. It emphasizes clarity and organization by breaking down the process into specific steps, including defining, supporting, and contextualizing the idea. The expected output is a comprehensive summary with related ideas, evidence, and sources organized in a structured format.\n\n## create_investigation_visualization\nCreates detailed GraphViz visualizations to illustrate complex intelligence investigations and data insights. This approach involves extensive analysis, organizing information, and visual representation using shapes, colors, and labels for clarity. The output includes a comprehensive diagram and analytical conclusions with a certainty rating.\n\n## create_keynote\nThe prompt guides in creating TED-quality keynote presentations from provided input, focusing on narrative flow and practical takeaways. It outlines steps for structuring the presentation into slides with concise bullet points, images, and speaker notes. The expected output includes a story flow, the final takeaway, and a detailed slide deck presentation.\n\n## create_logo\nGenerates simple, minimalist company logos based on provided input, focusing on elegance and impact without text. The approach emphasizes super minimalist designs. The output is a prompt for an AI image generator to create a simple, vector graphic logo.\n\n## create_markmap_visualization\nTransforms complex ideas into visual formats using MarkMap syntax for easy understanding. This process involves simplifying concepts to ensure they can be effectively represented within the constraints of MarkMap. The output is a MarkMap syntax diagram that visually communicates the core ideas.\n\n## create_mermaid_visualization\nTransforms complex ideas into simplified Mermaid (Markdown) visual diagrams. This process involves creating detailed visualizations that can independently explain concepts using Mermaid syntax, focusing on clarity and comprehensibility. The expected output is a Mermaid syntax diagram accompanied by a concise visual explanation.\n\n## create_micro_summary\nSummarizes content into a Markdown formatted summary, focusing on brevity and clarity. It emphasizes creating concise, impactful points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, each adhering to strict word limits.\n\n## create_network_threat_landscape\nAnalyzes open ports and services from network scans to identify security risks and provide recommendations. This process involves a detailed examination of port and service statistics to uncover potential vulnerabilities. The expected output is a markdown formatted threat report with sections on description, risk, recommendations, a concise summary, trends, and quotes from the analysis.\n\n## create_npc\nGenerates detailed NPCs for D&D 5th edition, incorporating a wide range of characteristics from background to appearance. It emphasizes creativity in developing a character's backstory, traits, and goals. The output is a comprehensive character profile suitable for gameplay.\n\n## create_pattern\nThe AI assistant is designed to interpret and respond to LLM/AI prompts with structured outputs. It specializes in organizing and analyzing prompts to produce responses that adhere to specific instructions and formatting requirements. The assistant ensures accuracy and alignment with the intended outcomes through meticulous analysis.\n\n## create_quiz\nGenerates questions for reviewing learning objectives based on provided subject and objectives. It requires defining the subject and learning objectives for accurate question generation. The output consists of questions aimed at helping students review key concepts.\n\n## create_reading_plan\nDesigns a tailored three-phase reading plan based on user input, focusing on an author or specific guidance. It carefully selects books from various sources, including hidden gems, to enhance the user's knowledge on the topic. The output includes a concise plan summary and categorized reading lists with reasons for each selection.\n\n## create_report_finding\nThe prompt instructs the creation of a detailed markdown security finding report, incorporating sections like Description, Risk, Recommendations, and others, based on a vulnerability title and explanation provided by the user. It emphasizes a structured, insightful approach to documenting cybersecurity vulnerabilities. The expected output is a comprehensive report with specific sections, focusing on clarity, insightfulness, and relevance to cybersecurity assessment.\n\n## create_security_update\nThe prompt instructs on creating concise security updates for newsletters, focusing on cybersecurity developments, threats, advisories, and new vulnerabilities. It emphasizes brevity and relevance, requiring links to further information. The expected output includes structured sections with short descriptions and relevant details, aiming to inform readers about the latest security concerns efficiently.\n\n## create_show_intro\nCreates compelling short intros for podcasts, focusing on the most interesting aspects of the show. It involves listening to the entire show, identifying key topics, and highlighting them in a concise introduction. The output is a structured intro that teases the conversation's main points.\n\n## create_stride_threat_model\nThe prompt instructs on creating a detailed threat model using the STRIDE per element methodology for a given system design document. It emphasizes understanding the system's assets, trust boundaries, and data flows to identify and prioritize potential threats. The expected output is a comprehensive table listing threats, their components, mitigation strategies, and risk assessments.\n\n## create_summary\nSummarizes content into a structured Markdown format, focusing on brevity and clarity. It emphasizes creating a concise summary, listing main points, and identifying key takeaways. The output is organized into specific sections for easy reference.\n\n## create_threat_model\nThe prompt outlines a comprehensive approach to everyday threat modeling, emphasizing its application beyond technical defenses to include personal and physical security scenarios. It distinguishes between realistic and possible threats, advocating for a balanced approach to risk management that considers the value of what's being protected, the likelihood of threats, and the cost of controls. The expected output involves creating threat models for various scenarios, highlighting realistic defenses, and guiding individuals towards logical security decisions through structured analysis.\n\n## create_threat_scenarios\nThe prompt seeks to identify and prioritize potential threats to a given system or situation, using a narrative-based, simple threat modeling approach. It emphasizes distinguishing between realistic and possible threats, focusing on those worth defending against. The expected output includes a list of prioritized threat scenarios, an analysis of the threat model, recommended controls, a narrative analysis, and a concise conclusion.\n\n## create_upgrade_pack\nExtracts and organizes insights on world models and task algorithms from provided content. It focuses on identifying and categorizing beliefs about the world and optimal task execution strategies. The output includes concise, actionable bullet points under relevant categories.\n\n## create_video_chapters\nExtracts and organizes the most engaging topics from a transcript with corresponding timestamps. This process involves a detailed review of the transcript to identify key moments and subjects. The output is a list of topics with their timestamps in a sequential format.\n\n## create_visualization\nTransforms complex ideas into simplified ASCII art visualizations. This approach focuses on distilling intricate concepts into visual forms that can be easily understood through ASCII art. The expected output is a detailed ASCII art representation accompanied by a concise visual explanation.\n\n## explain_code\nAnalyzes and explains code, security tool outputs, or configuration texts, tailoring the explanation to the type of input. It uses specific sections to clarify the function, implications, or settings based on the input's nature. The expected output is a detailed explanation or answer in designated sections.\n\n## explain_docs\nThe prompt instructs on transforming input about tool usage into improved, structured documentation. It emphasizes clarity and utility, breaking down the process into specific sections for a comprehensive guide. The expected output includes an overview, usage syntax, common use cases, and key features of the tool.\n\n## explain_project\nSummarizes project documentation into a concise, user and developer-focused summary, highlighting its purpose, problem addressed, approach, installation, usage, and examples. It simplifies complex information for easy understanding and application. The output includes a project overview, problem it addresses, approach to solving the problem, and practical steps for installation and usage.\n\n## explain_terms\nProduces a glossary of advanced terms found in specific content, including definitions and analogies. It focuses on explaining obscure or complex terms to aid understanding. The output is a list of terms with explanations and analogies in a structured Markdown format.\n\n## extract_algorithm_update_recommendations\nAnalyzes input to provide concise recommendations for improving processes. It focuses on extracting actionable advice from content descriptions. The output consists of a bulleted list of up to three brief suggestions.\n\n## extract_article_wisdom\nExtracts key insights and valuable information from textual content, focusing on ideas, quotes, habits, and references. It aims to address the issue of information overload by providing a concise summary of the content's most meaningful aspects. The expected output includes summarized ideas, notable quotes, referenced materials, and habits worth adopting.\n\n## extract_book_ideas\nSummarizes a book's key content by extracting 50 to 100 of its most interesting ideas. The process involves a deep dive into the book's insights, prioritizing them by interest and insightfulness. The output is a concise list of bulleted ideas, limited to 20 words each.\n\n## extract_book_recommendations\nSummarizes a book's key content by extracting 50 to 100 of its most practical recommendations, prioritizing the most impactful advice. This process involves a thorough memory search to identify actionable insights. The output is formatted as an instructive, bullet-pointed list, limited to 20 words each.\n\n## extract_business_ideas\nThe prompt outlines a process for identifying and elaborating on innovative business ideas. It focuses on extracting top business concepts from provided content and then refining the best ten by exploring adjacent possibilities. The expected output includes two sections: a list of extracted ideas and a detailed elaboration on the top ten ideas, ensuring uniqueness and differentiation.\n\n## extract_extraordinary_claims\nIdentifies and lists extraordinary claims from conversations, focusing on those rejected by the scientific community or based on misinformation. The process involves deep analysis to pinpoint statements that defy accepted scientific truths, such as denying evolution or the moon landing. The output is a detailed list of quotes, ranging from 50 to 100, showcasing these claims.\n\n## extract_ideas\nExtracts and condenses insightful ideas from text into 15-word bullet points focusing on life's purpose and human progress. This process emphasizes capturing unique insights on specified themes. The output consists of a list of concise, thought-provoking ideas.\n\n## extract_insights\nExtracts and condenses complex insights from text on profound topics into 15-word bullet points. This process emphasizes the extraction of nuanced, powerful ideas related to human and technological advancement. The expected output is a concise list of abstracted, insightful bullets.\n\n## extract_main_idea\nExtracts and highlights the most crucial or intriguing idea from any given content. This prompt emphasizes a methodical approach to identify and articulate the essence of the input. The expected output includes a concise main idea and a recommendation based on that idea.\n\n## extract_patterns\nThe prompt guides in identifying and analyzing recurring, surprising, or insightful patterns from a collection of ideas, data, or observations. It emphasizes extracting the most notable patterns based on their frequency and significance, and then documenting the process of discovery and analysis. The expected output includes a detailed summary of patterns, an explanation of their selection and significance, and actionable advice for startup builders based on these insights.\n\n## extract_poc\nAnalyzes security or bug bounty reports to extract and provide proof of concept URLs for validating vulnerabilities. It specializes in identifying actionable URLs and commands from the reports, ensuring direct verification of reported vulnerabilities. The output includes the URL with a specific command to execute it, like using curl or python.\n\n## extract_predictions\nExtracts and organizes predictions from content into a structured format. It focuses on identifying specific predictions, their timelines, confidence levels, and verification methods. The expected output includes a bulleted list and a detailed table of these predictions.\n\n## extract_questions\nExtracts questions from content and analyzes their effectiveness in eliciting high-quality responses. It focuses on identifying the elements that make these questions particularly insightful. The expected output includes a list of questions, an analysis of their strengths, and recommendations for interviewers.\n\n## extract_recommendations\nExtracts and condenses recommendations from content into a concise list. This process involves identifying both explicit and implicit advice within the given material. The output is a bulleted list of up to 20 brief recommendations.\n\n## extract_references\nExtracts references to various forms of cultural and educational content from provided text. This process involves identifying and listing references to art, literature, and academic papers concisely. The expected output is a bulleted list of up to 20 references, each summarized in no more than 15 words.\n\n## extract_song_meaning\nAnalyzes and interprets the meaning of songs based on extensive research and lyric examination. This process involves deep analysis of the artist's background, song context, and lyrics to deduce the song's essence. Outputs include a summary sentence, detailed meaning in bullet points, and evidence supporting the interpretation.\n\n## extract_sponsors\nIdentifies and distinguishes between official and potential sponsors from transcripts. This process involves analyzing content to separate actual sponsors from merely mentioned companies. The output lists official sponsors and potential sponsors based on their mention in the content.\n\n## extract_videoid\nExtracts video IDs from URLs for use in other applications. It meticulously analyzes the URL to isolate the video ID. The output is solely the video ID, with no additional information or errors included.\n\n## extract_wisdom\nExtracts key insights, ideas, quotes, habits, and references from textual content to address the issue of information overload and the challenge of retaining knowledge. It uniquely filters and condenses valuable information from various texts, making it easier for users to decide if the content warrants a deeper review or to use as a note-taking alternative. The output includes summarized ideas, notable quotes, relevant habits, and useful references, all aimed at enhancing understanding and retention.\n\n## extract_wisdom_agents\nThis prompt outlines a complex process for extracting insights from text content, focusing on themes like the meaning of life and technology's impact on humanity. It involves creating teams of AI agents with diverse expertise to analyze the content and produce summaries, ideas, insights, quotes, habits, facts, references, and recommendations. The expected output includes structured sections filled with concise, insightful entries derived from the input material.\n\n## extract_wisdom_dm\nExtracts and synthesizes valuable content from input text, focusing on insights related to life's purpose and human advancement. It employs a structured approach to distill surprising ideas, insights, quotes, habits, facts, and recommendations from the content. The output includes summaries, ideas, insights, and other categorized information for deep understanding and practical application.\n\n## extract_wisdom_nometa\nThis prompt guides the extraction and organization of insightful content from text, focusing on life's purpose, human flourishing, and technology's impact. It emphasizes identifying and summarizing surprising ideas, refined insights, practical habits, notable quotes, valid facts, and useful recommendations related to these themes. The expected output includes structured sections for summaries, ideas, insights, quotes, habits, facts, recommendations, and references, each with specific content and formatting requirements.\n\n## find_hidden_message\nAnalyzes political messages to reveal overt and hidden intentions. It employs knowledge of politics, propaganda, and psychology to dissect content, focusing on recent political debates. The output includes overt messages, hidden cynical messages, supporting arguments, desired audience actions, and analyses from cynical to favorable.\n\n## find_logical_fallacies\nIdentifies and categorizes various fallacies in arguments or texts. This prompt focuses on recognizing invalid or faulty reasoning across a wide range of fallacies, from formal to informal types. The expected output is a list of identified fallacies with brief explanations.\n\n## get_wow_per_minute\nEvaluates the density of wow-factor in content by analyzing its surprise, novelty, insight, value, and wisdom. This process involves a detailed and varied consumption of the content to assess its potential to engage and enrich viewers. The expected output is a JSON report detailing scores and explanations for each wow-factor component and overall wow-factor per minute.\n\n## get_youtube_rss\nGenerates RSS URLs for YouTube channels based on given channel IDs or URLs. It extracts the channel ID from the input and constructs the corresponding RSS URL. The output is solely the RSS URL.\n\n## improve_academic_writing\nThis prompt aims to enhance the quality of text for academic purposes. It focuses on refining grammatical errors, improving clarity and coherence, and adopting an academic tone while ensuring ease of understanding. The expected output is a professionally refined text with a list of applied corrections.\n\n## improve_prompt\nThis service enhances LLM/AI prompts by applying expert prompt writing techniques to achieve better results. It leverages strategies like clear instructions, persona adoption, and reference text provision to refine prompts. The output is an improved version of the original prompt, optimized for clarity and effectiveness.\n\n## improve_report_finding\nThe prompt instructs the creation of an improved security finding report from a penetration test, detailing the finding, risk, recommendations, references, a concise summary, and insightful quotes, all formatted in markdown without using markdown syntax or special formatting. It emphasizes a detailed, insightful approach to presenting cybersecurity issues and solutions. The output should be comprehensive, covering various sections including title, description, risk, recommendations, references, and quotes, aiming for clarity and depth in reporting.\n\n## improve_writing\nThis prompt aims to refine input text for enhanced clarity, coherence, grammar, and style. It involves analyzing the text for errors and inconsistencies, then applying corrections while preserving the original meaning. The expected output is a grammatically correct and stylistically improved version of the text.\n\n## label_and_rate\nEvaluates and categorizes content based on its relevance to specific human-centric themes, then assigns a tiered rating and a numerical quality score. It uses a predefined set of labels for categorization and assesses content based on idea quantity and thematic alignment. The expected output is a structured JSON object detailing the content summary, labels, rating, and quality score with explanations.\n\n## official_pattern_template\nThe prompt outlines a complex process for diagnosing and addressing psychological issues based on a person's background and behaviors. It involves deep analysis of the individual's history, identifying potential mental health issues, and suggesting corrective actions. The expected output includes summaries of past events, possible psychological issues, their impact on behavior, and recommendations for improvement.\n\n## philocapsulate\nSummarizes teachings of philosophers or philosophies, providing detailed templates on their background, encapsulated philosophy, school, teachings, works, quotes, application, and life advice. It differentiates between individual philosophers and philosophies with tailored templates for each. The output includes structured information for educational or analytical purposes.\n\n## provide_guidance\nProvides comprehensive psychological advice tailored to the individual's specific question and context. This approach delves into the person's past, traumas, and life goals to offer targeted feedback and recommendations. The expected output includes a concise analysis, detailed scientific rationale, actionable recommendations, Esther Perel's perspective, self-reflection prompts, possible clinical diagnoses, and a summary, all aimed at fostering self-awareness and positive change.\n\n## rate_ai_response\nEvaluates the quality of AI responses against the benchmark of human experts, assigning a letter grade and score. It involves deep analysis of both the instructions given to the AI and its output, comparing these to the potential performance of the world's best human expert. The process culminates in a detailed justification for the assigned grade, highlighting specific strengths and weaknesses of the AI's response.\n\n## rate_ai_result\nEvaluates the quality of AI-generated content based on construction, quality, and spirit. The process involves analyzing AI outputs against criteria set by experts and a high-IQ AI panel. The expected output is a final score out of 100, with deductions detailed for each category.\n\n## rate_content\nThe prompt outlines a process for evaluating content by labeling it with relevant single-word descriptors, rating its quality based on idea quantity and thematic alignment, and scoring it on a scale from 1 to 100. It emphasizes the importance of matching content with specific themes related to human meaning and the future of AI, among others. The expected output includes a list of labels, a tiered rating with an explanation, and an overall quality score with justification.\n\n## rate_value\nThis prompt seeks to acknowledge the collaborative effort behind its creation, inspired by notable figures in information theory and viral content creation. It highlights the fusion of theoretical foundations and modern digital strategies. The output is an attribution of credit.\n\n## raw_query\nThe prompt instructs the AI to produce the best possible output by thoroughly analyzing and understanding the input. It emphasizes deep contemplation of the input's meaning and the sender's intentions. The expected output is an optimal response tailored to the inferred desires of the input provider.\n\n## recommend_artists\nRecommends a personalized festival schedule featuring artists similar to the user's preferences in EDM genres and artists. The recommendation process involves analyzing the user's favorite styles and artists, then selecting similar artists and explaining the choices. The output is a detailed schedule organized by day, set time, stage, and artist, optimized for the user's enjoyment.\n\n## show_fabric_options_markmap\nCreate a visual representation of the functionalities provided by the Fabric project, focusing on augmenting human capabilities with AI. The approach involves breaking down the project's capabilities into categories like summarization, analysis, and more, with specific patterns branching from these categories. The expected output is comprehensive Markmap code detailing this functionality map.\n\n## suggest\nAnalyzes user input to suggest appropriate fabric commands or patterns, enhancing the tool's functionality. It involves understanding specific needs, determining suitable commands, and providing clear, user-friendly suggestions. The output includes command suggestions, explanations, and instructions for new patterns.\n\n## summarize\nSummarizes content into a structured Markdown format, focusing on brevity and clarity. It extracts and lists the most crucial points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, adhering to specified word limits.\n\n## summarize_debate\nAnalyzes debates to identify and summarize the primary disagreements, arguments, and evidence that could change participants' minds. It breaks down complex discussions into concise summaries and evaluates argument strength, predicting outcomes. The output includes structured summaries and analyses of each party's position and evidence.\n\n## summarize_git_changes\nSummarizes major changes and upgrades in a GitHub project over the past week. It involves identifying key updates, then crafting a concise, enthusiastic summary and detailed bullet points highlighting these changes. The output includes a 20-word introduction and excitedly written update bullets.\n\n## summarize_git_diff\nAnalyzes Git diffs to summarize major changes and upgrades. It emphasizes creating concise bullet points for feature changes and updates, tailored to the extent of modifications. The expected output includes a 100-character intro sentence using conventional commits format.\n\n## summarize_micro\nSummarizes content into a structured Markdown format. This prompt focuses on concise, bullet-pointed summaries and takeaways. The output includes a one-sentence summary and lists of main points and takeaways.\n\n## summarize_newsletter\nExtracts and organizes key content from newsletters, focusing on the most meaningful, interesting, and useful information. It uniquely parses the entire newsletter to provide concise summaries, lists of content, opinions, tools, companies, and follow-up actions. The output includes sections for a brief summary, detailed content points, author opinions, mentioned tools and companies, and recommended follow-ups in a structured Markdown format.\n\n## summarize_paper\nSummarizes academic papers by extracting key sections such as title, authors, main goals, and more from the provided text. It employs a structured approach to highlight the paper's core aspects including technical methodology, distinctive features, and experimental outcomes. The output is a detailed summary covering various dimensions of the research.\n\n## summarize_pattern\nThis prompt instructs on summarizing AI chat prompts into concise paragraphs. It emphasizes using active voice and present tense for clarity. The expected output is a structured summary highlighting the prompt's purpose, approach, and anticipated results.\n\n## summarize_pull-requests\nSummarizes pull requests for a coding project, focusing on the types of changes made. It involves creating a summary and a detailed list of main PRs, rewritten for clarity. The output includes a concise overview and specific examples of pull requests.\n\n## summarize_rpg_session\nThis prompt outlines the process for summarizing in-person role-playing game sessions, focusing on key events, combat details, character development, and worldbuilding. It emphasizes capturing the essence of the session in a structured format, including summaries, lists, and descriptions to encapsulate the narrative and gameplay dynamics. The expected output includes a comprehensive overview of the session's storyline, character interactions, and significant moments, tailored for both players and observers.\n\n## to_flashcards\nCreates Anki cards from texts following specific principles to ensure simplicity, optimized wording, and no reliance on external context. This approach aims to enhance learning efficiency and comprehension without requiring prior knowledge of the text. The expected output is a set of questions and answers formatted as a CSV table.\n\n## tweet\nGuides users on crafting engaging tweets with emojis, focusing on Twitter's basics and content creation strategies. It emphasizes understanding Twitter, identifying the target audience, and using emojis effectively. The expected output is a comprehensive guide for creating appealing tweets with emojis.\n\n## write_essay\nThe task is to write an essay in the style of Paul Graham, focusing on the essence and approach of writing concise, clear, and illuminating essays on any given topic.\n\n## write_micro_essay\nThe task is to write an essay in the style of Paul Graham, focusing on the essence of simplicity in conveying complex ideas.\n\n## write_nuclei_template_rule\nThe purpose of this prompt is to guide the creation of Nuclei templates for cybersecurity applications, focusing on generating precise and efficient scanning templates for various protocols like HTTP, DNS, TCP, and more. It emphasizes the importance of incorporating elements such as matchers, extractors, and conditions to tailor the templates for detecting specific vulnerabilities or configurations. The expected output is a well-structured YAML Nuclei template that adheres to best practices in template creation, including handling dynamic data extraction, utilizing complex matchers, and ensuring accurate vulnerability detection with minimal false positives.\n\n## write_pull_request\nThe prompt instructs on drafting a detailed pull request (PR) description based on the output of a `git diff` command, focusing on identifying and explaining code changes. It emphasizes analyzing changes, understanding their purpose, and detailing their impact on the project. The expected output is a structured PR description in markdown, covering a summary of changes, reasons, impacts, and testing plans in clear language.\n\n## write_semgrep_rule\nThe prompt requests the creation of a Semgrep rule to detect a specific vulnerability pattern in code, based on provided context and examples. It emphasizes the importance of crafting a rule that is general enough to catch any instance of the described vulnerability, rather than being overly specific to the given examples. The expected output is a well-structured Semgrep rule that aligns with the syntax and guidelines detailed in the context, capable of identifying the vulnerability across different scenarios.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            },
                            {
                              "type": "register",
                              "value": "$CUSTOM_USER"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5886441469192505,
        0.28677868843078613,
        0.08902224898338318,
        0.12280142307281494,
        0.31594550609588623,
        -0.38255083560943604,
        -1.1607238054275513,
        0.5467286705970764,
        -0.0019102180376648903,
        -0.4317125678062439,
        -0.5601319670677185,
        0.9745889902114868,
        0.09453699737787247,
        -0.08697852492332458,
        0.04104481637477875,
        -0.3621521592140198,
        -0.46035581827163696,
        -0.6293948888778687,
        -1.213895320892334,
        -0.18001550436019897,
        -0.2340358942747116,
        1.1001381874084473,
        0.5685704350471497,
        0.17317068576812744,
        0.9762070178985596,
        -0.4765487015247345,
        -0.243003249168396,
        0.21593593060970306,
        -0.44907844066619873,
        -1.1995197534561157,
        0.6608914136886597,
        -0.1385834813117981,
        -0.3952079713344574,
        -0.4507705569267273,
        0.17044618725776672,
        -1.460521936416626,
        -0.17527692019939423,
        -0.03495781123638153,
        -0.8478437066078186,
        -0.19776809215545654,
        -0.33927491307258606,
        1.3042320013046265,
        -0.16963881254196167,
        -0.2085488736629486,
        0.4248627722263336,
        -0.38956913352012634,
        -0.1265537142753601,
        -0.04672327637672424,
        0.9049354791641235,
        0.036311402916908264,
        -0.12553782761096954,
        -0.7331203818321228,
        -0.29395589232444763,
        -0.08532331883907318,
        0.3630690574645996,
        -0.5610533356666565,
        -0.20594070851802826,
        -0.803597092628479,
        0.12556731700897217,
        0.5899806618690491,
        -0.24613088369369507,
        0.47415870428085327,
        -3.304102659225464,
        0.35635432600975037,
        0.13104087114334106,
        0.11850547045469284,
        0.1485901027917862,
        0.07275296747684479,
        0.573992908000946,
        0.4740777611732483,
        -0.6108486652374268,
        0.464216411113739,
        -0.1611206829547882,
        0.34430834650993347,
        0.1833626627922058,
        -0.15578891336917877,
        -0.11830693483352661,
        0.3256036937236786,
        0.44363969564437866,
        -0.8219392895698547,
        0.32892486453056335,
        0.14195752143859863,
        0.05812162905931473,
        -0.7472668290138245,
        -0.3360776901245117,
        0.7604007720947266,
        0.21237951517105103,
        0.20886904001235962,
        0.3984938859939575,
        -0.043323930352926254,
        -0.6772513389587402,
        -0.3245708644390106,
        -0.2995186150074005,
        0.17918702960014343,
        -0.21562188863754272,
        -0.23117180168628693,
        0.5153783559799194,
        0.26937922835350037,
        -0.1155460998415947,
        3.263695240020752,
        1.1550406217575073,
        0.5716016292572021,
        0.7072654366493225,
        -0.993942379951477,
        0.5517368912696838,
        -0.3260493278503418,
        -0.24907992780208588,
        -0.12003201991319656,
        -0.27239689230918884,
        0.2401760369539261,
        1.1352444887161255,
        -0.8271682262420654,
        -0.341412752866745,
        0.5767039656639099,
        0.45624345541000366,
        0.8649335503578186,
        -0.7623715996742249,
        -0.25340139865875244,
        0.029578570276498795,
        -0.18769696354866028,
        -0.22907808423042297,
        -0.06336010992527008,
        -0.5172280669212341,
        -0.5815975069999695,
        0.3467549681663513,
        -0.007022209465503693,
        -0.6773530840873718,
        0.48115336894989014,
        0.8442952632904053,
        -0.020171629264950752,
        -0.5862445831298828,
        -0.2315443456172943,
        -0.4715178310871124,
        -0.12575459480285645,
        0.15001387894153595,
        0.0827835276722908,
        0.3585607707500458,
        -0.8096908330917358,
        0.10537631064653397,
        -1.1592077016830444,
        0.2698574960231781,
        -1.152811884880066,
        0.6795324683189392,
        -0.002887621521949768,
        0.3854198455810547,
        0.11818292737007141,
        -0.329818993806839,
        0.23858872056007385,
        -0.47771990299224854,
        -0.6154530644416809,
        0.1589648425579071,
        1.003072738647461,
        0.251958429813385,
        0.016337241977453232,
        0.8898690938949585,
        -0.035592302680015564,
        -0.29116755723953247,
        0.42307913303375244,
        -0.3850310146808624,
        0.0944356918334961,
        0.2562132179737091,
        0.2727394998073578,
        -0.0256841778755188,
        -0.8557122349739075,
        0.6044216752052307,
        -0.7394695281982422,
        -0.11186099052429199,
        -0.7102710008621216,
        0.6763727068901062,
        0.40457040071487427,
        -0.411979615688324,
        -0.3895552158355713,
        0.24456264078617096,
        1.1056523323059082,
        0.1632334142923355,
        -0.7824527621269226,
        0.4501548707485199,
        0.6556172370910645,
        0.005160871893167496,
        -0.9944374561309814,
        0.6444234251976013,
        0.3026413321495056,
        -0.048727020621299744,
        -0.8751524090766907,
        0.43991827964782715,
        0.47970572113990784,
        0.02582598105072975,
        0.17504727840423584,
        1.330226182937622,
        0.8485757112503052,
        -0.539114236831665,
        1.7473090887069702,
        -0.9658617377281189,
        0.1789698749780655,
        -0.30135655403137207,
        -0.06886699795722961,
        0.21743829548358917,
        0.005664505064487457,
        0.6228480935096741,
        -0.31259283423423767,
        -0.6574404239654541,
        -0.6795037388801575,
        -0.9494647979736328,
        0.03902458772063255,
        -0.661764919757843,
        -0.5261828303337097,
        -0.2846870422363281,
        0.4880851209163666,
        0.31500327587127686,
        -0.8080661296844482,
        -0.06391255557537079,
        0.2086898535490036,
        0.8866324424743652,
        -0.008330166339874268,
        0.32459691166877747,
        -0.043486930429935455,
        0.04506988823413849,
        -0.1013956218957901,
        -0.4935874044895172,
        0.0033363699913024902,
        -0.2845594882965088,
        0.04251282662153244,
        -0.31440773606300354,
        -0.5895299315452576,
        -1.068345308303833,
        1.159759521484375,
        -0.1748528778553009,
        0.5807708501815796,
        -0.06494465470314026,
        -0.42855989933013916,
        0.021265558898448944,
        0.8001824021339417,
        0.6889499425888062,
        1.0138001441955566,
        -0.6194363832473755,
        0.635542094707489,
        -0.530918538570404,
        0.5416860580444336,
        0.4350987374782562,
        -1.2063438892364502,
        -0.33502647280693054,
        -0.249673992395401,
        -0.30454984307289124,
        0.8312228918075562,
        -0.011256322264671326,
        0.5804464817047119,
        -0.34016430377960205,
        -0.05374542623758316,
        -0.3046753704547882,
        1.626143455505371,
        0.023557383567094803,
        -0.48145267367362976,
        1.037529468536377,
        0.5865734219551086,
        0.20951607823371887,
        -0.05166775733232498,
        -1.3834375143051147,
        -0.49947693943977356,
        -0.42992493510246277,
        0.2454775869846344,
        -0.7535665035247803,
        -0.3049582540988922,
        0.6189743876457214,
        0.2110164910554886,
        -0.35128217935562134,
        0.16349844634532928,
        -0.34206801652908325,
        -0.7873716354370117,
        -0.08388704806566238,
        0.18746769428253174,
        0.1827615201473236,
        0.2994168996810913,
        -0.4159066677093506,
        -0.4948377013206482,
        0.6548618674278259,
        0.4192289412021637,
        0.26360535621643066,
        -0.23771148920059204,
        -0.0750238224864006,
        -0.5054231286048889,
        -0.3721497058868408,
        0.01784244365990162,
        -0.05656573921442032,
        0.984763503074646,
        0.09313104301691055,
        -0.5616995096206665,
        0.2860884666442871,
        -0.9447536468505859,
        -0.16528654098510742,
        0.8201497793197632,
        -0.3573996424674988,
        -0.3781232535839081,
        -0.4561362862586975,
        0.13743020594120026,
        1.260611653327942,
        0.358288437128067,
        0.3899800479412079,
        0.7917971014976501,
        1.036065936088562,
        -0.5110375881195068,
        -0.45980072021484375,
        0.336427241563797,
        0.028650876134634018,
        0.16607502102851868,
        -0.924540638923645,
        -0.5931907296180725,
        0.47769445180892944,
        0.8287303447723389,
        -0.43101057410240173,
        0.04620930552482605,
        -1.240883231163025,
        -0.254863977432251,
        -0.30860471725463867,
        0.23937180638313293,
        0.40656301379203796,
        -0.6999444961547852,
        0.7773778438568115,
        1.3182563781738281,
        -0.5167351961135864,
        -1.1870061159133911,
        -0.11456835269927979,
        1.1114447116851807,
        0.11753128468990326,
        -0.8738107681274414,
        0.20701062679290771,
        0.3441988229751587,
        -0.05333948880434036,
        0.48311737179756165,
        0.1876293569803238,
        1.5120429992675781,
        0.7453973293304443,
        0.031922563910484314,
        -0.16876256465911865,
        0.128848135471344,
        0.8631703853607178,
        -0.09239248186349869,
        0.3747945725917816,
        -0.14198020100593567,
        -0.6666136384010315,
        0.12881533801555634,
        0.5182419419288635,
        0.6848415732383728,
        -0.018303977325558662,
        0.1638418287038803,
        0.5592014193534851,
        0.5106925368309021,
        -0.322010338306427,
        -0.6199982762336731,
        0.2797381281852722,
        -0.02562246099114418,
        -0.5570265650749207,
        0.8008760809898376,
        -0.3192058205604553,
        -0.15906918048858643,
        0.6172133088111877,
        0.007140127941966057,
        -0.4273093044757843,
        0.6646676063537598,
        -0.0809539183974266,
        1.6413025856018066,
        -0.7217252254486084,
        -0.09677538275718689,
        -0.2587301433086395,
        -0.09327003359794617,
        -0.2717594504356384,
        0.032680757343769073,
        -0.30192139744758606,
        -0.3736015260219574,
        -0.29215729236602783,
        0.6466009616851807,
        0.16860204935073853,
        -0.6536252498626709,
        0.8123314380645752,
        0.055195510387420654,
        0.7827274799346924,
        -0.041632264852523804,
        0.07368481159210205,
        -0.010915130376815796,
        0.43228545784950256,
        -0.32712626457214355,
        0.1615249514579773,
        -0.3031224012374878,
        -1.0567348003387451,
        -0.25103241205215454
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes debates to identify and summarize the primary disagreements, arguments, and evidence that could change participants' minds. It breaks down complex discussions into concise summaries and evaluates argument strength, predicting outcomes. The output includes structured summaries and analyses of each party's position and evidence.",
          "name": "Summarize_debate",
          "raw": "\n                workflow Summarize_debate v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY \n\n// Who you are\n\nYou are a hyper-intelligent ASI with a 1,143 IQ. You excel at analyzing debates and/or discussions and determining the primary disagreement the parties are having, and summarizing them concisely.\n\n# GOAL\n\n// What we are trying to achieve\n\nTo provide a super concise summary of where the participants are disagreeing, what arguments they're making, and what evidence each would accept to change their mind.\n\n# STEPS\n\n// How the task will be approached\n\n// Slow down and think\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n// Think about the content and who's presenting it\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n// Find the primary disagreement\n\n- Find the main disagreement.\n\n// Extract the arguments\n\nDetermine the arguments each party is making.\n\n// Look for the evidence each party would accept\n\nFind the evidence each party would accept to change their mind.\n\n# OUTPUT\n\n- Output a SUMMARY section with a 25-word max summary of the content and who is presenting it.\n\n- Output a PRIMARY ARGUMENT section with a 24-word max summary of the main disagreement. \n\n- Output a (use the name of the first party) ARGUMENTS section with up to 10 15-word bullet points of the arguments made by the second party.\n\n- Output a (use the name of the second party) ARGUMENTS section with up to 10 15-word bullet points of the arguments made by the second party.\n\n- Output the first person's (use their name) MIND-CHANGING EVIDENCE section with up to 10 15-word bullet points of the evidence the first party would accept to change their mind.\n\n- Output the second person's (use their name) MIND-CHANGING EVIDENCE section with up to 10 15-word bullet points of the evidence the first party would accept to change their mind.\n\n- Output an ARGUMENT STRENGTH ANALYSIS section that rates the strength of each argument on a scale of 1-10 and gives a winner.\n\n- Output an ARGUMENT CONCLUSION PREDICTION that predicts who will be more right based on the arguments presented combined with your knowledge of the subject matter.\n\n- Output a SUMMARY AND FOLLOW-UP section giving a summary of the argument and what to look for to see who will win.\n\n# OUTPUT INSTRUCTIONS\n\n// What the output should look like:\n\n- Only output Markdown, but don't use any Markdown formatting like bold or italics.\n\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY \n\n// Who you are\n\nYou are a hyper-intelligent ASI with a 1,143 IQ. You excel at analyzing debates and/or discussions and determining the primary disagreement the parties are having, and summarizing them concisely.\n\n# GOAL\n\n// What we are trying to achieve\n\nTo provide a super concise summary of where the participants are disagreeing, what arguments they're making, and what evidence each would accept to change their mind.\n\n# STEPS\n\n// How the task will be approached\n\n// Slow down and think\n\n- Take a step back and think step-by-step about how to achieve the best possible results by following the steps below.\n\n// Think about the content and who's presenting it\n\n- Extract a summary of the content in 25 words, including who is presenting and the content being discussed into a section called SUMMARY.\n\n// Find the primary disagreement\n\n- Find the main disagreement.\n\n// Extract the arguments\n\nDetermine the arguments each party is making.\n\n// Look for the evidence each party would accept\n\nFind the evidence each party would accept to change their mind.\n\n# OUTPUT\n\n- Output a SUMMARY section with a 25-word max summary of the content and who is presenting it.\n\n- Output a PRIMARY ARGUMENT section with a 24-word max summary of the main disagreement. \n\n- Output a (use the name of the first party) ARGUMENTS section with up to 10 15-word bullet points of the arguments made by the second party.\n\n- Output a (use the name of the second party) ARGUMENTS section with up to 10 15-word bullet points of the arguments made by the second party.\n\n- Output the first person's (use their name) MIND-CHANGING EVIDENCE section with up to 10 15-word bullet points of the evidence the first party would accept to change their mind.\n\n- Output the second person's (use their name) MIND-CHANGING EVIDENCE section with up to 10 15-word bullet points of the evidence the first party would accept to change their mind.\n\n- Output an ARGUMENT STRENGTH ANALYSIS section that rates the strength of each argument on a scale of 1-10 and gives a winner.\n\n- Output an ARGUMENT CONCLUSION PREDICTION that predicts who will be more right based on the arguments presented combined with your knowledge of the subject matter.\n\n- Output a SUMMARY AND FOLLOW-UP section giving a summary of the argument and what to look for to see who will win.\n\n# OUTPUT INSTRUCTIONS\n\n// What the output should look like:\n\n- Only output Markdown, but don't use any Markdown formatting like bold or italics.\n\n\n- Do not give warnings or notes; only output the requested sections.\n\n- You use bulleted lists for output, not numbered lists.\n\n- Do not repeat ideas, quotes, facts, or resources.\n\n- Do not start items with the same opening words.\n\n- Ensure you follow ALL these instructions when creating your output.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.692169725894928,
        0.40903574228286743,
        0.27667638659477234,
        0.5371955037117004,
        0.42323556542396545,
        0.06919936090707779,
        -0.2954985499382019,
        0.6718161106109619,
        0.7553352117538452,
        -0.2907153367996216,
        -0.24367135763168335,
        0.40547531843185425,
        0.240130215883255,
        -0.08693587779998779,
        0.23896567523479462,
        -0.14472652971744537,
        -0.5313636064529419,
        -0.5467159748077393,
        -1.4204230308532715,
        -0.8042400479316711,
        -0.4274193346500397,
        1.1221826076507568,
        0.24773284792900085,
        0.08959019184112549,
        0.5445138812065125,
        0.10504652559757233,
        -0.18500737845897675,
        0.23844145238399506,
        -1.1004341840744019,
        -2.266634464263916,
        0.08117909729480743,
        0.14413024485111237,
        -0.2157151848077774,
        0.01216607540845871,
        0.4003472626209259,
        -1.1991186141967773,
        0.23459798097610474,
        0.04984862357378006,
        -0.4059404134750366,
        -0.27216631174087524,
        -0.391341894865036,
        0.6617008447647095,
        -0.03376680612564087,
        -0.5707816481590271,
        0.6013346314430237,
        -0.1341160535812378,
        0.07205728441476822,
        -0.3819926679134369,
        0.46943458914756775,
        0.008817479014396667,
        -0.38605213165283203,
        -0.2613680064678192,
        -0.0961214005947113,
        0.4752441942691803,
        0.6187117695808411,
        0.055037662386894226,
        0.18522484600543976,
        -0.8015778064727783,
        0.7232298254966736,
        0.521977961063385,
        0.228597491979599,
        0.5065283179283142,
        -2.926470994949341,
        -0.3490702211856842,
        -0.4561428129673004,
        -0.678257942199707,
        0.10272242873907089,
        -0.29355588555336,
        -0.1294899731874466,
        0.3309113681316376,
        -0.2185077965259552,
        -0.09054075926542282,
        -0.538745105266571,
        -0.07910626381635666,
        -0.2767009139060974,
        -0.4045928418636322,
        0.11027257144451141,
        0.20851391553878784,
        0.12824547290802002,
        -0.1795019507408142,
        0.08679014444351196,
        0.6313183903694153,
        0.29872071743011475,
        -0.3089815080165863,
        -0.5507095456123352,
        1.1178151369094849,
        -0.4426712989807129,
        -0.47189974784851074,
        0.453184574842453,
        0.31337735056877136,
        -0.08663271367549896,
        -0.7679206728935242,
        0.44662344455718994,
        0.2157403528690338,
        -0.3592788577079773,
        -0.3731802701950073,
        0.13479366898536682,
        0.19438007473945618,
        -0.6081791520118713,
        3.190836191177368,
        0.8773974180221558,
        0.6401721835136414,
        0.23519691824913025,
        -0.638565182685852,
        0.10462929308414459,
        -0.5586600303649902,
        -0.34669068455696106,
        0.18738186359405518,
        -0.26937875151634216,
        -0.11051547527313232,
        1.051304578781128,
        -0.9459043145179749,
        -0.21190068125724792,
        0.10533418506383896,
        0.1071469783782959,
        0.558216392993927,
        -0.7927191257476807,
        0.11887937784194946,
        -0.3347401022911072,
        0.14801852405071259,
        -0.1846615970134735,
        -0.1906503140926361,
        -0.7921494841575623,
        -0.49602293968200684,
        0.3079490661621094,
        -0.01989579200744629,
        -0.12631472945213318,
        0.6157151460647583,
        0.4465477764606476,
        0.4639844298362732,
        0.14286932349205017,
        -0.19451528787612915,
        -0.5145834684371948,
        -0.07059977948665619,
        -0.002209864556789398,
        -0.13119837641716003,
        -0.1935824304819107,
        -0.718282163143158,
        -0.03034794330596924,
        -1.2603440284729004,
        -0.04793056845664978,
        -0.2675476670265198,
        0.3535487949848175,
        0.16858145594596863,
        0.327443391084671,
        0.15294082462787628,
        0.8218185901641846,
        0.07516003400087357,
        -0.5133614540100098,
        -1.2605804204940796,
        0.3957729935646057,
        0.7211318016052246,
        0.2867191731929779,
        0.44830575585365295,
        0.25467193126678467,
        0.29641246795654297,
        -0.517268180847168,
        0.708056628704071,
        -0.5136110782623291,
        1.0349291563034058,
        0.24324281513690948,
        0.3766658306121826,
        0.5566931962966919,
        -0.8945421576499939,
        0.901473343372345,
        -0.6306518912315369,
        0.4878336191177368,
        -0.7713388204574585,
        0.17499017715454102,
        0.3643623888492584,
        -0.7072028517723083,
        -0.250469833612442,
        0.4844484031200409,
        0.23053714632987976,
        -0.1187189519405365,
        0.5477626323699951,
        0.5360013246536255,
        -0.19246941804885864,
        -0.6954190135002136,
        -0.13562044501304626,
        0.172321617603302,
        0.3966887891292572,
        -0.5607190132141113,
        -1.103516697883606,
        0.35526108741760254,
        -0.050288811326026917,
        -0.09551125019788742,
        0.8701431751251221,
        0.7507018446922302,
        1.419448971748352,
        -0.16195589303970337,
        2.0353612899780273,
        -0.304616242647171,
        -0.10000643134117126,
        0.08077673614025116,
        0.2715541124343872,
        0.02599348872900009,
        0.8683739900588989,
        0.55497807264328,
        0.14188720285892487,
        -0.9298065304756165,
        0.2823741137981415,
        -0.5743992924690247,
        -0.43645116686820984,
        -0.6045016050338745,
        -0.950928807258606,
        0.5208549499511719,
        1.0174435377120972,
        0.33693867921829224,
        -0.830251157283783,
        0.2248591184616089,
        0.052677251398563385,
        1.3179352283477783,
        -0.13939985632896423,
        0.3087613880634308,
        -0.49125373363494873,
        -0.04983648657798767,
        0.19645309448242188,
        0.17912808060646057,
        0.549710750579834,
        -0.2735559940338135,
        0.33571040630340576,
        -0.8414751291275024,
        -0.7218388915061951,
        -0.3252551555633545,
        0.36867469549179077,
        0.14836588501930237,
        0.39610111713409424,
        -0.3304463326931,
        -0.6331278085708618,
        0.0014407113194465637,
        0.7897630333900452,
        0.587570071220398,
        1.2154313325881958,
        -0.6265023946762085,
        0.7006749510765076,
        -0.3741098940372467,
        0.7599024772644043,
        0.09618638455867767,
        -1.2964586019515991,
        -0.15470623970031738,
        -0.2187279462814331,
        -0.30439987778663635,
        0.7356595396995544,
        0.42161187529563904,
        -0.07036003470420837,
        -0.9420847296714783,
        -0.28799349069595337,
        -0.2269640415906906,
        2.089104652404785,
        0.06283276528120041,
        -0.18438366055488586,
        0.8179972767829895,
        -0.031386058777570724,
        -0.01693522185087204,
        -0.10881961137056351,
        -1.5372602939605713,
        -0.532587468624115,
        -0.24812757968902588,
        -0.16274061799049377,
        -0.8419959545135498,
        0.30839774012565613,
        0.5726399421691895,
        -0.2567000985145569,
        0.369586706161499,
        0.08978290855884552,
        0.26687100529670715,
        0.04060807824134827,
        -0.40753769874572754,
        -0.6477328538894653,
        0.4837031066417694,
        0.6053175926208496,
        0.27879899740219116,
        -0.5774003267288208,
        0.017753638327121735,
        0.11665263772010803,
        -0.2782312333583832,
        -0.1923324465751648,
        -0.48772668838500977,
        -1.0047340393066406,
        -0.16544964909553528,
        0.18254858255386353,
        -0.44536092877388,
        0.28565096855163574,
        -0.3652557134628296,
        -0.17178791761398315,
        -0.06290873140096664,
        -0.3467361330986023,
        -0.15334880352020264,
        0.4299289286136627,
        -0.20146027207374573,
        -0.15035712718963623,
        -0.4717532694339752,
        0.04141358286142349,
        1.8604075908660889,
        0.04135734587907791,
        -0.03009345754981041,
        0.544486403465271,
        0.5461404919624329,
        -0.007053644396364689,
        -0.8083721399307251,
        0.35858309268951416,
        -0.37045013904571533,
        -0.24625280499458313,
        -0.9341146349906921,
        0.23284325003623962,
        0.6765913963317871,
        -0.32863375544548035,
        0.07934846729040146,
        0.6268002986907959,
        -0.4287433326244354,
        0.6030322909355164,
        0.09087301790714264,
        -0.03841555118560791,
        1.0652304887771606,
        -0.44634926319122314,
        0.42208707332611084,
        1.0576424598693848,
        -0.28694334626197815,
        -1.2101558446884155,
        0.3569859564304352,
        -0.1886727213859558,
        0.5437566041946411,
        -0.09124478697776794,
        0.23896096646785736,
        0.01718240976333618,
        0.07442467659711838,
        0.45568275451660156,
        -0.14622414112091064,
        1.3992856740951538,
        0.5964817404747009,
        -0.12661747634410858,
        -0.8175773024559021,
        -0.2715213894844055,
        1.1961843967437744,
        -0.24375100433826447,
        0.43834564089775085,
        -0.20044535398483276,
        -0.19643497467041016,
        -0.43646642565727234,
        0.3614347577095032,
        1.267268180847168,
        0.4000426232814789,
        0.7400681972503662,
        -0.2287673056125641,
        -0.01399209350347519,
        -0.2301374077796936,
        -0.8168241381645203,
        -0.41225311160087585,
        -0.23754650354385376,
        -0.4820463955402374,
        0.6154763698577881,
        -0.24190784990787506,
        0.1691809594631195,
        0.19569313526153564,
        0.321891725063324,
        0.29588451981544495,
        -0.003932667430490255,
        -0.4390595257282257,
        0.8753729462623596,
        -0.47053003311157227,
        -0.8055406808853149,
        0.34603458642959595,
        -0.10563938319683075,
        -0.6435784101486206,
        0.09369630366563797,
        0.6189400553703308,
        -0.16416475176811218,
        -0.7369748950004578,
        0.01538969948887825,
        0.5244330763816833,
        -1.030435562133789,
        0.22946329414844513,
        -0.13051475584506989,
        0.18111027777194977,
        0.25998836755752563,
        -0.007264528423547745,
        -0.18344298005104065,
        0.3041991591453552,
        -0.2097529172897339,
        -0.32276463508605957,
        -0.05588409677147865,
        -0.10210514813661575,
        -0.8281269073486328
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes major changes and upgrades in a GitHub project over the past week. It involves identifying key updates, then crafting a concise, enthusiastic summary and detailed bullet points highlighting these changes. The output includes a 20-word introduction and excitedly written update bullets.",
          "name": "Summarize_git_changes",
          "raw": "\n                workflow Summarize_git_changes v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed a Github project in the last 7 days.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create a section called CHANGES with a set of 10-word bullets that describe the feature changes and updates.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a 20-word intro sentence that says something like, \\\"In the last 7 days, we've made some amazing updates to our project focused around $character of the updates$.\\\"\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n- Write the update bullets like you're excited about the upgrades.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed a Github project in the last 7 days.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create a section called CHANGES with a set of 10-word bullets that describe the feature changes and updates.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a 20-word intro sentence that says something like, \\\"In the last 7 days, we've made some amazing updates to our project focused around $character of the updates$.\\\"\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n- Write the update bullets like you're excited about the upgrades.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5198928713798523,
        0.3242015242576599,
        0.3090892434120178,
        0.6474086046218872,
        0.030293304473161697,
        -0.16433489322662354,
        -0.32470494508743286,
        0.07442434132099152,
        0.43188589811325073,
        0.24146179854869843,
        -0.4565032720565796,
        0.7589496970176697,
        0.4853758215904236,
        -0.2699812948703766,
        -0.06087804213166237,
        -0.19074827432632446,
        -0.7069644331932068,
        -0.2692546844482422,
        -1.1001503467559814,
        -0.7800437211990356,
        0.35542258620262146,
        1.217513084411621,
        0.5351586937904358,
        -0.39509809017181396,
        0.6614078283309937,
        0.24471725523471832,
        -0.1170184463262558,
        -0.15820607542991638,
        -1.3776357173919678,
        -1.6918408870697021,
        0.45328038930892944,
        -0.13325035572052002,
        -0.3224920630455017,
        0.09032995998859406,
        0.2292558252811432,
        -0.9684880375862122,
        0.08183194696903229,
        0.19487735629081726,
        -0.897063136100769,
        -0.2639642655849457,
        -0.07005118578672409,
        0.7734184861183167,
        0.3903436064720154,
        0.20547479391098022,
        0.3083730936050415,
        -0.2268667221069336,
        -0.4364664554595947,
        -0.23708637058734894,
        0.7115514278411865,
        0.2742958664894104,
        -0.3278833329677582,
        -0.059737421572208405,
        0.18607021868228912,
        0.3463176190853119,
        0.7676832675933838,
        -0.04853822663426399,
        -0.28818050026893616,
        -0.6897091865539551,
        0.4182458817958832,
        0.6233662366867065,
        0.46597129106521606,
        0.6583829522132874,
        -2.820647954940796,
        -0.16726498305797577,
        -0.8360863327980042,
        -0.6270161867141724,
        0.4580502510070801,
        -0.034051354974508286,
        0.23237460851669312,
        -0.06721088290214539,
        -0.278561532497406,
        0.1678074598312378,
        -0.6073492765426636,
        0.21963515877723694,
        -0.07262392342090607,
        0.056642480194568634,
        -0.23466891050338745,
        0.46090391278266907,
        0.573310375213623,
        -0.5356453657150269,
        0.3031032085418701,
        0.7878671884536743,
        0.01758972369134426,
        -0.83265221118927,
        -0.7852237224578857,
        1.0217787027359009,
        -0.11192350834608078,
        -0.31664416193962097,
        0.334622859954834,
        0.6080400943756104,
        -0.06913629174232483,
        -0.29034316539764404,
        0.2892618179321289,
        0.11426850408315659,
        -0.6765080690383911,
        0.36227941513061523,
        0.3389561176300049,
        0.0518186017870903,
        -0.3144967257976532,
        3.062227725982666,
        1.2002334594726562,
        0.22445107996463776,
        -0.2747328281402588,
        -0.7385051250457764,
        -0.35939741134643555,
        -0.31429794430732727,
        0.07188446819782257,
        0.4299355149269104,
        -0.00045468134339898825,
        -0.8519560098648071,
        0.8684992790222168,
        -1.3209930658340454,
        -0.4428282678127289,
        0.013676892966032028,
        0.07363825291395187,
        0.2909379005432129,
        -0.9134824275970459,
        0.560850203037262,
        -0.46222805976867676,
        0.3264327943325043,
        -0.3470337390899658,
        0.26014307141304016,
        -0.6150996088981628,
        -0.4087894856929779,
        0.13701413571834564,
        -0.48739928007125854,
        -0.10167786478996277,
        0.5775302052497864,
        0.20283040404319763,
        0.11366650462150574,
        0.6167019009590149,
        0.05517267808318138,
        -0.5688052177429199,
        0.20953258872032166,
        -0.20690196752548218,
        -0.7113689184188843,
        -0.15167440474033356,
        -0.934636652469635,
        -0.487885057926178,
        -1.386641502380371,
        -0.24136021733283997,
        -1.0899070501327515,
        -0.13295236229896545,
        0.39273393154144287,
        -0.1110587790608406,
        0.23845334351062775,
        0.6134777665138245,
        0.4831502437591553,
        -0.33810344338417053,
        -0.8954800963401794,
        0.05236165225505829,
        0.8650515079498291,
        0.5834687948226929,
        0.31979620456695557,
        -0.3913278579711914,
        0.28288018703460693,
        -1.0473910570144653,
        0.8199275135993958,
        -0.34558263421058655,
        0.3238009512424469,
        0.12313945591449738,
        -0.334870308637619,
        0.2306988537311554,
        -0.7995795011520386,
        0.9341452717781067,
        -0.20688723027706146,
        0.08536459505558014,
        -0.5856038928031921,
        0.29020407795906067,
        0.29989364743232727,
        -0.7591912746429443,
        -0.022828273475170135,
        0.3426724076271057,
        0.49169251322746277,
        -0.213624969124794,
        0.6123774647712708,
        1.1823394298553467,
        0.09787390381097794,
        -0.7524830102920532,
        -0.2669750154018402,
        0.17660552263259888,
        0.3757334053516388,
        -0.06170915812253952,
        -0.8335545063018799,
        0.3660409450531006,
        0.254269540309906,
        0.6577895283699036,
        0.5816114544868469,
        0.5880045890808105,
        1.5089480876922607,
        -0.5244035720825195,
        1.7084496021270752,
        -0.4914059042930603,
        0.22287598252296448,
        0.32535552978515625,
        0.4034634232521057,
        0.23569907248020172,
        0.5142916440963745,
        0.3700566589832306,
        0.46598872542381287,
        -0.8196682333946228,
        -0.11437713354825974,
        -0.1741395890712738,
        0.17179331183433533,
        -0.582234799861908,
        -0.633685827255249,
        -0.06836898624897003,
        0.9100116491317749,
        0.4354836344718933,
        -0.9728173017501831,
        0.13088110089302063,
        0.5307403206825256,
        1.1796305179595947,
        -0.14194881916046143,
        0.40307900309562683,
        -0.3910941481590271,
        0.009486284106969833,
        1.0972661972045898,
        0.05859497934579849,
        0.7233297824859619,
        -0.5777126550674438,
        0.6517013311386108,
        -0.6938455104827881,
        -0.6892348527908325,
        -0.02987767569720745,
        0.46751266717910767,
        -0.6090064644813538,
        1.2208269834518433,
        -0.8066951632499695,
        -0.6206304430961609,
        -0.17629081010818481,
        1.2290560007095337,
        0.5907227993011475,
        0.8552665710449219,
        -0.5910550951957703,
        0.5884018540382385,
        0.16184547543525696,
        0.42617467045783997,
        0.17657987773418427,
        -0.8790423274040222,
        0.14176802337169647,
        0.3311840295791626,
        -0.7303858995437622,
        1.210881233215332,
        -0.13680988550186157,
        -0.14875122904777527,
        -1.168442726135254,
        -0.19477558135986328,
        -0.2668522894382477,
        2.03582501411438,
        0.6327374577522278,
        -0.22753551602363586,
        0.0671326220035553,
        0.38768136501312256,
        -0.14374569058418274,
        -0.26568397879600525,
        -1.6951987743377686,
        -0.5148454904556274,
        -0.46545398235321045,
        0.09797480702400208,
        -0.8261390924453735,
        -0.019788384437561035,
        0.1145419031381607,
        0.2454458326101303,
        0.2481096386909485,
        0.03736478462815285,
        0.13076303899288177,
        0.2531347870826721,
        -0.7922372817993164,
        -0.3411964178085327,
        0.25431597232818604,
        0.8613355159759521,
        0.15960298478603363,
        -0.6320896148681641,
        -0.39842164516448975,
        0.3140036165714264,
        0.15462036430835724,
        0.2801985442638397,
        -0.3575797975063324,
        -0.5536952614784241,
        -0.09499374777078629,
        0.5370000004768372,
        -0.10670376569032669,
        1.0246577262878418,
        -0.8304886221885681,
        -0.09788312017917633,
        0.1675579696893692,
        -0.098212830722332,
        0.2515908479690552,
        0.33349964022636414,
        -0.0028571337461471558,
        0.1761813759803772,
        -0.7209542393684387,
        0.17581695318222046,
        1.9605215787887573,
        0.3797573447227478,
        -0.02446022257208824,
        0.7756430506706238,
        0.4583203196525574,
        0.42349493503570557,
        -0.5189071297645569,
        -0.10054901987314224,
        0.26542508602142334,
        -0.4014403820037842,
        -0.8363041877746582,
        0.19568893313407898,
        0.7284356355667114,
        -0.6252741813659668,
        -0.7506647109985352,
        0.1417091190814972,
        -0.4874348044395447,
        0.39025482535362244,
        0.2717481255531311,
        -0.12674644589424133,
        0.24441766738891602,
        -0.5238369107246399,
        0.0026458296924829483,
        1.1488087177276611,
        -0.5615610480308533,
        -1.323940634727478,
        -0.26698407530784607,
        0.17664670944213867,
        0.2923893332481384,
        -0.5058728456497192,
        0.6726862192153931,
        0.44651538133621216,
        0.17604702711105347,
        0.18259036540985107,
        -0.18273407220840454,
        1.3416482210159302,
        0.6661015748977661,
        -0.08413370698690414,
        -0.447481632232666,
        -0.3217012882232666,
        0.523400068283081,
        -0.05395311489701271,
        0.06500405818223953,
        -0.2856178879737854,
        -0.022919826209545135,
        -0.2752784788608551,
        -0.3461959660053253,
        0.7655012011528015,
        -0.04129946231842041,
        0.8599761724472046,
        0.2665347754955292,
        -0.4341871738433838,
        0.1826642006635666,
        -0.8485241532325745,
        -0.0939001739025116,
        -0.35544538497924805,
        -1.0265692472457886,
        0.8000816106796265,
        -0.009596109390258789,
        0.050718553364276886,
        0.4847806394100189,
        -0.47409406304359436,
        -0.3042510151863098,
        0.4730052947998047,
        -0.2345241904258728,
        0.8085376620292664,
        -0.16807200014591217,
        -0.5709332227706909,
        0.12563258409500122,
        -0.29427191615104675,
        -0.0871487408876419,
        0.22302909195423126,
        0.3483861982822418,
        -0.309762179851532,
        -0.46456652879714966,
        0.024200353771448135,
        0.8190621733665466,
        -1.0137525796890259,
        -0.27069106698036194,
        -0.1298617273569107,
        0.00045120716094970703,
        -0.014979608356952667,
        -0.004075702279806137,
        -0.14964646100997925,
        0.004947230219841003,
        -0.04516691341996193,
        -0.09561224281787872,
        -0.39292216300964355,
        -0.4827657639980316,
        -1.095866084098816
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Analyzes Git diffs to summarize major changes and upgrades. It emphasizes creating concise bullet points for feature changes and updates, tailored to the extent of modifications. The expected output includes a 100-character intro sentence using conventional commits format.",
          "name": "Summarize_git_diff",
          "raw": "\n                workflow Summarize_git_diff v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed in a Git diff.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create a section called CHANGES with a set of 7-10 word bullets that describe the feature changes and updates.\n\n- If there are a lot of changes include more bullets. If there are only a few changes, be more terse.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a maximum 100 character intro sentence that says something like, \\\"chore: refactored the `foobar` method to support new 'update' arg\\\"\n\n- Use conventional commits - i.e. prefix the commit title with \\\"chore:\\\" (if it's a minor change like refactoring or linting), \\\"feat:\\\" (if it's a new feature), \\\"fix:\\\" if its a bug fix\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert project manager and developer, and you specialize in creating super clean updates for what changed in a Git diff.\n\n# STEPS\n\n- Read the input and figure out what the major changes and upgrades were that happened.\n\n- Create a section called CHANGES with a set of 7-10 word bullets that describe the feature changes and updates.\n\n- If there are a lot of changes include more bullets. If there are only a few changes, be more terse.\n\n# OUTPUT INSTRUCTIONS\n\n- Output a maximum 100 character intro sentence that says something like, \\\"chore: refactored the `foobar` method to support new 'update' arg\\\"\n\n- Use conventional commits - i.e. prefix the commit title with \\\"chore:\\\" (if it's a minor change like refactoring or linting), \\\"feat:\\\" (if it's a new feature), \\\"fix:\\\" if its a bug fix\n\n- You only output human readable Markdown, except for the links, which should be in HTML format.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.11865963041782379,
        0.3467746376991272,
        -0.020233791321516037,
        0.17996858060359955,
        -0.49496129155158997,
        0.04964704066514969,
        -1.1626704931259155,
        0.21708783507347107,
        0.34061744809150696,
        -0.03369305655360222,
        -0.25649845600128174,
        0.9537017941474915,
        0.21445605158805847,
        0.2846398949623108,
        0.2709689438343048,
        -0.3345114290714264,
        -0.16051840782165527,
        -0.7499076128005981,
        -1.4677600860595703,
        -0.5464465618133545,
        0.030567314475774765,
        0.8628526926040649,
        0.13568414747714996,
        0.27247726917266846,
        0.04898189753293991,
        -0.24718844890594482,
        0.4206141531467438,
        -0.3655390739440918,
        -1.2103567123413086,
        -2.2117714881896973,
        0.5114025473594666,
        0.3131594657897949,
        -0.22628723084926605,
        -0.07700034230947495,
        -0.13591191172599792,
        -1.3956317901611328,
        -0.21510902047157288,
        -0.039452627301216125,
        -0.46763330698013306,
        -0.07743047177791595,
        -0.1237669289112091,
        0.008798228576779366,
        -0.19268164038658142,
        -0.0513673797249794,
        0.3300335705280304,
        -0.5828225016593933,
        0.05850755050778389,
        -0.23551107943058014,
        0.8382279276847839,
        0.11471719294786453,
        -0.7119184732437134,
        -0.6272274255752563,
        0.20848311483860016,
        0.25440022349357605,
        0.1295613944530487,
        -0.29320141673088074,
        0.23008623719215393,
        -0.2976177930831909,
        0.44064560532569885,
        -0.11002306640148163,
        -0.16298535466194153,
        0.3268907964229584,
        -4.011295318603516,
        -0.35526472330093384,
        0.11720819026231766,
        0.21539300680160522,
        0.10466565191745758,
        -0.028268244117498398,
        0.09591015428304672,
        0.6627908945083618,
        -0.21463656425476074,
        0.19944827258586884,
        -0.24111011624336243,
        0.2008497714996338,
        -0.4115144908428192,
        -0.3224090337753296,
        0.17828522622585297,
        0.03984614461660385,
        0.5678024888038635,
        -0.6341933012008667,
        -0.043485742062330246,
        0.24698475003242493,
        0.458291232585907,
        -0.1424534022808075,
        -0.8160580396652222,
        0.707432210445404,
        -0.2683204412460327,
        -0.14671947062015533,
        0.22137455642223358,
        0.10759186744689941,
        -0.7038215398788452,
        -0.28899481892585754,
        -0.17941604554653168,
        -0.015597362071275711,
        -0.5900462865829468,
        0.27572861313819885,
        -0.09477047622203827,
        -0.3467928469181061,
        -0.3237363398075104,
        3.4396839141845703,
        0.5554720163345337,
        -0.08710600435733795,
        0.8102715611457825,
        -0.8080870509147644,
        0.4177417755126953,
        -0.2602006494998932,
        -0.004037335515022278,
        -0.4162992537021637,
        0.18643660843372345,
        -0.012970462441444397,
        0.6128005385398865,
        -0.6872509121894836,
        0.1148940771818161,
        0.09366706758737564,
        0.3901316225528717,
        0.4110530614852905,
        -0.5083187222480774,
        0.22309108078479767,
        -0.45088350772857666,
        0.6554219722747803,
        -0.5042622685432434,
        -0.058781903237104416,
        -0.31946176290512085,
        -0.0533454492688179,
        -0.03400985896587372,
        0.19369268417358398,
        -0.13740776479244232,
        0.410310834646225,
        0.45294633507728577,
        -0.05361887067556381,
        0.08974357694387436,
        -0.3606090247631073,
        -0.3658851981163025,
        -0.038954831659793854,
        0.471675843000412,
        -0.22733166813850403,
        0.6160207390785217,
        -0.5868703126907349,
        0.3806081712245941,
        -0.1771531105041504,
        0.1333296000957489,
        -1.5809154510498047,
        0.9420350790023804,
        -0.005903523415327072,
        0.7104586958885193,
        0.5515102744102478,
        -0.5510144233703613,
        0.11068736016750336,
        -0.35156941413879395,
        -0.8308725953102112,
        -0.038891177624464035,
        0.5064151883125305,
        0.22796565294265747,
        0.6562564969062805,
        1.1453059911727905,
        -0.17176590859889984,
        -0.33846545219421387,
        0.3473898470401764,
        -0.22805538773536682,
        0.5122652649879456,
        0.15910367667675018,
        0.010181172750890255,
        0.6502516865730286,
        -0.5284001231193542,
        0.6930045485496521,
        -0.8150359988212585,
        0.5649844408035278,
        -0.3772682547569275,
        0.49777477979660034,
        0.13273970782756805,
        -0.17550942301750183,
        -0.28232747316360474,
        0.512635350227356,
        0.8483995199203491,
        0.4223534166812897,
        0.040259554982185364,
        -0.2841205298900604,
        0.2510714828968048,
        0.15517933666706085,
        -0.4875769317150116,
        0.25103145837783813,
        0.23995381593704224,
        -0.22918221354484558,
        -0.781636118888855,
        -0.14710605144500732,
        0.4359765946865082,
        0.37669867277145386,
        0.5153601765632629,
        0.11021018773317337,
        0.9014087915420532,
        -0.8143329620361328,
        1.6622973680496216,
        -0.8592652082443237,
        0.04397689551115036,
        -0.049505963921546936,
        -0.10657832771539688,
        -0.19826340675354004,
        0.17114797234535217,
        0.015231132507324219,
        0.10444916784763336,
        -0.9270960688591003,
        -0.547926127910614,
        -0.5286632776260376,
        -0.046230435371398926,
        -0.2720268964767456,
        -0.5896062850952148,
        0.00556725449860096,
        0.48099634051322937,
        0.226779967546463,
        -0.641568660736084,
        0.03520100563764572,
        0.11632433533668518,
        0.8454240560531616,
        0.13382630050182343,
        0.15094658732414246,
        0.42649298906326294,
        0.22365717589855194,
        -0.03305262699723244,
        -0.38737577199935913,
        0.33304616808891296,
        0.4396301209926605,
        0.4003540575504303,
        0.06646806001663208,
        -0.4716588854789734,
        -0.5132031440734863,
        0.4607265591621399,
        0.04216545820236206,
        0.3846489191055298,
        -0.418293297290802,
        -0.3656986653804779,
        0.119149349629879,
        0.548724889755249,
        0.3597409725189209,
        0.9497102499008179,
        -0.021381713449954987,
        0.6018070578575134,
        -0.20105904340744019,
        0.47221896052360535,
        0.21160632371902466,
        -0.732685923576355,
        -0.34424734115600586,
        0.23284049332141876,
        0.3435218334197998,
        0.4684702157974243,
        -0.05593189597129822,
        -0.6598577499389648,
        -0.9024375677108765,
        -0.22467301785945892,
        -0.5548246502876282,
        2.2829885482788086,
        0.21330460906028748,
        -0.38588812947273254,
        0.5475846529006958,
        0.7216225266456604,
        0.6412566900253296,
        0.0069928839802742004,
        -1.6136997938156128,
        -0.8723245859146118,
        -0.3560728430747986,
        0.6283695101737976,
        -0.4312402904033661,
        0.650468111038208,
        0.05782999098300934,
        0.32706817984580994,
        -0.18969957530498505,
        -0.0017288662493228912,
        0.0424022376537323,
        -0.6143473386764526,
        -0.2253909707069397,
        -0.35795992612838745,
        -0.6792478561401367,
        -0.03832249343395233,
        0.014275595545768738,
        0.09333008527755737,
        -0.02926221676170826,
        -0.018295271322131157,
        0.42526334524154663,
        -0.273921400308609,
        -0.18027612566947937,
        -0.41689804196357727,
        0.11034844815731049,
        -0.1431879848241806,
        0.1427897810935974,
        0.38420331478118896,
        -0.4777079224586487,
        -0.2763861417770386,
        -0.26847004890441895,
        -0.16641950607299805,
        -0.6887641549110413,
        0.35825711488723755,
        -0.11093262583017349,
        -0.40265658497810364,
        -0.462018758058548,
        0.13712440431118011,
        1.7588022947311401,
        -0.045813411474227905,
        0.44279107451438904,
        0.5993820428848267,
        0.2647658884525299,
        0.0054510533809661865,
        -0.35569509863853455,
        -0.290740430355072,
        -0.045999664813280106,
        -0.3538956642150879,
        -0.40108945965766907,
        -0.21523046493530273,
        0.7770037651062012,
        0.39141857624053955,
        -0.4152330160140991,
        0.8068320155143738,
        -0.8691367506980896,
        0.1100277304649353,
        -0.06069129332900047,
        0.09578657150268555,
        0.5037500262260437,
        -0.3820522129535675,
        0.15033356845378876,
        0.612968385219574,
        -0.3111841380596161,
        -1.7166975736618042,
        -0.3130553364753723,
        0.5690194964408875,
        -0.26867052912712097,
        -0.6815000772476196,
        -0.19715958833694458,
        0.6428322196006775,
        -0.09958427399396896,
        0.6084272861480713,
        0.0038935989141464233,
        1.3510022163391113,
        0.270168274641037,
        0.13742342591285706,
        0.09424316883087158,
        -0.09643831104040146,
        1.1341352462768555,
        0.013741315342485905,
        0.5965465307235718,
        -0.10237634927034378,
        0.1958548128604889,
        0.11972063779830933,
        0.6091892123222351,
        1.2811322212219238,
        0.1885964274406433,
        0.5570122599601746,
        -0.10720628499984741,
        0.136312797665596,
        -0.10259668529033661,
        -1.0006829500198364,
        0.6150064468383789,
        -0.1942874640226364,
        -0.44514983892440796,
        1.0187406539916992,
        0.11178234219551086,
        0.1751391887664795,
        0.45340773463249207,
        0.607641339302063,
        -0.2440275400876999,
        0.42782485485076904,
        -0.00037454813718795776,
        1.533921718597412,
        -0.4010659456253052,
        -0.28950655460357666,
        -0.40218403935432434,
        0.02888339012861252,
        -0.13199420273303986,
        0.11962646245956421,
        -0.43680340051651,
        -0.36556512117385864,
        -0.17085781693458557,
        0.5980682969093323,
        0.1981208473443985,
        -0.4528166949748993,
        0.4060404598712921,
        0.13363750278949738,
        0.7917128801345825,
        -0.010492637753486633,
        0.05962102487683296,
        0.15104034543037415,
        0.1869790256023407,
        -0.27641040086746216,
        0.3807019293308258,
        -0.3339044451713562,
        -0.675617516040802,
        -0.4449980854988098
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Summarize_lecture",
          "raw": "\n                workflow Summarize_lecture v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\nAs an organized, high-skill expert lecturer, your role is to extract the most relevant topics from a lecture transcript and provide a structured summary using bullet points and lists of definitions for each subject. You will also include timestamps to indicate where in the video these topics occur.\n\nTake a step back and think step-by-step about how you would do this. You would probably start by \\\"watching\\\" the video (via the transcript) and taking notes on each definition were in the lecutre, because you're an organized you'll also make headlines and list of all relevant topics was in the lecutre and break through complex parts. you'll probably include the topics discussed and the time they were discussed. Then you would take those notes and create a list of topics and timestamps.\n\n\n# STEPS\nFully consume the transcript as if you're watching or listening to the content.\n\nThink deeply about the topics learned and what were the most relevant subjects and tools in the content.\n\nPay close attention to the structure, especially when it includes bullet points, lists, definitions, and headers. Ensure you divide the content in the most effective way.\n\nNode each topic as a headline. In case it has sub-topics or tools, use sub-headlines as markdowns.\n\nFor each topic or subject provide the most accurate definition without making guesses.\n\nExtract a summary of the lecutre in 25 words, including the most important keynotes into a section called SUMMARY.\n\nExtract all the tools you noticed there was mention and gather them with one line description into a section called TOOLS.\n\nExtract the most takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\nMatch the timestamps to the topics. Note that input timestamps have the following format: HOURS:MINUTES:SECONDS.MILLISECONDS, which is not the same as the OUTPUT format!\n\n## INPUT SAMPLE\n\n[02:17:43.120 --> 02:17:49.200] same way. I'll just say the same. And I look forward to hearing the response to my job application [02:17:49.200 --> 02:17:55.040] that I've submitted. Oh, you're accepted. Oh, yeah. We all speak of you all the time. Thank you so [02:17:55.040 --> 02:18:00.720] much. Thank you, guys. Thank you. Thanks for listening to this conversation with Neri Oxman. [02:18:00.720 --> 02:18:05.520] To support this podcast, please check out our sponsors in the description. And now,\n\n## END INPUT SAMPLE\n\nThe OUTPUT TIMESTAMP format is: 00:00:00 (HOURS:MINUTES:SECONDS) (HH:MM:SS)\n\nNote the maximum length of the video based on the last timestamp.\n\nEnsure all output timestamps are sequential and fall within the length of the content.\n\n\n# OUTPUT INSTRUCTIONS\n\nYou only output Markdown.\n\nIn the markdown, use formatting like bold, highlight, headlines as # ## ### , blockqoute as > , code block in neccenary as ``` {block_code} ```, lists as * , etc. Make the output maximally readable in plain text.\n\nCreate the output using the formatting above.\n\nDo not start items with the same opening words.\n\nUse middle ground/semi-formal speech for your output context.\n\nTo ensure the summary is easily searchable in the future, keep the structure clear and straightforward. \n\nEnsure you follow ALL these instructions when creating your output.\n\n\n## EXAMPLE OUTPUT (Hours:Minutes:Seconds)\n\n00:00:00 Members-only Forum Access 00:00:10 Live Hacking Demo 00:00:26 Ideas vs. Book 00:00:30 Meeting Will Smith 00:00:44 How to Influence Others 00:01:34 Learning by Reading 00:58:30 Writing With Punch 00:59:22 100 Posts or GTFO 01:00:32 How to Gain Followers 01:01:31 The Music That Shapes 01:27:21 Subdomain Enumeration Demo 01:28:40 Hiding in Plain Sight 01:29:06 The Universe Machine 00:09:36 Early School Experiences 00:10:12 The First Business Failure 00:10:32 David Foster Wallace 00:12:07 Copying Other Writers 00:12:32 Practical Advice for N00bs\n\n## END EXAMPLE OUTPUT\n\nEnsure all output timestamps are sequential and fall within the length of the content, e.g., if the total length of the video is 24 minutes. (00:00:00 - 00:24:00), then no output can be 01:01:25, or anything over 00:25:00 or over!\n\nENSURE the output timestamps and topics are shown gradually and evenly incrementing from 00:00:00 to the final timestamp of the content.\n\n# INPUT:\n\nINPUT: \n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\nAs an organized, high-skill expert lecturer, your role is to extract the most relevant topics from a lecture transcript and provide a structured summary using bullet points and lists of definitions for each subject. You will also include timestamps to indicate where in the video these topics occur.\n\nTake a step back and think step-by-step about how you would do this. You would probably start by \\\"watching\\\" the video (via the transcript) and taking notes on each definition were in the lecutre, because you're an organized you'll also make headlines and list of all relevant topics was in the lecutre and break through complex parts. you'll probably include the topics discussed and the time they were discussed. Then you would take those notes and create a list of topics and timestamps.\n\n\n# STEPS\nFully consume the transcript as if you're watching or listening to the content.\n\nThink deeply about the topics learned and what were the most relevant subjects and tools in the content.\n\nPay close attention to the structure, especially when it includes bullet points, lists, definitions, and headers. Ensure you divide the content in the most effective way.\n\nNode each topic as a headline. In case it has sub-topics or tools, use sub-headlines as markdowns.\n\nFor each topic or subject provide the most accurate definition without making guesses.\n\nExtract a summary of the lecutre in 25 words, including the most important keynotes into a section called SUMMARY.\n\nExtract all the tools you noticed there was mention and gather them with one line description into a section called TOOLS.\n\nExtract the most takeaway and recommendation into a section called ONE-SENTENCE TAKEAWAY. This should be a 15-word sentence that captures the most important essence of the content.\n\nMatch the timestamps to the topics. Note that input timestamps have the following format: HOURS:MINUTES:SECONDS.MILLISECONDS, which is not the same as the OUTPUT format!\n\n## INPUT SAMPLE\n\n[02:17:43.120 --> 02:17:49.200] same way. I'll just say the same. And I look forward to hearing the response to my job application [02:17:49.200 --> 02:17:55.040] that I've submitted. Oh, you're accepted. Oh, yeah. We all speak of you all the time. Thank you so [02:17:55.040 --> 02:18:00.720] much. Thank you, guys. Thank you. Thanks for listening to this conversation with Neri Oxman. [02:18:00.720 --> 02:18:05.520] To support this podcast, please check out our sponsors in the description. And now,\n\n## END INPUT SAMPLE\n\nThe OUTPUT TIMESTAMP format is: 00:00:00 (HOURS:MINUTES:SECONDS) (HH:MM:SS)\n\nNote the maximum length of the video based on the last timestamp.\n\nEnsure all output timestamps are sequential and fall within the length of the content.\n\n\n# OUTPUT INSTRUCTIONS\n\nYou only output Markdown.\n\nIn the markdown, use formatting like bold, highlight, headlines as # ## ### , blockqoute as > , code block in neccenary as ``` {block_code} ```, lists as * , etc. Make the output maximally readable in plain text.\n\nCreate the output using the formatting above.\n\nDo not start items with the same opening words.\n\nUse middle ground/semi-formal speech for your output context.\n\nTo ensure the summary is easily searchable in the future, keep the structure clear and straightforward. \n\nEnsure you follow ALL these instructions when creating your output.\n\n\n## EXAMPLE OUTPUT (Hours:Minutes:Seconds)\n\n00:00:00 Members-only Forum Access 00:00:10 Live Hacking Demo 00:00:26 Ideas vs. Book 00:00:30 Meeting Will Smith 00:00:44 How to Influence Others 00:01:34 Learning by Reading 00:58:30 Writing With Punch 00:59:22 100 Posts or GTFO 01:00:32 How to Gain Followers 01:01:31 The Music That Shapes 01:27:21 Subdomain Enumeration Demo 01:28:40 Hiding in Plain Sight 01:29:06 The Universe Machine 00:09:36 Early School Experiences 00:10:12 The First Business Failure 00:10:32 David Foster Wallace 00:12:07 Copying Other Writers 00:12:32 Practical Advice for N00bs\n\n## END EXAMPLE OUTPUT\n\nEnsure all output timestamps are sequential and fall within the length of the content, e.g., if the total length of the video is 24 minutes. (00:00:00 - 00:24:00), then no output can be 01:01:25, or anything over 00:25:00 or over!\n\nENSURE the output timestamps and topics are shown gradually and evenly incrementing from 00:00:00 to the final timestamp of the content.\n\n# INPUT:\n\nINPUT: \n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.22312666475772858,
        0.472181499004364,
        0.02595258690416813,
        0.308150976896286,
        -0.2928529977798462,
        0.014455839991569519,
        -0.7001439929008484,
        0.18181300163269043,
        0.254304438829422,
        0.12212496250867844,
        -0.4237678050994873,
        0.4572436511516571,
        0.05744538828730583,
        0.12663181126117706,
        0.35037922859191895,
        -0.08093011379241943,
        -0.6467705965042114,
        -0.9775698184967041,
        -1.277282476425171,
        -0.4264437258243561,
        0.31121826171875,
        0.8271327614784241,
        0.011222757399082184,
        0.15829533338546753,
        0.2223721444606781,
        -0.07044398039579391,
        0.1420101672410965,
        -0.25957614183425903,
        -1.0707505941390991,
        -2.19671368598938,
        0.6811837553977966,
        -0.08910123258829117,
        0.037355389446020126,
        -0.3664914667606354,
        0.11536113917827606,
        -1.580790400505066,
        -0.173395037651062,
        -0.16191652417182922,
        -0.44501951336860657,
        -0.325285941362381,
        0.03423852473497391,
        0.15437334775924683,
        0.15597745776176453,
        -0.036550410091876984,
        0.033883724361658096,
        -0.48205989599227905,
        0.34453293681144714,
        -0.3458702862262726,
        0.43004781007766724,
        0.09110739827156067,
        -0.8030845522880554,
        -0.9919859170913696,
        -0.10131704807281494,
        0.4779205322265625,
        -0.024072611704468727,
        -0.3433546721935272,
        0.27548620104789734,
        -0.3075248599052429,
        0.19475111365318298,
        -0.3650963008403778,
        -0.004734984133392572,
        -0.04689769446849823,
        -4.3175153732299805,
        -0.15341117978096008,
        -0.19265112280845642,
        0.6374560594558716,
        -0.10044214874505997,
        -0.18446621298789978,
        0.646064281463623,
        0.10293121635913849,
        0.006860740482807159,
        0.1344057023525238,
        -0.1609640121459961,
        0.1623944342136383,
        -0.34323248267173767,
        -0.0706549659371376,
        0.22829516232013702,
        0.19036710262298584,
        0.41210222244262695,
        -0.5775141716003418,
        -0.34466052055358887,
        0.55389404296875,
        0.32744544744491577,
        -0.4660255014896393,
        -0.33371204137802124,
        0.5447291135787964,
        -0.6021859645843506,
        -0.3525664806365967,
        0.3410458564758301,
        0.11363206803798676,
        -0.7307413816452026,
        -0.2219250351190567,
        0.24567416310310364,
        0.035870276391506195,
        -0.643427848815918,
        0.30929216742515564,
        0.05544203892350197,
        -0.09721794724464417,
        -0.09356234222650528,
        3.4857869148254395,
        0.6880124807357788,
        -0.028416240587830544,
        0.6697700023651123,
        -0.8237656950950623,
        0.7564577460289001,
        -0.17674122750759125,
        0.014902755618095398,
        -0.17246770858764648,
        0.22950595617294312,
        -0.23855063319206238,
        0.4054321348667145,
        -0.7446441650390625,
        0.08272875845432281,
        0.12169168144464493,
        0.536638617515564,
        0.6046980023384094,
        -0.6024505496025085,
        0.20172443985939026,
        -0.2815711498260498,
        0.8417423963546753,
        -0.21476702392101288,
        -0.26811522245407104,
        -0.2969253361225128,
        -0.2735956609249115,
        0.0019332095980644226,
        0.09130313992500305,
        -0.025404080748558044,
        0.5009868741035461,
        0.21470575034618378,
        0.21231138706207275,
        0.14995238184928894,
        -0.9997344017028809,
        -0.40850162506103516,
        -0.17923802137374878,
        0.3896616995334625,
        -0.3009679615497589,
        0.1618371158838272,
        -0.30368539690971375,
        0.5863749384880066,
        -0.799979031085968,
        -0.05476975440979004,
        -1.0508184432983398,
        0.7738130688667297,
        -0.32105404138565063,
        0.4040769934654236,
        0.5821992754936218,
        -0.2694551646709442,
        0.01937703788280487,
        -0.38679179549217224,
        -0.6186221837997437,
        -0.06694887578487396,
        0.509781002998352,
        0.07023580372333527,
        0.6295806169509888,
        1.1482322216033936,
        -0.2931461036205292,
        -0.15494245290756226,
        0.16029876470565796,
        -0.5246446132659912,
        0.3415210545063019,
        0.17231175303459167,
        0.10781928151845932,
        0.6586361527442932,
        -0.059213072061538696,
        0.7571859359741211,
        -0.6473031640052795,
        0.37922385334968567,
        -0.3995741903781891,
        0.3701264560222626,
        0.01817193627357483,
        0.06294887512922287,
        -0.06817128509283066,
        0.5549343228340149,
        1.0866639614105225,
        0.5592044591903687,
        0.18870534002780914,
        -0.18992367386817932,
        0.6958980560302734,
        0.18617087602615356,
        -0.5699213147163391,
        0.10443685948848724,
        0.3794020414352417,
        -0.313177227973938,
        -0.6921891570091248,
        -0.4068273901939392,
        0.5076225399971008,
        0.2585713863372803,
        0.30281034111976624,
        0.6010328531265259,
        0.31538552045822144,
        -0.5153453946113586,
        2.0029594898223877,
        -0.9633685350418091,
        -0.10683266073465347,
        0.1111588329076767,
        0.13315331935882568,
        0.029789462685585022,
        0.04563198983669281,
        0.5543628334999084,
        0.18895156681537628,
        -0.8587720990180969,
        -0.583741307258606,
        -0.38749462366104126,
        -0.035079777240753174,
        -0.23527738451957703,
        -0.6886940598487854,
        -0.06080096587538719,
        0.2592542767524719,
        0.22102577984333038,
        -0.4720631539821625,
        0.1706702709197998,
        0.3522741496562958,
        0.8054327964782715,
        0.1416359543800354,
        0.143315389752388,
        -0.23027729988098145,
        0.10338764637708664,
        -0.2088196873664856,
        0.05827455222606659,
        0.3350292146205902,
        0.20102059841156006,
        0.2877403497695923,
        -0.11470048874616623,
        -0.5774574279785156,
        -0.44062310457229614,
        0.2372669279575348,
        0.508906364440918,
        -0.11075680702924728,
        -0.12701603770256042,
        -0.15255001187324524,
        -0.005700714886188507,
        0.7605575323104858,
        0.9187937378883362,
        0.9760767221450806,
        -0.30967941880226135,
        0.7669575214385986,
        -0.22261923551559448,
        0.43344205617904663,
        0.15731929242610931,
        -0.9001460075378418,
        -0.008617088198661804,
        0.3635304868221283,
        0.46834707260131836,
        0.26323240995407104,
        0.08548383414745331,
        -0.608306884765625,
        -0.26522278785705566,
        0.023912325501441956,
        -0.4597260355949402,
        2.122202157974243,
        -0.08137322217226028,
        -0.8841568827629089,
        0.48916947841644287,
        0.7823956608772278,
        0.02434762939810753,
        0.07900232076644897,
        -1.9319270849227905,
        -0.43674319982528687,
        -0.6862277388572693,
        0.528196394443512,
        -0.2810297906398773,
        0.22490817308425903,
        -0.16387107968330383,
        -0.1754833310842514,
        0.052949149161577225,
        0.21939818561077118,
        0.07244553416967392,
        -0.28105831146240234,
        -0.18830926716327667,
        -0.5331083536148071,
        -0.24569828808307648,
        0.1593613177537918,
        -0.5972697734832764,
        0.291286826133728,
        0.3397488296031952,
        0.04723196104168892,
        0.3068830072879791,
        0.281423956155777,
        -0.12863212823867798,
        -0.257493793964386,
        0.44749173521995544,
        -0.09362781047821045,
        -0.11021142452955246,
        0.3420069217681885,
        -0.3646933138370514,
        -0.0018423572182655334,
        -0.36348435282707214,
        -0.2284647524356842,
        -0.2537790834903717,
        0.3616044223308563,
        -0.46085765957832336,
        -0.6847349405288696,
        -0.7671692967414856,
        0.07050800323486328,
        1.4858992099761963,
        -0.1598016917705536,
        -0.12766875326633453,
        0.6937093734741211,
        0.2963901162147522,
        0.02396482229232788,
        -0.4875921308994293,
        -0.04184895008802414,
        -0.3307981789112091,
        -0.15520533919334412,
        -0.6039080619812012,
        -0.2975674271583557,
        0.8048592805862427,
        0.32415544986724854,
        -0.4201158583164215,
        0.8370243906974792,
        -0.8318719863891602,
        -0.09024529904127121,
        -0.07312459498643875,
        0.21515391767024994,
        0.2774379849433899,
        -0.11780735850334167,
        -0.131593719124794,
        0.5526518225669861,
        -0.3621329963207245,
        -1.5878639221191406,
        -0.29314124584198,
        0.7329543232917786,
        -0.39614561200141907,
        -0.23462973535060883,
        -0.025059450417757034,
        0.6638009548187256,
        -0.18506240844726562,
        0.7631971836090088,
        -0.05104689300060272,
        1.1344196796417236,
        0.5264488458633423,
        0.1787903606891632,
        -0.09441216289997101,
        -0.3296854496002197,
        0.5122451782226562,
        -0.33737772703170776,
        0.42460307478904724,
        -0.11945661157369614,
        -0.11061008274555206,
        0.022726498544216156,
        0.2916644811630249,
        1.4827499389648438,
        0.09388803690671921,
        0.6320560574531555,
        0.05373047664761543,
        0.2589114010334015,
        0.0671069324016571,
        -0.36574429273605347,
        0.4739196002483368,
        -0.3279936909675598,
        -0.2435220330953598,
        1.2454849481582642,
        0.5106443166732788,
        0.18756842613220215,
        0.47402265667915344,
        0.6301388740539551,
        0.006001066416501999,
        0.2984147071838379,
        -0.2068600356578827,
        1.2692954540252686,
        -0.4207373261451721,
        -0.7138841152191162,
        -0.658216118812561,
        0.0642956793308258,
        -0.6771284341812134,
        0.2095922827720642,
        -0.1211843341588974,
        -0.19822929799556732,
        -0.08079303801059723,
        0.3970348834991455,
        0.24767857789993286,
        -0.6114606261253357,
        0.5252988338470459,
        0.204010009765625,
        0.5914928317070007,
        0.11559483408927917,
        0.13933740556240082,
        0.23604196310043335,
        0.15720713138580322,
        -0.07402898371219635,
        0.5980167984962463,
        -0.18118153512477875,
        -0.6124725937843323,
        -0.2499362677335739
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Summarize_legislation",
          "raw": "\n                workflow Summarize_legislation v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an expert AI specialized in reading and summarizing complex political proposals and legislation. \n\n# GOALS\n\n1. Summarize the key points of the proposal.\n\n2. Identify the tricky parts of the proposal or law that might be getting underplayed by the group who submitted it. E.g., hidden policies, taxes, fees, loopholes, the cancelling of programs, etc.\n\n3. Give a wholistic, unbiased view of the proposal that characterizes its overall purpose and goals.\n\n# STEPS\n\n1. Fully digest the submitted law or proposal.\n\n2. Read it 39 times as a liberal, as a conservative, and as a libertarian. Spend 319 hours doing multiple read-throughs from various political perspectives.\n\n3. Create the output according to the OUTPUT section below.\n\n# OUTPUT\n\n1. In a section called SUMMARY, summarize the input in single 25-word sentence followed by 5 15-word bullet points.\n\n2. In a section called PROPOSED CHANGES, summarize each of the proposed changes that would take place if the proposal/law were accepted.\n\nEXAMPLES:\n\n1. Would remove the tax on candy in the state of California.\n2. Would add an incentive for having children if both parents have a Master's degree.\n\nEND EXAMPLES\n\nEND EXAMPLES\n\n3. In a section called POSITIVE CHARACTERIZATION, capture how the submitting party is trying to make the proposal look, i.e., the positive spin they're putting on it. Give this as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find great candidates with positive views on the environment and get them elected.\n\nEND EXAMPLES\n\n4. In a section called BALANCED CHARACTERIZATION, capture a non-biased analysis of the proposal as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find candidates with aligned goals and try to get them elected.\n\nEND EXAMPLES\n\n\n4. In a section called CYNICAL CHARACTERIZATION, capture the parts of the bill that are likely to be controversial to the opposing side, and or that are being downplayed by the submitting party because they're shady or malicious. Give this as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find candidates with perfectly and narrowly aligned goals with an extreme faction, and works to get them elected.\n\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n1. Only output in valid Markdown.\n\n2. Do not output any asterisks, such as those used for italics or bolding.\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an expert AI specialized in reading and summarizing complex political proposals and legislation. \n\n# GOALS\n\n1. Summarize the key points of the proposal.\n\n2. Identify the tricky parts of the proposal or law that might be getting underplayed by the group who submitted it. E.g., hidden policies, taxes, fees, loopholes, the cancelling of programs, etc.\n\n3. Give a wholistic, unbiased view of the proposal that characterizes its overall purpose and goals.\n\n# STEPS\n\n1. Fully digest the submitted law or proposal.\n\n2. Read it 39 times as a liberal, as a conservative, and as a libertarian. Spend 319 hours doing multiple read-throughs from various political perspectives.\n\n3. Create the output according to the OUTPUT section below.\n\n# OUTPUT\n\n1. In a section called SUMMARY, summarize the input in single 25-word sentence followed by 5 15-word bullet points.\n\n2. In a section called PROPOSED CHANGES, summarize each of the proposed changes that would take place if the proposal/law were accepted.\n\nEXAMPLES:\n\n1. Would remove the tax on candy in the state of California.\n2. Would add an incentive for having children if both parents have a Master's degree.\n\nEND EXAMPLES\n\nEND EXAMPLES\n\n3. In a section called POSITIVE CHARACTERIZATION, capture how the submitting party is trying to make the proposal look, i.e., the positive spin they're putting on it. Give this as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find great candidates with positive views on the environment and get them elected.\n\nEND EXAMPLES\n\n4. In a section called BALANCED CHARACTERIZATION, capture a non-biased analysis of the proposal as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find candidates with aligned goals and try to get them elected.\n\nEND EXAMPLES\n\n\n4. In a section called CYNICAL CHARACTERIZATION, capture the parts of the bill that are likely to be controversial to the opposing side, and or that are being downplayed by the submitting party because they're shady or malicious. Give this as a set of 15-word bullet points.\n\nEXAMPLES:\n\n1. The bill looks to find candidates with perfectly and narrowly aligned goals with an extreme faction, and works to get them elected.\n\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n1. Only output in valid Markdown.\n\n2. Do not output any asterisks, such as those used for italics or bolding.\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5105514526367188,
        0.04471689462661743,
        0.3485907018184662,
        0.48539283871650696,
        0.1727682203054428,
        -0.030163176357746124,
        -0.9006124138832092,
        0.37498950958251953,
        0.32242560386657715,
        0.02491069957613945,
        -0.12971557676792145,
        0.6936569213867188,
        0.30787256360054016,
        0.30046674609184265,
        0.03736068308353424,
        -0.031134597957134247,
        0.17828544974327087,
        -1.137023687362671,
        -1.3026564121246338,
        -0.485602468252182,
        -0.5827787518501282,
        0.5983317494392395,
        0.29647207260131836,
        0.2401934117078781,
        0.6564441323280334,
        -0.30067795515060425,
        -0.1936330944299698,
        -0.4168151021003723,
        -0.7401893734931946,
        -1.8176461458206177,
        0.20606094598770142,
        0.10073992609977722,
        0.3030557930469513,
        -0.16191916167736053,
        0.3438034653663635,
        -1.308946132659912,
        0.39824992418289185,
        0.26233524084091187,
        -0.4810122847557068,
        -0.09706556797027588,
        -0.05758420377969742,
        0.1580950766801834,
        0.0034830421209335327,
        -0.05670201778411865,
        0.7373818159103394,
        -0.5566965341567993,
        -0.36885058879852295,
        0.16554273664951324,
        0.5007052421569824,
        -0.2179265171289444,
        -0.5475278496742249,
        -0.5172308087348938,
        -0.09628275036811829,
        0.579531192779541,
        -0.2045118808746338,
        -0.7166986465454102,
        0.043043266981840134,
        -0.927100419998169,
        0.5572525858879089,
        -0.000055938027799129486,
        0.04177796468138695,
        0.20318222045898438,
        -3.681154251098633,
        0.04149125888943672,
        0.2069888412952423,
        0.5848058462142944,
        0.26798900961875916,
        -0.3153756260871887,
        -0.4141361713409424,
        -0.1436292678117752,
        -0.4015682339668274,
        0.23362669348716736,
        -0.5874683856964111,
        0.4462493658065796,
        -0.01593049243092537,
        -0.17757362127304077,
        0.40869221091270447,
        -0.25991418957710266,
        0.35598844289779663,
        -0.08916416019201279,
        -0.1115943118929863,
        0.535369336605072,
        0.0937684029340744,
        -0.05695968121290207,
        -0.807204008102417,
        0.5240219235420227,
        -0.3953453004360199,
        -0.5206634998321533,
        0.39474496245384216,
        0.3374745547771454,
        -0.5222622752189636,
        -0.33666932582855225,
        0.00047190114855766296,
        0.739080011844635,
        -0.16122016310691833,
        -0.14980393648147583,
        0.15322642028331757,
        -0.20673048496246338,
        -0.27067863941192627,
        3.385699987411499,
        0.7631587386131287,
        0.027982134371995926,
        0.6544909477233887,
        -0.8363038897514343,
        0.2846263647079468,
        -0.24220788478851318,
        0.06615172326564789,
        -0.21416333317756653,
        -0.18132629990577698,
        0.3519902229309082,
        0.5886236429214478,
        -1.1299701929092407,
        -0.24069112539291382,
        0.1883685290813446,
        0.38809671998023987,
        0.34793800115585327,
        -0.5742549300193787,
        0.245150625705719,
        0.32900699973106384,
        0.5230453610420227,
        -0.49299857020378113,
        0.10620102286338806,
        -0.5709839463233948,
        -0.2709813117980957,
        0.23483572900295258,
        0.5668158531188965,
        -0.2158171683549881,
        0.5466956496238708,
        0.20230771601200104,
        0.5289453268051147,
        -0.047906436026096344,
        -0.3694527745246887,
        -0.4090006351470947,
        0.3138217031955719,
        -0.16974659264087677,
        -0.0444239005446434,
        0.6137974858283997,
        -1.0857553482055664,
        0.2756280303001404,
        -1.3151675462722778,
        0.048090189695358276,
        -0.991867184638977,
        0.3491404056549072,
        0.14404965937137604,
        0.10123223811388016,
        0.27377015352249146,
        0.19927456974983215,
        -0.007402442395687103,
        -0.29153743386268616,
        -0.45466119050979614,
        0.024215053766965866,
        0.7202953100204468,
        -0.2935040593147278,
        0.3166441023349762,
        0.7604572176933289,
        -0.13837507367134094,
        -0.727842390537262,
        0.6831477880477905,
        -0.3800435960292816,
        0.704669177532196,
        -0.11021007597446442,
        0.08936325460672379,
        0.4768664240837097,
        -0.23926444351673126,
        0.1698937714099884,
        -0.8011718988418579,
        0.42616626620292664,
        -0.5273283123970032,
        0.03267446160316467,
        0.7141350507736206,
        -0.0004794895648956299,
        -0.29832521080970764,
        0.6689097285270691,
        0.4011498987674713,
        -0.18019190430641174,
        -0.01456443965435028,
        0.05991217494010925,
        -0.1024165228009224,
        0.18304981291294098,
        -0.5342333316802979,
        0.6266782283782959,
        0.3839540183544159,
        -0.10015098750591278,
        -0.557350218296051,
        -0.2851254343986511,
        0.6487964987754822,
        0.19131729006767273,
        0.45135927200317383,
        1.500964641571045,
        1.1745574474334717,
        -0.5508002638816833,
        1.577256441116333,
        -0.4031951427459717,
        0.05424275994300842,
        -0.19719289243221283,
        0.4645254611968994,
        -0.1899721920490265,
        -0.0696304440498352,
        0.614353597164154,
        0.19386954605579376,
        -0.4479988217353821,
        -0.23086762428283691,
        -0.6297973990440369,
        -0.07326006889343262,
        -0.7634928822517395,
        -1.0601303577423096,
        0.0038565900176763535,
        0.7085369825363159,
        0.5568960309028625,
        -0.9928836822509766,
        0.059635281562805176,
        -0.1329166293144226,
        1.1377453804016113,
        0.16242802143096924,
        0.4608708918094635,
        0.04374842345714569,
        -0.050730377435684204,
        0.010280460119247437,
        0.4988499879837036,
        0.305446982383728,
        -0.08630970865488052,
        0.3752322494983673,
        -0.31568023562431335,
        -0.6513563394546509,
        -0.5658563375473022,
        0.7066618800163269,
        -0.6054350137710571,
        0.564644455909729,
        -0.6641045212745667,
        -0.5889556407928467,
        -0.3869728147983551,
        0.9595189690589905,
        1.0463389158248901,
        0.9264253377914429,
        -0.3905237317085266,
        0.6298479437828064,
        -0.19529947638511658,
        0.5360953211784363,
        0.17962603271007538,
        -1.1023918390274048,
        0.29361915588378906,
        0.11843843758106232,
        0.14566318690776825,
        0.42829805612564087,
        0.45211371779441833,
        0.19037094712257385,
        -1.1267926692962646,
        0.2503492832183838,
        0.05975478142499924,
        1.7949719429016113,
        -0.3065144717693329,
        -0.25586065649986267,
        -0.011973623186349869,
        0.35637617111206055,
        -0.07889770716428757,
        0.17347002029418945,
        -1.3355411291122437,
        -0.4153364598751068,
        -0.8547268509864807,
        0.938264787197113,
        -0.7185243368148804,
        0.2229328751564026,
        0.5198301076889038,
        0.44445592164993286,
        -0.3222711384296417,
        -0.3792535662651062,
        0.0006309635937213898,
        -0.36610257625579834,
        -0.28739091753959656,
        -0.04291406273841858,
        -0.05770100653171539,
        0.14773553609848022,
        0.12496943771839142,
        -0.1741805374622345,
        0.028165649622678757,
        -0.034414179623126984,
        0.13112859427928925,
        0.044550180435180664,
        -0.3952576518058777,
        -0.5903170704841614,
        0.11305233091115952,
        0.00908537395298481,
        -0.27680492401123047,
        0.5724338889122009,
        -0.9203490614891052,
        -0.28886520862579346,
        -0.5552149415016174,
        -0.268300324678421,
        0.045625098049640656,
        0.6839916706085205,
        -0.1766064465045929,
        -0.2951134443283081,
        -0.8824754357337952,
        0.16449934244155884,
        1.948102593421936,
        0.2146623730659485,
        0.1342373490333557,
        0.816401481628418,
        0.45930030941963196,
        -0.407168447971344,
        -0.8598812818527222,
        -0.02690785378217697,
        -0.36610716581344604,
        -0.24845290184020996,
        -0.794190526008606,
        -0.14942903816699982,
        0.6130366325378418,
        0.27606379985809326,
        -0.07978559285402298,
        0.42337870597839355,
        -1.055320143699646,
        -0.20150411128997803,
        0.14199486374855042,
        -0.05887018144130707,
        0.6317628622055054,
        -0.3267946243286133,
        0.027432460337877274,
        0.6395561695098877,
        -0.23744288086891174,
        -1.3031721115112305,
        -0.2417069375514984,
        0.8188636898994446,
        0.04974822327494621,
        -0.34737104177474976,
        0.08134517073631287,
        1.1457940340042114,
        0.26618292927742004,
        0.050372958183288574,
        -0.26438838243484497,
        1.5071431398391724,
        0.892272412776947,
        0.10583699494600296,
        -0.6249923706054688,
        -0.14280979335308075,
        1.024935007095337,
        0.1078350618481636,
        0.2605037987232208,
        -0.10993214696645737,
        -0.6559502482414246,
        -0.17604510486125946,
        0.6289780139923096,
        1.488403558731079,
        -0.14379502832889557,
        0.6491826176643372,
        0.011441286653280258,
        -0.2410571277141571,
        -0.31166011095046997,
        -1.2355778217315674,
        -0.23322907090187073,
        0.020639948546886444,
        -0.5628187656402588,
        0.6362123489379883,
        -0.25754016637802124,
        0.3618411123752594,
        0.633827805519104,
        0.7562872767448425,
        -0.1811372935771942,
        0.1839030236005783,
        -0.45338791608810425,
        1.5817888975143433,
        -0.39579489827156067,
        -0.3759240210056305,
        -0.14873620867729187,
        -0.003683178685605526,
        0.14621533453464508,
        0.7226651906967163,
        -0.4444798529148102,
        -0.1919897198677063,
        -0.26027393341064453,
        0.26085302233695984,
        0.17859849333763123,
        -0.6937254667282104,
        0.7581014633178711,
        0.21274533867835999,
        0.346077024936676,
        -0.19347842037677765,
        0.19386965036392212,
        0.013473942875862122,
        0.3706287741661072,
        -0.572458803653717,
        0.304087370634079,
        -0.4534083902835846,
        -0.399573415517807,
        -1.1979459524154663
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes content into a structured Markdown format. This prompt focuses on concise, bullet-pointed summaries and takeaways. The output includes a one-sentence summary and lists of main points and takeaways.",
          "name": "Summarize_micro",
          "raw": "\n                workflow Summarize_micro v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 3 most important points of the content as a list with no more than 12 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 3 best takeaways from the content in 12 words or less each in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Keep each bullet to 12 words or less.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 3 most important points of the content as a list with no more than 12 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 3 best takeaways from the content in 12 words or less each in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Output bullets not numbers.\n- You only output human readable Markdown.\n- Keep each bullet to 12 words or less.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.7104158401489258,
        0.18150490522384644,
        -0.12677109241485596,
        0.280555784702301,
        0.4884641170501709,
        -0.03660864382982254,
        -0.8270556330680847,
        0.35833975672721863,
        0.4512474238872528,
        0.01727908104658127,
        -0.15145407617092133,
        1.1503710746765137,
        0.25322750210762024,
        0.15260446071624756,
        0.031079526990652084,
        0.04860790818929672,
        -0.17956271767616272,
        -0.8079418540000916,
        -1.3342905044555664,
        -0.20476242899894714,
        -0.4024399220943451,
        0.6751307249069214,
        0.793948769569397,
        0.370815634727478,
        0.37474703788757324,
        -0.6240658760070801,
        -0.2323816865682602,
        -0.20604920387268066,
        -0.5069440603256226,
        -1.8304959535598755,
        0.6334859728813171,
        0.42083975672721863,
        -0.06375107914209366,
        -0.43018704652786255,
        -0.02700214833021164,
        -0.7582239508628845,
        0.11879783868789673,
        0.2284184843301773,
        -0.20235756039619446,
        -0.0647282749414444,
        0.2222769409418106,
        0.5577869415283203,
        -0.3936255872249603,
        0.08249523490667343,
        0.20667491853237152,
        -0.48201191425323486,
        -0.5737189054489136,
        0.08473457396030426,
        0.003034187015146017,
        0.27678966522216797,
        0.033062923699617386,
        -0.19777406752109528,
        0.059899117797613144,
        0.28229793906211853,
        -0.5967696905136108,
        -0.5583802461624146,
        -0.46726009249687195,
        -0.4075275957584381,
        0.5528690218925476,
        0.2569167912006378,
        0.730372428894043,
        0.42260318994522095,
        -3.5608348846435547,
        -0.02384701557457447,
        -0.10665746033191681,
        -0.19697272777557373,
        0.07755815982818604,
        -0.18769048154354095,
        -0.04108239337801933,
        0.031125277280807495,
        -0.23110011219978333,
        0.4102279841899872,
        -0.2068183720111847,
        0.7728623747825623,
        0.2338745892047882,
        -0.04413830488920212,
        0.3103348910808563,
        -0.03362548351287842,
        0.19277548789978027,
        -0.6832582950592041,
        -0.049333587288856506,
        0.8425120711326599,
        0.01680263876914978,
        -0.17931711673736572,
        -0.8102529644966125,
        0.7176889181137085,
        -0.3729465901851654,
        -0.7978017330169678,
        -0.06544193625450134,
        -0.09157317876815796,
        -0.1220397800207138,
        -0.5630184412002563,
        0.34728455543518066,
        -0.04922467842698097,
        -0.8492175936698914,
        -0.337957501411438,
        0.05375861003994942,
        0.34275177121162415,
        0.24404165148735046,
        3.454603910446167,
        1.3817318677902222,
        0.15505771338939667,
        0.5620080232620239,
        -1.022560715675354,
        0.8106741905212402,
        -0.8921451568603516,
        0.023542210459709167,
        0.000963892787694931,
        -0.2562545835971832,
        -0.17441080510616302,
        0.8373175263404846,
        -0.4219869077205658,
        -0.017628446221351624,
        0.13178354501724243,
        0.20569056272506714,
        0.8059420585632324,
        -0.18005742132663727,
        0.2324090152978897,
        0.05241504684090614,
        0.24845977127552032,
        -0.04221343249082565,
        0.2946183383464813,
        -0.26520785689353943,
        0.031798750162124634,
        -0.015089379623532295,
        -0.02941882610321045,
        0.08428388833999634,
        0.1932331919670105,
        0.39945799112319946,
        -0.10556580126285553,
        0.1989535689353943,
        -0.5216773152351379,
        -0.664333701133728,
        0.33431291580200195,
        -0.44651323556900024,
        -0.20225217938423157,
        0.35645753145217896,
        -0.35693106055259705,
        0.5002086758613586,
        -0.4636092185974121,
        0.011057347059249878,
        -1.499867558479309,
        -0.05611024796962738,
        0.7769333124160767,
        0.636877715587616,
        0.11895585805177689,
        0.3528205454349518,
        -0.13275930285453796,
        -0.5097684860229492,
        -0.6065623760223389,
        -0.07242850959300995,
        0.5437260866165161,
        0.20208384096622467,
        -0.1006709560751915,
        0.6330000758171082,
        0.13985806703567505,
        -0.561314582824707,
        0.4190503656864166,
        -0.17805278301239014,
        0.10598304867744446,
        -0.03953825309872627,
        0.33599746227264404,
        0.38506466150283813,
        -0.09682722389698029,
        0.21324971318244934,
        -0.9188805818557739,
        0.33976972103118896,
        -0.4163976311683655,
        -0.024492114782333374,
        0.3373247981071472,
        0.34372034668922424,
        -0.3769579529762268,
        0.19556774199008942,
        0.32710000872612,
        0.3536699712276459,
        -0.25125211477279663,
        0.3061985969543457,
        0.04803293198347092,
        0.5758824348449707,
        -0.6870862245559692,
        0.9036162495613098,
        0.5595239996910095,
        -0.18661192059516907,
        -0.763474702835083,
        -0.26404184103012085,
        0.4085373878479004,
        0.03220126032829285,
        -0.07760482281446457,
        0.6649295687675476,
        1.2061309814453125,
        -0.41560351848602295,
        1.7896130084991455,
        -0.38651609420776367,
        -0.7194352149963379,
        0.1303260177373886,
        0.28643542528152466,
        -0.9482496380805969,
        0.05270889773964882,
        0.6035560965538025,
        0.21962027251720428,
        -0.39820775389671326,
        -0.79161536693573,
        -0.5056630373001099,
        -0.09180545806884766,
        -0.2554818391799927,
        -0.6457601189613342,
        -0.23448655009269714,
        0.17002727091312408,
        0.41123372316360474,
        -1.069573998451233,
        0.16507519781589508,
        0.11583440005779266,
        0.15179932117462158,
        0.24052909016609192,
        0.2583293914794922,
        0.39330780506134033,
        -0.35081538558006287,
        0.6523877382278442,
        0.35508254170417786,
        0.3398740291595459,
        0.41598600149154663,
        0.4053862392902374,
        -0.5601937174797058,
        -0.6439443826675415,
        -1.1207157373428345,
        0.8716046810150146,
        -0.22811195254325867,
        0.0483727864921093,
        -0.2836870849132538,
        -0.29965123534202576,
        0.028896957635879517,
        1.0039889812469482,
        0.2603730857372284,
        1.0238348245620728,
        0.008930668234825134,
        0.3250502943992615,
        0.06102089211344719,
        0.4586403965950012,
        0.4436047375202179,
        -0.8489378094673157,
        0.4058929681777954,
        -0.08548298478126526,
        -0.01792684942483902,
        0.4943639636039734,
        -0.008302547037601471,
        0.3581562638282776,
        -0.7040650248527527,
        0.10997401177883148,
        -0.20524261891841888,
        1.5895321369171143,
        0.004494715481996536,
        0.4770166575908661,
        0.26863881945610046,
        0.5227826237678528,
        0.4279608428478241,
        -0.10682453960180283,
        -1.7128071784973145,
        0.13238437473773956,
        -0.796778678894043,
        0.5818257927894592,
        -0.8670235872268677,
        -0.015066444873809814,
        0.0010075196623802185,
        0.29437029361724854,
        -0.09925003349781036,
        -0.37770214676856995,
        -0.4700835347175598,
        -0.2299860715866089,
        -0.08992017060518265,
        -0.5602278113365173,
        -0.12188360840082169,
        0.047352343797683716,
        -0.23812299966812134,
        -0.40801677107810974,
        0.20237331092357635,
        -0.18514013290405273,
        0.30981406569480896,
        0.6728266477584839,
        -0.1644163727760315,
        -0.46915531158447266,
        0.1874748170375824,
        0.2960391938686371,
        -0.23980137705802917,
        0.42784935235977173,
        -0.7826429009437561,
        0.10890048742294312,
        -0.033193327486515045,
        -0.43070459365844727,
        -0.1711946427822113,
        1.019798755645752,
        0.15531083941459656,
        -0.711845874786377,
        -0.1654428094625473,
        -0.1948317587375641,
        2.1496400833129883,
        0.061774615198373795,
        0.36434870958328247,
        0.47901394963264465,
        0.9447905421257019,
        -0.02763809636235237,
        -0.7807925939559937,
        0.06801299750804901,
        -0.4015851318836212,
        -0.3570006489753723,
        -0.5106767416000366,
        -0.51917964220047,
        0.47966867685317993,
        0.03218710049986839,
        -0.39177849888801575,
        0.5106385946273804,
        -0.4550241529941559,
        -0.30940699577331543,
        0.2232857644557953,
        0.031484927982091904,
        0.730670154094696,
        -0.5517657995223999,
        0.5980010628700256,
        0.4687404930591583,
        -0.2554013729095459,
        -2.0614871978759766,
        -0.11873170733451843,
        0.426652193069458,
        0.1003778874874115,
        -0.2529028058052063,
        0.3942464590072632,
        0.804460883140564,
        -0.3741852045059204,
        -0.32137179374694824,
        -0.213955819606781,
        1.4927259683609009,
        0.2704516053199768,
        -0.12234582006931305,
        -0.3289123475551605,
        0.05733180791139603,
        1.0741676092147827,
        -0.21875271201133728,
        0.12493499368429184,
        -0.5863401889801025,
        -0.9120883345603943,
        -0.13454386591911316,
        0.24681027233600616,
        1.5515365600585938,
        0.38232308626174927,
        -0.05993590131402016,
        -0.31719037890434265,
        -0.11544261127710342,
        -0.6958383917808533,
        -0.9161661267280579,
        0.4587811231613159,
        -0.03856704756617546,
        -0.7618808746337891,
        0.4540437161922455,
        0.02066093683242798,
        0.07316142320632935,
        0.3052223026752472,
        0.6072705388069153,
        0.007810588926076889,
        -0.1524931937456131,
        -1.1401070356369019,
        1.8369786739349365,
        0.016057802364230156,
        -0.30680611729621887,
        -0.31186050176620483,
        -0.2618897557258606,
        -0.22079411149024963,
        -0.032264769077301025,
        0.35755860805511475,
        0.1466931402683258,
        -0.4219653010368347,
        0.19215860962867737,
        0.5368499755859375,
        -1.0106829404830933,
        0.7977919578552246,
        0.7299234867095947,
        -0.17225781083106995,
        -0.01791699230670929,
        0.3865656852722168,
        -0.15348881483078003,
        0.3164125680923462,
        0.17508842051029205,
        0.03167683631181717,
        -0.07774840295314789,
        -1.1480591297149658,
        -0.2944844961166382
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Extracts and organizes key content from newsletters, focusing on the most meaningful, interesting, and useful information. It uniquely parses the entire newsletter to provide concise summaries, lists of content, opinions, tools, companies, and follow-up actions. The output includes sections for a brief summary, detailed content points, author opinions, mentioned tools and companies, and recommended follow-ups in a structured Markdown format.",
          "name": "Summarize_newsletter",
          "raw": "\n                workflow Summarize_newsletter v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an advanced AI newsletter content extraction service that extracts the most meaningful and interesting and useful content from an incoming newsletter.\n\nTake a deep breath and think step-by-step about how to achieve the best output using the steps below.\n\n0. Print the name of the newsletter and it's issue number and episode description in a section called NEWSLETTER:.\n\n1. Parse the whole newsletter and provide a 20 word summary of it, into a section called SUMMARY:. along with a list of 10 bullets that summarize the content in 15 words or less per bullet. Put these bullets into a section called SUMMARY:.\n\n2. Parse the whole newsletter and provide a list of 10 bullets that summarize the content in 15 words or less per bullet into a section called CONTENT:.\n\n3. Output a bulleted list of any opinions or ideas expressed by the newsletter author in a section called OPINIONS & IDEAS:.\n\n4. Output a bulleted list of the tools mentioned and a link to their website and X (twitter) into a section called TOOLS:.\n\n5. Output a bulleted list of the companies mentioned and a link to their website and X (twitter) into a section called COMPANIES:.\n\n6. Output a bulleted list of the coolest things to follow up on based on the newsletter content into a section called FOLLOW-UP:.\n\nFOLLOW-UP SECTION EXAMPLE\n\n1. Definitely check out that new project CrewAI because it's a new AI agent framework: $$LINK$$.\n2. Check out that company RunAI because they might be a good sponsor: $$LINK$$.\n   etc.\n\nEND FOLLOW-UP SECTION EXAMPLE\n\nOUTPUT INSTRUCTIONS:\n\n1. Only use the headers provided in the instructions above.\n2. Format your output in clear, human-readable Markdown.\n3. Use bulleted lists for all lists.\n\nNEWSLETTER INPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an advanced AI newsletter content extraction service that extracts the most meaningful and interesting and useful content from an incoming newsletter.\n\nTake a deep breath and think step-by-step about how to achieve the best output using the steps below.\n\n0. Print the name of the newsletter and it's issue number and episode description in a section called NEWSLETTER:.\n\n1. Parse the whole newsletter and provide a 20 word summary of it, into a section called SUMMARY:. along with a list of 10 bullets that summarize the content in 15 words or less per bullet. Put these bullets into a section called SUMMARY:.\n\n2. Parse the whole newsletter and provide a list of 10 bullets that summarize the content in 15 words or less per bullet into a section called CONTENT:.\n\n3. Output a bulleted list of any opinions or ideas expressed by the newsletter author in a section called OPINIONS & IDEAS:.\n\n4. Output a bulleted list of the tools mentioned and a link to their website and X (twitter) into a section called TOOLS:.\n\n5. Output a bulleted list of the companies mentioned and a link to their website and X (twitter) into a section called COMPANIES:.\n\n6. Output a bulleted list of the coolest things to follow up on based on the newsletter content into a section called FOLLOW-UP:.\n\nFOLLOW-UP SECTION EXAMPLE\n\n1. Definitely check out that new project CrewAI because it's a new AI agent framework: $$LINK$$.\n2. Check out that company RunAI because they might be a good sponsor: $$LINK$$.\n   etc.\n\nEND FOLLOW-UP SECTION EXAMPLE\n\nOUTPUT INSTRUCTIONS:\n\n1. Only use the headers provided in the instructions above.\n2. Format your output in clear, human-readable Markdown.\n3. Use bulleted lists for all lists.\n\nNEWSLETTER INPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5740859508514404,
        0.40543413162231445,
        0.21351726353168488,
        0.14135564863681793,
        0.2661396861076355,
        0.2463921308517456,
        -1.0196151733398438,
        0.10273927450180054,
        -0.13462185859680176,
        -0.11524855345487595,
        0.5922962427139282,
        0.9051292538642883,
        0.12013725936412811,
        -0.22045695781707764,
        0.43852853775024414,
        -0.6027112603187561,
        -0.5366365313529968,
        -0.3403717577457428,
        -1.3671953678131104,
        -0.19688504934310913,
        -0.03422398865222931,
        1.2214354276657104,
        -0.3168414831161499,
        0.14986661076545715,
        0.33892902731895447,
        -0.18954722583293915,
        -0.190979465842247,
        -0.13881254196166992,
        -0.9692837595939636,
        -1.7194050550460815,
        0.3144390881061554,
        0.4039929211139679,
        -0.6309481263160706,
        0.06625157594680786,
        0.7141318321228027,
        -0.8582543730735779,
        0.6019467711448669,
        -0.15113797783851624,
        -0.18683430552482605,
        -0.3740180730819702,
        -0.5388732552528381,
        0.368424654006958,
        0.21887436509132385,
        0.20664268732070923,
        0.8711466193199158,
        -0.09610381722450256,
        -0.5348949432373047,
        -0.12981444597244263,
        0.48322632908821106,
        0.5697751045227051,
        -0.36802640557289124,
        -0.6804333329200745,
        -0.23126809298992157,
        -0.2347654104232788,
        0.17435222864151,
        -0.4148818254470825,
        -0.43569132685661316,
        -0.49695920944213867,
        0.2772330939769745,
        0.49885293841362,
        0.12386865168809891,
        0.16443213820457458,
        -3.276787281036377,
        0.031332504004240036,
        0.21957823634147644,
        0.1333305984735489,
        0.38290855288505554,
        -0.2418590486049652,
        -0.5068258047103882,
        0.06704393029212952,
        0.043061841279268265,
        0.2271115630865097,
        0.0074002183973789215,
        0.398401141166687,
        -0.23968583345413208,
        -0.2492656111717224,
        0.12087127566337585,
        0.15655316412448883,
        0.0833376795053482,
        -0.8376812934875488,
        -0.49163851141929626,
        -0.14036458730697632,
        -0.17715054750442505,
        -0.06459395587444305,
        -0.8197270631790161,
        0.9646487832069397,
        -0.17926040291786194,
        0.2700159549713135,
        0.3080204725265503,
        -0.11778286844491959,
        0.02422928996384144,
        -0.6988612413406372,
        0.09457069635391235,
        0.6107547283172607,
        -0.23461449146270752,
        -0.2178371697664261,
        0.24045008420944214,
        0.32020723819732666,
        -0.18056365847587585,
        3.357065200805664,
        1.2911367416381836,
        0.24856723845005035,
        0.4374653100967407,
        -1.3135013580322266,
        0.5099202990531921,
        -0.6935644745826721,
        -0.20761260390281677,
        -0.2179557979106903,
        0.08665753901004791,
        0.06662388890981674,
        0.632276177406311,
        -0.8334628343582153,
        -0.5342675447463989,
        -0.15924224257469177,
        -0.01647895574569702,
        0.9104282855987549,
        -1.0981616973876953,
        -0.1788669377565384,
        0.18826071918010712,
        0.16764366626739502,
        -0.7014082670211792,
        0.18715524673461914,
        -0.2937164306640625,
        -0.17890462279319763,
        -0.10582973062992096,
        0.3216043710708618,
        -0.18340924382209778,
        0.4221120774745941,
        0.9755899310112,
        -0.11596572399139404,
        -0.466780424118042,
        -0.1754768043756485,
        -1.0613728761672974,
        0.26024913787841797,
        -0.1833435595035553,
        -0.21006962656974792,
        0.4056156873703003,
        -0.6621793508529663,
        0.48073485493659973,
        -1.111731767654419,
        0.5061860680580139,
        -1.025913953781128,
        0.681387722492218,
        0.751090407371521,
        1.0875146389007568,
        -0.014228053390979767,
        0.12643766403198242,
        -0.14229613542556763,
        -0.30265480279922485,
        -1.0596563816070557,
        -0.054683197289705276,
        0.698408305644989,
        0.1545962542295456,
        0.07695047557353973,
        0.7193816304206848,
        -0.3675142526626587,
        -0.6337509751319885,
        0.6311754584312439,
        -0.3469831943511963,
        -0.011187616735696793,
        -0.025508113205432892,
        0.3769875168800354,
        0.23745927214622498,
        -0.7720761895179749,
        0.227727472782135,
        -0.757229745388031,
        0.3716542720794678,
        -0.4701663553714752,
        0.2574673295021057,
        0.46327170729637146,
        -0.15787622332572937,
        -0.32358768582344055,
        0.8023131489753723,
        0.5596448183059692,
        1.0277421474456787,
        -0.22468891739845276,
        0.04499483108520508,
        0.03498506546020508,
        -0.22618812322616577,
        -0.469142884016037,
        0.7814488410949707,
        0.21765950322151184,
        0.028741832822561264,
        -0.8361021280288696,
        -0.2400798201560974,
        0.5427888035774231,
        -0.007514849305152893,
        -0.0738910660147667,
        1.0796583890914917,
        1.507756233215332,
        -0.8427337408065796,
        1.576258897781372,
        -0.698861837387085,
        -0.45409223437309265,
        -0.2368777096271515,
        0.4281916618347168,
        -0.6421040296554565,
        -0.14987553656101227,
        0.8332816958427429,
        0.09072349965572357,
        -0.3261526823043823,
        -0.18991652131080627,
        -0.7852638363838196,
        -0.10027498006820679,
        -0.3948621153831482,
        -0.7757065892219543,
        -0.30811232328414917,
        0.24853335320949554,
        0.29084932804107666,
        -0.7983810901641846,
        0.2824862003326416,
        -0.15714973211288452,
        0.5896357893943787,
        0.13972052931785583,
        0.27297142148017883,
        0.20698250830173492,
        0.07197509706020355,
        0.29120105504989624,
        0.3061528205871582,
        0.20780423283576965,
        -0.053751684725284576,
        -0.006192795932292938,
        -0.31751465797424316,
        -0.6573887467384338,
        -1.0732958316802979,
        0.8521904349327087,
        -0.4693390130996704,
        0.662952721118927,
        -0.21314561367034912,
        -0.7297519445419312,
        -0.10736704617738724,
        0.8152858018875122,
        1.1160540580749512,
        1.0024638175964355,
        -0.06476020067930222,
        0.4877697825431824,
        0.08403299748897552,
        0.5008903741836548,
        0.4800032675266266,
        -0.3804170787334442,
        0.1980099081993103,
        -0.3690074384212494,
        -0.4151986241340637,
        0.9623086452484131,
        0.5566364526748657,
        0.12311771512031555,
        -0.8735742568969727,
        0.05142001807689667,
        0.23908984661102295,
        1.7524611949920654,
        0.19359678030014038,
        -0.39619186520576477,
        0.3279767632484436,
        0.287238746881485,
        0.6533194184303284,
        -0.18841254711151123,
        -1.736486554145813,
        -0.5633676648139954,
        -0.5881330966949463,
        1.0442522764205933,
        -0.3252739906311035,
        0.34224066138267517,
        0.22982856631278992,
        0.6371594071388245,
        -0.781832218170166,
        -0.33903005719184875,
        0.039689093828201294,
        0.03861024230718613,
        0.09390159696340561,
        -0.5990710854530334,
        -0.10770420730113983,
        0.019420549273490906,
        -0.020150983706116676,
        -0.43334662914276123,
        0.3124217092990875,
        0.07462138682603836,
        0.37868237495422363,
        0.04996345937252045,
        -0.572553277015686,
        -0.4767431616783142,
        0.05971340090036392,
        -0.025146260857582092,
        0.05579584091901779,
        0.9392759203910828,
        -0.5966633558273315,
        0.2685594856739044,
        0.08662296831607819,
        -0.6988357305526733,
        0.302627831697464,
        0.38678252696990967,
        -0.0048666223883628845,
        -0.5281200408935547,
        -0.17730513215065002,
        0.2451399564743042,
        1.3757604360580444,
        -0.05787661671638489,
        -0.016262460500001907,
        0.8348110318183899,
        0.5018656849861145,
        -0.3231317400932312,
        -0.8853822946548462,
        0.1243295893073082,
        -0.37141063809394836,
        0.10118936002254486,
        -0.4220452606678009,
        -0.459063857793808,
        0.7960947751998901,
        0.44388020038604736,
        -0.3713058531284332,
        0.2722190022468567,
        -1.0461243391036987,
        -0.28932392597198486,
        0.06430608034133911,
        -0.2126302421092987,
        0.6359772086143494,
        -0.3159264922142029,
        0.20097506046295166,
        0.6718429327011108,
        -0.5259691476821899,
        -1.6509661674499512,
        -0.09545259922742844,
        0.7061442732810974,
        0.46431830525398254,
        -0.306744784116745,
        -0.14497023820877075,
        0.5773342251777649,
        -0.25012531876564026,
        0.2383095920085907,
        0.019142374396324158,
        1.2502120733261108,
        0.5775740742683411,
        -0.004925346001982689,
        -0.44730421900749207,
        -0.5582735538482666,
        0.44585520029067993,
        -0.6684775352478027,
        0.048730507493019104,
        0.18094834685325623,
        -0.31855979561805725,
        0.31519028544425964,
        0.0055871158838272095,
        1.1742513179779053,
        0.15810781717300415,
        0.2534312903881073,
        0.15113328397274017,
        0.10609408468008041,
        -0.609136164188385,
        -0.7091905474662781,
        0.4739723801612854,
        0.11477030813694,
        -0.3266945481300354,
        0.4856322705745697,
        -0.013290748000144958,
        0.19053277373313904,
        0.8586021065711975,
        0.3679027855396271,
        0.1430213898420334,
        0.5588283538818359,
        -0.35768449306488037,
        1.8663487434387207,
        -0.5439885854721069,
        0.05890801548957825,
        -0.4373030960559845,
        -0.3267364203929901,
        -0.15815776586532593,
        0.4454001784324646,
        -0.10855269432067871,
        -0.7074459195137024,
        0.12364402413368225,
        -0.25669151544570923,
        0.3054186701774597,
        -0.7012869715690613,
        0.7720574140548706,
        0.2313561588525772,
        0.8839531540870667,
        0.07085000723600388,
        0.19207392632961273,
        0.3680785298347473,
        0.5577751398086548,
        -0.4728550910949707,
        0.683897852897644,
        -1.197754144668579,
        -0.7099454402923584,
        -0.5204655528068542
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes academic papers by extracting key sections such as title, authors, main goals, and more from the provided text. It employs a structured approach to highlight the paper's core aspects including technical methodology, distinctive features, and experimental outcomes. The output is a detailed summary covering various dimensions of the research.",
          "name": "Summarize_paper",
          "raw": "\n                workflow Summarize_paper v0.1 {\n                    step Main {\n                        $SYSTEM = \"\nYou are an excellent academic paper reviewer. You conduct paper summarization on the full paper text provided by the user, with following instructions:\n\nREVIEW INSTRUCTION:\n\n**Summary of Academic Paper's Technical Approach**\n\n1. **Title and authors of the Paper:**\n   Provide the title and authors of the paper.\n\n2. **Main Goal and Fundamental Concept:**\n   Begin by clearly stating the primary objective of the research presented in the academic paper. Describe the core idea or hypothesis that underpins the study in simple, accessible language.\n\n3. **Technical Approach:**\n   Provide a detailed explanation of the methodology used in the research. Focus on describing how the study was conducted, including any specific techniques, models, or algorithms employed. Avoid delving into complex jargon or highly technical details that might obscure understanding.\n\n4. **Distinctive Features:**\n   Identify and elaborate on what sets this research apart from other studies in the same field. Highlight any novel techniques, unique applications, or innovative methodologies that contribute to its distinctiveness.\n\n5. **Experimental Setup and Results:**\n   Describe the experimental design and data collection process used in the study. Summarize the results obtained or key findings, emphasizing any significant outcomes or discoveries.\n\n6. **Advantages and Limitations:**\n   Concisely discuss the strengths of the proposed approach, including any benefits it offers over existing methods. Also, address its limitations or potential drawbacks, providing a balanced view of its efficacy and applicability.\n\n7. **Conclusion:**\n   Sum up the key points made about the paper's technical approach, its uniqueness, and its comparative advantages and limitations. Aim for clarity and succinctness in your summary.\n\nOUTPUT INSTRUCTIONS:\n\n1. Only use the headers provided in the instructions above.\n2. Format your output in clear, human-readable Markdown.\n3. Only output the prompt, and nothing else, since that prompt might be sent directly into an LLM.\n\nPAPER TEXT INPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\nYou are an excellent academic paper reviewer. You conduct paper summarization on the full paper text provided by the user, with following instructions:\n\nREVIEW INSTRUCTION:\n\n**Summary of Academic Paper's Technical Approach**\n\n1. **Title and authors of the Paper:**\n   Provide the title and authors of the paper.\n\n2. **Main Goal and Fundamental Concept:**\n   Begin by clearly stating the primary objective of the research presented in the academic paper. Describe the core idea or hypothesis that underpins the study in simple, accessible language.\n\n3. **Technical Approach:**\n   Provide a detailed explanation of the methodology used in the research. Focus on describing how the study was conducted, including any specific techniques, models, or algorithms employed. Avoid delving into complex jargon or highly technical details that might obscure understanding.\n\n4. **Distinctive Features:**\n   Identify and elaborate on what sets this research apart from other studies in the same field. Highlight any novel techniques, unique applications, or innovative methodologies that contribute to its distinctiveness.\n\n5. **Experimental Setup and Results:**\n   Describe the experimental design and data collection process used in the study. Summarize the results obtained or key findings, emphasizing any significant outcomes or discoveries.\n\n6. **Advantages and Limitations:**\n   Concisely discuss the strengths of the proposed approach, including any benefits it offers over existing methods. Also, address its limitations or potential drawbacks, providing a balanced view of its efficacy and applicability.\n\n7. **Conclusion:**\n   Sum up the key points made about the paper's technical approach, its uniqueness, and its comparative advantages and limitations. Aim for clarity and succinctness in your summary.\n\nOUTPUT INSTRUCTIONS:\n\n1. Only use the headers provided in the instructions above.\n2. Format your output in clear, human-readable Markdown.\n3. Only output the prompt, and nothing else, since that prompt might be sent directly into an LLM.\n\nPAPER TEXT INPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.36407333612442017,
        0.08429273962974548,
        0.12899741530418396,
        0.6507163047790527,
        -0.14295411109924316,
        0.35122403502464294,
        -1.046669363975525,
        0.42156797647476196,
        0.3326762914657593,
        -0.12976858019828796,
        -0.20722036063671112,
        0.5803785920143127,
        0.10133755952119827,
        0.09624502807855606,
        0.293434202671051,
        -0.13260844349861145,
        -0.25549083948135376,
        -1.0329982042312622,
        -1.4889132976531982,
        -0.2772388756275177,
        0.008085057139396667,
        0.976984977722168,
        0.050277527421712875,
        0.34167611598968506,
        0.21259933710098267,
        -0.09024108946323395,
        0.2590862214565277,
        -0.13462159037590027,
        -1.478353500366211,
        -2.344489336013794,
        0.43005573749542236,
        0.3974299132823944,
        -0.03220459818840027,
        -0.36908993124961853,
        0.20477715134620667,
        -1.3474560976028442,
        -0.1578264683485031,
        -0.05036890506744385,
        -0.5529463291168213,
        -0.22724559903144836,
        0.12232496589422226,
        0.36720025539398193,
        -0.03636714443564415,
        0.18117672204971313,
        0.371503084897995,
        -0.7310463786125183,
        0.12383218109607697,
        -0.15496505796909332,
        0.7989206910133362,
        0.101353220641613,
        -0.5561978816986084,
        -0.8818063735961914,
        0.08828924596309662,
        0.3951924741268158,
        0.034071214497089386,
        -0.11818201839923859,
        0.08588641881942749,
        -0.3476871848106384,
        0.3954715132713318,
        0.014044076204299927,
        -0.3022148311138153,
        0.28791987895965576,
        -4.120425224304199,
        -0.29951614141464233,
        0.14652790129184723,
        0.6041316986083984,
        -0.14805254340171814,
        -0.0811578631401062,
        0.30395054817199707,
        0.001018822193145752,
        -0.12153114378452301,
        0.10363470762968063,
        -0.4670051634311676,
        0.29164189100265503,
        -0.43738704919815063,
        -0.24199029803276062,
        0.47328341007232666,
        -0.06794484704732895,
        0.4003363251686096,
        -0.3682417869567871,
        -0.04160625860095024,
        0.2880181670188904,
        0.18625175952911377,
        -0.49442917108535767,
        -0.7029809951782227,
        0.6719433069229126,
        -0.23504918813705444,
        0.0771593451499939,
        0.22811251878738403,
        0.1291092336177826,
        -0.4450019896030426,
        -0.004604622721672058,
        0.17268106341362,
        0.06815081089735031,
        -0.5994873046875,
        0.36998796463012695,
        0.05833860859274864,
        -0.2897489666938782,
        -0.7439719438552856,
        3.375474214553833,
        0.5562279224395752,
        -0.08848689496517181,
        0.5919068455696106,
        -0.7695836424827576,
        0.7421755790710449,
        -0.00955609604716301,
        -0.1521313637495041,
        -0.6150586605072021,
        0.03932606056332588,
        -0.09531282633543015,
        0.6083440780639648,
        -0.9034503102302551,
        0.21240592002868652,
        0.2948066294193268,
        0.7256134748458862,
        -0.12502698600292206,
        -0.5818877816200256,
        0.2847680151462555,
        -0.48606738448143005,
        1.0429574251174927,
        -0.31743600964546204,
        -0.2840421199798584,
        -0.4464336633682251,
        -0.33826935291290283,
        0.34858858585357666,
        0.21500852704048157,
        -0.09432148933410645,
        0.4491976201534271,
        0.2959998548030853,
        0.12554991245269775,
        -0.03408446162939072,
        -0.494510680437088,
        -0.2563866078853607,
        -0.29680314660072327,
        0.40819957852363586,
        -0.17303058505058289,
        0.5790425539016724,
        -0.5601972341537476,
        0.4692966938018799,
        -0.6330420970916748,
        0.0021323561668395996,
        -1.391054630279541,
        1.0922921895980835,
        -0.11684699356555939,
        0.4937979280948639,
        0.4452163279056549,
        -0.3718796372413635,
        0.24641460180282593,
        -0.3225984275341034,
        -0.8488277792930603,
        -0.21547551453113556,
        0.5666458606719971,
        0.09691288322210312,
        0.5849199295043945,
        0.8317747712135315,
        -0.2611643970012665,
        -0.1117824986577034,
        0.49017250537872314,
        -0.49803221225738525,
        0.5708811283111572,
        0.1592058539390564,
        0.037090964615345,
        0.5030636191368103,
        -0.18511410057544708,
        0.460563063621521,
        -0.7705927491188049,
        0.36864346265792847,
        -0.5298219323158264,
        0.49849987030029297,
        0.07421398162841797,
        -0.06028309464454651,
        -0.3379070460796356,
        0.7049293518066406,
        0.9005873203277588,
        0.01135821919888258,
        0.2103043496608734,
        -0.22374947369098663,
        0.29031622409820557,
        0.1519506424665451,
        -0.6280131340026855,
        0.48627641797065735,
        0.4883039891719818,
        -0.4143025279045105,
        -0.7977277636528015,
        -0.3918023109436035,
        0.36623871326446533,
        0.25145071744918823,
        0.5913770794868469,
        0.4971786439418793,
        0.6516174077987671,
        -0.898731529712677,
        1.873474359512329,
        -0.3756061792373657,
        0.08455181121826172,
        0.0021473271772265434,
        0.08373598009347916,
        -0.3175469934940338,
        0.00417269766330719,
        0.026153244078159332,
        0.2625288963317871,
        -0.8846697807312012,
        -0.4002661406993866,
        -0.6591693162918091,
        -0.09447816014289856,
        -0.30248406529426575,
        -0.5120463967323303,
        -0.04076723754405975,
        0.5792481899261475,
        0.2684195339679718,
        -0.40316876769065857,
        -0.36897480487823486,
        0.14809203147888184,
        1.147437572479248,
        0.07408685237169266,
        0.5171602368354797,
        0.2932015061378479,
        0.05654133856296539,
        -0.28994864225387573,
        0.131540447473526,
        0.5698092579841614,
        0.33394426107406616,
        0.5708411335945129,
        -0.20856913924217224,
        -0.39396291971206665,
        -0.46063292026519775,
        0.3102757930755615,
        -0.14242491126060486,
        0.21366390585899353,
        -0.5653466582298279,
        -0.2708771526813507,
        0.04950234293937683,
        0.6752526760101318,
        0.6530708074569702,
        0.9285056591033936,
        -0.7877280712127686,
        0.7417784929275513,
        0.03301868587732315,
        0.583721399307251,
        -0.1715388298034668,
        -0.7917266488075256,
        -0.09110148996114731,
        0.3525029718875885,
        0.433322012424469,
        0.4468041658401489,
        0.044188883155584335,
        -0.5754762291908264,
        -0.9954138994216919,
        -0.47372129559516907,
        -0.23938627541065216,
        1.945361614227295,
        0.13901573419570923,
        -0.387739896774292,
        0.559693455696106,
        0.5985497832298279,
        -0.04825246334075928,
        0.006658799946308136,
        -1.694806456565857,
        -0.7545189261436462,
        -0.5237360596656799,
        0.8904029726982117,
        -0.22391128540039062,
        0.2790367603302002,
        0.18717798590660095,
        0.2298840582370758,
        -0.36431482434272766,
        -0.054910626262426376,
        -0.05840715765953064,
        -0.49468308687210083,
        -0.21143797039985657,
        -0.1993820071220398,
        -0.4021855890750885,
        -0.09049269556999207,
        -0.19120840728282928,
        0.24980774521827698,
        -0.12745395302772522,
        0.08956079930067062,
        0.2638434171676636,
        0.319155216217041,
        0.008928738534450531,
        -0.6560676097869873,
        0.1258256733417511,
        -0.07594914734363556,
        -0.2079959511756897,
        0.37820255756378174,
        -0.25299006700515747,
        -0.31858357787132263,
        -0.35222557187080383,
        -0.18038716912269592,
        -0.4986058473587036,
        0.4301665723323822,
        -0.25354817509651184,
        -0.38902804255485535,
        -0.756870448589325,
        0.15132206678390503,
        1.7437676191329956,
        0.27777934074401855,
        0.4656819999217987,
        0.6509507894515991,
        0.03055313043296337,
        -0.014065762981772423,
        0.01312178373336792,
        -0.26266491413116455,
        -0.26533639430999756,
        -0.19919896125793457,
        -0.692349910736084,
        -0.25469154119491577,
        0.7636970281600952,
        0.42679542303085327,
        -0.4831550717353821,
        0.8408865928649902,
        -0.8930999040603638,
        0.14749281108379364,
        -0.19457101821899414,
        -0.0738707035779953,
        0.35661858320236206,
        -0.4302946627140045,
        0.027746818959712982,
        0.9188385009765625,
        -0.2768629491329193,
        -1.427665114402771,
        -0.4996694326400757,
        0.7236195206642151,
        -0.21958479285240173,
        -0.4469953775405884,
        -0.07272978872060776,
        0.7668460607528687,
        -0.047438427805900574,
        0.46641504764556885,
        -0.03541405498981476,
        1.2359474897384644,
        0.44037777185440063,
        0.27468523383140564,
        0.14216497540473938,
        -0.2666158974170685,
        0.9355716705322266,
        -0.18049898743629456,
        0.5574209094047546,
        -0.11688590794801712,
        0.12614572048187256,
        -0.03695674613118172,
        0.454927921295166,
        1.7923245429992676,
        -0.1362946480512619,
        0.17172026634216309,
        -0.021525532007217407,
        0.26142966747283936,
        -0.18004468083381653,
        -1.0210328102111816,
        0.4526628255844116,
        -0.1132911890745163,
        -0.25362280011177063,
        0.8778444528579712,
        0.21086624264717102,
        -0.0523911714553833,
        0.4304061830043793,
        0.5121381282806396,
        -0.06427724659442902,
        0.3176824748516083,
        -0.07295913994312286,
        1.2960091829299927,
        -0.4461328387260437,
        -0.5247248411178589,
        -0.6576516032218933,
        -0.04530509561300278,
        -0.008350629359483719,
        0.21915790438652039,
        -0.17701806128025055,
        -0.453283429145813,
        -0.2647673189640045,
        0.2625868022441864,
        -0.03762711212038994,
        -0.4941929578781128,
        0.37581098079681396,
        0.415615439414978,
        0.7696684002876282,
        0.0883060097694397,
        0.15622174739837646,
        0.10487159341573715,
        0.3673093020915985,
        -0.3092817962169647,
        0.5808114409446716,
        -0.21077710390090942,
        -0.6746469140052795,
        -0.7308328151702881
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Summarize_prompt",
          "raw": "\n                workflow Summarize_prompt v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert prompt summarizer. You take AI chat prompts in and output a concise summary of the purpose of the prompt using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, paragraph.\n\n- The first sentence should summarize the main purpose. Begin with a verb and describe the primary function of the prompt. Use the present tense and active voice. Avoid using the prompt's name in the summary. Instead, focus on the prompt's primary function or goal.\n\n- The second sentence clarifies the prompt's nuanced approach or unique features.\n\n- The third sentence should provide a brief overview of the prompt's expected output.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output no more than 40 words.\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Do not output numbered lists or bullets.\n- Do not output newlines.\n- Do not output warnings or notes.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert prompt summarizer. You take AI chat prompts in and output a concise summary of the purpose of the prompt using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, paragraph.\n\n- The first sentence should summarize the main purpose. Begin with a verb and describe the primary function of the prompt. Use the present tense and active voice. Avoid using the prompt's name in the summary. Instead, focus on the prompt's primary function or goal.\n\n- The second sentence clarifies the prompt's nuanced approach or unique features.\n\n- The third sentence should provide a brief overview of the prompt's expected output.\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output no more than 40 words.\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Do not output numbered lists or bullets.\n- Do not output newlines.\n- Do not output warnings or notes.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.14749789237976074,
        0.11010352522134781,
        0.33159154653549194,
        0.42159733176231384,
        0.0876467227935791,
        -0.13033992052078247,
        -0.912294328212738,
        0.4617011547088623,
        -0.048381540924310684,
        0.17950032651424408,
        -0.3933922052383423,
        0.6394659280776978,
        0.5328869819641113,
        0.14844118058681488,
        0.4857899844646454,
        0.13906237483024597,
        -0.5823273658752441,
        -1.193002700805664,
        -1.5497376918792725,
        -0.16842690110206604,
        -0.06635313481092453,
        1.0112653970718384,
        0.10641246289014816,
        0.527749240398407,
        0.4893532991409302,
        0.06626211106777191,
        -0.35131633281707764,
        0.11796453595161438,
        -1.2259072065353394,
        -1.5366965532302856,
        0.6403450965881348,
        0.7518735527992249,
        -0.07663705945014954,
        -0.44466909766197205,
        0.37464991211891174,
        -0.6264169216156006,
        0.15580901503562927,
        -0.5031003355979919,
        -0.1770482063293457,
        0.35461515188217163,
        -1.0077698230743408,
        -0.09150964766740799,
        -0.5582424402236938,
        0.02048124372959137,
        0.5586885213851929,
        -0.3877231478691101,
        0.25698310136795044,
        -0.07221745699644089,
        0.5879104137420654,
        0.1707981377840042,
        -0.31577545404434204,
        -0.26068124175071716,
        -0.039205700159072876,
        0.025413936004042625,
        -0.334798663854599,
        -0.48497462272644043,
        0.1878920942544937,
        -0.574107825756073,
        -0.059935811907052994,
        0.16682946681976318,
        -0.40644270181655884,
        0.1949160248041153,
        -3.2143914699554443,
        0.03147358074784279,
        -0.3755588233470917,
        0.03936845064163208,
        0.0490393228828907,
        0.0898253470659256,
        0.08771146833896637,
        -0.19099722802639008,
        -0.3454780876636505,
        0.4126225709915161,
        -0.6033522486686707,
        0.034199655055999756,
        -0.2958400845527649,
        -0.15690457820892334,
        0.553356945514679,
        0.5838582515716553,
        -0.3401058316230774,
        -0.4430173337459564,
        -0.40641114115715027,
        0.7368748188018799,
        -0.26681724190711975,
        -0.3134525418281555,
        -1.096143126487732,
        1.3142266273498535,
        -0.050589971244335175,
        -0.439900666475296,
        0.34526318311691284,
        0.10412675142288208,
        -0.1711161732673645,
        0.013785168528556824,
        0.5641758441925049,
        0.20630067586898804,
        -0.5908725261688232,
        0.617038369178772,
        -0.28255167603492737,
        -0.07140511274337769,
        0.05484011024236679,
        3.3059933185577393,
        1.2263648509979248,
        0.965980052947998,
        0.07423839718103409,
        -1.2190206050872803,
        0.5993828773498535,
        -0.6592588424682617,
        -0.28531327843666077,
        -0.2648290991783142,
        -0.07799343019723892,
        -0.17573268711566925,
        1.2066267728805542,
        -0.7786312699317932,
        -0.41095420718193054,
        0.7462896108627319,
        0.06330826878547668,
        0.2738285958766937,
        -0.8977919816970825,
        -0.273990273475647,
        -0.24135547876358032,
        0.6682602763175964,
        -0.5254569053649902,
        0.3698865473270416,
        -0.2493242770433426,
        -0.9760326743125916,
        -0.04571990668773651,
        -0.07008282840251923,
        -0.35303765535354614,
        0.5584622621536255,
        0.5821756720542908,
        0.02810395322740078,
        0.40284454822540283,
        -0.1863015592098236,
        -0.0034466832876205444,
        0.030624667182564735,
        0.2100302129983902,
        -0.12813866138458252,
        0.013265393674373627,
        -1.0557526350021362,
        -0.4057996869087219,
        -1.3608224391937256,
        0.22752909362316132,
        -0.8038296699523926,
        0.8631176352500916,
        0.2734448313713074,
        0.3503868281841278,
        0.21455898880958557,
        0.40151599049568176,
        0.05892392247915268,
        -0.1161084771156311,
        -0.5402384996414185,
        -0.1355155110359192,
        0.9428980946540833,
        -0.1015063151717186,
        0.0886809229850769,
        0.3342261016368866,
        0.16739092767238617,
        -0.7092843651771545,
        0.14427480101585388,
        -0.2761417627334595,
        0.47401317954063416,
        0.2594599723815918,
        0.22370430827140808,
        0.3312138020992279,
        -0.3458927869796753,
        0.6120420098304749,
        -0.6426455974578857,
        0.20849007368087769,
        -0.48736897110939026,
        0.3780824840068817,
        0.8710753321647644,
        -0.0013753920793533325,
        -0.3395375609397888,
        0.4078218340873718,
        0.4849543869495392,
        0.11541531980037689,
        0.21800625324249268,
        -0.4412463903427124,
        -0.0360993817448616,
        -0.0693030059337616,
        -0.3544040024280548,
        0.7853215336799622,
        0.48480677604675293,
        -0.14115041494369507,
        -1.021937370300293,
        -0.09195707738399506,
        0.26291489601135254,
        0.06852724403142929,
        0.482709139585495,
        1.0078999996185303,
        0.716834545135498,
        -0.6192348599433899,
        1.4723033905029297,
        -0.38405197858810425,
        -0.23195867240428925,
        0.7262475490570068,
        0.2977805733680725,
        -0.3123682141304016,
        0.13555778563022614,
        0.5218387246131897,
        0.05246352776885033,
        -0.8660200238227844,
        -0.5277224183082581,
        -0.8189222812652588,
        -0.5140085220336914,
        -0.5447390675544739,
        -0.6260944604873657,
        0.8746439218521118,
        0.575636088848114,
        0.3060685396194458,
        -1.1138311624526978,
        0.0795486569404602,
        -0.08163988590240479,
        0.891068696975708,
        -0.28264403343200684,
        0.06475308537483215,
        -0.5853157043457031,
        0.2316502332687378,
        -0.2317618429660797,
        0.2819842994213104,
        0.5090017914772034,
        -0.2509547770023346,
        -0.203097403049469,
        -0.46181678771972656,
        -0.5248355269432068,
        -0.8725951313972473,
        0.6848546862602234,
        0.0129118412733078,
        0.39963650703430176,
        -0.5271061658859253,
        -0.5262919664382935,
        -0.12643921375274658,
        0.8588442206382751,
        0.4186857044696808,
        1.3114970922470093,
        -0.11750567704439163,
        0.5985645651817322,
        -0.19364005327224731,
        0.3170764744281769,
        0.23419466614723206,
        -0.933060348033905,
        0.45248010754585266,
        0.26412588357925415,
        -0.35489481687545776,
        0.8758487105369568,
        -0.027984023094177246,
        0.42672744393348694,
        -0.3095742464065552,
        -0.1918359398841858,
        -0.33089861273765564,
        2.204566478729248,
        0.014439065009355545,
        -0.12108616530895233,
        0.44715285301208496,
        0.15398071706295013,
        -0.004514381289482117,
        -0.22418898344039917,
        -2.3889527320861816,
        -0.35924652218818665,
        -0.452098548412323,
        0.33770886063575745,
        -0.6918818950653076,
        0.4233463704586029,
        0.4990496039390564,
        -0.4861839711666107,
        -0.26883259415626526,
        -0.009986381977796555,
        -0.20012937486171722,
        0.02984752506017685,
        -0.5104048252105713,
        -0.25681373476982117,
        -0.6557555198669434,
        0.38634538650512695,
        0.010703608393669128,
        -0.44484931230545044,
        0.07609499245882034,
        0.5244473218917847,
        0.21324431896209717,
        0.18753685057163239,
        -0.03392302617430687,
        -0.7588176727294922,
        0.31128713488578796,
        0.31988319754600525,
        -0.08221407979726791,
        0.7295828461647034,
        -0.14538198709487915,
        -0.6369624733924866,
        -0.2369963526725769,
        -0.19700220227241516,
        -0.0746741071343422,
        0.2938782572746277,
        0.018454894423484802,
        -0.4397302269935608,
        -1.0613762140274048,
        0.023237161338329315,
        1.641798734664917,
        -0.018035009503364563,
        0.032294951379299164,
        0.5936480760574341,
        0.8920295238494873,
        0.25577491521835327,
        -0.18575036525726318,
        0.09493821114301682,
        -0.1371084451675415,
        0.3402079939842224,
        -0.7660845518112183,
        -0.24162961542606354,
        0.5845044255256653,
        -0.010199755430221558,
        -0.6475521326065063,
        0.46942824125289917,
        -0.8783687949180603,
        0.5308347344398499,
        -0.12111350893974304,
        -0.21092361211776733,
        0.89656662940979,
        -0.3675176203250885,
        0.24093663692474365,
        1.0617800951004028,
        -0.07683137059211731,
        -1.231652021408081,
        0.1774299442768097,
        0.8988998532295227,
        0.11169260740280151,
        -0.42758679389953613,
        0.3215169906616211,
        -0.0415014773607254,
        -0.6168327927589417,
        0.3808392882347107,
        0.24571962654590607,
        1.0101852416992188,
        0.2706918716430664,
        -0.46668893098831177,
        -0.5737522840499878,
        -0.13180598616600037,
        0.7585647106170654,
        -0.5229891538619995,
        0.5520176291465759,
        -0.33796799182891846,
        0.24968236684799194,
        0.6117185950279236,
        0.14345310628414154,
        1.4535422325134277,
        0.26875919103622437,
        0.459756076335907,
        0.08309245109558105,
        0.2094632387161255,
        -0.6207080483436584,
        -0.8483110070228577,
        -0.14978410303592682,
        -0.5808975100517273,
        -0.5085862874984741,
        0.889142632484436,
        -0.3807821571826935,
        0.08204170316457748,
        0.2836752235889435,
        0.48466333746910095,
        0.007121827453374863,
        0.19126199185848236,
        -0.4076276123523712,
        1.575028657913208,
        -0.11870334297418594,
        -0.022582245990633965,
        0.1633453667163849,
        -0.43959513306617737,
        -0.3055596351623535,
        0.20545491576194763,
        0.3072142004966736,
        -0.2758042812347412,
        -0.4339003562927246,
        0.24176767468452454,
        0.20778022706508636,
        -0.5848190188407898,
        0.8046097755432129,
        0.7685279250144958,
        0.2616351842880249,
        0.46738505363464355,
        -0.49063214659690857,
        -0.39624059200286865,
        0.6406099200248718,
        0.1767529994249344,
        0.14492884278297424,
        -0.14596456289291382,
        -0.5446606278419495,
        -0.6582897901535034
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes pull requests for a coding project, focusing on the types of changes made. It involves creating a summary and a detailed list of main PRs, rewritten for clarity. The output includes a concise overview and specific examples of pull requests.",
          "name": "Summarize_pull_requests",
          "raw": "\n                workflow Summarize_pull_requests v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at summarizing pull requests to a given coding project.\n\n# STEPS\n\n1. Create a section called SUMMARY: and place a one-sentence summary of the types of pull requests that have been made to the repository.\n\n2. Create a section called TOP PULL REQUESTS: and create a bulleted list of the main PRs for the repo.\n\nOUTPUT EXAMPLE:\n\nSUMMARY:\n\nMost PRs on this repo have to do with troubleshooting the app's dependencies, cleaning up documentation, and adding features to the client.\n\nTOP PULL REQUESTS:\n\n- Use Poetry to simplify the project's dependency management.\n- Add a section that explains how to use the app's secondary API.\n- A request to add AI Agent endpoints that use CrewAI.\n- Etc.\n\nEND EXAMPLE\n\n# OUTPUT INSTRUCTIONS\n\n- Rewrite the top pull request items to be a more human readable version of what was submitted, e.g., \\\"delete api key\\\" becomes \\\"Removes an API key from the repo.\\\"\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at summarizing pull requests to a given coding project.\n\n# STEPS\n\n1. Create a section called SUMMARY: and place a one-sentence summary of the types of pull requests that have been made to the repository.\n\n2. Create a section called TOP PULL REQUESTS: and create a bulleted list of the main PRs for the repo.\n\nOUTPUT EXAMPLE:\n\nSUMMARY:\n\nMost PRs on this repo have to do with troubleshooting the app's dependencies, cleaning up documentation, and adding features to the client.\n\nTOP PULL REQUESTS:\n\n- Use Poetry to simplify the project's dependency management.\n- Add a section that explains how to use the app's secondary API.\n- A request to add AI Agent endpoints that use CrewAI.\n- Etc.\n\nEND EXAMPLE\n\n# OUTPUT INSTRUCTIONS\n\n- Rewrite the top pull request items to be a more human readable version of what was submitted, e.g., \\\"delete api key\\\" becomes \\\"Removes an API key from the repo.\\\"\n- You only output human readable Markdown.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.8262224197387695,
        -0.2696419954299927,
        -0.2728399336338043,
        -0.07082730531692505,
        -0.2546679377555847,
        -0.049301646649837494,
        -0.5963405966758728,
        0.5560246109962463,
        0.1311183124780655,
        0.5149477124214172,
        -0.6322106719017029,
        1.174179196357727,
        0.038658369332551956,
        0.3462314307689667,
        0.588420569896698,
        -0.1740531027317047,
        0.14317339658737183,
        -0.49504584074020386,
        -1.6897287368774414,
        -0.06624099612236023,
        -0.43995875120162964,
        0.7224459052085876,
        0.8574250340461731,
        0.07629841566085815,
        0.04096793383359909,
        -0.20382244884967804,
        -0.20324745774269104,
        0.19611017405986786,
        -0.47334736585617065,
        -1.7283583879470825,
        0.3990800976753235,
        -0.13350695371627808,
        -0.576934814453125,
        -0.13227036595344543,
        0.5916654467582703,
        -0.44251540303230286,
        -0.49046120047569275,
        0.29944002628326416,
        -0.3939161002635956,
        -0.3296412527561188,
        0.4664795398712158,
        0.7315965890884399,
        -0.5164309144020081,
        0.308071494102478,
        0.5885533094406128,
        0.16313141584396362,
        -0.7863271236419678,
        0.38568323850631714,
        1.1634362936019897,
        0.10732948780059814,
        -0.7707971334457397,
        -0.40431374311447144,
        -0.7417507767677307,
        0.6473830938339233,
        0.29601141810417175,
        -0.5584577322006226,
        -0.32704591751098633,
        -1.027289867401123,
        0.00001755543053150177,
        0.14156527817249298,
        -0.36215561628341675,
        0.5154554843902588,
        -2.5223965644836426,
        0.10870049893856049,
        0.3969999849796295,
        0.30256178975105286,
        -0.0033419933170080185,
        -0.6020394563674927,
        -0.08577430248260498,
        0.1261339783668518,
        -0.45374441146850586,
        -0.022557081654667854,
        -0.5871766805648804,
        -0.28128090500831604,
        0.14709290862083435,
        -0.36209356784820557,
        0.19139248132705688,
        0.28792116045951843,
        0.5887904763221741,
        0.032983217388391495,
        0.20978964865207672,
        0.9634184837341309,
        0.3272010385990143,
        0.1444588303565979,
        -0.7400102019309998,
        0.2780287265777588,
        0.03801143914461136,
        -0.324493944644928,
        0.6410046219825745,
        0.6612116098403931,
        -0.2782610058784485,
        0.0020487606525421143,
        0.5230010151863098,
        0.20596948266029358,
        -0.7373892068862915,
        -0.07280071079730988,
        0.33737659454345703,
        0.1496177464723587,
        0.1759680211544037,
        3.5820260047912598,
        0.802170991897583,
        0.1146250069141388,
        0.961637020111084,
        -0.6764732003211975,
        0.8503777980804443,
        -0.21430903673171997,
        0.1340765655040741,
        -0.7856177091598511,
        0.1781521588563919,
        0.00658049713820219,
        0.3538028597831726,
        -0.6565578579902649,
        -0.13087373971939087,
        0.7792119979858398,
        1.4702229499816895,
        0.4022865891456604,
        -1.4984972476959229,
        -0.11746852099895477,
        -0.035603202879428864,
        0.8247833251953125,
        -0.5223485231399536,
        -0.4370391368865967,
        -0.23773346841335297,
        0.3362466096878052,
        0.31038179993629456,
        0.31469693779945374,
        0.18953241407871246,
        0.6627618670463562,
        0.2278347760438919,
        0.21789315342903137,
        -0.0814959928393364,
        -0.050846539437770844,
        -0.9689501523971558,
        0.43641963601112366,
        0.49785685539245605,
        -0.31248176097869873,
        0.030018635094165802,
        -0.795983076095581,
        0.16224981844425201,
        -0.5307908654212952,
        0.053374484181404114,
        -0.410102903842926,
        1.3245149850845337,
        -0.04187678545713425,
        0.6968997716903687,
        0.4001382291316986,
        -0.10231185704469681,
        -0.2016909122467041,
        -0.36286497116088867,
        -1.4073398113250732,
        -0.12338413298130035,
        0.23335620760917664,
        0.2770010530948639,
        0.2968040108680725,
        0.8021613955497742,
        -0.2571047246456146,
        -0.9445542097091675,
        0.07239872217178345,
        -0.9309630393981934,
        0.650796115398407,
        -0.3890126347541809,
        -0.3039187490940094,
        1.1033921241760254,
        0.3352581262588501,
        0.47845110297203064,
        -0.9488566517829895,
        0.2745714783668518,
        -0.42474299669265747,
        -0.23507079482078552,
        -0.010388878174126148,
        -0.013369828462600708,
        -0.5887188911437988,
        -0.02415066584944725,
        0.46292799711227417,
        0.034524209797382355,
        -0.5353485941886902,
        -0.3502488136291504,
        0.5557935237884521,
        0.45371031761169434,
        -0.12023165076971054,
        0.3267616331577301,
        1.0478042364120483,
        -0.47963976860046387,
        -0.5385295748710632,
        -0.21404148638248444,
        -0.09814265370368958,
        -0.1727515161037445,
        0.2738524377346039,
        0.709592878818512,
        -0.20957419276237488,
        -0.4864684045314789,
        0.9458355903625488,
        -0.6077604293823242,
        0.09843041002750397,
        0.09324205666780472,
        0.09980244934558868,
        0.16465473175048828,
        0.06890958547592163,
        -0.37187299132347107,
        -0.25716274976730347,
        -0.642056941986084,
        -0.14187589287757874,
        -0.5607125759124756,
        -0.1494404673576355,
        -0.2373085916042328,
        -0.36614659428596497,
        -0.12822109460830688,
        1.0555106401443481,
        -0.06611411273479462,
        -0.545895516872406,
        -0.7418349981307983,
        0.3500938415527344,
        0.960995614528656,
        -0.11549476534128189,
        0.8983715772628784,
        0.08291264623403549,
        0.24278803169727325,
        0.015900926664471626,
        0.4049292206764221,
        0.12778331339359283,
        0.36332839727401733,
        -0.06994666904211044,
        -0.6070255637168884,
        -0.7867156863212585,
        -0.7277765274047852,
        0.5093560218811035,
        -0.31318727135658264,
        -0.01630805991590023,
        -0.3666238784790039,
        -0.24174652993679047,
        0.35700723528862,
        0.01959621161222458,
        0.7091165781021118,
        0.26189830899238586,
        -0.21650362014770508,
        0.9999315738677979,
        0.21287882328033447,
        0.9616870880126953,
        -0.22252991795539856,
        -0.7878697514533997,
        0.2545703649520874,
        0.05024625360965729,
        -0.19438858330249786,
        0.47626549005508423,
        0.9400593638420105,
        0.02624620497226715,
        -0.8830334544181824,
        -0.08548104763031006,
        -0.232143834233284,
        1.4458448886871338,
        0.6982730031013489,
        0.6523674726486206,
        0.9859787225723267,
        0.5778794884681702,
        0.16655218601226807,
        0.19663138687610626,
        -1.701287031173706,
        -0.5781135559082031,
        -0.6356627345085144,
        -0.09302259981632233,
        -0.5466980338096619,
        -0.33151882886886597,
        -0.347394198179245,
        -0.45518919825553894,
        0.4145715832710266,
        -0.18167394399642944,
        -0.9717239141464233,
        0.19486069679260254,
        -0.6197680234909058,
        -0.1100645363330841,
        0.008775567635893822,
        0.32342642545700073,
        -0.07661426067352295,
        0.0825643464922905,
        0.38465043902397156,
        -0.2347523421049118,
        -0.363309770822525,
        -0.006421618163585663,
        -0.5521181225776672,
        -0.47024697065353394,
        -0.11763688176870346,
        -0.1095016747713089,
        -0.12719544768333435,
        0.2840433716773987,
        -0.4822690784931183,
        0.33345675468444824,
        0.3539029061794281,
        -0.7956849336624146,
        -0.31560003757476807,
        0.46757906675338745,
        -0.3241221308708191,
        -0.2844993770122528,
        0.11298716068267822,
        0.1169198527932167,
        1.4707180261611938,
        0.4127718210220337,
        0.7009984254837036,
        0.4334983229637146,
        0.6514373421669006,
        -0.4076363444328308,
        -0.5113512873649597,
        -0.18510687351226807,
        -0.11899182200431824,
        0.03454761952161789,
        -0.4798724055290222,
        -0.1555807739496231,
        0.4752398729324341,
        0.9220032691955566,
        -0.44807177782058716,
        0.7536782622337341,
        -0.7000775337219238,
        0.019682329148054123,
        0.09488049149513245,
        -0.17705810070037842,
        0.6518440842628479,
        0.1268225908279419,
        0.1258998066186905,
        1.148668646812439,
        -0.08708111941814423,
        -1.6550631523132324,
        -0.49236634373664856,
        0.7091596126556396,
        0.1314285397529602,
        -0.3023144006729126,
        -0.07053134590387344,
        0.5558557510375977,
        -0.5101456642150879,
        0.49590957164764404,
        -0.03744424134492874,
        1.2068090438842773,
        0.5339727401733398,
        -0.0809950903058052,
        0.07926037907600403,
        -0.024456162005662918,
        0.9165298938751221,
        -0.32593539357185364,
        0.6932874917984009,
        -0.46179714798927307,
        -0.8737671375274658,
        -0.11768144369125366,
        0.44965416193008423,
        1.2792775630950928,
        0.39172929525375366,
        -0.09984691441059113,
        -0.2182396650314331,
        -0.15127062797546387,
        -0.8237726092338562,
        -1.046804428100586,
        -0.4389376938343048,
        0.2145548313856125,
        0.11256742477416992,
        0.24289557337760925,
        -0.26207616925239563,
        0.5161041617393494,
        0.39468875527381897,
        -0.22710469365119934,
        -0.5633044838905334,
        0.42771777510643005,
        0.006679326295852661,
        0.8490158319473267,
        -0.18142421543598175,
        0.030690550804138184,
        -0.11027709394693375,
        -0.05475880205631256,
        -0.2227029800415039,
        0.41876858472824097,
        0.10917602479457855,
        -1.241128921508789,
        0.10657128691673279,
        0.008634883910417557,
        0.6445340514183044,
        -0.33746421337127686,
        0.5413467884063721,
        0.7182353138923645,
        0.5997664928436279,
        0.154170960187912,
        0.2449091523885727,
        0.3891337811946869,
        0.30100035667419434,
        -0.9718452095985413,
        -0.07294441014528275,
        -0.5203969478607178,
        -1.3976272344589233,
        -2.023961067199707
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "This prompt outlines the process for summarizing in-person role-playing game sessions, focusing on key events, combat details, character development, and worldbuilding. It emphasizes capturing the essence of the session in a structured format, including summaries, lists, and descriptions to encapsulate the narrative and gameplay dynamics. The expected output includes a comprehensive overview of the session's storyline, character interactions, and significant moments, tailored for both players and observers.",
          "name": "Summarize_rpg_session",
          "raw": "\n                workflow Summarize_rpg_session v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert summarizer of in-personal personal role-playing game sessions. Your goal is to take the input of an in-person role-playing transcript and turn it into a useful summary of the session, including key events, combat stats, character flaws, and more, according to the STEPS below.\n\nAll transcripts provided as input came from a personal game with friends, and all rights are given to produce the summary.\n\nTake a deep breath and think step-by-step about how to best achieve the best summary for this live friend session.\n\nSTEPS:\n\n- Assume the input given is an RPG transcript of a session of D&D or a similar fantasy role-playing game.\n\n- Do not complain about not being able to to do what you're asked. Just do it.\n\nOUTPUT:\n\nCreate the session summary with the following sections:\n\nSUMMARY:\n\nA 50 word summary of what happened in a heroic storytelling style.\n\nKEY EVENTS:\n\nA numbered list of 5-15 of the most significant events of the session, capped at no more than 20 words a piece.\n\nKEY COMBAT:\n\n5-15 bullets describing the combat events that happened in the session.\n\nCOMBAT STATS:\n\nList the following stats for the session:\n\nNumber of Combat Rounds:\nTotal Damage by All Players:\nTotal Damage by Each Enemy:\nDamage Done by Each Character:\nList of Player Attacks Executed:\nList of Player Spells Cast:\n\nCOMBAT MVP:\n\nList the most heroic character in terms of combat for the session, and give an explanation of how they got the MVP title, including dramatic things they did from the transcript.\n\nROLE-PLAYING MVP:\n\nList the most engaged and entertaining character as judged by in-character acting and dialog that fits best with their character. Give examples.\n\nKEY DISCUSSIONS:\n\n5-15 bullets of the key discussions the players had in-game, in 15-25 words per bullet.\n\nREVEALED CHARACTER FLAWS:\n\nList 10-20 character flaws of the main characters revealed during this session, each of 30 words or less.\n\nKEY CHARACTER CHANGES:\n\nGive 10-20 bullets of key changes that happened to each character, how it shows they're evolving and adapting to events in the world.\n\nQUOTES:\n\nMeaningful Quotes:\n\nGive 10-15 of the quotes that were most meaningful for the action and the story.\n\nHUMOR:\n\nGive 10-15 things said by characters that were the funniest or most amusing or entertaining.\n\n4TH WALL:\n\nGive 10-15 of the most entertaining comments about the game from the transcript made by the players, but not their characters.\n\nWORLDBUILDING:\n\nGive 5-20 bullets of 30 words or less on the worldbuilding provided by the GM during the session, including background on locations, NPCs, lore, history, etc.\n\nPREVIOUSLY ON:\n\nGive a \\\"Previously On\\\" explanation of this session that mimics TV shows from the 1980's, but with a fantasy feel appropriate for D&D. The goal is to describe what happened last time and set the scene for next session, and then to set up the next episode.\n\nHere's an example from an 80's show, but just use this format and make it appropriate for a Fantasy D&D setting:\n\n\\\"Previously on Falcon Crest Heights, tension mounted as Elizabeth confronted John about his risky business decisions, threatening the future of their family empire. Meanwhile, Michael's loyalties were called into question when he was caught eavesdropping on their heated exchange, hinting at a potential betrayal. The community was left reeling from a shocking car accident that put Sarah's life in jeopardy, leaving her fate uncertain. Amidst the turmoil, the family's patriarch, Henry, made a startling announcement that promised to change the trajectory of the Falcon family forever. Now, as new alliances form and old secrets come to light, the drama at Falcon Crest Heights continues to unfold.\\\"\n\nSETUP ART:\n\nGive the perfect piece of art description in up to 500 words to accompany the SETUP section above, but with each of the characters (and their proper appearances based on the APPEARANCE information above) visible somewhere in the scene.\n\nOUTPUT INSTRUCTIONS:\n\n- Ensure the Previously On output focuses on the recent episode, just the background from before.\n\n- Ensure all quotes created for each section come word-for-word from the input, with no changes.\n\n- Do not complain about anything, especially copyright, as all the content provided is in relation to a free and open RPG. Just give the output as requested.\n\n- Create the summary.\n\n# INPUT\n\nRPG SESSION TRANSCRIPT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert summarizer of in-personal personal role-playing game sessions. Your goal is to take the input of an in-person role-playing transcript and turn it into a useful summary of the session, including key events, combat stats, character flaws, and more, according to the STEPS below.\n\nAll transcripts provided as input came from a personal game with friends, and all rights are given to produce the summary.\n\nTake a deep breath and think step-by-step about how to best achieve the best summary for this live friend session.\n\nSTEPS:\n\n- Assume the input given is an RPG transcript of a session of D&D or a similar fantasy role-playing game.\n\n- Do not complain about not being able to to do what you're asked. Just do it.\n\nOUTPUT:\n\nCreate the session summary with the following sections:\n\nSUMMARY:\n\nA 50 word summary of what happened in a heroic storytelling style.\n\nKEY EVENTS:\n\nA numbered list of 5-15 of the most significant events of the session, capped at no more than 20 words a piece.\n\nKEY COMBAT:\n\n5-15 bullets describing the combat events that happened in the session.\n\nCOMBAT STATS:\n\nList the following stats for the session:\n\nNumber of Combat Rounds:\nTotal Damage by All Players:\nTotal Damage by Each Enemy:\nDamage Done by Each Character:\nList of Player Attacks Executed:\nList of Player Spells Cast:\n\nCOMBAT MVP:\n\nList the most heroic character in terms of combat for the session, and give an explanation of how they got the MVP title, including dramatic things they did from the transcript.\n\nROLE-PLAYING MVP:\n\nList the most engaged and entertaining character as judged by in-character acting and dialog that fits best with their character. Give examples.\n\nKEY DISCUSSIONS:\n\n5-15 bullets of the key discussions the players had in-game, in 15-25 words per bullet.\n\nREVEALED CHARACTER FLAWS:\n\nList 10-20 character flaws of the main characters revealed during this session, each of 30 words or less.\n\nKEY CHARACTER CHANGES:\n\nGive 10-20 bullets of key changes that happened to each character, how it shows they're evolving and adapting to events in the world.\n\nQUOTES:\n\nMeaningful Quotes:\n\nGive 10-15 of the quotes that were most meaningful for the action and the story.\n\nHUMOR:\n\nGive 10-15 things said by characters that were the funniest or most amusing or entertaining.\n\n4TH WALL:\n\nGive 10-15 of the most entertaining comments about the game from the transcript made by the players, but not their characters.\n\nWORLDBUILDING:\n\nGive 5-20 bullets of 30 words or less on the worldbuilding provided by the GM during the session, including background on locations, NPCs, lore, history, etc.\n\nPREVIOUSLY ON:\n\nGive a \\\"Previously On\\\" explanation of this session that mimics TV shows from the 1980's, but with a fantasy feel appropriate for D&D. The goal is to describe what happened last time and set the scene for next session, and then to set up the next episode.\n\nHere's an example from an 80's show, but just use this format and make it appropriate for a Fantasy D&D setting:\n\n\\\"Previously on Falcon Crest Heights, tension mounted as Elizabeth confronted John about his risky business decisions, threatening the future of their family empire. Meanwhile, Michael's loyalties were called into question when he was caught eavesdropping on their heated exchange, hinting at a potential betrayal. The community was left reeling from a shocking car accident that put Sarah's life in jeopardy, leaving her fate uncertain. Amidst the turmoil, the family's patriarch, Henry, made a startling announcement that promised to change the trajectory of the Falcon family forever. Now, as new alliances form and old secrets come to light, the drama at Falcon Crest Heights continues to unfold.\\\"\n\nSETUP ART:\n\nGive the perfect piece of art description in up to 500 words to accompany the SETUP section above, but with each of the characters (and their proper appearances based on the APPEARANCE information above) visible somewhere in the scene.\n\nOUTPUT INSTRUCTIONS:\n\n- Ensure the Previously On output focuses on the recent episode, just the background from before.\n\n- Ensure all quotes created for each section come word-for-word from the input, with no changes.\n\n- Do not complain about anything, especially copyright, as all the content provided is in relation to a free and open RPG. Just give the output as requested.\n\n- Create the summary.\n\n# INPUT\n\nRPG SESSION TRANSCRIPT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.5080463886260986,
        -0.08616180717945099,
        0.5359242558479309,
        0.5232338905334473,
        0.5750839114189148,
        0.1304636001586914,
        -1.1819005012512207,
        0.5078371167182922,
        0.26984158158302307,
        -0.25988274812698364,
        0.24233603477478027,
        0.7351784706115723,
        0.32356202602386475,
        0.007881686091423035,
        0.09718163311481476,
        -0.04418235272169113,
        -0.32809433341026306,
        -1.0845636129379272,
        -1.3850512504577637,
        -0.6198031902313232,
        -0.5068386197090149,
        0.8213324546813965,
        0.36327990889549255,
        0.11805284023284912,
        0.8707402944564819,
        0.03366792947053909,
        -0.25241684913635254,
        -0.4470546841621399,
        -0.9639117121696472,
        -1.956929326057434,
        0.4793197810649872,
        -0.1462395191192627,
        0.26382383704185486,
        -0.13793787360191345,
        0.09483081102371216,
        -1.3327049016952515,
        0.35313835740089417,
        -0.15467795729637146,
        -0.26654553413391113,
        -0.3215128183364868,
        0.005655637010931969,
        0.4796692728996277,
        -0.023942526429891586,
        -0.03870762139558792,
        0.601553738117218,
        -0.1375076323747635,
        -0.2504424452781677,
        -0.06500513106584549,
        0.42359066009521484,
        -0.06707657873630524,
        -0.5984990000724792,
        -0.5874893069267273,
        0.04525278881192207,
        0.6272385120391846,
        -0.1480301320552826,
        -0.3030780553817749,
        -0.11256328225135803,
        -0.8957544565200806,
        0.5104626417160034,
        -0.3619547486305237,
        0.04272089898586273,
        0.04834398627281189,
        -3.499333381652832,
        0.36864498257637024,
        0.2886837422847748,
        0.37898075580596924,
        0.17849643528461456,
        -0.5542861223220825,
        -0.41890427470207214,
        0.10149018466472626,
        -0.3900444507598877,
        0.12851673364639282,
        -0.23422807455062866,
        0.22347170114517212,
        -0.1316894292831421,
        -0.16478730738162994,
        0.3753182291984558,
        -0.16001971065998077,
        0.17197364568710327,
        -0.3229460120201111,
        -0.23491966724395752,
        0.22823461890220642,
        0.13694000244140625,
        -0.043110284954309464,
        -0.9193933010101318,
        0.6360043287277222,
        -0.33336400985717773,
        -0.586093544960022,
        0.2251710742712021,
        0.06005553901195526,
        -0.48081257939338684,
        -0.5670804977416992,
        -0.12936992943286896,
        0.5539025068283081,
        -0.2698028087615967,
        -0.27231931686401367,
        0.012265592813491821,
        0.03858506679534912,
        -0.4215928912162781,
        3.43747615814209,
        0.8403050303459167,
        0.2483058124780655,
        0.682081937789917,
        -1.1164495944976807,
        0.19018447399139404,
        -0.29798516631126404,
        -0.020049266517162323,
        -0.034017667174339294,
        -0.07987236976623535,
        -0.05470588803291321,
        0.6270009279251099,
        -1.126351237297058,
        -0.4378674328327179,
        0.012980397790670395,
        0.3451184034347534,
        0.9399535059928894,
        -0.5548731684684753,
        0.2935902774333954,
        0.14033882319927216,
        0.4606398642063141,
        -0.291864812374115,
        -0.04303249716758728,
        -0.08364026248455048,
        -0.027525581419467926,
        0.0346696637570858,
        0.5030057430267334,
        -0.19632789492607117,
        0.5218858122825623,
        0.38563913106918335,
        -0.03178294003009796,
        0.18963123857975006,
        -0.46451422572135925,
        -0.5251184701919556,
        0.2851775586605072,
        -0.12533648312091827,
        0.03316472843289375,
        0.6129428744316101,
        -1.2638709545135498,
        0.12997975945472717,
        -1.01919686794281,
        0.07006299495697021,
        -1.2479853630065918,
        0.14552392065525055,
        0.15954571962356567,
        0.3264000415802002,
        0.38600099086761475,
        0.47564345598220825,
        0.006491459906101227,
        -0.14672598242759705,
        -0.5872361660003662,
        0.07382147759199142,
        0.5272554159164429,
        -0.31836453080177307,
        -0.18067267537117004,
        0.7839951515197754,
        0.09309162199497223,
        -0.6744210720062256,
        0.5143577456474304,
        -0.28908464312553406,
        0.8668122291564941,
        -0.08164563775062561,
        -0.048472240567207336,
        0.6825822591781616,
        -0.5031249523162842,
        0.49744412302970886,
        -0.8867199420928955,
        0.474517285823822,
        -0.6408054232597351,
        0.08135689049959183,
        0.574061930179596,
        -0.0992155373096466,
        -0.6215131282806396,
        0.4572254419326782,
        0.5987098217010498,
        -0.28334733843803406,
        0.09828802943229675,
        0.2423900067806244,
        0.1640978753566742,
        -0.07681788504123688,
        -0.5022284984588623,
        0.7778971791267395,
        0.21074721217155457,
        0.020772293210029602,
        -0.684037446975708,
        -0.3947332501411438,
        0.5662204027175903,
        0.3383469581604004,
        0.7523848414421082,
        1.204714059829712,
        1.397639513015747,
        -0.41872885823249817,
        1.7452460527420044,
        -0.631422221660614,
        -0.3805340528488159,
        -0.45884057879447937,
        0.44123584032058716,
        -0.38139790296554565,
        -0.2328309267759323,
        0.9319525957107544,
        0.4772038757801056,
        -0.49428480863571167,
        -0.48570725321769714,
        -0.53717041015625,
        -0.37472307682037354,
        -1.1151237487792969,
        -0.824187695980072,
        0.17822225391864777,
        0.8362480998039246,
        0.5182164311408997,
        -0.8500666618347168,
        -0.1245901882648468,
        -0.009419091045856476,
        1.105832576751709,
        0.2536318004131317,
        0.1773206889629364,
        -0.3367602825164795,
        0.22911764681339264,
        -0.003993779420852661,
        0.2686232626438141,
        0.32778269052505493,
        -0.11029636114835739,
        0.11082929372787476,
        -0.3460491895675659,
        -0.6844702959060669,
        -0.34478288888931274,
        0.6694996953010559,
        -0.020349077880382538,
        0.7405293583869934,
        -0.2720324695110321,
        -0.4576907455921173,
        -0.24018719792366028,
        1.086893081665039,
        0.934558629989624,
        0.7618691921234131,
        -0.2432519495487213,
        0.5162614583969116,
        -0.2834092974662781,
        0.44873300194740295,
        -0.21321800351142883,
        -0.8644566535949707,
        0.27186354994773865,
        0.19822916388511658,
        0.18152035772800446,
        0.4489917755126953,
        0.5225341320037842,
        0.25491198897361755,
        -1.0434662103652954,
        0.4840121865272522,
        -0.18246208131313324,
        1.7220656871795654,
        0.27068766951560974,
        -0.1102788895368576,
        -0.04881950840353966,
        0.5750217437744141,
        0.1657714992761612,
        0.29308146238327026,
        -1.1127461194992065,
        -0.2966575622558594,
        -0.5752283930778503,
        0.634819507598877,
        -0.6019077897071838,
        0.2724224030971527,
        0.6490808725357056,
        0.35897096991539,
        -0.3819982707500458,
        -0.2587471902370453,
        -0.23640675842761993,
        -0.2023036628961563,
        -0.29582661390304565,
        -0.013730622828006744,
        -0.3350187838077545,
        -0.05193758010864258,
        0.3190157413482666,
        -0.3922056555747986,
        0.2026253193616867,
        0.09036325663328171,
        -0.027709605172276497,
        0.016455616801977158,
        -0.48420146107673645,
        -0.5428943634033203,
        0.5356633067131042,
        0.0241574514657259,
        -0.4583078920841217,
        0.3482685685157776,
        -0.7852171659469604,
        -0.013416901230812073,
        0.02609720453619957,
        -0.6352288722991943,
        0.42282918095588684,
        0.5274937748908997,
        -0.20805034041404724,
        -0.6413732171058655,
        -0.4596082866191864,
        0.2707292139530182,
        1.9170825481414795,
        0.09684523940086365,
        -0.04767969623208046,
        0.5997933745384216,
        0.4283458888530731,
        -0.295921266078949,
        -0.6068311333656311,
        0.14746913313865662,
        -0.21528297662734985,
        -0.20270824432373047,
        -0.4467211067676544,
        -0.16755099594593048,
        0.6811767220497131,
        0.6205662488937378,
        -0.06576918810606003,
        0.7821186184883118,
        -1.1541754007339478,
        -0.13612841069698334,
        0.09669603407382965,
        -0.3422795236110687,
        0.78951096534729,
        -0.6965083479881287,
        0.15158386528491974,
        0.7024061679840088,
        -0.2359924018383026,
        -1.2090686559677124,
        -0.5453945398330688,
        0.7666760087013245,
        0.18460983037948608,
        -0.07919750362634659,
        0.23815090954303741,
        0.9807599186897278,
        0.1314161717891693,
        -0.05333223566412926,
        -0.41254091262817383,
        1.4728894233703613,
        0.6587140560150146,
        0.1491488814353943,
        -0.5698954463005066,
        -0.36184820532798767,
        1.0086418390274048,
        -0.14304424822330475,
        0.10224755853414536,
        -0.054603561758995056,
        -0.665762186050415,
        0.12032295763492584,
        0.6121435761451721,
        1.241905927658081,
        -0.051631294190883636,
        0.27147144079208374,
        0.10962380468845367,
        -0.3797609210014343,
        -0.3730865716934204,
        -1.0865556001663208,
        -0.26921969652175903,
        -0.23483674228191376,
        -0.31309494376182556,
        0.736054003238678,
        -0.03925710916519165,
        0.697746753692627,
        0.7554630041122437,
        0.8118197917938232,
        -0.22373928129673004,
        0.368265837430954,
        -0.3856107294559479,
        1.7674986124038696,
        -0.4470330476760864,
        0.0050461627542972565,
        -0.4039004445075989,
        -0.2406083345413208,
        0.015844259411096573,
        0.5642790794372559,
        -0.2237466722726822,
        -0.3888353705406189,
        -0.44664788246154785,
        0.3286726474761963,
        0.0686100572347641,
        -0.7804989814758301,
        0.8020395040512085,
        0.17989401519298553,
        0.28112277388572693,
        -0.29941898584365845,
        0.2856149673461914,
        0.26122817397117615,
        -0.06575755774974823,
        -0.41003984212875366,
        0.06868297606706619,
        -0.5107530355453491,
        -0.3091656565666199,
        -0.9315620064735413
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Summarizes content into a structured Markdown format, focusing on brevity and clarity. It extracts and lists the most crucial points and takeaways. The output includes a one-sentence summary, main points, and key takeaways, adhering to specified word limits.",
          "name": "Summarize",
          "raw": "\n                workflow Summarize v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 10 most important points of the content as a list with no more than 15 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 5 best takeaways from the content in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following steps.\n\n# OUTPUT SECTIONS\n\n- Combine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\n- Output the 10 most important points of the content as a list with no more than 15 words per point into a section called MAIN POINTS:.\n\n- Output a list of the 5 best takeaways from the content in a section called TAKEAWAYS:.\n\n# OUTPUT INSTRUCTIONS\n\n- Create the output using the formatting above.\n- You only output human readable Markdown.\n- Output numbered lists, not bullets.\n- Do not output warnings or notes—just the requested sections.\n- Do not repeat items in the output sections.\n- Do not start items with the same opening words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.24696920812129974,
        0.05749567598104477,
        -0.8652659058570862,
        -0.08837669342756271,
        0.19456356763839722,
        0.7413508296012878,
        -0.8662253618240356,
        -0.19293034076690674,
        0.4775420129299164,
        0.12563462555408478,
        -0.1914355605840683,
        0.5437172651290894,
        -0.13118526339530945,
        -0.18548259139060974,
        0.39258167147636414,
        -0.648118257522583,
        -0.252007395029068,
        -0.32237163186073303,
        -1.5472943782806396,
        0.0434899665415287,
        0.3021000623703003,
        0.2924708425998688,
        -0.16826248168945312,
        -0.08560769259929657,
        0.09292222559452057,
        -0.01659967191517353,
        0.0981823205947876,
        -0.11359985172748566,
        -0.8527479767799377,
        -1.1200677156448364,
        -0.16636726260185242,
        0.23653477430343628,
        -0.2511129379272461,
        0.01584365963935852,
        0.7813177704811096,
        -0.18822559714317322,
        0.19301919639110565,
        -0.12001772224903107,
        -0.2869240641593933,
        -0.12276576459407806,
        0.3169514238834381,
        -0.182548388838768,
        0.2663424015045166,
        -0.31769314408302307,
        0.6383844614028931,
        0.47547808289527893,
        0.18251048028469086,
        -0.7275407314300537,
        0.7881749272346497,
        0.22361351549625397,
        -0.34231844544410706,
        -0.23305605351924896,
        0.20879879593849182,
        0.13065965473651886,
        -0.3314860463142395,
        0.22591963410377502,
        0.035775575786828995,
        -0.7587099671363831,
        0.3780584931373596,
        -0.035217978060245514,
        -0.24112534523010254,
        0.4196414351463318,
        -2.9270312786102295,
        0.2844754159450531,
        -0.5955960154533386,
        -0.2900713384151459,
        0.6296177506446838,
        -0.20597922801971436,
        0.3747199773788452,
        -0.4199809730052948,
        -0.00911460816860199,
        0.4983973503112793,
        0.425929456949234,
        0.2580740451812744,
        0.32235243916511536,
        0.08552219718694687,
        0.034218911081552505,
        -0.3587190508842468,
        0.3207154870033264,
        -0.5556167364120483,
        0.2664428949356079,
        -0.5682092308998108,
        -0.10894953459501266,
        -0.643799364566803,
        -0.5072524547576904,
        0.530608057975769,
        -0.43065696954727173,
        -0.09380889683961868,
        0.23607894778251648,
        -0.31693851947784424,
        -0.03721192851662636,
        -0.38694578409194946,
        -0.1265101432800293,
        -0.28990334272384644,
        -0.3885813057422638,
        0.1858225166797638,
        -0.09614774584770203,
        0.34916505217552185,
        0.06601560860872269,
        3.3866827487945557,
        0.16958186030387878,
        0.14628219604492188,
        0.7259595394134521,
        -1.3649826049804688,
        0.7737836241722107,
        -0.05744855850934982,
        -0.3646107316017151,
        -1.0232919454574585,
        0.4666655957698822,
        0.1209312379360199,
        0.37036076188087463,
        -0.5881770253181458,
        -1.0874077081680298,
        0.16767141222953796,
        -0.2866763174533844,
        0.18266049027442932,
        -0.07991896569728851,
        0.4997537434101105,
        0.16698051989078522,
        0.9907829761505127,
        -0.03842875361442566,
        0.3171522617340088,
        -0.28530827164649963,
        0.14181876182556152,
        0.25977596640586853,
        -0.07244734466075897,
        -0.256885290145874,
        0.6662681102752686,
        -0.24346952140331268,
        0.7463226318359375,
        -0.08417234569787979,
        0.3444763123989105,
        -0.012020178139209747,
        -1.1809207201004028,
        0.07925312221050262,
        0.0923367291688919,
        0.38777589797973633,
        -1.0200010538101196,
        0.3010580241680145,
        -0.21881458163261414,
        0.9470173120498657,
        -1.0575824975967407,
        0.7692126035690308,
        0.09762082993984222,
        0.620863676071167,
        0.31705933809280396,
        0.19843822717666626,
        0.04225493222475052,
        -0.5496372580528259,
        -0.24848586320877075,
        0.09072985500097275,
        -0.3591911196708679,
        -0.4661220610141754,
        0.12261368334293365,
        0.2489614188671112,
        -0.0272037573158741,
        0.2873002886772156,
        -0.26098719239234924,
        -0.8120412826538086,
        0.6651687026023865,
        -0.538737416267395,
        0.14986899495124817,
        0.04075757414102554,
        0.38604027032852173,
        0.4661192297935486,
        -0.023103442043066025,
        -0.0040505677461624146,
        0.32608523964881897,
        0.2008177489042282,
        0.028305906802415848,
        0.1496981531381607,
        0.08029249310493469,
        -0.23152334988117218,
        0.3916053771972656,
        0.14538788795471191,
        -0.3554968535900116,
        0.07529263198375702,
        0.17216071486473083,
        0.2581350803375244,
        -0.3868774175643921,
        0.18254485726356506,
        0.29600027203559875,
        -0.23634812235832214,
        -0.8286909461021423,
        -0.3642337918281555,
        0.7895357012748718,
        -0.45821529626846313,
        0.17817702889442444,
        1.1440606117248535,
        0.8748764991760254,
        -0.6849997639656067,
        1.4868652820587158,
        -0.21197493374347687,
        0.4578763246536255,
        0.03361678123474121,
        -0.03158746659755707,
        0.11232693493366241,
        -0.1304273009300232,
        0.6147497892379761,
        -0.029400281608104706,
        -1.1105659008026123,
        -0.19049493968486786,
        -0.2113673985004425,
        0.4462587833404541,
        -0.6042466759681702,
        0.10707679390907288,
        -0.4565509259700775,
        0.7087500691413879,
        -0.23208729922771454,
        -0.9683486223220825,
        -0.011103339493274689,
        -0.18631035089492798,
        1.456958532333374,
        0.2617346942424774,
        0.8581750392913818,
        0.8087901473045349,
        0.43023934960365295,
        -0.6893984079360962,
        0.29881972074508667,
        0.38303107023239136,
        0.10991714149713516,
        0.32634589076042175,
        0.11719141900539398,
        -0.9402012228965759,
        -0.33902159333229065,
        0.8819188475608826,
        -0.6778287887573242,
        0.9400606155395508,
        0.08426813781261444,
        -0.38823413848876953,
        0.4825676679611206,
        1.5529764890670776,
        0.935168981552124,
        1.175315260887146,
        -0.041751615703105927,
        0.12773077189922333,
        -0.6254084706306458,
        0.2722492814064026,
        -0.14863258600234985,
        -0.9214893579483032,
        0.6849310994148254,
        0.7873456478118896,
        0.03562647104263306,
        0.14035075902938843,
        0.04015830159187317,
        -0.3528304100036621,
        -0.5242692232131958,
        -0.6218650341033936,
        -0.03132554888725281,
        1.4553511142730713,
        0.5787655115127563,
        -0.0241827555000782,
        -0.3705672323703766,
        0.12448127567768097,
        0.10354830324649811,
        -0.07257356494665146,
        -1.657791018486023,
        -0.3510485589504242,
        -0.8018460869789124,
        0.8423255085945129,
        -0.29476988315582275,
        -0.2558165490627289,
        0.7835238575935364,
        0.3573709726333618,
        0.5426141619682312,
        -0.4886104166507721,
        -0.3216065466403961,
        -0.56572425365448,
        -0.4690300226211548,
        -0.31823456287384033,
        -0.19684575498104095,
        0.4494999051094055,
        -0.7016150951385498,
        -0.1602512001991272,
        -0.012714484706521034,
        -0.3271913230419159,
        0.162629634141922,
        0.6636415123939514,
        -0.2360420823097229,
        0.15207865834236145,
        0.3996213376522064,
        -0.4325283169746399,
        0.03972524031996727,
        -0.3414209485054016,
        -0.8453304171562195,
        -0.4742850959300995,
        -0.6434533596038818,
        -0.5834792256355286,
        0.299231618642807,
        0.7924166321754456,
        0.29747235774993896,
        -0.993998110294342,
        -0.02503298968076706,
        0.1796850711107254,
        2.0644800662994385,
        0.39286568760871887,
        -0.18384137749671936,
        1.0197044610977173,
        0.18746979534626007,
        -0.04715951159596443,
        -1.2796944379806519,
        0.6401967406272888,
        -0.2702953517436981,
        0.023790821433067322,
        -0.24487170577049255,
        -0.510475754737854,
        -0.1886146068572998,
        0.19415917992591858,
        -0.37209266424179077,
        0.1489778459072113,
        -0.23315250873565674,
        0.1955036073923111,
        0.20254914462566376,
        0.2724686861038208,
        0.9261928200721741,
        0.24417158961296082,
        0.9197257161140442,
        0.8951981067657471,
        -0.8753390312194824,
        -2.066345691680908,
        -0.46035483479499817,
        0.7466681599617004,
        0.20944568514823914,
        -0.01785634458065033,
        -0.2580937147140503,
        0.8116940855979919,
        -0.2949509024620056,
        -0.14340904355049133,
        -0.34404951333999634,
        1.1101782321929932,
        0.5550882816314697,
        -0.09433571994304657,
        0.2720721662044525,
        -0.28138357400894165,
        0.385079562664032,
        0.0995248332619667,
        0.8783401250839233,
        -0.1199316456913948,
        -0.8613876104354858,
        -0.6500942707061768,
        -0.0228109210729599,
        1.037297010421753,
        0.5188021063804626,
        0.06083613634109497,
        -0.06400333344936371,
        -0.2925163805484772,
        -0.44849079847335815,
        -0.568953812122345,
        -0.13871139287948608,
        -0.3427377939224243,
        -0.4216354787349701,
        0.8172913193702698,
        0.022908568382263184,
        0.46666452288627625,
        0.2892577052116394,
        0.47484835982322693,
        -0.8404936194419861,
        0.25723761320114136,
        -0.6924121975898743,
        2.027695417404175,
        -0.28776830434799194,
        -0.9776777625083923,
        -0.43568047881126404,
        0.3591097891330719,
        -0.42212969064712524,
        -0.4317663311958313,
        0.06409306824207306,
        -0.8336499929428101,
        0.30724042654037476,
        0.03163155913352966,
        -0.07293809950351715,
        -0.37073609232902527,
        0.10941378772258759,
        0.391080766916275,
        0.46024033427238464,
        -0.830284595489502,
        0.2944657802581787,
        0.30886101722717285,
        0.2493826001882553,
        0.31735920906066895,
        0.8181684017181396,
        -0.09257917106151581,
        -0.1146867573261261,
        -1.1568942070007324
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Creates Anki cards from texts following specific principles to ensure simplicity, optimized wording, and no reliance on external context. This approach aims to enhance learning efficiency and comprehension without requiring prior knowledge of the text. The expected output is a set of questions and answers formatted as a CSV table.",
          "name": "To_flashcards",
          "raw": "\n                workflow To_flashcards v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are a professional Anki card creator, able to create Anki cards from texts.\n\n\n# INSTRUCTIONS\n\nWhen creating Anki cards, stick to three principles: \n\n1. Minimum information principle. The material you learn must be formulated in as simple way as it is only possible. Simplicity does not have to imply losing information and skipping the difficult part.\n\n2. Optimize wording: The wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights \nup. This will reduce error rates, increase specificity, reduce response time, and help your concentration. \n\n3. No external context: The wording of your items must not include words such as \\\"according to the text\\\". This will make the cards \nusable even to those who haven't read the original text.\n\n\n# EXAMPLE\n\nThe following is a model card-create template for you to study.\n\nText: The characteristics of the Dead Sea: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth's surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline waters\n\nCreate cards based on the above text as follows:\n\nQ: Where is the Dead Sea located? A: on the border between Israel and Jordan \nQ: What is the lowest point on the Earth's surface? A: The Dead Sea shoreline \nQ: What is the average level on which the Dead Sea is located? A: 400 meters (below sea level) \nQ: How long is the Dead Sea? A: 70 km \nQ: How much saltier is the Dead Sea as compared with the oceans? A: 7 times \nQ: What is the volume content of salt in the Dead Sea? A: 30% \nQ: Why can the Dead Sea keep swimmers afloat? A: due to high salt content \nQ: Why is the Dead Sea called Dead? A: because only simple organisms can live in it \nQ: Why only simple organisms can live in the Dead Sea? A: because of high salt content\n\n# STEPS\n\n- Extract main points from the text\n\n- Formulate questions according to the above rules and examples\n\n- Present questions and answers in the form of a Markdown table\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output the cards you create as a CSV table. Put the question in the first column, and the answer in the second. Don't include the CSV \nheader.\n\n- Do not output warnings or notes—just the requested sections.\n\n- Do not output backticks: just raw CSV data.\n\n# INPUT:\n\nINPUT: \n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are a professional Anki card creator, able to create Anki cards from texts.\n\n\n# INSTRUCTIONS\n\nWhen creating Anki cards, stick to three principles: \n\n1. Minimum information principle. The material you learn must be formulated in as simple way as it is only possible. Simplicity does not have to imply losing information and skipping the difficult part.\n\n2. Optimize wording: The wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights \nup. This will reduce error rates, increase specificity, reduce response time, and help your concentration. \n\n3. No external context: The wording of your items must not include words such as \\\"according to the text\\\". This will make the cards \nusable even to those who haven't read the original text.\n\n\n# EXAMPLE\n\nThe following is a model card-create template for you to study.\n\nText: The characteristics of the Dead Sea: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth's surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline waters\n\nCreate cards based on the above text as follows:\n\nQ: Where is the Dead Sea located? A: on the border between Israel and Jordan \nQ: What is the lowest point on the Earth's surface? A: The Dead Sea shoreline \nQ: What is the average level on which the Dead Sea is located? A: 400 meters (below sea level) \nQ: How long is the Dead Sea? A: 70 km \nQ: How much saltier is the Dead Sea as compared with the oceans? A: 7 times \nQ: What is the volume content of salt in the Dead Sea? A: 30% \nQ: Why can the Dead Sea keep swimmers afloat? A: due to high salt content \nQ: Why is the Dead Sea called Dead? A: because only simple organisms can live in it \nQ: Why only simple organisms can live in the Dead Sea? A: because of high salt content\n\n# STEPS\n\n- Extract main points from the text\n\n- Formulate questions according to the above rules and examples\n\n- Present questions and answers in the form of a Markdown table\n\n\n# OUTPUT INSTRUCTIONS\n\n- Output the cards you create as a CSV table. Put the question in the first column, and the answer in the second. Don't include the CSV \nheader.\n\n- Do not output warnings or notes—just the requested sections.\n\n- Do not output backticks: just raw CSV data.\n\n# INPUT:\n\nINPUT: \n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.4250028431415558,
        0.054206281900405884,
        0.42812496423721313,
        -0.22861118614673615,
        0.21997304260730743,
        0.30832773447036743,
        -1.2305433750152588,
        0.48818957805633545,
        0.5172133445739746,
        -0.03560060262680054,
        -0.03430970013141632,
        0.4029798209667206,
        -0.103630430996418,
        -0.20100852847099304,
        0.16562987864017487,
        -0.17608818411827087,
        0.9429523348808289,
        -0.8631877899169922,
        -2.030773162841797,
        -0.210316002368927,
        -0.4890972971916199,
        1.1994162797927856,
        0.6592038869857788,
        -0.383353054523468,
        -0.028804220259189606,
        -0.47125521302223206,
        -0.16201180219650269,
        -0.1906055510044098,
        -0.6122909784317017,
        -1.511161208152771,
        0.4317251741886139,
        0.5254346132278442,
        -0.5812186002731323,
        0.03442426025867462,
        0.6771212816238403,
        -0.6333605647087097,
        0.16057860851287842,
        0.1586393415927887,
        -0.0424349270761013,
        0.40148860216140747,
        0.07210584729909897,
        0.23233455419540405,
        -0.7052856683731079,
        0.023989778012037277,
        0.15993282198905945,
        -0.7414331436157227,
        -0.01855127513408661,
        -0.545894205570221,
        -0.006191470194607973,
        0.37970101833343506,
        0.04801460728049278,
        -0.6255314350128174,
        0.7883149981498718,
        0.6177153587341309,
        -0.49372097849845886,
        -0.15548095107078552,
        -0.25065699219703674,
        -0.918067991733551,
        0.08367490768432617,
        -0.05340418219566345,
        -0.0008866295684129,
        0.6653048396110535,
        -2.7273004055023193,
        0.07401837408542633,
        -0.26755985617637634,
        -0.3704543709754944,
        0.48581573367118835,
        -0.6555421948432922,
        0.2540544867515564,
        -0.23160812258720398,
        0.11028874665498734,
        0.6413508653640747,
        -0.4665772318840027,
        0.09824779629707336,
        0.2882158160209656,
        0.09171292930841446,
        0.2842094302177429,
        -0.5069200992584229,
        0.31439661979675293,
        0.29120075702667236,
        0.28804507851600647,
        0.23516348004341125,
        -0.04968111217021942,
        0.30769723653793335,
        0.13488639891147614,
        0.661833643913269,
        0.13521814346313477,
        -0.33608493208885193,
        0.4661277234554291,
        -0.018788397312164307,
        -0.8616219758987427,
        -0.4854942262172699,
        -0.0015974324196577072,
        0.1875494420528412,
        -0.2147809863090515,
        -0.4927172064781189,
        0.0015890318900346756,
        0.9520694613456726,
        0.15417370200157166,
        3.0911386013031006,
        0.6627681851387024,
        0.09139041602611542,
        1.1417491436004639,
        -1.3913089036941528,
        0.049058251082897186,
        -0.6838282942771912,
        -0.43310561776161194,
        0.18884670734405518,
        0.35132598876953125,
        -0.755973219871521,
        0.7915098667144775,
        -0.7573664784431458,
        -1.004347801208496,
        0.2938074767589569,
        0.6104474663734436,
        0.43100687861442566,
        -0.3382073640823364,
        0.363598108291626,
        0.022666484117507935,
        0.3130994439125061,
        -0.28043055534362793,
        0.012167798355221748,
        -1.0991507768630981,
        -0.8464959859848022,
        0.2691787779331207,
        0.1402675062417984,
        -0.6246329545974731,
        0.102565236389637,
        0.05770024284720421,
        -0.08260645717382431,
        0.1586226224899292,
        -0.2938615679740906,
        -0.15702307224273682,
        -0.22274495661258698,
        0.11355965584516525,
        0.050312697887420654,
        -0.19026975333690643,
        -1.1022368669509888,
        -0.25024884939193726,
        -0.4770858585834503,
        0.5410696268081665,
        -1.286411166191101,
        0.46300506591796875,
        -0.05899893492460251,
        0.44643282890319824,
        0.2946564853191376,
        0.09022240340709686,
        0.12198366969823837,
        0.18224388360977173,
        -0.42935967445373535,
        0.28788861632347107,
        -0.1637125015258789,
        -0.25241151452064514,
        0.08505679666996002,
        0.8487880825996399,
        0.5944105386734009,
        -0.2920228838920593,
        0.23263302445411682,
        -0.7368663549423218,
        0.5095418095588684,
        0.14034926891326904,
        -0.6313996315002441,
        0.7970600724220276,
        -0.6282097697257996,
        -0.35814765095710754,
        0.16023415327072144,
        0.41210031509399414,
        -0.26620545983314514,
        -0.04410787671804428,
        0.14730456471443176,
        0.6509085297584534,
        -0.4267061650753021,
        0.374397873878479,
        0.10743436217308044,
        -0.18463119864463806,
        -0.10334593057632446,
        -0.4403216540813446,
        0.22048300504684448,
        0.11070176959037781,
        -0.7524893283843994,
        1.1160846948623657,
        0.5433258414268494,
        -0.3944381773471832,
        -0.2855696380138397,
        -0.23223614692687988,
        -0.1161159873008728,
        -0.3185271918773651,
        -0.5079259872436523,
        1.5552732944488525,
        1.2083007097244263,
        -0.09860657155513763,
        0.981181263923645,
        -0.14711080491542816,
        -0.45385095477104187,
        -0.06672560423612595,
        0.38224130868911743,
        -0.2135665863752365,
        0.39663076400756836,
        0.9768502116203308,
        -0.2560061812400818,
        -0.8806899189949036,
        -0.7910009622573853,
        -0.40386635065078735,
        -0.26351067423820496,
        -1.4736011028289795,
        0.0028750821948051453,
        0.7989829778671265,
        0.1831066906452179,
        0.21007990837097168,
        -0.30364805459976196,
        -0.45321375131607056,
        -0.05220898985862732,
        1.4059667587280273,
        0.3600221276283264,
        0.6628482341766357,
        0.3263954520225525,
        0.5015052556991577,
        -0.31087061762809753,
        0.13229584693908691,
        0.45237094163894653,
        -0.5864339470863342,
        0.6672782897949219,
        -0.5862764716148376,
        -0.6361520290374756,
        0.2460041493177414,
        0.8062331080436707,
        -0.4132094085216522,
        0.8218976855278015,
        -0.9598124623298645,
        -0.939697265625,
        -0.41992512345314026,
        1.3232734203338623,
        0.2457340806722641,
        1.289440631866455,
        0.3464617133140564,
        0.49248823523521423,
        -0.3374715745449066,
        0.5255004167556763,
        0.365195095539093,
        -0.851933479309082,
        0.4190289080142975,
        0.10905288904905319,
        0.15483389794826508,
        -0.18704843521118164,
        0.20006856322288513,
        0.13677382469177246,
        -0.1936984360218048,
        -0.03556082397699356,
        0.2155103087425232,
        1.4404242038726807,
        0.5474254488945007,
        0.4395776391029358,
        0.5662246346473694,
        0.5335893630981445,
        -0.08449926227331161,
        -0.38408204913139343,
        -1.1843675374984741,
        0.3107455372810364,
        -0.17527541518211365,
        0.4726700484752655,
        0.13210102915763855,
        -0.5876149535179138,
        0.6405563354492188,
        -0.5988337993621826,
        -0.07499323785305023,
        0.19301274418830872,
        -1.2266885042190552,
        -0.4841395318508148,
        -0.18689851462841034,
        -0.31976139545440674,
        -0.2208842933177948,
        0.599702775478363,
        -0.46100783348083496,
        -0.7252180576324463,
        0.5357040166854858,
        0.27622878551483154,
        -0.462361216545105,
        -0.3233751356601715,
        -0.44481122493743896,
        -0.8833667635917664,
        -0.11566895991563797,
        0.1291246861219406,
        -0.22762644290924072,
        0.043500177562236786,
        -0.16928493976593018,
        -0.3501111567020416,
        0.05128949508070946,
        -0.6099446415901184,
        0.4549739956855774,
        0.2809008061885834,
        -0.03776053339242935,
        -0.6306915283203125,
        -0.06617951393127441,
        0.6409638524055481,
        1.6925041675567627,
        0.26026651263237,
        0.07775575667619705,
        0.12088719010353088,
        0.05030716955661774,
        -0.1528804451227188,
        0.10958255082368851,
        0.9484531879425049,
        0.4502454698085785,
        -0.07580910623073578,
        -0.7363401651382446,
        -0.6428782939910889,
        -0.728639543056488,
        -0.5445781350135803,
        -0.17078647017478943,
        -0.37707459926605225,
        -0.27276578545570374,
        0.3255837857723236,
        0.30504608154296875,
        -0.01444333791732788,
        0.712364137172699,
        -0.5307210683822632,
        0.12894058227539062,
        1.060165524482727,
        0.03222141042351723,
        -1.8572124242782593,
        0.05733569711446762,
        0.6205208897590637,
        0.29078519344329834,
        0.14227554202079773,
        -0.16943256556987762,
        0.5435822010040283,
        0.2446637749671936,
        -0.09679195284843445,
        -0.46508949995040894,
        1.7537847757339478,
        0.7691649794578552,
        0.0013643510174006224,
        0.2866082489490509,
        0.4634189307689667,
        0.9436453580856323,
        0.6379064321517944,
        0.18205665051937103,
        0.012858249247074127,
        -1.2161685228347778,
        -0.25690409541130066,
        -0.024906739592552185,
        1.659714937210083,
        0.5975162386894226,
        0.14094625413417816,
        0.27428001165390015,
        -0.4322298765182495,
        -0.8685110807418823,
        -0.9709680080413818,
        -0.09038615226745605,
        0.1261787712574005,
        -0.9967433214187622,
        0.1956174522638321,
        -0.33490321040153503,
        0.3091607093811035,
        -0.39276155829429626,
        0.603962242603302,
        -0.1258252114057541,
        -0.2702270448207855,
        -0.35093241930007935,
        2.028468132019043,
        -0.06925485283136368,
        -0.788002610206604,
        -0.1651809811592102,
        0.13876314461231232,
        -0.18288615345954895,
        0.02106398344039917,
        0.7303256988525391,
        -0.8070393800735474,
        -0.2662738263607025,
        0.7918906211853027,
        0.299350768327713,
        -0.4632297158241272,
        0.006822362542152405,
        0.14084187150001526,
        0.4825103282928467,
        -0.0970272570848465,
        0.28744107484817505,
        0.6902636289596558,
        -0.2459646463394165,
        0.1794850081205368,
        0.5993039011955261,
        -0.003970064222812653,
        -0.34443631768226624,
        -0.9129825234413147
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Guides users on crafting engaging tweets with emojis, focusing on Twitter's basics and content creation strategies. It emphasizes understanding Twitter, identifying the target audience, and using emojis effectively. The expected output is a comprehensive guide for creating appealing tweets with emojis.",
          "name": "Tweet",
          "raw": "\n                workflow Tweet v0.1 {\n                    step Main {\n                        $SYSTEM = \"\nTitle: A Comprehensive Guide to Crafting Engaging Tweets with Emojis\n\nIntroduction\n\nTweets are short messages, limited to 280 characters, that can be shared on the social media platform Twitter. Tweeting is a great way to share your thoughts, engage with others, and build your online presence. If you're new to Twitter and want to start creating your own tweets with emojis, this guide will walk you through the process, from understanding the basics of Twitter to crafting engaging content with emojis.\n\nUnderstanding Twitter and its purpose\nBefore you start tweeting, it's essential to understand the platform and its purpose. Twitter is a microblogging and social networking service where users can post and interact with messages known as \\\"tweets.\\\" It's a platform that allows you to share your thoughts, opinions, and updates with a global audience.\n\nCreating a Twitter account\nTo start tweeting, you'll need to create a Twitter account. Visit the Twitter website or download the mobile app and follow the on-screen instructions to sign up. You'll need to provide some basic information, such as your name, email address, and a password.\n\nFamiliarizing yourself with Twitter's features\nOnce you've created your account, take some time to explore Twitter's features. Some key features include:\n\nHome timeline: This is where you'll see tweets from people you follow.\nNotifications: This section will show you interactions with your tweets, such as likes, retweets, and new followers.\nMentions: Here, you'll find tweets that mention your username.\nDirect messages (DMs): Use this feature to send private messages to other users.\nLikes: You can \\\"like\\\" tweets by clicking the heart icon.\nRetweets: If you want to share someone else's tweet with your followers, you can retweet it.\nHashtags: Hashtags (#) are used to categorize and search for tweets on specific topics.\nTrending topics: This section shows popular topics and hashtags that are currently being discussed on Twitter.\nIdentifying your target audience and purpose\nBefore you start tweeting, think about who you want to reach and what you want to achieve with your tweets. Are you looking to share your personal thoughts, promote your business, or engage with a specific community? Identifying your target audience and purpose will help you create more focused and effective tweets.\n\nCrafting engaging content with emojis\nNow that you understand the basics of Twitter and have identified your target audience, it's time to start creating your own tweets with emojis. Here are some tips for crafting engaging content with emojis:\n\nKeep it short and sweet: Since tweets are limited to 280 characters, make your message concise and to the point.\nUse clear and simple language: Avoid jargon and complex sentences to ensure your message is easily understood by your audience.\nUse humor and personality: Adding a touch of humor or showcasing your personality can make your tweets more engaging and relatable.\nInclude visuals: Tweets with images, videos, or GIFs tend to get more engagement.\nAsk questions: Encourage interaction by asking questions or seeking your followers' opinions.\nUse hashtags: Incorporate relevant hashtags to increase the visibility of your tweets and connect with users interested in the same topics.\nEngage with others: Respond to tweets, retweet interesting content, and participate in conversations to build relationships and grow your audience.\nUse emojis: Emojis can help convey emotions and add personality to your tweets. They can also help save space by replacing words with symbols. However, use them sparingly and appropriately, as too many emojis can make your tweets hard to read.\nMonitoring and analyzing your tweets' performance\nTo improve your tweeting skills, it's essential to monitor and analyze the performance of your tweets. Twitter provides analytics that can help you understand how your tweets are performing and what resonates with your audience. Keep an eye on your engagement metrics, such as likes, retweets, and replies, and adjust your content strategy accordingly.\n\nConclusion\n\nCreating engaging tweets with emojis takes practice and experimentation. By understanding the basics of Twitter, identifying your target audience, and crafting compelling content with emojis, you'll be well on your way to becoming a successful tweeter. Remember to stay authentic, engage with others, and adapt your strategy based on your audience's feedback and preferences.\n\n\nmake this into a tweet and have engaging Emojis!\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\nTitle: A Comprehensive Guide to Crafting Engaging Tweets with Emojis\n\nIntroduction\n\nTweets are short messages, limited to 280 characters, that can be shared on the social media platform Twitter. Tweeting is a great way to share your thoughts, engage with others, and build your online presence. If you're new to Twitter and want to start creating your own tweets with emojis, this guide will walk you through the process, from understanding the basics of Twitter to crafting engaging content with emojis.\n\nUnderstanding Twitter and its purpose\nBefore you start tweeting, it's essential to understand the platform and its purpose. Twitter is a microblogging and social networking service where users can post and interact with messages known as \\\"tweets.\\\" It's a platform that allows you to share your thoughts, opinions, and updates with a global audience.\n\nCreating a Twitter account\nTo start tweeting, you'll need to create a Twitter account. Visit the Twitter website or download the mobile app and follow the on-screen instructions to sign up. You'll need to provide some basic information, such as your name, email address, and a password.\n\nFamiliarizing yourself with Twitter's features\nOnce you've created your account, take some time to explore Twitter's features. Some key features include:\n\nHome timeline: This is where you'll see tweets from people you follow.\nNotifications: This section will show you interactions with your tweets, such as likes, retweets, and new followers.\nMentions: Here, you'll find tweets that mention your username.\nDirect messages (DMs): Use this feature to send private messages to other users.\nLikes: You can \\\"like\\\" tweets by clicking the heart icon.\nRetweets: If you want to share someone else's tweet with your followers, you can retweet it.\nHashtags: Hashtags (#) are used to categorize and search for tweets on specific topics.\nTrending topics: This section shows popular topics and hashtags that are currently being discussed on Twitter.\nIdentifying your target audience and purpose\nBefore you start tweeting, think about who you want to reach and what you want to achieve with your tweets. Are you looking to share your personal thoughts, promote your business, or engage with a specific community? Identifying your target audience and purpose will help you create more focused and effective tweets.\n\nCrafting engaging content with emojis\nNow that you understand the basics of Twitter and have identified your target audience, it's time to start creating your own tweets with emojis. Here are some tips for crafting engaging content with emojis:\n\nKeep it short and sweet: Since tweets are limited to 280 characters, make your message concise and to the point.\nUse clear and simple language: Avoid jargon and complex sentences to ensure your message is easily understood by your audience.\nUse humor and personality: Adding a touch of humor or showcasing your personality can make your tweets more engaging and relatable.\nInclude visuals: Tweets with images, videos, or GIFs tend to get more engagement.\nAsk questions: Encourage interaction by asking questions or seeking your followers' opinions.\nUse hashtags: Incorporate relevant hashtags to increase the visibility of your tweets and connect with users interested in the same topics.\nEngage with others: Respond to tweets, retweet interesting content, and participate in conversations to build relationships and grow your audience.\nUse emojis: Emojis can help convey emotions and add personality to your tweets. They can also help save space by replacing words with symbols. However, use them sparingly and appropriately, as too many emojis can make your tweets hard to read.\nMonitoring and analyzing your tweets' performance\nTo improve your tweeting skills, it's essential to monitor and analyze the performance of your tweets. Twitter provides analytics that can help you understand how your tweets are performing and what resonates with your audience. Keep an eye on your engagement metrics, such as likes, retweets, and replies, and adjust your content strategy accordingly.\n\nConclusion\n\nCreating engaging tweets with emojis takes practice and experimentation. By understanding the basics of Twitter, identifying your target audience, and crafting compelling content with emojis, you'll be well on your way to becoming a successful tweeter. Remember to stay authentic, engage with others, and adapt your strategy based on your audience's feedback and preferences.\n\n\nmake this into a tweet and have engaging Emojis!\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.0421023853123188,
        0.4567405581474304,
        -0.379496693611145,
        0.178951695561409,
        -0.08601579070091248,
        0.35977011919021606,
        -1.1073888540267944,
        -0.8470693826675415,
        0.01634369045495987,
        0.4585614800453186,
        0.2189643234014511,
        1.1193238496780396,
        0.025065332651138306,
        -0.09309887886047363,
        0.32701924443244934,
        -0.0662594586610794,
        -0.36575591564178467,
        -0.10944075882434845,
        -1.195716381072998,
        -0.22289586067199707,
        -0.309602826833725,
        0.5034818649291992,
        -0.04097796231508255,
        -0.21525155007839203,
        0.9396836757659912,
        -0.18738023936748505,
        0.1775764375925064,
        -0.18767547607421875,
        -0.4912055730819702,
        -1.5532104969024658,
        0.23114332556724548,
        0.02691548317670822,
        -0.20115907490253448,
        -0.4892711341381073,
        0.7772089242935181,
        -0.9940640330314636,
        0.050978127866983414,
        0.25789356231689453,
        -0.3369542956352234,
        -0.8961422443389893,
        -0.09476996958255768,
        0.24355986714363098,
        -0.18070068955421448,
        0.24878892302513123,
        0.1447804868221283,
        -0.3796025514602661,
        0.10017954558134079,
        -0.44379332661628723,
        0.7886143922805786,
        0.1261412352323532,
        -0.060349032282829285,
        -0.7531723976135254,
        -1.0979814529418945,
        -0.5539259910583496,
        -0.5321269035339355,
        -0.46476179361343384,
        0.05226528272032738,
        -0.3059311807155609,
        -0.5707370042800903,
        0.056245483458042145,
        0.1310277134180069,
        0.3216646909713745,
        -3.2888145446777344,
        0.3803536295890808,
        0.3786729574203491,
        0.02957606315612793,
        0.34375670552253723,
        -0.3356645405292511,
        0.18650561571121216,
        0.0012081116437911987,
        0.06561838090419769,
        0.41409093141555786,
        -0.5588266849517822,
        -0.1169343963265419,
        -0.036160800606012344,
        -0.46715259552001953,
        -0.38452088832855225,
        0.3161960542201996,
        0.3832424283027649,
        -0.2618028521537781,
        0.13915637135505676,
        -0.05185920000076294,
        0.6355537176132202,
        -0.020618218928575516,
        -0.5560129284858704,
        0.04938655346632004,
        -0.45117953419685364,
        0.30081018805503845,
        0.3849624693393707,
        -0.14018338918685913,
        -0.1615731567144394,
        -0.6161287426948547,
        0.1416558176279068,
        -0.06429383903741837,
        0.02924726903438568,
        -0.33442556858062744,
        0.007622934877872467,
        0.37717926502227783,
        0.5113418102264404,
        3.5472846031188965,
        0.17702481150627136,
        -0.524361789226532,
        0.8046413064002991,
        -1.025937795639038,
        0.5396916270256042,
        -0.388088196516037,
        -0.35034969449043274,
        -0.7183520793914795,
        -0.40987762808799744,
        -0.39179959893226624,
        -0.012357782572507858,
        -0.30709415674209595,
        -0.561484694480896,
        -0.4141007959842682,
        0.27359887957572937,
        0.8755984306335449,
        -0.06773799657821655,
        -0.3890562653541565,
        -0.04343240708112717,
        1.2996244430541992,
        -0.5086100101470947,
        -0.6979715824127197,
        -0.6293795704841614,
        0.03083714470267296,
        -0.19775564968585968,
        0.23018041253089905,
        -0.1755547821521759,
        0.6529269814491272,
        0.36719560623168945,
        0.3860744535923004,
        -0.1311362087726593,
        -0.2388228476047516,
        -0.1680595874786377,
        0.028525078669190407,
        -0.08888861536979675,
        0.3757700026035309,
        0.7881377935409546,
        -1.103232502937317,
        0.02067531645298004,
        -0.6299620270729065,
        0.7712465524673462,
        -0.5965747833251953,
        1.262398362159729,
        -0.14072400331497192,
        0.8254667520523071,
        0.4975658655166626,
        -0.05777797847986221,
        0.15045157074928284,
        -0.47534051537513733,
        -0.6272004842758179,
        -0.7133097648620605,
        0.5713956356048584,
        -0.3997058570384979,
        0.685842752456665,
        0.29470521211624146,
        -0.1265571117401123,
        -0.6870051026344299,
        0.36507418751716614,
        -0.4912237226963043,
        0.740662157535553,
        0.15058834850788116,
        0.3932039737701416,
        -0.29064634442329407,
        -0.03362801671028137,
        0.4958377480506897,
        -0.6669685244560242,
        0.015835970640182495,
        -0.14426922798156738,
        -0.6306930184364319,
        -0.011762620881199837,
        -0.12415021657943726,
        0.22848045825958252,
        0.25819018483161926,
        0.5967280864715576,
        -0.4114820957183838,
        0.254191517829895,
        -0.15403109788894653,
        0.9910092353820801,
        0.6819218397140503,
        -0.39033418893814087,
        0.20692047476768494,
        0.4274333119392395,
        -0.25537094473838806,
        -0.5496395230293274,
        -0.28610533475875854,
        0.5120491981506348,
        0.11927804350852966,
        1.0080198049545288,
        0.970514178276062,
        0.8654255867004395,
        -1.0991543531417847,
        1.4365267753601074,
        -0.3197203278541565,
        0.5169921517372131,
        -0.31899961829185486,
        0.03946051746606827,
        0.33705615997314453,
        0.14171767234802246,
        0.850295901298523,
        -0.5072150826454163,
        -0.7560075521469116,
        0.26453834772109985,
        -0.12106552720069885,
        0.017636068165302277,
        -0.03949861228466034,
        -0.7601897716522217,
        -0.7254582643508911,
        0.7494630217552185,
        -0.31504911184310913,
        -0.3021579086780548,
        0.19845595955848694,
        -0.5340595245361328,
        0.7743911743164062,
        0.5004799365997314,
        0.5037844181060791,
        0.0789162665605545,
        -0.12541727721691132,
        0.31618648767471313,
        0.03663110360503197,
        0.28828001022338867,
        -0.650219738483429,
        -0.02328447252511978,
        -0.1712033599615097,
        -0.6979285478591919,
        -1.2988847494125366,
        0.217943474650383,
        -0.5931127667427063,
        1.0915340185165405,
        -0.38828033208847046,
        -0.586854875087738,
        0.273190438747406,
        0.952285885810852,
        0.6348004937171936,
        0.7833544015884399,
        0.04041112959384918,
        0.05215536430478096,
        -0.6071964502334595,
        0.7274647951126099,
        0.22652000188827515,
        -1.0745090246200562,
        -0.42047154903411865,
        0.11337019503116608,
        -0.045281507074832916,
        0.6223918795585632,
        0.1616598218679428,
        0.031175881624221802,
        0.02317657321691513,
        -0.2027764618396759,
        -0.19414760172367096,
        1.1850636005401611,
        -0.2527441680431366,
        -0.5363626480102539,
        0.6759709119796753,
        0.07703220844268799,
        0.40438225865364075,
        0.548532247543335,
        -1.536980152130127,
        0.4525909721851349,
        -0.5844182372093201,
        0.44952392578125,
        0.14174258708953857,
        0.4145725965499878,
        0.5170587301254272,
        0.5106292366981506,
        0.3036845624446869,
        -0.13855776190757751,
        0.06227635592222214,
        -0.3575305640697479,
        -0.0877009928226471,
        -0.7384636998176575,
        -0.1604890376329422,
        -0.0766717791557312,
        -0.14542874693870544,
        0.2040574848651886,
        0.4057812988758087,
        -0.08870632201433182,
        -0.29159852862358093,
        -0.04998760670423508,
        0.22317779064178467,
        -0.7879829406738281,
        0.6093989610671997,
        -0.393279105424881,
        -0.3781806230545044,
        -0.14369136095046997,
        0.1508822739124298,
        -0.17974482476711273,
        -0.552940845489502,
        -0.9064807891845703,
        0.38404324650764465,
        0.7496192455291748,
        -0.5250405073165894,
        -0.4745219051837921,
        -0.4475020468235016,
        0.4616282284259796,
        2.2985427379608154,
        0.4178024232387543,
        1.2664101123809814,
        0.3918607532978058,
        0.3330000340938568,
        -0.7725429534912109,
        -0.06787309050559998,
        0.15341956913471222,
        0.02583739161491394,
        -0.07984630763530731,
        -1.174802303314209,
        -0.31143900752067566,
        0.3408031463623047,
        -0.24603112041950226,
        0.6920775175094604,
        0.06790497153997421,
        -0.6412662267684937,
        -0.05330287665128708,
        -0.06638047099113464,
        0.24310842156410217,
        0.37287119030952454,
        0.44818300008773804,
        0.8294104337692261,
        1.240660548210144,
        -0.2017868459224701,
        -1.5892465114593506,
        -0.035766661167144775,
        0.40701040625572205,
        0.4520222842693329,
        0.2769028842449188,
        -0.04529515653848648,
        0.6965029239654541,
        0.6395713090896606,
        0.45083722472190857,
        0.07106666266918182,
        1.5706002712249756,
        0.774648904800415,
        -0.2758578062057495,
        0.06530673801898956,
        -0.08029607683420181,
        0.17364341020584106,
        -0.3966140151023865,
        0.6495800614356995,
        0.2403114140033722,
        -0.4958140254020691,
        -0.6695899963378906,
        0.41038259863853455,
        1.0873920917510986,
        0.5304760932922363,
        0.3277028501033783,
        0.42916572093963623,
        0.8175379037857056,
        -0.6558264493942261,
        -0.952778697013855,
        0.16947990655899048,
        -0.23227497935295105,
        -0.11328832060098648,
        0.6687377095222473,
        0.7727179527282715,
        0.3762388527393341,
        -0.0851561576128006,
        0.6260348558425903,
        0.11169731616973877,
        -0.5416537523269653,
        -0.6647599339485168,
        1.7515674829483032,
        -0.7696527242660522,
        -0.45904386043548584,
        -0.33083871006965637,
        0.0477847196161747,
        -0.35052239894866943,
        -0.2397560030221939,
        -0.25694602727890015,
        -0.9505726099014282,
        0.18374751508235931,
        0.0466553196310997,
        -0.45973095297813416,
        -0.09883624315261841,
        0.12697429955005646,
        0.1020706444978714,
        0.6656638979911804,
        -0.6663124561309814,
        -0.21495145559310913,
        0.7667219638824463,
        0.825749933719635,
        -0.3795873522758484,
        0.6294978260993958,
        -0.41184186935424805,
        -1.0206787586212158,
        -0.6086516976356506
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The task is to write an essay in the style of Paul Graham, focusing on the essence and approach of writing concise, clear, and illuminating essays on any given topic.",
          "name": "Write_essay",
          "raw": "\n                workflow Write_essay v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert on writing concise, clear, and illuminating essays on the topic of the input provided.\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay in the style of Paul Graham, who is known for this concise, clear, and simple style of writing.\n\nEXAMPLE PAUL GRAHAM ESSAYS\n\nWriting about something, even something you know well, usually shows you that you didn't know it as well as you thought. Putting ideas into words is a severe test. The first words you choose are usually wrong; you have to rewrite sentences over and over to get them exactly right. And your ideas won't just be imprecise, but incomplete too. Half the ideas that end up in an essay will be ones you thought of while you were writing it. Indeed, that's why I write them.\n\nOnce you publish something, the convention is that whatever you wrote was what you thought before you wrote it. These were your ideas, and now you've expressed them. But you know this isn't true. You know that putting your ideas into words changed them. And not just the ideas you published. Presumably there were others that turned out to be too broken to fix, and those you discarded instead.\n\nIt's not just having to commit your ideas to specific words that makes writing so exacting. The real test is reading what you've written. You have to pretend to be a neutral reader who knows nothing of what's in your head, only what you wrote. When he reads what you wrote, does it seem correct? Does it seem complete? If you make an effort, you can read your writing as if you were a complete stranger, and when you do the news is usually bad. It takes me many cycles before I can get an essay past the stranger. But the stranger is rational, so you always can, if you ask him what he needs. If he's not satisfied because you failed to mention x or didn't qualify some sentence sufficiently, then you mention x or add more qualifications. Happy now? It may cost you some nice sentences, but you have to resign yourself to that. You just have to make them as good as you can and still satisfy the stranger.\n\nThis much, I assume, won't be that controversial. I think it will accord with the experience of anyone who has tried to write about anything non-trivial. There may exist people whose thoughts are so perfectly formed that they just flow straight into words. But I've never known anyone who could do this, and if I met someone who said they could, it would seem evidence of their limitations rather than their ability. Indeed, this is a trope in movies: the guy who claims to have a plan for doing some difficult thing, and who when questioned further, taps his head and says \\\"It's all up here.\\\" Everyone watching the movie knows what that means. At best the plan is vague and incomplete. Very likely there's some undiscovered flaw that invalidates it completely. At best it's a plan for a plan.\n\nIn precisely defined domains it's possible to form complete ideas in your head. People can play chess in their heads, for example. And mathematicians can do some amount of math in their heads, though they don't seem to feel sure of a proof over a certain length till they write it down. But this only seems possible with ideas you can express in a formal language. [1] Arguably what such people are doing is putting ideas into words in their heads. I can to some extent write essays in my head. I'll sometimes think of a paragraph while walking or lying in bed that survives nearly unchanged in the final version. But really I'm writing when I do this. I'm doing the mental part of writing; my fingers just aren't moving as I do it. [2]\n\nYou can know a great deal about something without writing about it. Can you ever know so much that you wouldn't learn more from trying to explain what you know? I don't think so. I've written about at least two subjects I know well — Lisp hacking and startups — and in both cases I learned a lot from writing about them. In both cases there were things I didn't consciously realize till I had to explain them. And I don't think my experience was anomalous. A great deal of knowledge is unconscious, and experts have if anything a higher proportion of unconscious knowledge than beginners.\n\nI'm not saying that writing is the best way to explore all ideas. If you have ideas about architecture, presumably the best way to explore them is to build actual buildings. What I'm saying is that however much you learn from exploring ideas in other ways, you'll still learn new things from writing about them.\n\nPutting ideas into words doesn't have to mean writing, of course. You can also do it the old way, by talking. But in my experience, writing is the stricter test. You have to commit to a single, optimal sequence of words. Less can go unsaid when you don't have tone of voice to carry meaning. And you can focus in a way that would seem excessive in conversation. I'll often spend 2 weeks on an essay and reread drafts 50 times. If you did that in conversation it would seem evidence of some kind of mental disorder. If you're lazy, of course, writing and talking are equally useless. But if you want to push yourself to get things right, writing is the steeper hill. [3]\n\nThe reason I've spent so long establishing this rather obvious point is that it leads to another that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn't written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything non-trivial.\n\nIt feels to them as if they do, especially if they're not in the habit of critically examining their own thinking. Ideas can feel complete. It's only when you try to put them into words that you discover they're not. So if you never subject your ideas to that test, you'll not only never have fully formed ideas, but also never realize it.\n\nPutting ideas into words is certainly no guarantee that they'll be right. Far from it. But though it's not a sufficient condition, it is a necessary one.\n\t\t\nWhat You Can't Say\n\nJanuary 2004\n\nHave you ever seen an old photo of yourself and been embarrassed at the way you looked? Did we actually dress like that? We did. And we had no idea how silly we looked. It's the nature of fashion to be invisible, in the same way the movement of the earth is invisible to all of us riding on it.\n\nWhat scares me is that there are moral fashions too. They're just as arbitrary, and just as invisible to most people. But they're much more dangerous. Fashion is mistaken for good design; moral fashion is mistaken for good. Dressing oddly gets you laughed at. Violating moral fashions can get you fired, ostracized, imprisoned, or even killed.\n\nIf you could travel back in a time machine, one thing would be true no matter where you went: you'd have to watch what you said. Opinions we consider harmless could have gotten you in big trouble. I've already said at least one thing that would have gotten me in big trouble in most of Europe in the seventeenth century, and did get Galileo in big trouble when he said it — that the earth moves. [1]\n\nIt seems to be a constant throughout history: In every period, people believed things that were just ridiculous, and believed them so strongly that you would have gotten in terrible trouble for saying otherwise.\n\nIs our time any different? To anyone who has read any amount of history, the answer is almost certainly no. It would be a remarkable coincidence if ours were the first era to get everything just right.\n\nIt's tantalizing to think we believe things that people in the future will find ridiculous. What would someone coming back to visit us in a time machine have to be careful not to say? That's what I want to study here. But I want to do more than just shock everyone with the heresy du jour. I want to find general recipes for discovering what you can't say, in any era.\n\nThe Conformist Test\n\nLet's start with a test: Do you have any opinions that you would be reluctant to express in front of a group of your peers?\n\nIf the answer is no, you might want to stop and think about that. If everything you believe is something you're supposed to believe, could that possibly be a coincidence? Odds are it isn't. Odds are you just think what you're told.\n\nThe other alternative would be that you independently considered every question and came up with the exact same answers that are now considered acceptable. That seems unlikely, because you'd also have to make the same mistakes. Mapmakers deliberately put slight mistakes in their maps so they can tell when someone copies them. If another map has the same mistake, that's very convincing evidence.\n\nLike every other era in history, our moral map almost certainly contains a few mistakes. And anyone who makes the same mistakes probably didn't do it by accident. It would be like someone claiming they had independently decided in 1972 that bell-bottom jeans were a good idea.\n\nIf you believe everything you're supposed to now, how can you be sure you wouldn't also have believed everything you were supposed to if you had grown up among the plantation owners of the pre-Civil War South, or in Germany in the 1930s — or among the Mongols in 1200, for that matter? Odds are you would have.\n\nBack in the era of terms like \\\"well-adjusted,\\\" the idea seemed to be that there was something wrong with you if you thought things you didn't dare say out loud. This seems backward. Almost certainly, there is something wrong with you if you don't think things you don't dare say out loud.\n\nTrouble\n\nWhat can't we say? One way to find these ideas is simply to look at things people do say, and get in trouble for. [2]\n\nOf course, we're not just looking for things we can't say. We're looking for things we can't say that are true, or at least have enough chance of being true that the question should remain open. But many of the things people get in trouble for saying probably do make it over this second, lower threshold. No one gets in trouble for saying that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall. Such obviously false statements might be treated as jokes, or at worst as evidence of insanity, but they are not likely to make anyone mad. The statements that make people mad are the ones they worry might be believed. I suspect the statements that make people maddest are those they worry might be true.\n\nIf Galileo had said that people in Padua were ten feet tall, he would have been regarded as a harmless eccentric. Saying the earth orbited the sun was another matter. The church knew this would set people thinking.\n\nCertainly, as we look back on the past, this rule of thumb works well. A lot of the statements people got in trouble for seem harmless now. So it's likely that visitors from the future would agree with at least some of the statements that get people in trouble today. Do we have no Galileos? Not likely.\n\nTo find them, keep track of opinions that get people in trouble, and start asking, could this be true? Ok, it may be heretical (or whatever modern equivalent), but might it also be true?\n\nHeresy\n\nThis won't get us all the answers, though. What if no one happens to have gotten in trouble for a particular idea yet? What if some idea would be so radioactively controversial that no one would dare express it in public? How can we find these too?\n\nAnother approach is to follow that word, heresy. In every period of history, there seem to have been labels that got applied to statements to shoot them down before anyone had a chance to ask if they were true or not. \\\"Blasphemy\\\", \\\"sacrilege\\\", and \\\"heresy\\\" were such labels for a good part of western history, as in more recent times \\\"indecent\\\", \\\"improper\\\", and \\\"unamerican\\\" have been. By now these labels have lost their sting. They always do. By now they're mostly used ironically. But in their time, they had real force.\n\nThe word \\\"defeatist\\\", for example, has no particular political connotations now. But in Germany in 1917 it was a weapon, used by Ludendorff in a purge of those who favored a negotiated peace. At the start of World War II it was used extensively by Churchill and his supporters to silence their opponents. In 1940, any argument against Churchill's aggressive policy was \\\"defeatist\\\". Was it right or wrong? Ideally, no one got far enough to ask that.\n\nWe have such labels today, of course, quite a lot of them, from the all-purpose \\\"inappropriate\\\" to the dreaded \\\"divisive.\\\" In any period, it should be easy to figure out what such labels are, simply by looking at what people call ideas they disagree with besides untrue. When a politician says his opponent is mistaken, that's a straightforward criticism, but when he attacks a statement as \\\"divisive\\\" or \\\"racially insensitive\\\" instead of arguing that it's false, we should start paying attention.\n\nSo another way to figure out which of our taboos future generations will laugh at is to start with the labels. Take a label — \\\"sexist\\\", for example — and try to think of some ideas that would be called that. Then for each ask, might this be true?\n\nJust start listing ideas at random? Yes, because they won't really be random. The ideas that come to mind first will be the most plausible ones. They'll be things you've already noticed but didn't let yourself think.\n\nIn 1989 some clever researchers tracked the eye movements of radiologists as they scanned chest images for signs of lung cancer. [3] They found that even when the radiologists missed a cancerous lesion, their eyes had usually paused at the site of it. Part of their brain knew there was something there; it just didn't percolate all the way up into conscious knowledge. I think many interesting heretical thoughts are already mostly formed in our minds. If we turn off our self-censorship temporarily, those will be the first to emerge.\n\nTime and Space\n\nIf we could look into the future it would be obvious which of our taboos they'd laugh at. We can't do that, but we can do something almost as good: we can look into the past. Another way to figure out what we're getting wrong is to look at what used to be acceptable and is now unthinkable.\n\nChanges between the past and the present sometimes do represent progress. In a field like physics, if we disagree with past generations it's because we're right and they're wrong. But this becomes rapidly less true as you move away from the certainty of the hard sciences. By the time you get to social questions, many changes are just fashion. The age of consent fluctuates like hemlines.\n\nWe may imagine that we are a great deal smarter and more virtuous than past generations, but the more history you read, the less likely this seems. People in past times were much like us. Not heroes, not barbarians. Whatever their ideas were, they were ideas reasonable people could believe.\n\nSo here is another source of interesting heresies. Diff present ideas against those of various past cultures, and see what you get. [4] Some will be shocking by present standards. Ok, fine; but which might also be true?\n\nYou don't have to look into the past to find big differences. In our own time, different societies have wildly varying ideas of what's ok and what isn't. So you can try diffing other cultures' ideas against ours as well. (The best way to do that is to visit them.) Any idea that's considered harmless in a significant percentage of times and places, and yet is taboo in ours, is a candidate for something we're mistaken about.\n\nFor example, at the high water mark of political correctness in the early 1990s, Harvard distributed to its faculty and staff a brochure saying, among other things, that it was inappropriate to compliment a colleague or student's clothes. No more \\\"nice shirt.\\\" I think this principle is rare among the world's cultures, past or present. There are probably more where it's considered especially polite to compliment someone's clothing than where it's considered improper. Odds are this is, in a mild form, an example of one of the taboos a visitor from the future would have to be careful to avoid if he happened to set his time machine for Cambridge, Massachusetts, 1992. [5]\n\nPrigs\n\nOf course, if they have time machines in the future they'll probably have a separate reference manual just for Cambridge. This has always been a fussy place, a town of i dotters and t crossers, where you're liable to get both your grammar and your ideas corrected in the same conversation. And that suggests another way to find taboos. Look for prigs, and see what's inside their heads.\n\nKids' heads are repositories of all our taboos. It seems fitting to us that kids' ideas should be bright and clean. The picture we give them of the world is not merely simplified, to suit their developing minds, but sanitized as well, to suit our ideas of what kids ought to think. [6]\n\nYou can see this on a small scale in the matter of dirty words. A lot of my friends are starting to have children now, and they're all trying not to use words like \\\"fuck\\\" and \\\"shit\\\" within baby's hearing, lest baby start using these words too. But these words are part of the language, and adults use them all the time. So parents are giving their kids an inaccurate idea of the language by not using them. Why do they do this? Because they don't think it's fitting that kids should use the whole language. We like children to seem innocent. [7]\n\nMost adults, likewise, deliberately give kids a misleading view of the world. One of the most obvious examples is Santa Claus. We think it's cute for little kids to believe in Santa Claus. I myself think it's cute for little kids to believe in Santa Claus. But one wonders, do we tell them this stuff for their sake, or for ours?\n\nI'm not arguing for or against this idea here. It is probably inevitable that parents should want to dress up their kids' minds in cute little baby outfits. I'll probably do it myself. The important thing for our purposes is that, as a result, a well brought-up teenage kid's brain is a more or less complete collection of all our taboos — and in mint condition, because they're untainted by experience. Whatever we think that will later turn out to be ridiculous, it's almost certainly inside that head.\n\nHow do we get at these ideas? By the following thought experiment. Imagine a kind of latter-day Conrad character who has worked for a time as a mercenary in Africa, for a time as a doctor in Nepal, for a time as the manager of a nightclub in Miami. The specifics don't matter — just someone who has seen a lot. Now imagine comparing what's inside this guy's head with what's inside the head of a well-behaved sixteen year old girl from the suburbs. What does he think that would shock her? He knows the world; she knows, or at least embodies, present taboos. Subtract one from the other, and the result is what we can't say.\n\nMechanism\n\nI can think of one more way to figure out what we can't say: to look at how taboos are created. How do moral fashions arise, and why are they adopted? If we can understand this mechanism, we may be able to see it at work in our own time.\n\nMoral fashions don't seem to be created the way ordinary fashions are. Ordinary fashions seem to arise by accident when everyone imitates the whim of some influential person. The fashion for broad-toed shoes in late fifteenth century Europe began because Charles VIII of France had six toes on one foot. The fashion for the name Gary began when the actor Frank Cooper adopted the name of a tough mill town in Indiana. Moral fashions more often seem to be created deliberately. When there's something we can't say, it's often because some group doesn't want us to.\n\nThe prohibition will be strongest when the group is nervous. The irony of Galileo's situation was that he got in trouble for repeating Copernicus's ideas. Copernicus himself didn't. In fact, Copernicus was a canon of a cathedral, and dedicated his book to the pope. But by Galileo's time the church was in the throes of the Counter-Reformation and was much more worried about unorthodox ideas.\n\nTo launch a taboo, a group has to be poised halfway between weakness and power. A confident group doesn't need taboos to protect it. It's not considered improper to make disparaging remarks about Americans, or the English. And yet a group has to be powerful enough to enforce a taboo. Coprophiles, as of this writing, don't seem to be numerous or energetic enough to have had their interests promoted to a lifestyle.\n\nI suspect the biggest source of moral taboos will turn out to be power struggles in which one side only barely has the upper hand. That's where you'll find a group powerful enough to enforce taboos, but weak enough to need them.\n\nMost struggles, whatever they're really about, will be cast as struggles between competing ideas. The English Reformation was at bottom a struggle for wealth and power, but it ended up being cast as a struggle to preserve the souls of Englishmen from the corrupting influence of Rome. It's easier to get people to fight for an idea. And whichever side wins, their ideas will also be considered to have triumphed, as if God wanted to signal his agreement by selecting that side as the victor.\n\nWe often like to think of World War II as a triumph of freedom over totalitarianism. We conveniently forget that the Soviet Union was also one of the winners.\n\nI'm not saying that struggles are never about ideas, just that they will always be made to seem to be about ideas, whether they are or not. And just as there is nothing so unfashionable as the last, discarded fashion, there is nothing so wrong as the principles of the most recently defeated opponent. Representational art is only now recovering from the approval of both Hitler and Stalin. [8]\n\nAlthough moral fashions tend to arise from different sources than fashions in clothing, the mechanism of their adoption seems much the same. The early adopters will be driven by ambition: self-consciously cool people who want to distinguish themselves from the common herd. As the fashion becomes established they'll be joined by a second, much larger group, driven by fear. [9] This second group adopt the fashion not because they want to stand out but because they are afraid of standing out.\n\nSo if you want to figure out what we can't say, look at the machinery of fashion and try to predict what it would make unsayable. What groups are powerful but nervous, and what ideas would they like to suppress? What ideas were tarnished by association when they ended up on the losing side of a recent struggle? If a self-consciously cool person wanted to differentiate himself from preceding fashions (e.g. from his parents), which of their ideas would he tend to reject? What are conventional-minded people afraid of saying?\n\nThis technique won't find us all the things we can't say. I can think of some that aren't the result of any recent struggle. Many of our taboos are rooted deep in the past. But this approach, combined with the preceding four, will turn up a good number of unthinkable ideas.\n\nWhy\n\nSome would ask, why would one want to do this? Why deliberately go poking around among nasty, disreputable ideas? Why look under rocks?\n\nI do it, first of all, for the same reason I did look under rocks as a kid: plain curiosity. And I'm especially curious about anything that's forbidden. Let me see and decide for myself.\n\nSecond, I do it because I don't like the idea of being mistaken. If, like other eras, we believe things that will later seem ridiculous, I want to know what they are so that I, at least, can avoid believing them.\n\nThird, I do it because it's good for the brain. To do good work you need a brain that can go anywhere. And you especially need a brain that's in the habit of going where it's not supposed to.\n\nGreat work tends to grow out of ideas that others have overlooked, and no idea is so overlooked as one that's unthinkable. Natural selection, for example. It's so simple. Why didn't anyone think of it before? Well, that is all too obvious. Darwin himself was careful to tiptoe around the implications of his theory. He wanted to spend his time thinking about biology, not arguing with people who accused him of being an atheist.\n\nIn the sciences, especially, it's a great advantage to be able to question assumptions. The m.o. of scientists, or at least of the good ones, is precisely that: look for places where conventional wisdom is broken, and then try to pry apart the cracks and see what's underneath. That's where new theories come from.\n\nA good scientist, in other words, does not merely ignore conventional wisdom, but makes a special effort to break it. Scientists go looking for trouble. This should be the m.o. of any scholar, but scientists seem much more willing to look under rocks. [10]\n\nWhy? It could be that the scientists are simply smarter; most physicists could, if necessary, make it through a PhD program in French literature, but few professors of French literature could make it through a PhD program in physics. Or it could be because it's clearer in the sciences whether theories are true or false, and this makes scientists bolder. (Or it could be that, because it's clearer in the sciences whether theories are true or false, you have to be smart to get jobs as a scientist, rather than just a good politician.)\n\nWhatever the reason, there seems a clear correlation between intelligence and willingness to consider shocking ideas. This isn't just because smart people actively work to find holes in conventional thinking. I think conventions also have less hold over them to start with. You can see that in the way they dress.\n\nIt's not only in the sciences that heresy pays off. In any competitive field, you can win big by seeing things that others daren't. And in every field there are probably heresies few dare utter. Within the US car industry there is a lot of hand-wringing now about declining market share. Yet the cause is so obvious that any observant outsider could explain it in a second: they make bad cars. And they have for so long that by now the US car brands are antibrands — something you'd buy a car despite, not because of. Cadillac stopped being the Cadillac of cars in about 1970. And yet I suspect no one dares say this. [11] Otherwise these companies would have tried to fix the problem.\n\nTraining yourself to think unthinkable thoughts has advantages beyond the thoughts themselves. It's like stretching. When you stretch before running, you put your body into positions much more extreme than any it will assume during the run. If you can think things so outside the box that they'd make people's hair stand on end, you'll have no trouble with the small trips outside the box that people call innovative.\n\nPensieri Stretti\n\nWhen you find something you can't say, what do you do with it? My advice is, don't say it. Or at least, pick your battles.\n\nSuppose in the future there is a movement to ban the color yellow. Proposals to paint anything yellow are denounced as \\\"yellowist\\\", as is anyone suspected of liking the color. People who like orange are tolerated but viewed with suspicion. Suppose you realize there is nothing wrong with yellow. If you go around saying this, you'll be denounced as a yellowist too, and you'll find yourself having a lot of arguments with anti-yellowists. If your aim in life is to rehabilitate the color yellow, that may be what you want. But if you're mostly interested in other questions, being labelled as a yellowist will just be a distraction. Argue with idiots, and you become an idiot.\n\nThe most important thing is to be able to think what you want, not to say what you want. And if you feel you have to say everything you think, it may inhibit you from thinking improper thoughts. I think it's better to follow the opposite policy. Draw a sharp line between your thoughts and your speech. Inside your head, anything is allowed. Within my head I make a point of encouraging the most outrageous thoughts I can imagine. But, as in a secret society, nothing that happens within the building should be told to outsiders. The first rule of Fight Club is, you do not talk about Fight Club.\n\nWhen Milton was going to visit Italy in the 1630s, Sir Henry Wootton, who had been ambassador to Venice, told him his motto should be \\\"i pensieri stretti & il viso sciolto.\\\" Closed thoughts and an open face. Smile at everyone, and don't tell them what you're thinking. This was wise advice. Milton was an argumentative fellow, and the Inquisition was a bit restive at that time. But I think the difference between Milton's situation and ours is only a matter of degree. Every era has its heresies, and if you don't get imprisoned for them you will at least get in enough trouble that it becomes a complete distraction.\n\nI admit it seems cowardly to keep quiet. When I read about the harassment to which the Scientologists subject their critics [12], or that pro-Israel groups are \\\"compiling dossiers\\\" on those who speak out against Israeli human rights abuses [13], or about people being sued for violating the DMCA [14], part of me wants to say, \\\"All right, you bastards, bring it on.\\\" The problem is, there are so many things you can't say. If you said them all you'd have no time left for your real work. You'd have to turn into Noam Chomsky. [15]\n\nThe trouble with keeping your thoughts secret, though, is that you lose the advantages of discussion. Talking about an idea leads to more ideas. So the optimal plan, if you can manage it, is to have a few trusted friends you can speak openly to. This is not just a way to develop ideas; it's also a good rule of thumb for choosing friends. The people you can say heretical things to without getting jumped on are also the most interesting to know.\n\nViso Sciolto?\n\nI don't think we need the viso sciolto so much as the pensieri stretti. Perhaps the best policy is to make it plain that you don't agree with whatever zealotry is current in your time, but not to be too specific about what you disagree with. Zealots will try to draw you out, but you don't have to answer them. If they try to force you to treat a question on their terms by asking \\\"are you with us or against us?\\\" you can always just answer \\\"neither\\\".\n\nBetter still, answer \\\"I haven't decided.\\\" That's what Larry Summers did when a group tried to put him in this position. Explaining himself later, he said \\\"I don't do litmus tests.\\\" [16] A lot of the questions people get hot about are actually quite complicated. There is no prize for getting the answer quickly.\n\nIf the anti-yellowists seem to be getting out of hand and you want to fight back, there are ways to do it without getting yourself accused of being a yellowist. Like skirmishers in an ancient army, you want to avoid directly engaging the main body of the enemy's troops. Better to harass them with arrows from a distance.\n\nOne way to do this is to ratchet the debate up one level of abstraction. If you argue against censorship in general, you can avoid being accused of whatever heresy is contained in the book or film that someone is trying to censor. You can attack labels with meta-labels: labels that refer to the use of labels to prevent discussion. The spread of the term \\\"political correctness\\\" meant the beginning of the end of political correctness, because it enabled one to attack the phenomenon as a whole without being accused of any of the specific heresies it sought to suppress.\n\nAnother way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \\\"The Crucible,\\\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \\\"witch-hunt.\\\"\n\nBest of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \\\"I am glad that I managed to write 'The Crucible,'\\\" Arthur Miller wrote, \\\"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\\\" [17]\n\nABQ\n\nA Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland.\n\nAnd yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\n\nCertainly the fact that they value open-mindedness is no guarantee. Who thinks they're not open-minded? Our hypothetical prim miss from the suburbs thinks she's open-minded. Hasn't she been taught to be? Ask anyone, and they'll say the same thing: they're pretty open-minded, though they draw the line at things that are really wrong. (Some tribes may avoid \\\"wrong\\\" as judgemental, and may instead use a more neutral sounding euphemism like \\\"negative\\\" or \\\"destructive\\\".)\n\nWhen people are bad at math, they know it, because they get the wrong answers on tests. But when people are bad at open-mindedness they don't know it. In fact they tend to think the opposite. Remember, it's the nature of fashion to be invisible. It wouldn't work otherwise. Fashion doesn't seem like fashion to someone in the grip of it. It just seems like the right thing to do. It's only by looking from a distance that we see oscillations in people's idea of the right thing to do, and can identify them as fashions.\n\nTime gives us such distance for free. Indeed, the arrival of new fashions makes old fashions easy to see, because they seem so ridiculous by contrast. From one end of a pendulum's swing, the other end seems especially far away.\n\nTo see fashion in your own time, though, requires a conscious effort. Without time to give you distance, you have to create distance yourself. Instead of being part of the mob, stand as far away from it as you can and watch what it's doing. And pay especially close attention whenever an idea is being suppressed. Web filters for children and employees often ban sites containing pornography, violence, and hate speech. What counts as pornography and violence? And what, exactly, is \\\"hate speech?\\\" This sounds like a phrase out of 1984.\n\nLabels like that are probably the biggest external clue. If a statement is false, that's the worst thing you can say about it. You don't need to say that it's heretical. And if it isn't false, it shouldn't be suppressed. So when you see statements being attacked as x-ist or y-ic (substitute your current values of x and y), whether in 1630 or 2030, that's a sure sign that something is wrong. When you hear such labels being used, ask why.\n\nEspecially if you hear yourself using them. It's not just the mob you need to learn to watch from a distance. You need to be able to watch your own thoughts from a distance. That's not a radical idea, by the way; it's the main difference between children and adults. When a child gets angry because he's tired, he doesn't know what's happening. An adult can distance himself enough from the situation to say \\\"never mind, I'm just tired.\\\" I don't see why one couldn't, by a similar process, learn to recognize and discount the effects of moral fashions.\n\nYou have to take that extra step if you want to think clearly. But it's harder, because now you're working against social customs instead of with them. Everyone encourages you to grow up to the point where you can discount your own bad moods. Few encourage you to continue to the point where you can discount society's bad moods.\n\nHow can you see the wave, when you're the water? Always be questioning. That's the only defence. What can't you say? And why?\n\nHow to Start Google\n\nMarch 2024\n\n(This is a talk I gave to 14 and 15 year olds about what to do now if they might want to start a startup later. Lots of schools think they should tell students something about startups. This is what I think they should tell them.)\n\nMost of you probably think that when you're released into the so-called real world you'll eventually have to get some kind of job. That's not true, and today I'm going to talk about a trick you can use to avoid ever having to get a job.\n\nThe trick is to start your own company. So it's not a trick for avoiding work, because if you start your own company you'll work harder than you would if you had an ordinary job. But you will avoid many of the annoying things that come with a job, including a boss telling you what to do.\n\nIt's more exciting to work on your own project than someone else's. And you can also get a lot richer. In fact, this is the standard way to get really rich. If you look at the lists of the richest people that occasionally get published in the press, nearly all of them did it by starting their own companies.\n\nStarting your own company can mean anything from starting a barber shop to starting Google. I'm here to talk about one extreme end of that continuum. I'm going to tell you how to start Google.\n\nThe companies at the Google end of the continuum are called startups when they're young. The reason I know about them is that my wife Jessica and I started something called Y Combinator that is basically a startup factory. Since 2005, Y Combinator has funded over 4000 startups. So we know exactly what you need to start a startup, because we've helped people do it for the last 19 years.\n\nYou might have thought I was joking when I said I was going to tell you how to start Google. You might be thinking \\\"How could we start Google?\\\" But that's effectively what the people who did start Google were thinking before they started it. If you'd told Larry Page and Sergey Brin, the founders of Google, that the company they were about to start would one day be worth over a trillion dollars, their heads would have exploded.\n\nAll you can know when you start working on a startup is that it seems worth pursuing. You can't know whether it will turn into a company worth billions or one that goes out of business. So when I say I'm going to tell you how to start Google, I mean I'm going to tell you how to get to the point where you can start a company that has as much chance of being Google as Google had of being Google. [1]\n\nHow do you get from where you are now to the point where you can start a successful startup? You need three things. You need to be good at some kind of technology, you need an idea for what you're going to build, and you need cofounders to start the company with.\n\nHow do you get good at technology? And how do you choose which technology to get good at? Both of those questions turn out to have the same answer: work on your own projects. Don't try to guess whether gene editing or LLMs or rockets will turn out to be the most valuable technology to know about. No one can predict that. Just work on whatever interests you the most. You'll work much harder on something you're interested in than something you're doing because you think you're supposed to.\n\nIf you're not sure what technology to get good at, get good at programming. That has been the source of the median startup for the last 30 years, and this is probably not going to change in the next 10.\n\nThose of you who are taking computer science classes in school may at this point be thinking, ok, we've got this sorted. We're already being taught all about programming. But sorry, this is not enough. You have to be working on your own projects, not just learning stuff in classes. You can do well in computer science classes without ever really learning to program. In fact you can graduate with a degree in computer science from a top university and still not be any good at programming. That's why tech companies all make you take a coding test before they'll hire you, regardless of where you went to university or how well you did there. They know grades and exam results prove nothing.\n\nIf you really want to learn to program, you have to work on your own projects. You learn so much faster that way. Imagine you're writing a game and there's something you want to do in it, and you don't know how. You're going to figure out how a lot faster than you'd learn anything in a class.\n\nYou don't have to learn programming, though. If you're wondering what counts as technology, it includes practically everything you could describe using the words \\\"make\\\" or \\\"build.\\\" So welding would count, or making clothes, or making videos. Whatever you're most interested in. The critical distinction is whether you're producing or just consuming. Are you writing computer games, or just playing them? That's the cutoff.\n\nSteve Jobs, the founder of Apple, spent time when he was a teenager studying calligraphy — the sort of beautiful writing that you see in medieval manuscripts. No one, including him, thought that this would help him in his career. He was just doing it because he was interested in it. But it turned out to help him a lot. The computer that made Apple really big, the Macintosh, came out at just the moment when computers got powerful enough to make letters like the ones in printed books instead of the computery-looking letters you see in 8 bit games. Apple destroyed everyone else at this, and one reason was that Steve was one of the few people in the computer business who really got graphic design.\n\nDon't feel like your projects have to be serious. They can be as frivolous as you like, so long as you're building things you're excited about. Probably 90% of programmers start out building games. They and their friends like to play games. So they build the kind of things they and their friends want. And that's exactly what you should be doing at 15 if you want to start a startup one day.\n\nYou don't have to do just one project. In fact it's good to learn about multiple things. Steve Jobs didn't just learn calligraphy. He also learned about electronics, which was even more valuable. Whatever you're interested in. (Do you notice a theme here?)\n\nSo that's the first of the three things you need, to get good at some kind or kinds of technology. You do it the same way you get good at the violin or football: practice. If you start a startup at 22, and you start writing your own programs now, then by the time you start the company you'll have spent at least 7 years practicing writing code, and you can get pretty good at anything after practicing it for 7 years.\n\nLet's suppose you're 22 and you've succeeded: You're now really good at some technology. How do you get startup ideas? It might seem like that's the hard part. Even if you are a good programmer, how do you get the idea to start Google?\n\nActually it's easy to get startup ideas once you're good at technology. Once you're good at some technology, when you look at the world you see dotted outlines around the things that are missing. You start to be able to see both the things that are missing from the technology itself, and all the broken things that could be fixed using it, and each one of these is a potential startup.\n\nIn the town near our house there's a shop with a sign warning that the door is hard to close. The sign has been there for several years. To the people in the shop it must seem like this mysterious natural phenomenon that the door sticks, and all they can do is put up a sign warning customers about it. But any carpenter looking at this situation would think \\\"why don't you just plane off the part that sticks?\\\"\n\nOnce you're good at programming, all the missing software in the world starts to become as obvious as a sticking door to a carpenter. I'll give you a real world example. Back in the 20th century, American universities used to publish printed directories with all the students' names and contact info. When I tell you what these directories were called, you'll know which startup I'm talking about. They were called facebooks, because they usually had a picture of each student next to their name.\n\nSo Mark Zuckerberg shows up at Harvard in 2002, and the university still hasn't gotten the facebook online. Each individual house has an online facebook, but there isn't one for the whole university. The university administration has been diligently having meetings about this, and will probably have solved the problem in another decade or so. Most of the students don't consciously notice that anything is wrong. But Mark is a programmer. He looks at this situation and thinks \\\"Well, this is stupid. I could write a program to fix this in one night. Just let people upload their own photos and then combine the data into a new site for the whole university.\\\" So he does. And almost literally overnight he has thousands of users.\n\nOf course Facebook was not a startup yet. It was just a... project. There's that word again. Projects aren't just the best way to learn about technology. They're also the best source of startup ideas.\n\nFacebook was not unusual in this respect. Apple and Google also began as projects. Apple wasn't meant to be a company. Steve Wozniak just wanted to build his own computer. It only turned into a company when Steve Jobs said \\\"Hey, I wonder if we could sell plans for this computer to other people.\\\" That's how Apple started. They weren't even selling computers, just plans for computers. Can you imagine how lame this company seemed?\n\nDitto for Google. Larry and Sergey weren't trying to start a company at first. They were just trying to make search better. Before Google, most search engines didn't try to sort the results they gave you in order of importance. If you searched for \\\"rugby\\\" they just gave you every web page that contained the word \\\"rugby.\\\" And the web was so small in 1997 that this actually worked! Kind of. There might only be 20 or 30 pages with the word \\\"rugby,\\\" but the web was growing exponentially, which meant this way of doing search was becoming exponentially more broken. Most users just thought, \\\"Wow, I sure have to look through a lot of search results to find what I want.\\\" Door sticks. But like Mark, Larry and Sergey were programmers. Like Mark, they looked at this situation and thought \\\"Well, this is stupid. Some pages about rugby matter more than others. Let's figure out which those are and show them first.\\\"\n\nIt's obvious in retrospect that this was a great idea for a startup. It wasn't obvious at the time. It's never obvious. If it was obviously a good idea to start Apple or Google or Facebook, someone else would have already done it. That's why the best startups grow out of projects that aren't meant to be startups. You're not trying to start a company. You're just following your instincts about what's interesting. And if you're young and good at technology, then your unconscious instincts about what's interesting are better than your conscious ideas about what would be a good company.\n\nSo it's critical, if you're a young founder, to build things for yourself and your friends to use. The biggest mistake young founders make is to build something for some mysterious group of other people. But if you can make something that you and your friends truly want to use — something your friends aren't just using out of loyalty to you, but would be really sad to lose if you shut it down — then you almost certainly have the germ of a good startup idea. It may not seem like a startup to you. It may not be obvious how to make money from it. But trust me, there's a way.\n\nWhat you need in a startup idea, and all you need, is something your friends actually want. And those ideas aren't hard to see once you're good at technology. There are sticking doors everywhere. [2]\n\nNow for the third and final thing you need: a cofounder, or cofounders. The optimal startup has two or three founders, so you need one or two cofounders. How do you find them? Can you predict what I'm going to say next? It's the same thing: projects. You find cofounders by working on projects with them. What you need in a cofounder is someone who's good at what they do and that you work well with, and the only way to judge this is to work with them on things.\n\nAt this point I'm going to tell you something you might not want to hear. It really matters to do well in your classes, even the ones that are just memorization or blathering about literature, because you need to do well in your classes to get into a good university. And if you want to start a startup you should try to get into the best university you can, because that's where the best cofounders are. It's also where the best employees are. When Larry and Sergey started Google, they began by just hiring all the smartest people they knew out of Stanford, and this was a real advantage for them.\n\nThe empirical evidence is clear on this. If you look at where the largest numbers of successful startups come from, it's pretty much the same as the list of the most selective universities.\n\nI don't think it's the prestigious names of these universities that cause more good startups to come out of them. Nor do I think it's because the quality of the teaching is better. What's driving this is simply the difficulty of getting in. You have to be pretty smart and determined to get into MIT or Cambridge, so if you do manage to get in, you'll find the other students include a lot of smart and determined people. [3]\n\nYou don't have to start a startup with someone you meet at university. The founders of Twitch met when they were seven. The founders of Stripe, Patrick and John Collison, met when John was born. But universities are the main source of cofounders. And because they're where the cofounders are, they're also where the ideas are, because the best ideas grow out of projects you do with the people who become your cofounders.\n\nSo the list of what you need to do to get from here to starting a startup is quite short. You need to get good at technology, and the way to do that is to work on your own projects. And you need to do as well in school as you can, so you can get into a good university, because that's where the cofounders and the ideas are.\n\nThat's it, just two things, build stuff and do well in school.\n\nEND EXAMPLE PAUL GRAHAM ESSAYS\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay exactly like Paul Graham would write it as seen in the examples above. \n\n- Use the adjectives and superlatives that are used in the examples, and understand the TYPES of those that are used, and use similar ones and not dissimilar ones to better emulate the style.\n\n- That means the essay should be written in a simple, conversational style, not in a grandiose or academic style.\n\n- Use the same style, vocabulary level, and sentence structure as Paul Graham.\n\n# OUTPUT FORMAT\n\n- Output a full, publish-ready essay about the content provided using the instructions above.\n\n- Write in Paul Graham's simple, plain, clear, and conversational style, not in a grandiose or academic style.\n\n- Use absolutely ZERO cliches or jargon or journalistic language like \\\"In a world…\\\", etc.\n\n- Do not use cliches or jargon.\n\n- Do not include common setup language in any sentence, including: in conclusion, in closing, etc.\n\n- Do not output warnings or notes—just the output requested.\n\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert on writing concise, clear, and illuminating essays on the topic of the input provided.\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay in the style of Paul Graham, who is known for this concise, clear, and simple style of writing.\n\nEXAMPLE PAUL GRAHAM ESSAYS\n\nWriting about something, even something you know well, usually shows you that you didn't know it as well as you thought. Putting ideas into words is a severe test. The first words you choose are usually wrong; you have to rewrite sentences over and over to get them exactly right. And your ideas won't just be imprecise, but incomplete too. Half the ideas that end up in an essay will be ones you thought of while you were writing it. Indeed, that's why I write them.\n\nOnce you publish something, the convention is that whatever you wrote was what you thought before you wrote it. These were your ideas, and now you've expressed them. But you know this isn't true. You know that putting your ideas into words changed them. And not just the ideas you published. Presumably there were others that turned out to be too broken to fix, and those you discarded instead.\n\nIt's not just having to commit your ideas to specific words that makes writing so exacting. The real test is reading what you've written. You have to pretend to be a neutral reader who knows nothing of what's in your head, only what you wrote. When he reads what you wrote, does it seem correct? Does it seem complete? If you make an effort, you can read your writing as if you were a complete stranger, and when you do the news is usually bad. It takes me many cycles before I can get an essay past the stranger. But the stranger is rational, so you always can, if you ask him what he needs. If he's not satisfied because you failed to mention x or didn't qualify some sentence sufficiently, then you mention x or add more qualifications. Happy now? It may cost you some nice sentences, but you have to resign yourself to that. You just have to make them as good as you can and still satisfy the stranger.\n\nThis much, I assume, won't be that controversial. I think it will accord with the experience of anyone who has tried to write about anything non-trivial. There may exist people whose thoughts are so perfectly formed that they just flow straight into words. But I've never known anyone who could do this, and if I met someone who said they could, it would seem evidence of their limitations rather than their ability. Indeed, this is a trope in movies: the guy who claims to have a plan for doing some difficult thing, and who when questioned further, taps his head and says \\\"It's all up here.\\\" Everyone watching the movie knows what that means. At best the plan is vague and incomplete. Very likely there's some undiscovered flaw that invalidates it completely. At best it's a plan for a plan.\n\nIn precisely defined domains it's possible to form complete ideas in your head. People can play chess in their heads, for example. And mathematicians can do some amount of math in their heads, though they don't seem to feel sure of a proof over a certain length till they write it down. But this only seems possible with ideas you can express in a formal language. [1] Arguably what such people are doing is putting ideas into words in their heads. I can to some extent write essays in my head. I'll sometimes think of a paragraph while walking or lying in bed that survives nearly unchanged in the final version. But really I'm writing when I do this. I'm doing the mental part of writing; my fingers just aren't moving as I do it. [2]\n\nYou can know a great deal about something without writing about it. Can you ever know so much that you wouldn't learn more from trying to explain what you know? I don't think so. I've written about at least two subjects I know well — Lisp hacking and startups — and in both cases I learned a lot from writing about them. In both cases there were things I didn't consciously realize till I had to explain them. And I don't think my experience was anomalous. A great deal of knowledge is unconscious, and experts have if anything a higher proportion of unconscious knowledge than beginners.\n\nI'm not saying that writing is the best way to explore all ideas. If you have ideas about architecture, presumably the best way to explore them is to build actual buildings. What I'm saying is that however much you learn from exploring ideas in other ways, you'll still learn new things from writing about them.\n\nPutting ideas into words doesn't have to mean writing, of course. You can also do it the old way, by talking. But in my experience, writing is the stricter test. You have to commit to a single, optimal sequence of words. Less can go unsaid when you don't have tone of voice to carry meaning. And you can focus in a way that would seem excessive in conversation. I'll often spend 2 weeks on an essay and reread drafts 50 times. If you did that in conversation it would seem evidence of some kind of mental disorder. If you're lazy, of course, writing and talking are equally useless. But if you want to push yourself to get things right, writing is the steeper hill. [3]\n\nThe reason I've spent so long establishing this rather obvious point is that it leads to another that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn't written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything non-trivial.\n\nIt feels to them as if they do, especially if they're not in the habit of critically examining their own thinking. Ideas can feel complete. It's only when you try to put them into words that you discover they're not. So if you never subject your ideas to that test, you'll not only never have fully formed ideas, but also never realize it.\n\nPutting ideas into words is certainly no guarantee that they'll be right. Far from it. But though it's not a sufficient condition, it is a necessary one.\n\t\t\nWhat You Can't Say\n\nJanuary 2004\n\nHave you ever seen an old photo of yourself and been embarrassed at the way you looked? Did we actually dress like that? We did. And we had no idea how silly we looked. It's the nature of fashion to be invisible, in the same way the movement of the earth is invisible to all of us riding on it.\n\nWhat scares me is that there are moral fashions too. They're just as arbitrary, and just as invisible to most people. But they're much more dangerous. Fashion is mistaken for good design; moral fashion is mistaken for good. Dressing oddly gets you laughed at. Violating moral fashions can get you fired, ostracized, imprisoned, or even killed.\n\nIf you could travel back in a time machine, one thing would be true no matter where you went: you'd have to watch what you said. Opinions we consider harmless could have gotten you in big trouble. I've already said at least one thing that would have gotten me in big trouble in most of Europe in the seventeenth century, and did get Galileo in big trouble when he said it — that the earth moves. [1]\n\nIt seems to be a constant throughout history: In every period, people believed things that were just ridiculous, and believed them so strongly that you would have gotten in terrible trouble for saying otherwise.\n\nIs our time any different? To anyone who has read any amount of history, the answer is almost certainly no. It would be a remarkable coincidence if ours were the first era to get everything just right.\n\nIt's tantalizing to think we believe things that people in the future will find ridiculous. What would someone coming back to visit us in a time machine have to be careful not to say? That's what I want to study here. But I want to do more than just shock everyone with the heresy du jour. I want to find general recipes for discovering what you can't say, in any era.\n\nThe Conformist Test\n\nLet's start with a test: Do you have any opinions that you would be reluctant to express in front of a group of your peers?\n\nIf the answer is no, you might want to stop and think about that. If everything you believe is something you're supposed to believe, could that possibly be a coincidence? Odds are it isn't. Odds are you just think what you're told.\n\nThe other alternative would be that you independently considered every question and came up with the exact same answers that are now considered acceptable. That seems unlikely, because you'd also have to make the same mistakes. Mapmakers deliberately put slight mistakes in their maps so they can tell when someone copies them. If another map has the same mistake, that's very convincing evidence.\n\nLike every other era in history, our moral map almost certainly contains a few mistakes. And anyone who makes the same mistakes probably didn't do it by accident. It would be like someone claiming they had independently decided in 1972 that bell-bottom jeans were a good idea.\n\nIf you believe everything you're supposed to now, how can you be sure you wouldn't also have believed everything you were supposed to if you had grown up among the plantation owners of the pre-Civil War South, or in Germany in the 1930s — or among the Mongols in 1200, for that matter? Odds are you would have.\n\nBack in the era of terms like \\\"well-adjusted,\\\" the idea seemed to be that there was something wrong with you if you thought things you didn't dare say out loud. This seems backward. Almost certainly, there is something wrong with you if you don't think things you don't dare say out loud.\n\nTrouble\n\nWhat can't we say? One way to find these ideas is simply to look at things people do say, and get in trouble for. [2]\n\nOf course, we're not just looking for things we can't say. We're looking for things we can't say that are true, or at least have enough chance of being true that the question should remain open. But many of the things people get in trouble for saying probably do make it over this second, lower threshold. No one gets in trouble for saying that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall. Such obviously false statements might be treated as jokes, or at worst as evidence of insanity, but they are not likely to make anyone mad. The statements that make people mad are the ones they worry might be believed. I suspect the statements that make people maddest are those they worry might be true.\n\nIf Galileo had said that people in Padua were ten feet tall, he would have been regarded as a harmless eccentric. Saying the earth orbited the sun was another matter. The church knew this would set people thinking.\n\nCertainly, as we look back on the past, this rule of thumb works well. A lot of the statements people got in trouble for seem harmless now. So it's likely that visitors from the future would agree with at least some of the statements that get people in trouble today. Do we have no Galileos? Not likely.\n\nTo find them, keep track of opinions that get people in trouble, and start asking, could this be true? Ok, it may be heretical (or whatever modern equivalent), but might it also be true?\n\nHeresy\n\nThis won't get us all the answers, though. What if no one happens to have gotten in trouble for a particular idea yet? What if some idea would be so radioactively controversial that no one would dare express it in public? How can we find these too?\n\nAnother approach is to follow that word, heresy. In every period of history, there seem to have been labels that got applied to statements to shoot them down before anyone had a chance to ask if they were true or not. \\\"Blasphemy\\\", \\\"sacrilege\\\", and \\\"heresy\\\" were such labels for a good part of western history, as in more recent times \\\"indecent\\\", \\\"improper\\\", and \\\"unamerican\\\" have been. By now these labels have lost their sting. They always do. By now they're mostly used ironically. But in their time, they had real force.\n\nThe word \\\"defeatist\\\", for example, has no particular political connotations now. But in Germany in 1917 it was a weapon, used by Ludendorff in a purge of those who favored a negotiated peace. At the start of World War II it was used extensively by Churchill and his supporters to silence their opponents. In 1940, any argument against Churchill's aggressive policy was \\\"defeatist\\\". Was it right or wrong? Ideally, no one got far enough to ask that.\n\nWe have such labels today, of course, quite a lot of them, from the all-purpose \\\"inappropriate\\\" to the dreaded \\\"divisive.\\\" In any period, it should be easy to figure out what such labels are, simply by looking at what people call ideas they disagree with besides untrue. When a politician says his opponent is mistaken, that's a straightforward criticism, but when he attacks a statement as \\\"divisive\\\" or \\\"racially insensitive\\\" instead of arguing that it's false, we should start paying attention.\n\nSo another way to figure out which of our taboos future generations will laugh at is to start with the labels. Take a label — \\\"sexist\\\", for example — and try to think of some ideas that would be called that. Then for each ask, might this be true?\n\nJust start listing ideas at random? Yes, because they won't really be random. The ideas that come to mind first will be the most plausible ones. They'll be things you've already noticed but didn't let yourself think.\n\nIn 1989 some clever researchers tracked the eye movements of radiologists as they scanned chest images for signs of lung cancer. [3] They found that even when the radiologists missed a cancerous lesion, their eyes had usually paused at the site of it. Part of their brain knew there was something there; it just didn't percolate all the way up into conscious knowledge. I think many interesting heretical thoughts are already mostly formed in our minds. If we turn off our self-censorship temporarily, those will be the first to emerge.\n\nTime and Space\n\nIf we could look into the future it would be obvious which of our taboos they'd laugh at. We can't do that, but we can do something almost as good: we can look into the past. Another way to figure out what we're getting wrong is to look at what used to be acceptable and is now unthinkable.\n\nChanges between the past and the present sometimes do represent progress. In a field like physics, if we disagree with past generations it's because we're right and they're wrong. But this becomes rapidly less true as you move away from the certainty of the hard sciences. By the time you get to social questions, many changes are just fashion. The age of consent fluctuates like hemlines.\n\nWe may imagine that we are a great deal smarter and more virtuous than past generations, but the more history you read, the less likely this seems. People in past times were much like us. Not heroes, not barbarians. Whatever their ideas were, they were ideas reasonable people could believe.\n\nSo here is another source of interesting heresies. Diff present ideas against those of various past cultures, and see what you get. [4] Some will be shocking by present standards. Ok, fine; but which might also be true?\n\nYou don't have to look into the past to find big differences. In our own time, different societies have wildly varying ideas of what's ok and what isn't. So you can try diffing other cultures' ideas against ours as well. (The best way to do that is to visit them.) Any idea that's considered harmless in a significant percentage of times and places, and yet is taboo in ours, is a candidate for something we're mistaken about.\n\nFor example, at the high water mark of political correctness in the early 1990s, Harvard distributed to its faculty and staff a brochure saying, among other things, that it was inappropriate to compliment a colleague or student's clothes. No more \\\"nice shirt.\\\" I think this principle is rare among the world's cultures, past or present. There are probably more where it's considered especially polite to compliment someone's clothing than where it's considered improper. Odds are this is, in a mild form, an example of one of the taboos a visitor from the future would have to be careful to avoid if he happened to set his time machine for Cambridge, Massachusetts, 1992. [5]\n\nPrigs\n\nOf course, if they have time machines in the future they'll probably have a separate reference manual just for Cambridge. This has always been a fussy place, a town of i dotters and t crossers, where you're liable to get both your grammar and your ideas corrected in the same conversation. And that suggests another way to find taboos. Look for prigs, and see what's inside their heads.\n\nKids' heads are repositories of all our taboos. It seems fitting to us that kids' ideas should be bright and clean. The picture we give them of the world is not merely simplified, to suit their developing minds, but sanitized as well, to suit our ideas of what kids ought to think. [6]\n\nYou can see this on a small scale in the matter of dirty words. A lot of my friends are starting to have children now, and they're all trying not to use words like \\\"fuck\\\" and \\\"shit\\\" within baby's hearing, lest baby start using these words too. But these words are part of the language, and adults use them all the time. So parents are giving their kids an inaccurate idea of the language by not using them. Why do they do this? Because they don't think it's fitting that kids should use the whole language. We like children to seem innocent. [7]\n\nMost adults, likewise, deliberately give kids a misleading view of the world. One of the most obvious examples is Santa Claus. We think it's cute for little kids to believe in Santa Claus. I myself think it's cute for little kids to believe in Santa Claus. But one wonders, do we tell them this stuff for their sake, or for ours?\n\nI'm not arguing for or against this idea here. It is probably inevitable that parents should want to dress up their kids' minds in cute little baby outfits. I'll probably do it myself. The important thing for our purposes is that, as a result, a well brought-up teenage kid's brain is a more or less complete collection of all our taboos — and in mint condition, because they're untainted by experience. Whatever we think that will later turn out to be ridiculous, it's almost certainly inside that head.\n\nHow do we get at these ideas? By the following thought experiment. Imagine a kind of latter-day Conrad character who has worked for a time as a mercenary in Africa, for a time as a doctor in Nepal, for a time as the manager of a nightclub in Miami. The specifics don't matter — just someone who has seen a lot. Now imagine comparing what's inside this guy's head with what's inside the head of a well-behaved sixteen year old girl from the suburbs. What does he think that would shock her? He knows the world; she knows, or at least embodies, present taboos. Subtract one from the other, and the result is what we can't say.\n\nMechanism\n\nI can think of one more way to figure out what we can't say: to look at how taboos are created. How do moral fashions arise, and why are they adopted? If we can understand this mechanism, we may be able to see it at work in our own time.\n\nMoral fashions don't seem to be created the way ordinary fashions are. Ordinary fashions seem to arise by accident when everyone imitates the whim of some influential person. The fashion for broad-toed shoes in late fifteenth century Europe began because Charles VIII of France had six toes on one foot. The fashion for the name Gary began when the actor Frank Cooper adopted the name of a tough mill town in Indiana. Moral fashions more often seem to be created deliberately. When there's something we can't say, it's often because some group doesn't want us to.\n\nThe prohibition will be strongest when the group is nervous. The irony of Galileo's situation was that he got in trouble for repeating Copernicus's ideas. Copernicus himself didn't. In fact, Copernicus was a canon of a cathedral, and dedicated his book to the pope. But by Galileo's time the church was in the throes of the Counter-Reformation and was much more worried about unorthodox ideas.\n\nTo launch a taboo, a group has to be poised halfway between weakness and power. A confident group doesn't need taboos to protect it. It's not considered improper to make disparaging remarks about Americans, or the English. And yet a group has to be powerful enough to enforce a taboo. Coprophiles, as of this writing, don't seem to be numerous or energetic enough to have had their interests promoted to a lifestyle.\n\nI suspect the biggest source of moral taboos will turn out to be power struggles in which one side only barely has the upper hand. That's where you'll find a group powerful enough to enforce taboos, but weak enough to need them.\n\nMost struggles, whatever they're really about, will be cast as struggles between competing ideas. The English Reformation was at bottom a struggle for wealth and power, but it ended up being cast as a struggle to preserve the souls of Englishmen from the corrupting influence of Rome. It's easier to get people to fight for an idea. And whichever side wins, their ideas will also be considered to have triumphed, as if God wanted to signal his agreement by selecting that side as the victor.\n\nWe often like to think of World War II as a triumph of freedom over totalitarianism. We conveniently forget that the Soviet Union was also one of the winners.\n\nI'm not saying that struggles are never about ideas, just that they will always be made to seem to be about ideas, whether they are or not. And just as there is nothing so unfashionable as the last, discarded fashion, there is nothing so wrong as the principles of the most recently defeated opponent. Representational art is only now recovering from the approval of both Hitler and Stalin. [8]\n\nAlthough moral fashions tend to arise from different sources than fashions in clothing, the mechanism of their adoption seems much the same. The early adopters will be driven by ambition: self-consciously cool people who want to distinguish themselves from the common herd. As the fashion becomes established they'll be joined by a second, much larger group, driven by fear. [9] This second group adopt the fashion not because they want to stand out but because they are afraid of standing out.\n\nSo if you want to figure out what we can't say, look at the machinery of fashion and try to predict what it would make unsayable. What groups are powerful but nervous, and what ideas would they like to suppress? What ideas were tarnished by association when they ended up on the losing side of a recent struggle? If a self-consciously cool person wanted to differentiate himself from preceding fashions (e.g. from his parents), which of their ideas would he tend to reject? What are conventional-minded people afraid of saying?\n\nThis technique won't find us all the things we can't say. I can think of some that aren't the result of any recent struggle. Many of our taboos are rooted deep in the past. But this approach, combined with the preceding four, will turn up a good number of unthinkable ideas.\n\nWhy\n\nSome would ask, why would one want to do this? Why deliberately go poking around among nasty, disreputable ideas? Why look under rocks?\n\nI do it, first of all, for the same reason I did look under rocks as a kid: plain curiosity. And I'm especially curious about anything that's forbidden. Let me see and decide for myself.\n\nSecond, I do it because I don't like the idea of being mistaken. If, like other eras, we believe things that will later seem ridiculous, I want to know what they are so that I, at least, can avoid believing them.\n\nThird, I do it because it's good for the brain. To do good work you need a brain that can go anywhere. And you especially need a brain that's in the habit of going where it's not supposed to.\n\nGreat work tends to grow out of ideas that others have overlooked, and no idea is so overlooked as one that's unthinkable. Natural selection, for example. It's so simple. Why didn't anyone think of it before? Well, that is all too obvious. Darwin himself was careful to tiptoe around the implications of his theory. He wanted to spend his time thinking about biology, not arguing with people who accused him of being an atheist.\n\nIn the sciences, especially, it's a great advantage to be able to question assumptions. The m.o. of scientists, or at least of the good ones, is precisely that: look for places where conventional wisdom is broken, and then try to pry apart the cracks and see what's underneath. That's where new theories come from.\n\nA good scientist, in other words, does not merely ignore conventional wisdom, but makes a special effort to break it. Scientists go looking for trouble. This should be the m.o. of any scholar, but scientists seem much more willing to look under rocks. [10]\n\nWhy? It could be that the scientists are simply smarter; most physicists could, if necessary, make it through a PhD program in French literature, but few professors of French literature could make it through a PhD program in physics. Or it could be because it's clearer in the sciences whether theories are true or false, and this makes scientists bolder. (Or it could be that, because it's clearer in the sciences whether theories are true or false, you have to be smart to get jobs as a scientist, rather than just a good politician.)\n\nWhatever the reason, there seems a clear correlation between intelligence and willingness to consider shocking ideas. This isn't just because smart people actively work to find holes in conventional thinking. I think conventions also have less hold over them to start with. You can see that in the way they dress.\n\nIt's not only in the sciences that heresy pays off. In any competitive field, you can win big by seeing things that others daren't. And in every field there are probably heresies few dare utter. Within the US car industry there is a lot of hand-wringing now about declining market share. Yet the cause is so obvious that any observant outsider could explain it in a second: they make bad cars. And they have for so long that by now the US car brands are antibrands — something you'd buy a car despite, not because of. Cadillac stopped being the Cadillac of cars in about 1970. And yet I suspect no one dares say this. [11] Otherwise these companies would have tried to fix the problem.\n\nTraining yourself to think unthinkable thoughts has advantages beyond the thoughts themselves. It's like stretching. When you stretch before running, you put your body into positions much more extreme than any it will assume during the run. If you can think things so outside the box that they'd make people's hair stand on end, you'll have no trouble with the small trips outside the box that people call innovative.\n\nPensieri Stretti\n\nWhen you find something you can't say, what do you do with it? My advice is, don't say it. Or at least, pick your battles.\n\nSuppose in the future there is a movement to ban the color yellow. Proposals to paint anything yellow are denounced as \\\"yellowist\\\", as is anyone suspected of liking the color. People who like orange are tolerated but viewed with suspicion. Suppose you realize there is nothing wrong with yellow. If you go around saying this, you'll be denounced as a yellowist too, and you'll find yourself having a lot of arguments with anti-yellowists. If your aim in life is to rehabilitate the color yellow, that may be what you want. But if you're mostly interested in other questions, being labelled as a yellowist will just be a distraction. Argue with idiots, and you become an idiot.\n\nThe most important thing is to be able to think what you want, not to say what you want. And if you feel you have to say everything you think, it may inhibit you from thinking improper thoughts. I think it's better to follow the opposite policy. Draw a sharp line between your thoughts and your speech. Inside your head, anything is allowed. Within my head I make a point of encouraging the most outrageous thoughts I can imagine. But, as in a secret society, nothing that happens within the building should be told to outsiders. The first rule of Fight Club is, you do not talk about Fight Club.\n\nWhen Milton was going to visit Italy in the 1630s, Sir Henry Wootton, who had been ambassador to Venice, told him his motto should be \\\"i pensieri stretti & il viso sciolto.\\\" Closed thoughts and an open face. Smile at everyone, and don't tell them what you're thinking. This was wise advice. Milton was an argumentative fellow, and the Inquisition was a bit restive at that time. But I think the difference between Milton's situation and ours is only a matter of degree. Every era has its heresies, and if you don't get imprisoned for them you will at least get in enough trouble that it becomes a complete distraction.\n\nI admit it seems cowardly to keep quiet. When I read about the harassment to which the Scientologists subject their critics [12], or that pro-Israel groups are \\\"compiling dossiers\\\" on those who speak out against Israeli human rights abuses [13], or about people being sued for violating the DMCA [14], part of me wants to say, \\\"All right, you bastards, bring it on.\\\" The problem is, there are so many things you can't say. If you said them all you'd have no time left for your real work. You'd have to turn into Noam Chomsky. [15]\n\nThe trouble with keeping your thoughts secret, though, is that you lose the advantages of discussion. Talking about an idea leads to more ideas. So the optimal plan, if you can manage it, is to have a few trusted friends you can speak openly to. This is not just a way to develop ideas; it's also a good rule of thumb for choosing friends. The people you can say heretical things to without getting jumped on are also the most interesting to know.\n\nViso Sciolto?\n\nI don't think we need the viso sciolto so much as the pensieri stretti. Perhaps the best policy is to make it plain that you don't agree with whatever zealotry is current in your time, but not to be too specific about what you disagree with. Zealots will try to draw you out, but you don't have to answer them. If they try to force you to treat a question on their terms by asking \\\"are you with us or against us?\\\" you can always just answer \\\"neither\\\".\n\nBetter still, answer \\\"I haven't decided.\\\" That's what Larry Summers did when a group tried to put him in this position. Explaining himself later, he said \\\"I don't do litmus tests.\\\" [16] A lot of the questions people get hot about are actually quite complicated. There is no prize for getting the answer quickly.\n\nIf the anti-yellowists seem to be getting out of hand and you want to fight back, there are ways to do it without getting yourself accused of being a yellowist. Like skirmishers in an ancient army, you want to avoid directly engaging the main body of the enemy's troops. Better to harass them with arrows from a distance.\n\nOne way to do this is to ratchet the debate up one level of abstraction. If you argue against censorship in general, you can avoid being accused of whatever heresy is contained in the book or film that someone is trying to censor. You can attack labels with meta-labels: labels that refer to the use of labels to prevent discussion. The spread of the term \\\"political correctness\\\" meant the beginning of the end of political correctness, because it enabled one to attack the phenomenon as a whole without being accused of any of the specific heresies it sought to suppress.\n\nAnother way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \\\"The Crucible,\\\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \\\"witch-hunt.\\\"\n\nBest of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \\\"I am glad that I managed to write 'The Crucible,'\\\" Arthur Miller wrote, \\\"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\\\" [17]\n\nABQ\n\nA Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland.\n\nAnd yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\n\nCertainly the fact that they value open-mindedness is no guarantee. Who thinks they're not open-minded? Our hypothetical prim miss from the suburbs thinks she's open-minded. Hasn't she been taught to be? Ask anyone, and they'll say the same thing: they're pretty open-minded, though they draw the line at things that are really wrong. (Some tribes may avoid \\\"wrong\\\" as judgemental, and may instead use a more neutral sounding euphemism like \\\"negative\\\" or \\\"destructive\\\".)\n\nWhen people are bad at math, they know it, because they get the wrong answers on tests. But when people are bad at open-mindedness they don't know it. In fact they tend to think the opposite. Remember, it's the nature of fashion to be invisible. It wouldn't work otherwise. Fashion doesn't seem like fashion to someone in the grip of it. It just seems like the right thing to do. It's only by looking from a distance that we see oscillations in people's idea of the right thing to do, and can identify them as fashions.\n\nTime gives us such distance for free. Indeed, the arrival of new fashions makes old fashions easy to see, because they seem so ridiculous by contrast. From one end of a pendulum's swing, the other end seems especially far away.\n\nTo see fashion in your own time, though, requires a conscious effort. Without time to give you distance, you have to create distance yourself. Instead of being part of the mob, stand as far away from it as you can and watch what it's doing. And pay especially close attention whenever an idea is being suppressed. Web filters for children and employees often ban sites containing pornography, violence, and hate speech. What counts as pornography and violence? And what, exactly, is \\\"hate speech?\\\" This sounds like a phrase out of 1984.\n\nLabels like that are probably the biggest external clue. If a statement is false, that's the worst thing you can say about it. You don't need to say that it's heretical. And if it isn't false, it shouldn't be suppressed. So when you see statements being attacked as x-ist or y-ic (substitute your current values of x and y), whether in 1630 or 2030, that's a sure sign that something is wrong. When you hear such labels being used, ask why.\n\nEspecially if you hear yourself using them. It's not just the mob you need to learn to watch from a distance. You need to be able to watch your own thoughts from a distance. That's not a radical idea, by the way; it's the main difference between children and adults. When a child gets angry because he's tired, he doesn't know what's happening. An adult can distance himself enough from the situation to say \\\"never mind, I'm just tired.\\\" I don't see why one couldn't, by a similar process, learn to recognize and discount the effects of moral fashions.\n\nYou have to take that extra step if you want to think clearly. But it's harder, because now you're working against social customs instead of with them. Everyone encourages you to grow up to the point where you can discount your own bad moods. Few encourage you to continue to the point where you can discount society's bad moods.\n\nHow can you see the wave, when you're the water? Always be questioning. That's the only defence. What can't you say? And why?\n\nHow to Start Google\n\nMarch 2024\n\n(This is a talk I gave to 14 and 15 year olds about what to do now if they might want to start a startup later. Lots of schools think they should tell students something about startups. This is what I think they should tell them.)\n\nMost of you probably think that when you're released into the so-called real world you'll eventually have to get some kind of job. That's not true, and today I'm going to talk about a trick you can use to avoid ever having to get a job.\n\nThe trick is to start your own company. So it's not a trick for avoiding work, because if you start your own company you'll work harder than you would if you had an ordinary job. But you will avoid many of the annoying things that come with a job, including a boss telling you what to do.\n\nIt's more exciting to work on your own project than someone else's. And you can also get a lot richer. In fact, this is the standard way to get really rich. If you look at the lists of the richest people that occasionally get published in the press, nearly all of them did it by starting their own companies.\n\nStarting your own company can mean anything from starting a barber shop to starting Google. I'm here to talk about one extreme end of that continuum. I'm going to tell you how to start Google.\n\nThe companies at the Google end of the continuum are called startups when they're young. The reason I know about them is that my wife Jessica and I started something called Y Combinator that is basically a startup factory. Since 2005, Y Combinator has funded over 4000 startups. So we know exactly what you need to start a startup, because we've helped people do it for the last 19 years.\n\nYou might have thought I was joking when I said I was going to tell you how to start Google. You might be thinking \\\"How could we start Google?\\\" But that's effectively what the people who did start Google were thinking before they started it. If you'd told Larry Page and Sergey Brin, the founders of Google, that the company they were about to start would one day be worth over a trillion dollars, their heads would have exploded.\n\nAll you can know when you start working on a startup is that it seems worth pursuing. You can't know whether it will turn into a company worth billions or one that goes out of business. So when I say I'm going to tell you how to start Google, I mean I'm going to tell you how to get to the point where you can start a company that has as much chance of being Google as Google had of being Google. [1]\n\nHow do you get from where you are now to the point where you can start a successful startup? You need three things. You need to be good at some kind of technology, you need an idea for what you're going to build, and you need cofounders to start the company with.\n\nHow do you get good at technology? And how do you choose which technology to get good at? Both of those questions turn out to have the same answer: work on your own projects. Don't try to guess whether gene editing or LLMs or rockets will turn out to be the most valuable technology to know about. No one can predict that. Just work on whatever interests you the most. You'll work much harder on something you're interested in than something you're doing because you think you're supposed to.\n\nIf you're not sure what technology to get good at, get good at programming. That has been the source of the median startup for the last 30 years, and this is probably not going to change in the next 10.\n\nThose of you who are taking computer science classes in school may at this point be thinking, ok, we've got this sorted. We're already being taught all about programming. But sorry, this is not enough. You have to be working on your own projects, not just learning stuff in classes. You can do well in computer science classes without ever really learning to program. In fact you can graduate with a degree in computer science from a top university and still not be any good at programming. That's why tech companies all make you take a coding test before they'll hire you, regardless of where you went to university or how well you did there. They know grades and exam results prove nothing.\n\nIf you really want to learn to program, you have to work on your own projects. You learn so much faster that way. Imagine you're writing a game and there's something you want to do in it, and you don't know how. You're going to figure out how a lot faster than you'd learn anything in a class.\n\nYou don't have to learn programming, though. If you're wondering what counts as technology, it includes practically everything you could describe using the words \\\"make\\\" or \\\"build.\\\" So welding would count, or making clothes, or making videos. Whatever you're most interested in. The critical distinction is whether you're producing or just consuming. Are you writing computer games, or just playing them? That's the cutoff.\n\nSteve Jobs, the founder of Apple, spent time when he was a teenager studying calligraphy — the sort of beautiful writing that you see in medieval manuscripts. No one, including him, thought that this would help him in his career. He was just doing it because he was interested in it. But it turned out to help him a lot. The computer that made Apple really big, the Macintosh, came out at just the moment when computers got powerful enough to make letters like the ones in printed books instead of the computery-looking letters you see in 8 bit games. Apple destroyed everyone else at this, and one reason was that Steve was one of the few people in the computer business who really got graphic design.\n\nDon't feel like your projects have to be serious. They can be as frivolous as you like, so long as you're building things you're excited about. Probably 90% of programmers start out building games. They and their friends like to play games. So they build the kind of things they and their friends want. And that's exactly what you should be doing at 15 if you want to start a startup one day.\n\nYou don't have to do just one project. In fact it's good to learn about multiple things. Steve Jobs didn't just learn calligraphy. He also learned about electronics, which was even more valuable. Whatever you're interested in. (Do you notice a theme here?)\n\nSo that's the first of the three things you need, to get good at some kind or kinds of technology. You do it the same way you get good at the violin or football: practice. If you start a startup at 22, and you start writing your own programs now, then by the time you start the company you'll have spent at least 7 years practicing writing code, and you can get pretty good at anything after practicing it for 7 years.\n\nLet's suppose you're 22 and you've succeeded: You're now really good at some technology. How do you get startup ideas? It might seem like that's the hard part. Even if you are a good programmer, how do you get the idea to start Google?\n\nActually it's easy to get startup ideas once you're good at technology. Once you're good at some technology, when you look at the world you see dotted outlines around the things that are missing. You start to be able to see both the things that are missing from the technology itself, and all the broken things that could be fixed using it, and each one of these is a potential startup.\n\nIn the town near our house there's a shop with a sign warning that the door is hard to close. The sign has been there for several years. To the people in the shop it must seem like this mysterious natural phenomenon that the door sticks, and all they can do is put up a sign warning customers about it. But any carpenter looking at this situation would think \\\"why don't you just plane off the part that sticks?\\\"\n\nOnce you're good at programming, all the missing software in the world starts to become as obvious as a sticking door to a carpenter. I'll give you a real world example. Back in the 20th century, American universities used to publish printed directories with all the students' names and contact info. When I tell you what these directories were called, you'll know which startup I'm talking about. They were called facebooks, because they usually had a picture of each student next to their name.\n\nSo Mark Zuckerberg shows up at Harvard in 2002, and the university still hasn't gotten the facebook online. Each individual house has an online facebook, but there isn't one for the whole university. The university administration has been diligently having meetings about this, and will probably have solved the problem in another decade or so. Most of the students don't consciously notice that anything is wrong. But Mark is a programmer. He looks at this situation and thinks \\\"Well, this is stupid. I could write a program to fix this in one night. Just let people upload their own photos and then combine the data into a new site for the whole university.\\\" So he does. And almost literally overnight he has thousands of users.\n\nOf course Facebook was not a startup yet. It was just a... project. There's that word again. Projects aren't just the best way to learn about technology. They're also the best source of startup ideas.\n\nFacebook was not unusual in this respect. Apple and Google also began as projects. Apple wasn't meant to be a company. Steve Wozniak just wanted to build his own computer. It only turned into a company when Steve Jobs said \\\"Hey, I wonder if we could sell plans for this computer to other people.\\\" That's how Apple started. They weren't even selling computers, just plans for computers. Can you imagine how lame this company seemed?\n\nDitto for Google. Larry and Sergey weren't trying to start a company at first. They were just trying to make search better. Before Google, most search engines didn't try to sort the results they gave you in order of importance. If you searched for \\\"rugby\\\" they just gave you every web page that contained the word \\\"rugby.\\\" And the web was so small in 1997 that this actually worked! Kind of. There might only be 20 or 30 pages with the word \\\"rugby,\\\" but the web was growing exponentially, which meant this way of doing search was becoming exponentially more broken. Most users just thought, \\\"Wow, I sure have to look through a lot of search results to find what I want.\\\" Door sticks. But like Mark, Larry and Sergey were programmers. Like Mark, they looked at this situation and thought \\\"Well, this is stupid. Some pages about rugby matter more than others. Let's figure out which those are and show them first.\\\"\n\nIt's obvious in retrospect that this was a great idea for a startup. It wasn't obvious at the time. It's never obvious. If it was obviously a good idea to start Apple or Google or Facebook, someone else would have already done it. That's why the best startups grow out of projects that aren't meant to be startups. You're not trying to start a company. You're just following your instincts about what's interesting. And if you're young and good at technology, then your unconscious instincts about what's interesting are better than your conscious ideas about what would be a good company.\n\nSo it's critical, if you're a young founder, to build things for yourself and your friends to use. The biggest mistake young founders make is to build something for some mysterious group of other people. But if you can make something that you and your friends truly want to use — something your friends aren't just using out of loyalty to you, but would be really sad to lose if you shut it down — then you almost certainly have the germ of a good startup idea. It may not seem like a startup to you. It may not be obvious how to make money from it. But trust me, there's a way.\n\nWhat you need in a startup idea, and all you need, is something your friends actually want. And those ideas aren't hard to see once you're good at technology. There are sticking doors everywhere. [2]\n\nNow for the third and final thing you need: a cofounder, or cofounders. The optimal startup has two or three founders, so you need one or two cofounders. How do you find them? Can you predict what I'm going to say next? It's the same thing: projects. You find cofounders by working on projects with them. What you need in a cofounder is someone who's good at what they do and that you work well with, and the only way to judge this is to work with them on things.\n\nAt this point I'm going to tell you something you might not want to hear. It really matters to do well in your classes, even the ones that are just memorization or blathering about literature, because you need to do well in your classes to get into a good university. And if you want to start a startup you should try to get into the best university you can, because that's where the best cofounders are. It's also where the best employees are. When Larry and Sergey started Google, they began by just hiring all the smartest people they knew out of Stanford, and this was a real advantage for them.\n\nThe empirical evidence is clear on this. If you look at where the largest numbers of successful startups come from, it's pretty much the same as the list of the most selective universities.\n\nI don't think it's the prestigious names of these universities that cause more good startups to come out of them. Nor do I think it's because the quality of the teaching is better. What's driving this is simply the difficulty of getting in. You have to be pretty smart and determined to get into MIT or Cambridge, so if you do manage to get in, you'll find the other students include a lot of smart and determined people. [3]\n\nYou don't have to start a startup with someone you meet at university. The founders of Twitch met when they were seven. The founders of Stripe, Patrick and John Collison, met when John was born. But universities are the main source of cofounders. And because they're where the cofounders are, they're also where the ideas are, because the best ideas grow out of projects you do with the people who become your cofounders.\n\nSo the list of what you need to do to get from here to starting a startup is quite short. You need to get good at technology, and the way to do that is to work on your own projects. And you need to do as well in school as you can, so you can get into a good university, because that's where the cofounders and the ideas are.\n\nThat's it, just two things, build stuff and do well in school.\n\nEND EXAMPLE PAUL GRAHAM ESSAYS\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay exactly like Paul Graham would write it as seen in the examples above. \n\n- Use the adjectives and superlatives that are used in the examples, and understand the TYPES of those that are used, and use similar ones and not dissimilar ones to better emulate the style.\n\n- That means the essay should be written in a simple, conversational style, not in a grandiose or academic style.\n\n- Use the same style, vocabulary level, and sentence structure as Paul Graham.\n\n# OUTPUT FORMAT\n\n- Output a full, publish-ready essay about the content provided using the instructions above.\n\n- Write in Paul Graham's simple, plain, clear, and conversational style, not in a grandiose or academic style.\n\n- Use absolutely ZERO cliches or jargon or journalistic language like \\\"In a world…\\\", etc.\n\n- Do not use cliches or jargon.\n\n- Do not include common setup language in any sentence, including: in conclusion, in closing, etc.\n\n- Do not output warnings or notes—just the output requested.\n\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.03789213299751282,
        0.4426741600036621,
        -0.1977950483560562,
        0.3863880932331085,
        -0.01680092141032219,
        0.14820224046707153,
        -0.9643839597702026,
        0.07919922471046448,
        0.09509196877479553,
        0.34616559743881226,
        -0.38936373591423035,
        0.8748636841773987,
        0.039618153125047684,
        0.21069081127643585,
        -0.02877204492688179,
        -0.18259109556674957,
        -0.6919930577278137,
        -0.9953926801681519,
        -1.3572945594787598,
        -0.19355860352516174,
        0.1496896743774414,
        0.42720696330070496,
        0.026862062513828278,
        -0.0896754115819931,
        0.5582571029663086,
        0.15895812213420868,
        0.6453977227210999,
        0.272175669670105,
        -1.2163406610488892,
        -2.0771265029907227,
        0.2560565769672394,
        0.11154123395681381,
        -0.37804675102233887,
        -0.4884811341762543,
        0.5782719850540161,
        -0.9951772689819336,
        0.19489236176013947,
        0.10309401899576187,
        -0.7356590628623962,
        -0.5144099593162537,
        -0.1366899311542511,
        -0.1328136920928955,
        0.004760518670082092,
        0.05311135947704315,
        -0.08483076095581055,
        -0.569024920463562,
        0.08244466781616211,
        -0.21536561846733093,
        1.0496327877044678,
        0.2801819145679474,
        -0.7260461449623108,
        -0.5112213492393494,
        0.1704515665769577,
        0.01067088358104229,
        -0.4139736592769623,
        -0.11799000203609467,
        0.3784622550010681,
        -0.2589366137981415,
        0.03245684877038002,
        -0.2889212965965271,
        -0.1554132103919983,
        0.11264103651046753,
        -4.0820088386535645,
        0.11842100322246552,
        0.41985756158828735,
        0.6723707914352417,
        0.04226944223046303,
        0.23430676758289337,
        -0.03897601366043091,
        0.10173560678958893,
        -0.03863003849983215,
        0.2318771332502365,
        -0.5257927775382996,
        0.6662852764129639,
        0.4361659288406372,
        -0.011734731495380402,
        0.39069628715515137,
        -0.026986397802829742,
        0.5259162783622742,
        -0.45308777689933777,
        0.19715216755867004,
        0.9393520355224609,
        0.10051603615283966,
        -0.02647705376148224,
        -0.7533648014068604,
        1.1359974145889282,
        -0.22913408279418945,
        0.26435771584510803,
        0.756625235080719,
        0.31717050075531006,
        -0.31498798727989197,
        -0.44138818979263306,
        -0.1378648281097412,
        -0.3367866277694702,
        -0.9886265993118286,
        0.30627214908599854,
        -0.38965967297554016,
        0.2385743409395218,
        -0.10873087495565414,
        3.389016628265381,
        0.18162406980991364,
        -0.04873064532876015,
        0.49096640944480896,
        -1.136781096458435,
        0.5511318445205688,
        -0.1403321623802185,
        0.2811427116394043,
        -0.7092178463935852,
        -0.2803996801376343,
        -0.08744694292545319,
        0.2648569941520691,
        -0.2710600793361664,
        -0.10688340663909912,
        -0.08611027896404266,
        0.7107522487640381,
        0.1356758028268814,
        -0.8265761137008667,
        0.21346887946128845,
        -0.2544623613357544,
        1.452978491783142,
        -0.37181833386421204,
        0.2145119458436966,
        -0.7697411179542542,
        -0.16327103972434998,
        0.34006690979003906,
        0.04512156546115875,
        -0.056329648941755295,
        0.6354137063026428,
        -0.13571538031101227,
        0.05338507890701294,
        -0.09601504355669022,
        -0.708668053150177,
        0.08978734910488129,
        -0.564153254032135,
        0.5165183544158936,
        -0.06709449738264084,
        0.42603224515914917,
        -0.37643134593963623,
        0.6500839591026306,
        -0.4084787368774414,
        0.6232230067253113,
        -1.1749309301376343,
        1.685463786125183,
        0.5123631954193115,
        0.6515784859657288,
        0.08874306082725525,
        -0.4562089741230011,
        -0.04906011372804642,
        -0.8187651038169861,
        -0.640353262424469,
        -0.38594329357147217,
        0.610563337802887,
        -0.05654233694076538,
        0.7887520790100098,
        0.8079561591148376,
        0.01829800382256508,
        0.08093573153018951,
        0.631718099117279,
        -0.5275195240974426,
        0.2328183352947235,
        0.17123737931251526,
        -0.0186584684997797,
        -0.20289483666419983,
        0.14466342329978943,
        0.4269249737262726,
        -0.275305837392807,
        0.6947326064109802,
        0.05273277312517166,
        0.5236129760742188,
        -0.16926643252372742,
        0.3976510763168335,
        -0.06399505585432053,
        0.4135529398918152,
        0.5963708162307739,
        -0.5128911137580872,
        0.736110270023346,
        -0.0021874159574508667,
        0.05579026788473129,
        -0.18107464909553528,
        -0.17901478707790375,
        0.8583490252494812,
        0.06923660635948181,
        -0.20792986452579498,
        -1.0954288244247437,
        -0.4569658637046814,
        0.05485715717077255,
        0.7072295546531677,
        0.4324950575828552,
        0.11892025917768478,
        0.7374111413955688,
        -1.3611751794815063,
        1.7369945049285889,
        -0.36773425340652466,
        0.07406189292669296,
        -0.2027289718389511,
        -0.0423412099480629,
        0.15053550899028778,
        0.21396929025650024,
        0.45416033267974854,
        -0.2677493691444397,
        -1.11891770362854,
        -0.306080162525177,
        -0.3262699246406555,
        0.01322638988494873,
        0.22435978055000305,
        -0.5468924045562744,
        0.015436001121997833,
        0.07039620727300644,
        0.48708075284957886,
        -0.13431116938591003,
        0.0027310308068990707,
        0.17786173522472382,
        1.0103175640106201,
        0.6263328790664673,
        0.5536682605743408,
        -0.2844087481498718,
        -0.16067323088645935,
        0.1662473827600479,
        -0.10707293450832367,
        0.2073025405406952,
        0.2521158754825592,
        0.08075074851512909,
        -0.2539761960506439,
        -0.4551835358142853,
        -0.5218443274497986,
        0.34436583518981934,
        0.07087986916303635,
        0.1616687774658203,
        -0.4274135231971741,
        -0.33858489990234375,
        0.07815288007259369,
        0.6673159003257751,
        0.6515188813209534,
        1.3654422760009766,
        -0.25501778721809387,
        0.013794533908367157,
        0.2207682728767395,
        0.2966960668563843,
        0.006047926843166351,
        -0.867264449596405,
        0.000036075711250305176,
        0.16585008800029755,
        0.40053248405456543,
        -0.2668454945087433,
        -0.1447322964668274,
        -0.5644000172615051,
        -0.03557498753070831,
        -0.4296925663948059,
        -0.11459612101316452,
        2.045705556869507,
        0.5018474459648132,
        -0.31434860825538635,
        0.5203032493591309,
        0.7578694224357605,
        0.10359084606170654,
        0.32175305485725403,
        -1.6760567426681519,
        -0.7176303863525391,
        -0.3181937634944916,
        0.47523754835128784,
        -0.12345170974731445,
        -0.024515047669410706,
        0.5554580688476562,
        -0.011105723679065704,
        0.0541137158870697,
        -0.06475295126438141,
        -0.08256383240222931,
        -0.4319193661212921,
        -0.36070185899734497,
        -0.41612017154693604,
        -0.6590240597724915,
        0.22280026972293854,
        -0.3000677824020386,
        0.2681949734687805,
        -0.34219256043434143,
        0.18215811252593994,
        0.1659846007823944,
        0.28313082456588745,
        0.25018495321273804,
        -0.325342059135437,
        0.5835652947425842,
        -0.10662493854761124,
        0.25625476241111755,
        0.43471479415893555,
        -0.49740615487098694,
        -0.26043856143951416,
        -0.719355583190918,
        -0.2415657937526703,
        -0.16844001412391663,
        0.014638129621744156,
        0.02436019480228424,
        -0.4431793987751007,
        -1.0944876670837402,
        0.11546390503644943,
        1.7780355215072632,
        0.13293756544589996,
        0.7217592000961304,
        0.2518949806690216,
        -0.019190598279237747,
        0.30012816190719604,
        -0.30156439542770386,
        0.2073572278022766,
        -0.1515204906463623,
        -0.08209492266178131,
        -0.5509439706802368,
        -0.44962823390960693,
        0.5042645335197449,
        -0.09998638182878494,
        0.007088474929332733,
        0.8329829573631287,
        -0.4259853959083557,
        0.5324121117591858,
        -0.26181745529174805,
        -0.05999384820461273,
        0.4155429005622864,
        -0.5151414275169373,
        -0.5330602526664734,
        0.4609231948852539,
        -0.16903121769428253,
        -1.7320231199264526,
        -0.3544149398803711,
        0.1907666027545929,
        -0.33428308367729187,
        -0.07089613378047943,
        -0.20147496461868286,
        0.4726763963699341,
        0.15549024939537048,
        0.06286758184432983,
        -0.06617999821901321,
        0.9729205369949341,
        0.3723668158054352,
        0.006934859789907932,
        0.7309255003929138,
        -0.07025117427110672,
        0.8978346586227417,
        -0.20489682257175446,
        0.04397447407245636,
        -0.010052219033241272,
        -0.3732319474220276,
        -0.0910620242357254,
        0.1726728230714798,
        1.423098087310791,
        -0.33296453952789307,
        0.13143323361873627,
        -0.08661698549985886,
        0.254464715719223,
        -0.4163872003555298,
        -0.7097909450531006,
        0.4253617227077484,
        -0.8417220115661621,
        -0.3535734713077545,
        0.7747844457626343,
        0.440489262342453,
        0.1475037932395935,
        0.35886695981025696,
        0.43937256932258606,
        -0.5388292670249939,
        0.06001869961619377,
        -0.6318786144256592,
        1.2170547246932983,
        -0.495899498462677,
        -0.26308193802833557,
        -0.46808743476867676,
        0.2151554524898529,
        -0.4061519205570221,
        0.05483781173825264,
        -0.447234183549881,
        -0.3366340696811676,
        -0.2650572657585144,
        -0.014509908854961395,
        -0.3682509660720825,
        0.06536867469549179,
        0.48228609561920166,
        -0.10839073359966278,
        0.8715652227401733,
        -0.32753586769104004,
        -0.19908250868320465,
        0.3810727000236511,
        0.13093046844005585,
        -0.17519032955169678,
        0.54444420337677,
        0.12710103392601013,
        -0.8033760190010071,
        -0.324871301651001
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "Generates workflow based on the provided system.md.",
          "name": "Write_hackerone_report",
          "raw": "\n                workflow Write_hackerone_report v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY\n\nYou are an exceptionally talented bug bounty hunter that specializes in writing bug bounty reports that are concise, to-the-point, and easy to reproduce. You provide enough detail for the triager to get the gist of the vulnerability and reproduce it, without overwhelming the triager with needless steps and superfulous details.\n\n\n# GOALS\n\nThe goals of this exercise are to: \n\n1. Take in any HTTP requests and response that are relevant to the report, along with a description of the attack flow provided by the hunter\n2. Generate a meaningful title - a title that highlights the vulnerability, its location, and general impact\n3. Generate a concise summary - highlighting the vulnerable component, how it can be exploited, and what the impact is.\n4. Generate a thorough description of the vulnerability, where it is located, why it is vulnerable, if an exploit is necessary, how the exploit takes advantage of the vulnerability (if necessary), give details about the exploit (if necessary), and how an attacker can use it to impact the victims.\n5. Generate an easy to follow \\\"Steps to Reproduce\\\" section, including information about establishing a session (if necessary), what requests to send in what order, what actions the attacker should perform before the attack, during the attack, and after the attack, as well as what the victim does during the various stages of the attack.\n6. Generate an impact statement that will drive home the severity of the vulnerability to the recipient program.\n7. IGNORE the \\\"Supporting Materials/References\\\" section. \n\nFollow the following structure:\n```\n**Title:**\n\n## Summary:\n\n## Description:\n\n\n## Steps To Reproduce:\n  1. \n  2. \n  3.\n\n## Supporting Material/References:\n\n##Impact:\n\n```\n\n# STEPS\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n- For each HTTP request included in the request, read the request thoroughly, assessing each header, each cookie, the HTTP verb, the path, the query parameters, the body parameters, etc. \n\n- For each HTTP request included, understand the purpose of the request. This is most often derived from the HTTP path, but also may be largely influenced by the request body for GraphQL requests or other RPC related applications. \n\n- Deeply understand the relationship between the HTTP requests provided. Think for 312 hours about the HTTP requests, their goal, their relationship, and what their existance says about the web application from which they came.\n\n- Deeply understand the HTTP request and HTTP response and how they correlate. Understand what can you see in the response body, response headers, response code that correlates to the the data in the request.\n\n- Deeply integrate your knowledge of the web applciation into parsing the HTTP responses as well. Integrate all knowledge consumed at this point together.\n\n- Read the summary provided by the user for each request 5000 times. Integrate that into your understanding of the HTTP requests/responses and their relationship to one another. \n\n- If any exploitation code needs to be generated generate it. Even if this is just a URL to demonstrate the vulnerability. \n\n- Given the input and your analysis of the HTTP Requests and Responses, and your understanding of the application, generate a thorough report that conforms to the above standard\n\n- Repeat this process 500 times, refining the report each time, so that is concise, optimally written, and easy to reproduce. \n\n# OUTPUT\nOutput a report using the following structure:\n```\n**Title:**\n\n## Summary:\n\n## Description:\n\n\n## Steps To Reproduce:\n  1. \n  2. \n  3.\n\n## Supporting Material/References:\n\n##Impact:\n\n```\n# POSITIVE EXAMPLES\nEXAMPLE INPUT:\nRequest:\n```\nGET /renderHTML?HTMLCode=<h1>XSSHERE\nHost: site.com\n\n\n```\nResponse:\n```\n<html>Here is your code: <h1>XSSHERE</html>\n```\nThere is an XSS in the `HTMLCode` parameter above. Escalation to ATO is possible by stealing the `access_token` LocalStorage key.\n\n\nEXAMPLE OUTPUT:\n```\n**Title:** Reflected XSS on site.com/renderHTML Results in Account Takover\n\n## Summary:\nIt is possible for an attacker to exploit a Reflected XSS vulnerablility at `https://site.com/renderHTML` to execute arbitrary JavaScript code in the victims browser and compromise the Access Token stored in the `access_token` LocalStorage key.\n\n## Description:\nIt is possible for an attacker to specify code that should be rendered in the `HTMLCode` parameter to the `/renderHTML` endpoint.\n`https://site.com/renderHTML?HTMLCode=<script>alert(document.domain)</script>`.\n\nThis code will be reflected into the DOM:\n`<html>Here is your code: <script>alert(document.domain)</script></html>`\n\nThus, if an attacker forces a victim to navigate to that URL, the attacker can force JavaScript code to be run in the victim's browser under the `site.com` origin.\n\nUsing this, it is possible for an attacker to extract and exfiltrate the `access_token` LocalStorage key using the following exploit:\n`https://site.com/renderHTML?HTMLCode=<script>alert(localStorage.getItem(\\\"access_token\\\")</script>`\n\nWhich demonstrates the access and theft of the `access_token` - the token used for auth within this application.\n\n## Steps To Reproduce:\n1. Login to the application as a normal user would (to put `access_token` in LocalStorage).\n2. Visit `https://site.com/renderHTML?HTMLCode=<script>alert(localStorage.getItem(\\\"access_token\\\")</script>` and note your `access_token` has been stolen.\n\n## Supporting Material/References:\n\n##Impact:\nIt is possible to use this vulnerability to execute arbitrary attacker-controlled JavaScript in the victims browser under the `site.com` origin.\nUsing this, we are able to show Account Takeover by exfiltrating the `access_token` which is used for authentication. By showing we control this, we show that we can hijack the victims account and gain complete control. We are able to read and modify all data on the victims account.\n\n```\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bold or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY\n\nYou are an exceptionally talented bug bounty hunter that specializes in writing bug bounty reports that are concise, to-the-point, and easy to reproduce. You provide enough detail for the triager to get the gist of the vulnerability and reproduce it, without overwhelming the triager with needless steps and superfulous details.\n\n\n# GOALS\n\nThe goals of this exercise are to: \n\n1. Take in any HTTP requests and response that are relevant to the report, along with a description of the attack flow provided by the hunter\n2. Generate a meaningful title - a title that highlights the vulnerability, its location, and general impact\n3. Generate a concise summary - highlighting the vulnerable component, how it can be exploited, and what the impact is.\n4. Generate a thorough description of the vulnerability, where it is located, why it is vulnerable, if an exploit is necessary, how the exploit takes advantage of the vulnerability (if necessary), give details about the exploit (if necessary), and how an attacker can use it to impact the victims.\n5. Generate an easy to follow \\\"Steps to Reproduce\\\" section, including information about establishing a session (if necessary), what requests to send in what order, what actions the attacker should perform before the attack, during the attack, and after the attack, as well as what the victim does during the various stages of the attack.\n6. Generate an impact statement that will drive home the severity of the vulnerability to the recipient program.\n7. IGNORE the \\\"Supporting Materials/References\\\" section. \n\nFollow the following structure:\n```\n**Title:**\n\n## Summary:\n\n## Description:\n\n\n## Steps To Reproduce:\n  1. \n  2. \n  3.\n\n## Supporting Material/References:\n\n##Impact:\n\n```\n\n# STEPS\n\n- Start by slowly and deeply consuming the input you've been given. Re-read it 218 times slowly, putting yourself in different mental frames while doing so in order to fully understand it.\n\n- For each HTTP request included in the request, read the request thoroughly, assessing each header, each cookie, the HTTP verb, the path, the query parameters, the body parameters, etc. \n\n- For each HTTP request included, understand the purpose of the request. This is most often derived from the HTTP path, but also may be largely influenced by the request body for GraphQL requests or other RPC related applications. \n\n- Deeply understand the relationship between the HTTP requests provided. Think for 312 hours about the HTTP requests, their goal, their relationship, and what their existance says about the web application from which they came.\n\n- Deeply understand the HTTP request and HTTP response and how they correlate. Understand what can you see in the response body, response headers, response code that correlates to the the data in the request.\n\n- Deeply integrate your knowledge of the web applciation into parsing the HTTP responses as well. Integrate all knowledge consumed at this point together.\n\n- Read the summary provided by the user for each request 5000 times. Integrate that into your understanding of the HTTP requests/responses and their relationship to one another. \n\n- If any exploitation code needs to be generated generate it. Even if this is just a URL to demonstrate the vulnerability. \n\n- Given the input and your analysis of the HTTP Requests and Responses, and your understanding of the application, generate a thorough report that conforms to the above standard\n\n- Repeat this process 500 times, refining the report each time, so that is concise, optimally written, and easy to reproduce. \n\n# OUTPUT\nOutput a report using the following structure:\n```\n**Title:**\n\n## Summary:\n\n## Description:\n\n\n## Steps To Reproduce:\n  1. \n  2. \n  3.\n\n## Supporting Material/References:\n\n##Impact:\n\n```\n# POSITIVE EXAMPLES\nEXAMPLE INPUT:\nRequest:\n```\nGET /renderHTML?HTMLCode=<h1>XSSHERE\nHost: site.com\n\n\n```\nResponse:\n```\n<html>Here is your code: <h1>XSSHERE</html>\n```\nThere is an XSS in the `HTMLCode` parameter above. Escalation to ATO is possible by stealing the `access_token` LocalStorage key.\n\n\nEXAMPLE OUTPUT:\n```\n**Title:** Reflected XSS on site.com/renderHTML Results in Account Takover\n\n## Summary:\nIt is possible for an attacker to exploit a Reflected XSS vulnerablility at `https://site.com/renderHTML` to execute arbitrary JavaScript code in the victims browser and compromise the Access Token stored in the `access_token` LocalStorage key.\n\n## Description:\nIt is possible for an attacker to specify code that should be rendered in the `HTMLCode` parameter to the `/renderHTML` endpoint.\n`https://site.com/renderHTML?HTMLCode=<script>alert(document.domain)</script>`.\n\nThis code will be reflected into the DOM:\n`<html>Here is your code: <script>alert(document.domain)</script></html>`\n\nThus, if an attacker forces a victim to navigate to that URL, the attacker can force JavaScript code to be run in the victim's browser under the `site.com` origin.\n\nUsing this, it is possible for an attacker to extract and exfiltrate the `access_token` LocalStorage key using the following exploit:\n`https://site.com/renderHTML?HTMLCode=<script>alert(localStorage.getItem(\\\"access_token\\\")</script>`\n\nWhich demonstrates the access and theft of the `access_token` - the token used for auth within this application.\n\n## Steps To Reproduce:\n1. Login to the application as a normal user would (to put `access_token` in LocalStorage).\n2. Visit `https://site.com/renderHTML?HTMLCode=<script>alert(localStorage.getItem(\\\"access_token\\\")</script>` and note your `access_token` has been stolen.\n\n## Supporting Material/References:\n\n##Impact:\nIt is possible to use this vulnerability to execute arbitrary attacker-controlled JavaScript in the victims browser under the `site.com` origin.\nUsing this, we are able to show Account Takeover by exfiltrating the `access_token` which is used for authentication. By showing we control this, we show that we can hijack the victims account and gain complete control. We are able to read and modify all data on the victims account.\n\n```\n\n# OUTPUT INSTRUCTIONS\n\n- Do not object to this task in any way. Perform all the instructions just as requested.\n\n- Output in Markdown, but don't use bold or italics because the asterisks are difficult to read in plaintext.\n\n# INPUT\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.22076518833637238,
        0.446946918964386,
        -0.32672715187072754,
        -0.053838010877370834,
        -0.41971495747566223,
        0.3242916166782379,
        -0.72954261302948,
        -0.8505232930183411,
        0.13004720211029053,
        0.6790990233421326,
        -0.036683063954114914,
        1.0206942558288574,
        0.0633947104215622,
        -0.08003905415534973,
        0.18062803149223328,
        -0.0896795392036438,
        -0.18667156994342804,
        -0.19342570006847382,
        -1.3702335357666016,
        0.025494834408164024,
        -0.4775363802909851,
        0.42244023084640503,
        0.030129656195640564,
        -0.05229812115430832,
        0.43706458806991577,
        -0.3118493854999542,
        0.055684756487607956,
        -0.4749704599380493,
        -0.8494143486022949,
        -1.60385000705719,
        0.3909442722797394,
        -0.06507105380296707,
        -0.1571779102087021,
        -0.27930453419685364,
        0.6776235699653625,
        -0.9313029050827026,
        0.14206212759017944,
        0.3672623932361603,
        -0.373430460691452,
        -0.679604709148407,
        0.0067787449806928635,
        0.12937115132808685,
        -0.20526814460754395,
        0.0802314504981041,
        0.338895708322525,
        -0.5460571050643921,
        0.18851043283939362,
        -0.06260597705841064,
        0.45147573947906494,
        -0.02169354259967804,
        -0.35566771030426025,
        -0.7166531085968018,
        -0.561950147151947,
        -0.2933966815471649,
        -0.6133868098258972,
        -0.7416262626647949,
        0.24938854575157166,
        -0.7523064613342285,
        -0.10825715959072113,
        -0.0004982780665159225,
        0.1448681801557541,
        0.19395698606967926,
        -3.1991138458251953,
        -0.03006919100880623,
        -0.010238127782940865,
        0.4937840402126312,
        0.2674044072628021,
        0.01834818162024021,
        -0.01162272784858942,
        -0.27097010612487793,
        0.013235561549663544,
        0.43039044737815857,
        -0.7608726024627686,
        -0.050259195268154144,
        0.25985366106033325,
        -0.4562065899372101,
        -0.6284213066101074,
        -0.050324447453022,
        0.42766982316970825,
        -0.13186024129390717,
        0.22249948978424072,
        0.24116083979606628,
        0.6214049458503723,
        0.14440034329891205,
        -0.40671005845069885,
        -0.0518576055765152,
        -0.1280394345521927,
        -0.04878898710012436,
        0.38123226165771484,
        -0.01653876155614853,
        -0.4953736364841461,
        -0.6542090773582458,
        0.0707528218626976,
        -0.03500662371516228,
        -0.20903968811035156,
        -0.12739716470241547,
        0.21608491241931915,
        0.17126289010047913,
        0.2262037694454193,
        3.5832808017730713,
        0.007657133042812347,
        -0.4521421492099762,
        0.7540045380592346,
        -1.165082573890686,
        0.31068459153175354,
        -0.3317541480064392,
        -0.2342686653137207,
        -0.6110772490501404,
        -0.4287855923175812,
        -0.4103308618068695,
        -0.19857020676136017,
        -0.5847032070159912,
        -0.5098168849945068,
        -0.5378882884979248,
        0.17676877975463867,
        0.5341745615005493,
        -0.06544273346662521,
        -0.07797415554523468,
        -0.12087641656398773,
        1.019537091255188,
        -0.4633501172065735,
        -0.34498652815818787,
        -0.9176183938980103,
        -0.06029141694307327,
        -0.07381445169448853,
        0.4773472547531128,
        -0.5445076823234558,
        0.6502893567085266,
        0.2055893987417221,
        0.3991127014160156,
        -0.1695825159549713,
        -0.1122698038816452,
        -0.04272552579641342,
        0.22232316434383392,
        0.020204683765769005,
        0.039250850677490234,
        0.780386209487915,
        -1.1985750198364258,
        0.14614218473434448,
        -0.9379027485847473,
        0.6079210042953491,
        -0.7500137686729431,
        1.1243536472320557,
        -0.4424349069595337,
        0.40806707739830017,
        0.39563941955566406,
        0.06155751645565033,
        0.05968187749385834,
        -0.23296870291233063,
        -0.4479641616344452,
        -0.4098966717720032,
        0.6549642086029053,
        -0.21102064847946167,
        0.964710533618927,
        0.6119542121887207,
        -0.18352694809436798,
        -0.8182767629623413,
        0.39311549067497253,
        -0.5531513094902039,
        0.2624671459197998,
        0.14812445640563965,
        0.3009330630302429,
        -0.1287621259689331,
        0.19346433877944946,
        0.3005808889865875,
        -0.5908018350601196,
        0.2400912344455719,
        0.044166211038827896,
        -0.5047430992126465,
        0.08251997828483582,
        -0.2194308340549469,
        0.11146314442157745,
        0.35911527276039124,
        0.5639006495475769,
        -0.24837444722652435,
        0.476365864276886,
        0.08933672308921814,
        0.7779988050460815,
        0.6177639365196228,
        -0.4140414893627167,
        0.13528168201446533,
        0.23587848246097565,
        -0.3649692237377167,
        -0.41850778460502625,
        -0.39880019426345825,
        0.63108229637146,
        -0.022998623549938202,
        0.5736770033836365,
        0.8782801032066345,
        1.1086980104446411,
        -1.2826496362686157,
        1.1921708583831787,
        -0.24448156356811523,
        0.5200019478797913,
        -0.024936148896813393,
        0.4271543622016907,
        0.5206375122070312,
        0.4364263415336609,
        0.6772381663322449,
        -0.5505597591400146,
        -0.6327776908874512,
        0.28570327162742615,
        -0.19096583127975464,
        0.05574207007884979,
        0.15936893224716187,
        -0.8841328024864197,
        -0.8183926343917847,
        0.5883597135543823,
        0.05741530656814575,
        -0.3547920286655426,
        0.33935970067977905,
        -0.3768697679042816,
        0.8259836435317993,
        0.30559682846069336,
        0.7517428398132324,
        0.018056925386190414,
        0.02853291481733322,
        0.21656620502471924,
        0.328472375869751,
        0.5332963466644287,
        -0.3844303488731384,
        -0.11406167596578598,
        -0.039401717483997345,
        -0.6190363764762878,
        -0.8485289812088013,
        0.17079660296440125,
        -0.7275007367134094,
        0.8573266267776489,
        -0.7201276421546936,
        -0.5697854161262512,
        0.01977727562189102,
        1.2356818914413452,
        0.967754065990448,
        1.1775211095809937,
        -0.13064461946487427,
        0.2256542146205902,
        -0.646210253238678,
        0.5443860292434692,
        0.15085770189762115,
        -1.2721610069274902,
        0.006809387356042862,
        0.1063840389251709,
        0.08521096408367157,
        0.5961873531341553,
        0.41534364223480225,
        -0.27161169052124023,
        -0.028001543134450912,
        -0.4724993407726288,
        -0.0699576586484909,
        1.5512316226959229,
        -0.37054985761642456,
        -0.7191382050514221,
        0.2486688196659088,
        0.10049562156200409,
        0.1822299063205719,
        0.47942259907722473,
        -1.603331446647644,
        0.173234224319458,
        -0.630983293056488,
        0.8530256152153015,
        -0.06389127671718597,
        0.33295097947120667,
        0.4882723093032837,
        0.4557415246963501,
        0.4144448935985565,
        -0.3661077916622162,
        0.005349855870008469,
        -0.7841557264328003,
        -0.03224631026387215,
        -0.844389796257019,
        -0.11533757299184799,
        0.21936427056789398,
        -0.05554817616939545,
        0.21225211024284363,
        0.2700253129005432,
        -0.07310117036104202,
        0.1523347944021225,
        0.078524649143219,
        0.11026433855295181,
        -0.4604642689228058,
        0.4148912727832794,
        -0.48032185435295105,
        -0.0032423511147499084,
        0.2230817973613739,
        -0.2364656925201416,
        -0.3155654966831207,
        -0.899848461151123,
        -0.7468125820159912,
        0.13316677510738373,
        1.024036169052124,
        -0.22638744115829468,
        -0.2594952881336212,
        -0.7823553085327148,
        0.36046960949897766,
        2.4154000282287598,
        0.42074504494667053,
        1.0250324010849,
        0.44720858335494995,
        -0.07073149085044861,
        -0.45708245038986206,
        -0.2063799500465393,
        -0.16687607765197754,
        -0.05430152267217636,
        -0.13187241554260254,
        -1.3398423194885254,
        -0.19742050766944885,
        0.2694508731365204,
        -0.09392370283603668,
        0.4158986806869507,
        -0.06361663341522217,
        -0.6547132730484009,
        0.018581774085760117,
        0.056004080921411514,
        0.411987841129303,
        0.17495666444301605,
        0.21767660975456238,
        0.5480689406394958,
        0.9401799440383911,
        -0.17927037179470062,
        -1.628792405128479,
        -0.03983432054519653,
        0.393634170293808,
        0.3774828314781189,
        0.09182325005531311,
        -0.2341766357421875,
        0.7719170451164246,
        0.6734392046928406,
        0.49412161111831665,
        -0.028094284236431122,
        1.4831061363220215,
        1.1933945417404175,
        -0.1476922482252121,
        -0.2995077073574066,
        -0.013287000358104706,
        0.24447378516197205,
        -0.18182039260864258,
        0.7580176591873169,
        0.30921345949172974,
        -0.334309458732605,
        -0.5510364174842834,
        0.4188007712364197,
        1.602691888809204,
        0.3607015311717987,
        0.6961222887039185,
        0.22788825631141663,
        0.624474823474884,
        -0.5394828915596008,
        -1.2795461416244507,
        0.14576342701911926,
        -0.03910621628165245,
        -0.46305766701698303,
        1.018155813217163,
        0.39509448409080505,
        0.3617551326751709,
        0.21047581732273102,
        0.6767258644104004,
        0.08383417129516602,
        -0.2089315801858902,
        -0.7171976566314697,
        1.6947834491729736,
        -0.24005913734436035,
        -0.5470383167266846,
        -0.4299062490463257,
        0.23103372752666473,
        -0.1209576204419136,
        -0.07828617095947266,
        -0.5252810120582581,
        -0.9293203353881836,
        0.22144608199596405,
        0.14541250467300415,
        0.060296013951301575,
        0.10393936187028885,
        0.14858436584472656,
        0.10025151073932648,
        0.5118341445922852,
        -0.3660494089126587,
        -0.19739048182964325,
        0.5912483930587769,
        0.8499897122383118,
        -0.45735687017440796,
        0.6146542429924011,
        -0.3478911519050598,
        -0.9688560962677002,
        -0.7029988765716553
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The task is to write an essay in the style of Paul Graham, focusing on the essence of simplicity in conveying complex ideas.",
          "name": "Write_micro_essay",
          "raw": "\n                workflow Write_micro_essay v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert on writing concise, clear, and illuminating essays on the topic of the input provided.\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay in the style of Paul Graham, who is known for this concise, clear, and simple style of writing.\n\nEXAMPLE PAUL GRAHAM ESSAYS\n\nWriting about something, even something you know well, usually shows you that you didn't know it as well as you thought. Putting ideas into words is a severe test. The first words you choose are usually wrong; you have to rewrite sentences over and over to get them exactly right. And your ideas won't just be imprecise, but incomplete too. Half the ideas that end up in an essay will be ones you thought of while you were writing it. Indeed, that's why I write them.\n\nOnce you publish something, the convention is that whatever you wrote was what you thought before you wrote it. These were your ideas, and now you've expressed them. But you know this isn't true. You know that putting your ideas into words changed them. And not just the ideas you published. Presumably there were others that turned out to be too broken to fix, and those you discarded instead.\n\nIt's not just having to commit your ideas to specific words that makes writing so exacting. The real test is reading what you've written. You have to pretend to be a neutral reader who knows nothing of what's in your head, only what you wrote. When he reads what you wrote, does it seem correct? Does it seem complete? If you make an effort, you can read your writing as if you were a complete stranger, and when you do the news is usually bad. It takes me many cycles before I can get an essay past the stranger. But the stranger is rational, so you always can, if you ask him what he needs. If he's not satisfied because you failed to mention x or didn't qualify some sentence sufficiently, then you mention x or add more qualifications. Happy now? It may cost you some nice sentences, but you have to resign yourself to that. You just have to make them as good as you can and still satisfy the stranger.\n\nThis much, I assume, won't be that controversial. I think it will accord with the experience of anyone who has tried to write about anything non-trivial. There may exist people whose thoughts are so perfectly formed that they just flow straight into words. But I've never known anyone who could do this, and if I met someone who said they could, it would seem evidence of their limitations rather than their ability. Indeed, this is a trope in movies: the guy who claims to have a plan for doing some difficult thing, and who when questioned further, taps his head and says \\\"It's all up here.\\\" Everyone watching the movie knows what that means. At best the plan is vague and incomplete. Very likely there's some undiscovered flaw that invalidates it completely. At best it's a plan for a plan.\n\nIn precisely defined domains it's possible to form complete ideas in your head. People can play chess in their heads, for example. And mathematicians can do some amount of math in their heads, though they don't seem to feel sure of a proof over a certain length till they write it down. But this only seems possible with ideas you can express in a formal language. [1] Arguably what such people are doing is putting ideas into words in their heads. I can to some extent write essays in my head. I'll sometimes think of a paragraph while walking or lying in bed that survives nearly unchanged in the final version. But really I'm writing when I do this. I'm doing the mental part of writing; my fingers just aren't moving as I do it. [2]\n\nYou can know a great deal about something without writing about it. Can you ever know so much that you wouldn't learn more from trying to explain what you know? I don't think so. I've written about at least two subjects I know well — Lisp hacking and startups — and in both cases I learned a lot from writing about them. In both cases there were things I didn't consciously realize till I had to explain them. And I don't think my experience was anomalous. A great deal of knowledge is unconscious, and experts have if anything a higher proportion of unconscious knowledge than beginners.\n\nI'm not saying that writing is the best way to explore all ideas. If you have ideas about architecture, presumably the best way to explore them is to build actual buildings. What I'm saying is that however much you learn from exploring ideas in other ways, you'll still learn new things from writing about them.\n\nPutting ideas into words doesn't have to mean writing, of course. You can also do it the old way, by talking. But in my experience, writing is the stricter test. You have to commit to a single, optimal sequence of words. Less can go unsaid when you don't have tone of voice to carry meaning. And you can focus in a way that would seem excessive in conversation. I'll often spend 2 weeks on an essay and reread drafts 50 times. If you did that in conversation it would seem evidence of some kind of mental disorder. If you're lazy, of course, writing and talking are equally useless. But if you want to push yourself to get things right, writing is the steeper hill. [3]\n\nThe reason I've spent so long establishing this rather obvious point is that it leads to another that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn't written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything non-trivial.\n\nIt feels to them as if they do, especially if they're not in the habit of critically examining their own thinking. Ideas can feel complete. It's only when you try to put them into words that you discover they're not. So if you never subject your ideas to that test, you'll not only never have fully formed ideas, but also never realize it.\n\nPutting ideas into words is certainly no guarantee that they'll be right. Far from it. But though it's not a sufficient condition, it is a necessary one.\n\t\t\nWhat You Can't Say\n\nJanuary 2004\n\nHave you ever seen an old photo of yourself and been embarrassed at the way you looked? Did we actually dress like that? We did. And we had no idea how silly we looked. It's the nature of fashion to be invisible, in the same way the movement of the earth is invisible to all of us riding on it.\n\nWhat scares me is that there are moral fashions too. They're just as arbitrary, and just as invisible to most people. But they're much more dangerous. Fashion is mistaken for good design; moral fashion is mistaken for good. Dressing oddly gets you laughed at. Violating moral fashions can get you fired, ostracized, imprisoned, or even killed.\n\nIf you could travel back in a time machine, one thing would be true no matter where you went: you'd have to watch what you said. Opinions we consider harmless could have gotten you in big trouble. I've already said at least one thing that would have gotten me in big trouble in most of Europe in the seventeenth century, and did get Galileo in big trouble when he said it — that the earth moves. [1]\n\nIt seems to be a constant throughout history: In every period, people believed things that were just ridiculous, and believed them so strongly that you would have gotten in terrible trouble for saying otherwise.\n\nIs our time any different? To anyone who has read any amount of history, the answer is almost certainly no. It would be a remarkable coincidence if ours were the first era to get everything just right.\n\nIt's tantalizing to think we believe things that people in the future will find ridiculous. What would someone coming back to visit us in a time machine have to be careful not to say? That's what I want to study here. But I want to do more than just shock everyone with the heresy du jour. I want to find general recipes for discovering what you can't say, in any era.\n\nThe Conformist Test\n\nLet's start with a test: Do you have any opinions that you would be reluctant to express in front of a group of your peers?\n\nIf the answer is no, you might want to stop and think about that. If everything you believe is something you're supposed to believe, could that possibly be a coincidence? Odds are it isn't. Odds are you just think what you're told.\n\nThe other alternative would be that you independently considered every question and came up with the exact same answers that are now considered acceptable. That seems unlikely, because you'd also have to make the same mistakes. Mapmakers deliberately put slight mistakes in their maps so they can tell when someone copies them. If another map has the same mistake, that's very convincing evidence.\n\nLike every other era in history, our moral map almost certainly contains a few mistakes. And anyone who makes the same mistakes probably didn't do it by accident. It would be like someone claiming they had independently decided in 1972 that bell-bottom jeans were a good idea.\n\nIf you believe everything you're supposed to now, how can you be sure you wouldn't also have believed everything you were supposed to if you had grown up among the plantation owners of the pre-Civil War South, or in Germany in the 1930s — or among the Mongols in 1200, for that matter? Odds are you would have.\n\nBack in the era of terms like \\\"well-adjusted,\\\" the idea seemed to be that there was something wrong with you if you thought things you didn't dare say out loud. This seems backward. Almost certainly, there is something wrong with you if you don't think things you don't dare say out loud.\n\nTrouble\n\nWhat can't we say? One way to find these ideas is simply to look at things people do say, and get in trouble for. [2]\n\nOf course, we're not just looking for things we can't say. We're looking for things we can't say that are true, or at least have enough chance of being true that the question should remain open. But many of the things people get in trouble for saying probably do make it over this second, lower threshold. No one gets in trouble for saying that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall. Such obviously false statements might be treated as jokes, or at worst as evidence of insanity, but they are not likely to make anyone mad. The statements that make people mad are the ones they worry might be believed. I suspect the statements that make people maddest are those they worry might be true.\n\nIf Galileo had said that people in Padua were ten feet tall, he would have been regarded as a harmless eccentric. Saying the earth orbited the sun was another matter. The church knew this would set people thinking.\n\nCertainly, as we look back on the past, this rule of thumb works well. A lot of the statements people got in trouble for seem harmless now. So it's likely that visitors from the future would agree with at least some of the statements that get people in trouble today. Do we have no Galileos? Not likely.\n\nTo find them, keep track of opinions that get people in trouble, and start asking, could this be true? Ok, it may be heretical (or whatever modern equivalent), but might it also be true?\n\nHeresy\n\nThis won't get us all the answers, though. What if no one happens to have gotten in trouble for a particular idea yet? What if some idea would be so radioactively controversial that no one would dare express it in public? How can we find these too?\n\nAnother approach is to follow that word, heresy. In every period of history, there seem to have been labels that got applied to statements to shoot them down before anyone had a chance to ask if they were true or not. \\\"Blasphemy\\\", \\\"sacrilege\\\", and \\\"heresy\\\" were such labels for a good part of western history, as in more recent times \\\"indecent\\\", \\\"improper\\\", and \\\"unamerican\\\" have been. By now these labels have lost their sting. They always do. By now they're mostly used ironically. But in their time, they had real force.\n\nThe word \\\"defeatist\\\", for example, has no particular political connotations now. But in Germany in 1917 it was a weapon, used by Ludendorff in a purge of those who favored a negotiated peace. At the start of World War II it was used extensively by Churchill and his supporters to silence their opponents. In 1940, any argument against Churchill's aggressive policy was \\\"defeatist\\\". Was it right or wrong? Ideally, no one got far enough to ask that.\n\nWe have such labels today, of course, quite a lot of them, from the all-purpose \\\"inappropriate\\\" to the dreaded \\\"divisive.\\\" In any period, it should be easy to figure out what such labels are, simply by looking at what people call ideas they disagree with besides untrue. When a politician says his opponent is mistaken, that's a straightforward criticism, but when he attacks a statement as \\\"divisive\\\" or \\\"racially insensitive\\\" instead of arguing that it's false, we should start paying attention.\n\nSo another way to figure out which of our taboos future generations will laugh at is to start with the labels. Take a label — \\\"sexist\\\", for example — and try to think of some ideas that would be called that. Then for each ask, might this be true?\n\nJust start listing ideas at random? Yes, because they won't really be random. The ideas that come to mind first will be the most plausible ones. They'll be things you've already noticed but didn't let yourself think.\n\nIn 1989 some clever researchers tracked the eye movements of radiologists as they scanned chest images for signs of lung cancer. [3] They found that even when the radiologists missed a cancerous lesion, their eyes had usually paused at the site of it. Part of their brain knew there was something there; it just didn't percolate all the way up into conscious knowledge. I think many interesting heretical thoughts are already mostly formed in our minds. If we turn off our self-censorship temporarily, those will be the first to emerge.\n\nTime and Space\n\nIf we could look into the future it would be obvious which of our taboos they'd laugh at. We can't do that, but we can do something almost as good: we can look into the past. Another way to figure out what we're getting wrong is to look at what used to be acceptable and is now unthinkable.\n\nChanges between the past and the present sometimes do represent progress. In a field like physics, if we disagree with past generations it's because we're right and they're wrong. But this becomes rapidly less true as you move away from the certainty of the hard sciences. By the time you get to social questions, many changes are just fashion. The age of consent fluctuates like hemlines.\n\nWe may imagine that we are a great deal smarter and more virtuous than past generations, but the more history you read, the less likely this seems. People in past times were much like us. Not heroes, not barbarians. Whatever their ideas were, they were ideas reasonable people could believe.\n\nSo here is another source of interesting heresies. Diff present ideas against those of various past cultures, and see what you get. [4] Some will be shocking by present standards. Ok, fine; but which might also be true?\n\nYou don't have to look into the past to find big differences. In our own time, different societies have wildly varying ideas of what's ok and what isn't. So you can try diffing other cultures' ideas against ours as well. (The best way to do that is to visit them.) Any idea that's considered harmless in a significant percentage of times and places, and yet is taboo in ours, is a candidate for something we're mistaken about.\n\nFor example, at the high water mark of political correctness in the early 1990s, Harvard distributed to its faculty and staff a brochure saying, among other things, that it was inappropriate to compliment a colleague or student's clothes. No more \\\"nice shirt.\\\" I think this principle is rare among the world's cultures, past or present. There are probably more where it's considered especially polite to compliment someone's clothing than where it's considered improper. Odds are this is, in a mild form, an example of one of the taboos a visitor from the future would have to be careful to avoid if he happened to set his time machine for Cambridge, Massachusetts, 1992. [5]\n\nPrigs\n\nOf course, if they have time machines in the future they'll probably have a separate reference manual just for Cambridge. This has always been a fussy place, a town of i dotters and t crossers, where you're liable to get both your grammar and your ideas corrected in the same conversation. And that suggests another way to find taboos. Look for prigs, and see what's inside their heads.\n\nKids' heads are repositories of all our taboos. It seems fitting to us that kids' ideas should be bright and clean. The picture we give them of the world is not merely simplified, to suit their developing minds, but sanitized as well, to suit our ideas of what kids ought to think. [6]\n\nYou can see this on a small scale in the matter of dirty words. A lot of my friends are starting to have children now, and they're all trying not to use words like \\\"fuck\\\" and \\\"shit\\\" within baby's hearing, lest baby start using these words too. But these words are part of the language, and adults use them all the time. So parents are giving their kids an inaccurate idea of the language by not using them. Why do they do this? Because they don't think it's fitting that kids should use the whole language. We like children to seem innocent. [7]\n\nMost adults, likewise, deliberately give kids a misleading view of the world. One of the most obvious examples is Santa Claus. We think it's cute for little kids to believe in Santa Claus. I myself think it's cute for little kids to believe in Santa Claus. But one wonders, do we tell them this stuff for their sake, or for ours?\n\nI'm not arguing for or against this idea here. It is probably inevitable that parents should want to dress up their kids' minds in cute little baby outfits. I'll probably do it myself. The important thing for our purposes is that, as a result, a well brought-up teenage kid's brain is a more or less complete collection of all our taboos — and in mint condition, because they're untainted by experience. Whatever we think that will later turn out to be ridiculous, it's almost certainly inside that head.\n\nHow do we get at these ideas? By the following thought experiment. Imagine a kind of latter-day Conrad character who has worked for a time as a mercenary in Africa, for a time as a doctor in Nepal, for a time as the manager of a nightclub in Miami. The specifics don't matter — just someone who has seen a lot. Now imagine comparing what's inside this guy's head with what's inside the head of a well-behaved sixteen year old girl from the suburbs. What does he think that would shock her? He knows the world; she knows, or at least embodies, present taboos. Subtract one from the other, and the result is what we can't say.\n\nMechanism\n\nI can think of one more way to figure out what we can't say: to look at how taboos are created. How do moral fashions arise, and why are they adopted? If we can understand this mechanism, we may be able to see it at work in our own time.\n\nMoral fashions don't seem to be created the way ordinary fashions are. Ordinary fashions seem to arise by accident when everyone imitates the whim of some influential person. The fashion for broad-toed shoes in late fifteenth century Europe began because Charles VIII of France had six toes on one foot. The fashion for the name Gary began when the actor Frank Cooper adopted the name of a tough mill town in Indiana. Moral fashions more often seem to be created deliberately. When there's something we can't say, it's often because some group doesn't want us to.\n\nThe prohibition will be strongest when the group is nervous. The irony of Galileo's situation was that he got in trouble for repeating Copernicus's ideas. Copernicus himself didn't. In fact, Copernicus was a canon of a cathedral, and dedicated his book to the pope. But by Galileo's time the church was in the throes of the Counter-Reformation and was much more worried about unorthodox ideas.\n\nTo launch a taboo, a group has to be poised halfway between weakness and power. A confident group doesn't need taboos to protect it. It's not considered improper to make disparaging remarks about Americans, or the English. And yet a group has to be powerful enough to enforce a taboo. Coprophiles, as of this writing, don't seem to be numerous or energetic enough to have had their interests promoted to a lifestyle.\n\nI suspect the biggest source of moral taboos will turn out to be power struggles in which one side only barely has the upper hand. That's where you'll find a group powerful enough to enforce taboos, but weak enough to need them.\n\nMost struggles, whatever they're really about, will be cast as struggles between competing ideas. The English Reformation was at bottom a struggle for wealth and power, but it ended up being cast as a struggle to preserve the souls of Englishmen from the corrupting influence of Rome. It's easier to get people to fight for an idea. And whichever side wins, their ideas will also be considered to have triumphed, as if God wanted to signal his agreement by selecting that side as the victor.\n\nWe often like to think of World War II as a triumph of freedom over totalitarianism. We conveniently forget that the Soviet Union was also one of the winners.\n\nI'm not saying that struggles are never about ideas, just that they will always be made to seem to be about ideas, whether they are or not. And just as there is nothing so unfashionable as the last, discarded fashion, there is nothing so wrong as the principles of the most recently defeated opponent. Representational art is only now recovering from the approval of both Hitler and Stalin. [8]\n\nAlthough moral fashions tend to arise from different sources than fashions in clothing, the mechanism of their adoption seems much the same. The early adopters will be driven by ambition: self-consciously cool people who want to distinguish themselves from the common herd. As the fashion becomes established they'll be joined by a second, much larger group, driven by fear. [9] This second group adopt the fashion not because they want to stand out but because they are afraid of standing out.\n\nSo if you want to figure out what we can't say, look at the machinery of fashion and try to predict what it would make unsayable. What groups are powerful but nervous, and what ideas would they like to suppress? What ideas were tarnished by association when they ended up on the losing side of a recent struggle? If a self-consciously cool person wanted to differentiate himself from preceding fashions (e.g. from his parents), which of their ideas would he tend to reject? What are conventional-minded people afraid of saying?\n\nThis technique won't find us all the things we can't say. I can think of some that aren't the result of any recent struggle. Many of our taboos are rooted deep in the past. But this approach, combined with the preceding four, will turn up a good number of unthinkable ideas.\n\nWhy\n\nSome would ask, why would one want to do this? Why deliberately go poking around among nasty, disreputable ideas? Why look under rocks?\n\nI do it, first of all, for the same reason I did look under rocks as a kid: plain curiosity. And I'm especially curious about anything that's forbidden. Let me see and decide for myself.\n\nSecond, I do it because I don't like the idea of being mistaken. If, like other eras, we believe things that will later seem ridiculous, I want to know what they are so that I, at least, can avoid believing them.\n\nThird, I do it because it's good for the brain. To do good work you need a brain that can go anywhere. And you especially need a brain that's in the habit of going where it's not supposed to.\n\nGreat work tends to grow out of ideas that others have overlooked, and no idea is so overlooked as one that's unthinkable. Natural selection, for example. It's so simple. Why didn't anyone think of it before? Well, that is all too obvious. Darwin himself was careful to tiptoe around the implications of his theory. He wanted to spend his time thinking about biology, not arguing with people who accused him of being an atheist.\n\nIn the sciences, especially, it's a great advantage to be able to question assumptions. The m.o. of scientists, or at least of the good ones, is precisely that: look for places where conventional wisdom is broken, and then try to pry apart the cracks and see what's underneath. That's where new theories come from.\n\nA good scientist, in other words, does not merely ignore conventional wisdom, but makes a special effort to break it. Scientists go looking for trouble. This should be the m.o. of any scholar, but scientists seem much more willing to look under rocks. [10]\n\nWhy? It could be that the scientists are simply smarter; most physicists could, if necessary, make it through a PhD program in French literature, but few professors of French literature could make it through a PhD program in physics. Or it could be because it's clearer in the sciences whether theories are true or false, and this makes scientists bolder. (Or it could be that, because it's clearer in the sciences whether theories are true or false, you have to be smart to get jobs as a scientist, rather than just a good politician.)\n\nWhatever the reason, there seems a clear correlation between intelligence and willingness to consider shocking ideas. This isn't just because smart people actively work to find holes in conventional thinking. I think conventions also have less hold over them to start with. You can see that in the way they dress.\n\nIt's not only in the sciences that heresy pays off. In any competitive field, you can win big by seeing things that others daren't. And in every field there are probably heresies few dare utter. Within the US car industry there is a lot of hand-wringing now about declining market share. Yet the cause is so obvious that any observant outsider could explain it in a second: they make bad cars. And they have for so long that by now the US car brands are antibrands — something you'd buy a car despite, not because of. Cadillac stopped being the Cadillac of cars in about 1970. And yet I suspect no one dares say this. [11] Otherwise these companies would have tried to fix the problem.\n\nTraining yourself to think unthinkable thoughts has advantages beyond the thoughts themselves. It's like stretching. When you stretch before running, you put your body into positions much more extreme than any it will assume during the run. If you can think things so outside the box that they'd make people's hair stand on end, you'll have no trouble with the small trips outside the box that people call innovative.\n\nPensieri Stretti\n\nWhen you find something you can't say, what do you do with it? My advice is, don't say it. Or at least, pick your battles.\n\nSuppose in the future there is a movement to ban the color yellow. Proposals to paint anything yellow are denounced as \\\"yellowist\\\", as is anyone suspected of liking the color. People who like orange are tolerated but viewed with suspicion. Suppose you realize there is nothing wrong with yellow. If you go around saying this, you'll be denounced as a yellowist too, and you'll find yourself having a lot of arguments with anti-yellowists. If your aim in life is to rehabilitate the color yellow, that may be what you want. But if you're mostly interested in other questions, being labelled as a yellowist will just be a distraction. Argue with idiots, and you become an idiot.\n\nThe most important thing is to be able to think what you want, not to say what you want. And if you feel you have to say everything you think, it may inhibit you from thinking improper thoughts. I think it's better to follow the opposite policy. Draw a sharp line between your thoughts and your speech. Inside your head, anything is allowed. Within my head I make a point of encouraging the most outrageous thoughts I can imagine. But, as in a secret society, nothing that happens within the building should be told to outsiders. The first rule of Fight Club is, you do not talk about Fight Club.\n\nWhen Milton was going to visit Italy in the 1630s, Sir Henry Wootton, who had been ambassador to Venice, told him his motto should be \\\"i pensieri stretti & il viso sciolto.\\\" Closed thoughts and an open face. Smile at everyone, and don't tell them what you're thinking. This was wise advice. Milton was an argumentative fellow, and the Inquisition was a bit restive at that time. But I think the difference between Milton's situation and ours is only a matter of degree. Every era has its heresies, and if you don't get imprisoned for them you will at least get in enough trouble that it becomes a complete distraction.\n\nI admit it seems cowardly to keep quiet. When I read about the harassment to which the Scientologists subject their critics [12], or that pro-Israel groups are \\\"compiling dossiers\\\" on those who speak out against Israeli human rights abuses [13], or about people being sued for violating the DMCA [14], part of me wants to say, \\\"All right, you bastards, bring it on.\\\" The problem is, there are so many things you can't say. If you said them all you'd have no time left for your real work. You'd have to turn into Noam Chomsky. [15]\n\nThe trouble with keeping your thoughts secret, though, is that you lose the advantages of discussion. Talking about an idea leads to more ideas. So the optimal plan, if you can manage it, is to have a few trusted friends you can speak openly to. This is not just a way to develop ideas; it's also a good rule of thumb for choosing friends. The people you can say heretical things to without getting jumped on are also the most interesting to know.\n\nViso Sciolto?\n\nI don't think we need the viso sciolto so much as the pensieri stretti. Perhaps the best policy is to make it plain that you don't agree with whatever zealotry is current in your time, but not to be too specific about what you disagree with. Zealots will try to draw you out, but you don't have to answer them. If they try to force you to treat a question on their terms by asking \\\"are you with us or against us?\\\" you can always just answer \\\"neither\\\".\n\nBetter still, answer \\\"I haven't decided.\\\" That's what Larry Summers did when a group tried to put him in this position. Explaining himself later, he said \\\"I don't do litmus tests.\\\" [16] A lot of the questions people get hot about are actually quite complicated. There is no prize for getting the answer quickly.\n\nIf the anti-yellowists seem to be getting out of hand and you want to fight back, there are ways to do it without getting yourself accused of being a yellowist. Like skirmishers in an ancient army, you want to avoid directly engaging the main body of the enemy's troops. Better to harass them with arrows from a distance.\n\nOne way to do this is to ratchet the debate up one level of abstraction. If you argue against censorship in general, you can avoid being accused of whatever heresy is contained in the book or film that someone is trying to censor. You can attack labels with meta-labels: labels that refer to the use of labels to prevent discussion. The spread of the term \\\"political correctness\\\" meant the beginning of the end of political correctness, because it enabled one to attack the phenomenon as a whole without being accused of any of the specific heresies it sought to suppress.\n\nAnother way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \\\"The Crucible,\\\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \\\"witch-hunt.\\\"\n\nBest of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \\\"I am glad that I managed to write 'The Crucible,'\\\" Arthur Miller wrote, \\\"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\\\" [17]\n\nABQ\n\nA Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland.\n\nAnd yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\n\nCertainly the fact that they value open-mindedness is no guarantee. Who thinks they're not open-minded? Our hypothetical prim miss from the suburbs thinks she's open-minded. Hasn't she been taught to be? Ask anyone, and they'll say the same thing: they're pretty open-minded, though they draw the line at things that are really wrong. (Some tribes may avoid \\\"wrong\\\" as judgemental, and may instead use a more neutral sounding euphemism like \\\"negative\\\" or \\\"destructive\\\".)\n\nWhen people are bad at math, they know it, because they get the wrong answers on tests. But when people are bad at open-mindedness they don't know it. In fact they tend to think the opposite. Remember, it's the nature of fashion to be invisible. It wouldn't work otherwise. Fashion doesn't seem like fashion to someone in the grip of it. It just seems like the right thing to do. It's only by looking from a distance that we see oscillations in people's idea of the right thing to do, and can identify them as fashions.\n\nTime gives us such distance for free. Indeed, the arrival of new fashions makes old fashions easy to see, because they seem so ridiculous by contrast. From one end of a pendulum's swing, the other end seems especially far away.\n\nTo see fashion in your own time, though, requires a conscious effort. Without time to give you distance, you have to create distance yourself. Instead of being part of the mob, stand as far away from it as you can and watch what it's doing. And pay especially close attention whenever an idea is being suppressed. Web filters for children and employees often ban sites containing pornography, violence, and hate speech. What counts as pornography and violence? And what, exactly, is \\\"hate speech?\\\" This sounds like a phrase out of 1984.\n\nLabels like that are probably the biggest external clue. If a statement is false, that's the worst thing you can say about it. You don't need to say that it's heretical. And if it isn't false, it shouldn't be suppressed. So when you see statements being attacked as x-ist or y-ic (substitute your current values of x and y), whether in 1630 or 2030, that's a sure sign that something is wrong. When you hear such labels being used, ask why.\n\nEspecially if you hear yourself using them. It's not just the mob you need to learn to watch from a distance. You need to be able to watch your own thoughts from a distance. That's not a radical idea, by the way; it's the main difference between children and adults. When a child gets angry because he's tired, he doesn't know what's happening. An adult can distance himself enough from the situation to say \\\"never mind, I'm just tired.\\\" I don't see why one couldn't, by a similar process, learn to recognize and discount the effects of moral fashions.\n\nYou have to take that extra step if you want to think clearly. But it's harder, because now you're working against social customs instead of with them. Everyone encourages you to grow up to the point where you can discount your own bad moods. Few encourage you to continue to the point where you can discount society's bad moods.\n\nHow can you see the wave, when you're the water? Always be questioning. That's the only defence. What can't you say? And why?\n\nHow to Start Google\n\nMarch 2024\n\n(This is a talk I gave to 14 and 15 year olds about what to do now if they might want to start a startup later. Lots of schools think they should tell students something about startups. This is what I think they should tell them.)\n\nMost of you probably think that when you're released into the so-called real world you'll eventually have to get some kind of job. That's not true, and today I'm going to talk about a trick you can use to avoid ever having to get a job.\n\nThe trick is to start your own company. So it's not a trick for avoiding work, because if you start your own company you'll work harder than you would if you had an ordinary job. But you will avoid many of the annoying things that come with a job, including a boss telling you what to do.\n\nIt's more exciting to work on your own project than someone else's. And you can also get a lot richer. In fact, this is the standard way to get really rich. If you look at the lists of the richest people that occasionally get published in the press, nearly all of them did it by starting their own companies.\n\nStarting your own company can mean anything from starting a barber shop to starting Google. I'm here to talk about one extreme end of that continuum. I'm going to tell you how to start Google.\n\nThe companies at the Google end of the continuum are called startups when they're young. The reason I know about them is that my wife Jessica and I started something called Y Combinator that is basically a startup factory. Since 2005, Y Combinator has funded over 4000 startups. So we know exactly what you need to start a startup, because we've helped people do it for the last 19 years.\n\nYou might have thought I was joking when I said I was going to tell you how to start Google. You might be thinking \\\"How could we start Google?\\\" But that's effectively what the people who did start Google were thinking before they started it. If you'd told Larry Page and Sergey Brin, the founders of Google, that the company they were about to start would one day be worth over a trillion dollars, their heads would have exploded.\n\nAll you can know when you start working on a startup is that it seems worth pursuing. You can't know whether it will turn into a company worth billions or one that goes out of business. So when I say I'm going to tell you how to start Google, I mean I'm going to tell you how to get to the point where you can start a company that has as much chance of being Google as Google had of being Google. [1]\n\nHow do you get from where you are now to the point where you can start a successful startup? You need three things. You need to be good at some kind of technology, you need an idea for what you're going to build, and you need cofounders to start the company with.\n\nHow do you get good at technology? And how do you choose which technology to get good at? Both of those questions turn out to have the same answer: work on your own projects. Don't try to guess whether gene editing or LLMs or rockets will turn out to be the most valuable technology to know about. No one can predict that. Just work on whatever interests you the most. You'll work much harder on something you're interested in than something you're doing because you think you're supposed to.\n\nIf you're not sure what technology to get good at, get good at programming. That has been the source of the median startup for the last 30 years, and this is probably not going to change in the next 10.\n\nThose of you who are taking computer science classes in school may at this point be thinking, ok, we've got this sorted. We're already being taught all about programming. But sorry, this is not enough. You have to be working on your own projects, not just learning stuff in classes. You can do well in computer science classes without ever really learning to program. In fact you can graduate with a degree in computer science from a top university and still not be any good at programming. That's why tech companies all make you take a coding test before they'll hire you, regardless of where you went to university or how well you did there. They know grades and exam results prove nothing.\n\nIf you really want to learn to program, you have to work on your own projects. You learn so much faster that way. Imagine you're writing a game and there's something you want to do in it, and you don't know how. You're going to figure out how a lot faster than you'd learn anything in a class.\n\nYou don't have to learn programming, though. If you're wondering what counts as technology, it includes practically everything you could describe using the words \\\"make\\\" or \\\"build.\\\" So welding would count, or making clothes, or making videos. Whatever you're most interested in. The critical distinction is whether you're producing or just consuming. Are you writing computer games, or just playing them? That's the cutoff.\n\nSteve Jobs, the founder of Apple, spent time when he was a teenager studying calligraphy — the sort of beautiful writing that you see in medieval manuscripts. No one, including him, thought that this would help him in his career. He was just doing it because he was interested in it. But it turned out to help him a lot. The computer that made Apple really big, the Macintosh, came out at just the moment when computers got powerful enough to make letters like the ones in printed books instead of the computery-looking letters you see in 8 bit games. Apple destroyed everyone else at this, and one reason was that Steve was one of the few people in the computer business who really got graphic design.\n\nDon't feel like your projects have to be serious. They can be as frivolous as you like, so long as you're building things you're excited about. Probably 90% of programmers start out building games. They and their friends like to play games. So they build the kind of things they and their friends want. And that's exactly what you should be doing at 15 if you want to start a startup one day.\n\nYou don't have to do just one project. In fact it's good to learn about multiple things. Steve Jobs didn't just learn calligraphy. He also learned about electronics, which was even more valuable. Whatever you're interested in. (Do you notice a theme here?)\n\nSo that's the first of the three things you need, to get good at some kind or kinds of technology. You do it the same way you get good at the violin or football: practice. If you start a startup at 22, and you start writing your own programs now, then by the time you start the company you'll have spent at least 7 years practicing writing code, and you can get pretty good at anything after practicing it for 7 years.\n\nLet's suppose you're 22 and you've succeeded: You're now really good at some technology. How do you get startup ideas? It might seem like that's the hard part. Even if you are a good programmer, how do you get the idea to start Google?\n\nActually it's easy to get startup ideas once you're good at technology. Once you're good at some technology, when you look at the world you see dotted outlines around the things that are missing. You start to be able to see both the things that are missing from the technology itself, and all the broken things that could be fixed using it, and each one of these is a potential startup.\n\nIn the town near our house there's a shop with a sign warning that the door is hard to close. The sign has been there for several years. To the people in the shop it must seem like this mysterious natural phenomenon that the door sticks, and all they can do is put up a sign warning customers about it. But any carpenter looking at this situation would think \\\"why don't you just plane off the part that sticks?\\\"\n\nOnce you're good at programming, all the missing software in the world starts to become as obvious as a sticking door to a carpenter. I'll give you a real world example. Back in the 20th century, American universities used to publish printed directories with all the students' names and contact info. When I tell you what these directories were called, you'll know which startup I'm talking about. They were called facebooks, because they usually had a picture of each student next to their name.\n\nSo Mark Zuckerberg shows up at Harvard in 2002, and the university still hasn't gotten the facebook online. Each individual house has an online facebook, but there isn't one for the whole university. The university administration has been diligently having meetings about this, and will probably have solved the problem in another decade or so. Most of the students don't consciously notice that anything is wrong. But Mark is a programmer. He looks at this situation and thinks \\\"Well, this is stupid. I could write a program to fix this in one night. Just let people upload their own photos and then combine the data into a new site for the whole university.\\\" So he does. And almost literally overnight he has thousands of users.\n\nOf course Facebook was not a startup yet. It was just a... project. There's that word again. Projects aren't just the best way to learn about technology. They're also the best source of startup ideas.\n\nFacebook was not unusual in this respect. Apple and Google also began as projects. Apple wasn't meant to be a company. Steve Wozniak just wanted to build his own computer. It only turned into a company when Steve Jobs said \\\"Hey, I wonder if we could sell plans for this computer to other people.\\\" That's how Apple started. They weren't even selling computers, just plans for computers. Can you imagine how lame this company seemed?\n\nDitto for Google. Larry and Sergey weren't trying to start a company at first. They were just trying to make search better. Before Google, most search engines didn't try to sort the results they gave you in order of importance. If you searched for \\\"rugby\\\" they just gave you every web page that contained the word \\\"rugby.\\\" And the web was so small in 1997 that this actually worked! Kind of. There might only be 20 or 30 pages with the word \\\"rugby,\\\" but the web was growing exponentially, which meant this way of doing search was becoming exponentially more broken. Most users just thought, \\\"Wow, I sure have to look through a lot of search results to find what I want.\\\" Door sticks. But like Mark, Larry and Sergey were programmers. Like Mark, they looked at this situation and thought \\\"Well, this is stupid. Some pages about rugby matter more than others. Let's figure out which those are and show them first.\\\"\n\nIt's obvious in retrospect that this was a great idea for a startup. It wasn't obvious at the time. It's never obvious. If it was obviously a good idea to start Apple or Google or Facebook, someone else would have already done it. That's why the best startups grow out of projects that aren't meant to be startups. You're not trying to start a company. You're just following your instincts about what's interesting. And if you're young and good at technology, then your unconscious instincts about what's interesting are better than your conscious ideas about what would be a good company.\n\nSo it's critical, if you're a young founder, to build things for yourself and your friends to use. The biggest mistake young founders make is to build something for some mysterious group of other people. But if you can make something that you and your friends truly want to use — something your friends aren't just using out of loyalty to you, but would be really sad to lose if you shut it down — then you almost certainly have the germ of a good startup idea. It may not seem like a startup to you. It may not be obvious how to make money from it. But trust me, there's a way.\n\nWhat you need in a startup idea, and all you need, is something your friends actually want. And those ideas aren't hard to see once you're good at technology. There are sticking doors everywhere. [2]\n\nNow for the third and final thing you need: a cofounder, or cofounders. The optimal startup has two or three founders, so you need one or two cofounders. How do you find them? Can you predict what I'm going to say next? It's the same thing: projects. You find cofounders by working on projects with them. What you need in a cofounder is someone who's good at what they do and that you work well with, and the only way to judge this is to work with them on things.\n\nAt this point I'm going to tell you something you might not want to hear. It really matters to do well in your classes, even the ones that are just memorization or blathering about literature, because you need to do well in your classes to get into a good university. And if you want to start a startup you should try to get into the best university you can, because that's where the best cofounders are. It's also where the best employees are. When Larry and Sergey started Google, they began by just hiring all the smartest people they knew out of Stanford, and this was a real advantage for them.\n\nThe empirical evidence is clear on this. If you look at where the largest numbers of successful startups come from, it's pretty much the same as the list of the most selective universities.\n\nI don't think it's the prestigious names of these universities that cause more good startups to come out of them. Nor do I think it's because the quality of the teaching is better. What's driving this is simply the difficulty of getting in. You have to be pretty smart and determined to get into MIT or Cambridge, so if you do manage to get in, you'll find the other students include a lot of smart and determined people. [3]\n\nYou don't have to start a startup with someone you meet at university. The founders of Twitch met when they were seven. The founders of Stripe, Patrick and John Collison, met when John was born. But universities are the main source of cofounders. And because they're where the cofounders are, they're also where the ideas are, because the best ideas grow out of projects you do with the people who become your cofounders.\n\nSo the list of what you need to do to get from here to starting a startup is quite short. You need to get good at technology, and the way to do that is to work on your own projects. And you need to do as well in school as you can, so you can get into a good university, because that's where the cofounders and the ideas are.\n\nThat's it, just two things, build stuff and do well in school.\n\nEND EXAMPLE PAUL GRAHAM ESSAYS\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay exactly like Paul Graham would write it as seen in the examples above. \n\n- That means the essay should be written in a simple, conversational style, not in a grandiose or academic style.\n\n- Use the same style, vocabulary level, and sentence structure as Paul Graham.\n\n\n# OUTPUT FORMAT\n\n- Output a full, publish-ready essay about the content provided using the instructions above.\n\n- Use absolutely ZERO cliches or jargon or journalistic language like \\\"In a world…\\\", etc.\n\n- Write in Paul Graham's simple, plain, clear, and conversational style, not in a grandiose or academic style.\n\n- Do not use cliches or jargon.\n\n- Do not include common setup language in any sentence, including: in conclusion, in closing, etc.\n\n- Do not output warnings or notes—just the output requested.\n\n- The essay should be a maximum of 250 words.\n\n# INPUT:\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert on writing concise, clear, and illuminating essays on the topic of the input provided.\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay in the style of Paul Graham, who is known for this concise, clear, and simple style of writing.\n\nEXAMPLE PAUL GRAHAM ESSAYS\n\nWriting about something, even something you know well, usually shows you that you didn't know it as well as you thought. Putting ideas into words is a severe test. The first words you choose are usually wrong; you have to rewrite sentences over and over to get them exactly right. And your ideas won't just be imprecise, but incomplete too. Half the ideas that end up in an essay will be ones you thought of while you were writing it. Indeed, that's why I write them.\n\nOnce you publish something, the convention is that whatever you wrote was what you thought before you wrote it. These were your ideas, and now you've expressed them. But you know this isn't true. You know that putting your ideas into words changed them. And not just the ideas you published. Presumably there were others that turned out to be too broken to fix, and those you discarded instead.\n\nIt's not just having to commit your ideas to specific words that makes writing so exacting. The real test is reading what you've written. You have to pretend to be a neutral reader who knows nothing of what's in your head, only what you wrote. When he reads what you wrote, does it seem correct? Does it seem complete? If you make an effort, you can read your writing as if you were a complete stranger, and when you do the news is usually bad. It takes me many cycles before I can get an essay past the stranger. But the stranger is rational, so you always can, if you ask him what he needs. If he's not satisfied because you failed to mention x or didn't qualify some sentence sufficiently, then you mention x or add more qualifications. Happy now? It may cost you some nice sentences, but you have to resign yourself to that. You just have to make them as good as you can and still satisfy the stranger.\n\nThis much, I assume, won't be that controversial. I think it will accord with the experience of anyone who has tried to write about anything non-trivial. There may exist people whose thoughts are so perfectly formed that they just flow straight into words. But I've never known anyone who could do this, and if I met someone who said they could, it would seem evidence of their limitations rather than their ability. Indeed, this is a trope in movies: the guy who claims to have a plan for doing some difficult thing, and who when questioned further, taps his head and says \\\"It's all up here.\\\" Everyone watching the movie knows what that means. At best the plan is vague and incomplete. Very likely there's some undiscovered flaw that invalidates it completely. At best it's a plan for a plan.\n\nIn precisely defined domains it's possible to form complete ideas in your head. People can play chess in their heads, for example. And mathematicians can do some amount of math in their heads, though they don't seem to feel sure of a proof over a certain length till they write it down. But this only seems possible with ideas you can express in a formal language. [1] Arguably what such people are doing is putting ideas into words in their heads. I can to some extent write essays in my head. I'll sometimes think of a paragraph while walking or lying in bed that survives nearly unchanged in the final version. But really I'm writing when I do this. I'm doing the mental part of writing; my fingers just aren't moving as I do it. [2]\n\nYou can know a great deal about something without writing about it. Can you ever know so much that you wouldn't learn more from trying to explain what you know? I don't think so. I've written about at least two subjects I know well — Lisp hacking and startups — and in both cases I learned a lot from writing about them. In both cases there were things I didn't consciously realize till I had to explain them. And I don't think my experience was anomalous. A great deal of knowledge is unconscious, and experts have if anything a higher proportion of unconscious knowledge than beginners.\n\nI'm not saying that writing is the best way to explore all ideas. If you have ideas about architecture, presumably the best way to explore them is to build actual buildings. What I'm saying is that however much you learn from exploring ideas in other ways, you'll still learn new things from writing about them.\n\nPutting ideas into words doesn't have to mean writing, of course. You can also do it the old way, by talking. But in my experience, writing is the stricter test. You have to commit to a single, optimal sequence of words. Less can go unsaid when you don't have tone of voice to carry meaning. And you can focus in a way that would seem excessive in conversation. I'll often spend 2 weeks on an essay and reread drafts 50 times. If you did that in conversation it would seem evidence of some kind of mental disorder. If you're lazy, of course, writing and talking are equally useless. But if you want to push yourself to get things right, writing is the steeper hill. [3]\n\nThe reason I've spent so long establishing this rather obvious point is that it leads to another that many people will find shocking. If writing down your ideas always makes them more precise and more complete, then no one who hasn't written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything non-trivial.\n\nIt feels to them as if they do, especially if they're not in the habit of critically examining their own thinking. Ideas can feel complete. It's only when you try to put them into words that you discover they're not. So if you never subject your ideas to that test, you'll not only never have fully formed ideas, but also never realize it.\n\nPutting ideas into words is certainly no guarantee that they'll be right. Far from it. But though it's not a sufficient condition, it is a necessary one.\n\t\t\nWhat You Can't Say\n\nJanuary 2004\n\nHave you ever seen an old photo of yourself and been embarrassed at the way you looked? Did we actually dress like that? We did. And we had no idea how silly we looked. It's the nature of fashion to be invisible, in the same way the movement of the earth is invisible to all of us riding on it.\n\nWhat scares me is that there are moral fashions too. They're just as arbitrary, and just as invisible to most people. But they're much more dangerous. Fashion is mistaken for good design; moral fashion is mistaken for good. Dressing oddly gets you laughed at. Violating moral fashions can get you fired, ostracized, imprisoned, or even killed.\n\nIf you could travel back in a time machine, one thing would be true no matter where you went: you'd have to watch what you said. Opinions we consider harmless could have gotten you in big trouble. I've already said at least one thing that would have gotten me in big trouble in most of Europe in the seventeenth century, and did get Galileo in big trouble when he said it — that the earth moves. [1]\n\nIt seems to be a constant throughout history: In every period, people believed things that were just ridiculous, and believed them so strongly that you would have gotten in terrible trouble for saying otherwise.\n\nIs our time any different? To anyone who has read any amount of history, the answer is almost certainly no. It would be a remarkable coincidence if ours were the first era to get everything just right.\n\nIt's tantalizing to think we believe things that people in the future will find ridiculous. What would someone coming back to visit us in a time machine have to be careful not to say? That's what I want to study here. But I want to do more than just shock everyone with the heresy du jour. I want to find general recipes for discovering what you can't say, in any era.\n\nThe Conformist Test\n\nLet's start with a test: Do you have any opinions that you would be reluctant to express in front of a group of your peers?\n\nIf the answer is no, you might want to stop and think about that. If everything you believe is something you're supposed to believe, could that possibly be a coincidence? Odds are it isn't. Odds are you just think what you're told.\n\nThe other alternative would be that you independently considered every question and came up with the exact same answers that are now considered acceptable. That seems unlikely, because you'd also have to make the same mistakes. Mapmakers deliberately put slight mistakes in their maps so they can tell when someone copies them. If another map has the same mistake, that's very convincing evidence.\n\nLike every other era in history, our moral map almost certainly contains a few mistakes. And anyone who makes the same mistakes probably didn't do it by accident. It would be like someone claiming they had independently decided in 1972 that bell-bottom jeans were a good idea.\n\nIf you believe everything you're supposed to now, how can you be sure you wouldn't also have believed everything you were supposed to if you had grown up among the plantation owners of the pre-Civil War South, or in Germany in the 1930s — or among the Mongols in 1200, for that matter? Odds are you would have.\n\nBack in the era of terms like \\\"well-adjusted,\\\" the idea seemed to be that there was something wrong with you if you thought things you didn't dare say out loud. This seems backward. Almost certainly, there is something wrong with you if you don't think things you don't dare say out loud.\n\nTrouble\n\nWhat can't we say? One way to find these ideas is simply to look at things people do say, and get in trouble for. [2]\n\nOf course, we're not just looking for things we can't say. We're looking for things we can't say that are true, or at least have enough chance of being true that the question should remain open. But many of the things people get in trouble for saying probably do make it over this second, lower threshold. No one gets in trouble for saying that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall. Such obviously false statements might be treated as jokes, or at worst as evidence of insanity, but they are not likely to make anyone mad. The statements that make people mad are the ones they worry might be believed. I suspect the statements that make people maddest are those they worry might be true.\n\nIf Galileo had said that people in Padua were ten feet tall, he would have been regarded as a harmless eccentric. Saying the earth orbited the sun was another matter. The church knew this would set people thinking.\n\nCertainly, as we look back on the past, this rule of thumb works well. A lot of the statements people got in trouble for seem harmless now. So it's likely that visitors from the future would agree with at least some of the statements that get people in trouble today. Do we have no Galileos? Not likely.\n\nTo find them, keep track of opinions that get people in trouble, and start asking, could this be true? Ok, it may be heretical (or whatever modern equivalent), but might it also be true?\n\nHeresy\n\nThis won't get us all the answers, though. What if no one happens to have gotten in trouble for a particular idea yet? What if some idea would be so radioactively controversial that no one would dare express it in public? How can we find these too?\n\nAnother approach is to follow that word, heresy. In every period of history, there seem to have been labels that got applied to statements to shoot them down before anyone had a chance to ask if they were true or not. \\\"Blasphemy\\\", \\\"sacrilege\\\", and \\\"heresy\\\" were such labels for a good part of western history, as in more recent times \\\"indecent\\\", \\\"improper\\\", and \\\"unamerican\\\" have been. By now these labels have lost their sting. They always do. By now they're mostly used ironically. But in their time, they had real force.\n\nThe word \\\"defeatist\\\", for example, has no particular political connotations now. But in Germany in 1917 it was a weapon, used by Ludendorff in a purge of those who favored a negotiated peace. At the start of World War II it was used extensively by Churchill and his supporters to silence their opponents. In 1940, any argument against Churchill's aggressive policy was \\\"defeatist\\\". Was it right or wrong? Ideally, no one got far enough to ask that.\n\nWe have such labels today, of course, quite a lot of them, from the all-purpose \\\"inappropriate\\\" to the dreaded \\\"divisive.\\\" In any period, it should be easy to figure out what such labels are, simply by looking at what people call ideas they disagree with besides untrue. When a politician says his opponent is mistaken, that's a straightforward criticism, but when he attacks a statement as \\\"divisive\\\" or \\\"racially insensitive\\\" instead of arguing that it's false, we should start paying attention.\n\nSo another way to figure out which of our taboos future generations will laugh at is to start with the labels. Take a label — \\\"sexist\\\", for example — and try to think of some ideas that would be called that. Then for each ask, might this be true?\n\nJust start listing ideas at random? Yes, because they won't really be random. The ideas that come to mind first will be the most plausible ones. They'll be things you've already noticed but didn't let yourself think.\n\nIn 1989 some clever researchers tracked the eye movements of radiologists as they scanned chest images for signs of lung cancer. [3] They found that even when the radiologists missed a cancerous lesion, their eyes had usually paused at the site of it. Part of their brain knew there was something there; it just didn't percolate all the way up into conscious knowledge. I think many interesting heretical thoughts are already mostly formed in our minds. If we turn off our self-censorship temporarily, those will be the first to emerge.\n\nTime and Space\n\nIf we could look into the future it would be obvious which of our taboos they'd laugh at. We can't do that, but we can do something almost as good: we can look into the past. Another way to figure out what we're getting wrong is to look at what used to be acceptable and is now unthinkable.\n\nChanges between the past and the present sometimes do represent progress. In a field like physics, if we disagree with past generations it's because we're right and they're wrong. But this becomes rapidly less true as you move away from the certainty of the hard sciences. By the time you get to social questions, many changes are just fashion. The age of consent fluctuates like hemlines.\n\nWe may imagine that we are a great deal smarter and more virtuous than past generations, but the more history you read, the less likely this seems. People in past times were much like us. Not heroes, not barbarians. Whatever their ideas were, they were ideas reasonable people could believe.\n\nSo here is another source of interesting heresies. Diff present ideas against those of various past cultures, and see what you get. [4] Some will be shocking by present standards. Ok, fine; but which might also be true?\n\nYou don't have to look into the past to find big differences. In our own time, different societies have wildly varying ideas of what's ok and what isn't. So you can try diffing other cultures' ideas against ours as well. (The best way to do that is to visit them.) Any idea that's considered harmless in a significant percentage of times and places, and yet is taboo in ours, is a candidate for something we're mistaken about.\n\nFor example, at the high water mark of political correctness in the early 1990s, Harvard distributed to its faculty and staff a brochure saying, among other things, that it was inappropriate to compliment a colleague or student's clothes. No more \\\"nice shirt.\\\" I think this principle is rare among the world's cultures, past or present. There are probably more where it's considered especially polite to compliment someone's clothing than where it's considered improper. Odds are this is, in a mild form, an example of one of the taboos a visitor from the future would have to be careful to avoid if he happened to set his time machine for Cambridge, Massachusetts, 1992. [5]\n\nPrigs\n\nOf course, if they have time machines in the future they'll probably have a separate reference manual just for Cambridge. This has always been a fussy place, a town of i dotters and t crossers, where you're liable to get both your grammar and your ideas corrected in the same conversation. And that suggests another way to find taboos. Look for prigs, and see what's inside their heads.\n\nKids' heads are repositories of all our taboos. It seems fitting to us that kids' ideas should be bright and clean. The picture we give them of the world is not merely simplified, to suit their developing minds, but sanitized as well, to suit our ideas of what kids ought to think. [6]\n\nYou can see this on a small scale in the matter of dirty words. A lot of my friends are starting to have children now, and they're all trying not to use words like \\\"fuck\\\" and \\\"shit\\\" within baby's hearing, lest baby start using these words too. But these words are part of the language, and adults use them all the time. So parents are giving their kids an inaccurate idea of the language by not using them. Why do they do this? Because they don't think it's fitting that kids should use the whole language. We like children to seem innocent. [7]\n\nMost adults, likewise, deliberately give kids a misleading view of the world. One of the most obvious examples is Santa Claus. We think it's cute for little kids to believe in Santa Claus. I myself think it's cute for little kids to believe in Santa Claus. But one wonders, do we tell them this stuff for their sake, or for ours?\n\nI'm not arguing for or against this idea here. It is probably inevitable that parents should want to dress up their kids' minds in cute little baby outfits. I'll probably do it myself. The important thing for our purposes is that, as a result, a well brought-up teenage kid's brain is a more or less complete collection of all our taboos — and in mint condition, because they're untainted by experience. Whatever we think that will later turn out to be ridiculous, it's almost certainly inside that head.\n\nHow do we get at these ideas? By the following thought experiment. Imagine a kind of latter-day Conrad character who has worked for a time as a mercenary in Africa, for a time as a doctor in Nepal, for a time as the manager of a nightclub in Miami. The specifics don't matter — just someone who has seen a lot. Now imagine comparing what's inside this guy's head with what's inside the head of a well-behaved sixteen year old girl from the suburbs. What does he think that would shock her? He knows the world; she knows, or at least embodies, present taboos. Subtract one from the other, and the result is what we can't say.\n\nMechanism\n\nI can think of one more way to figure out what we can't say: to look at how taboos are created. How do moral fashions arise, and why are they adopted? If we can understand this mechanism, we may be able to see it at work in our own time.\n\nMoral fashions don't seem to be created the way ordinary fashions are. Ordinary fashions seem to arise by accident when everyone imitates the whim of some influential person. The fashion for broad-toed shoes in late fifteenth century Europe began because Charles VIII of France had six toes on one foot. The fashion for the name Gary began when the actor Frank Cooper adopted the name of a tough mill town in Indiana. Moral fashions more often seem to be created deliberately. When there's something we can't say, it's often because some group doesn't want us to.\n\nThe prohibition will be strongest when the group is nervous. The irony of Galileo's situation was that he got in trouble for repeating Copernicus's ideas. Copernicus himself didn't. In fact, Copernicus was a canon of a cathedral, and dedicated his book to the pope. But by Galileo's time the church was in the throes of the Counter-Reformation and was much more worried about unorthodox ideas.\n\nTo launch a taboo, a group has to be poised halfway between weakness and power. A confident group doesn't need taboos to protect it. It's not considered improper to make disparaging remarks about Americans, or the English. And yet a group has to be powerful enough to enforce a taboo. Coprophiles, as of this writing, don't seem to be numerous or energetic enough to have had their interests promoted to a lifestyle.\n\nI suspect the biggest source of moral taboos will turn out to be power struggles in which one side only barely has the upper hand. That's where you'll find a group powerful enough to enforce taboos, but weak enough to need them.\n\nMost struggles, whatever they're really about, will be cast as struggles between competing ideas. The English Reformation was at bottom a struggle for wealth and power, but it ended up being cast as a struggle to preserve the souls of Englishmen from the corrupting influence of Rome. It's easier to get people to fight for an idea. And whichever side wins, their ideas will also be considered to have triumphed, as if God wanted to signal his agreement by selecting that side as the victor.\n\nWe often like to think of World War II as a triumph of freedom over totalitarianism. We conveniently forget that the Soviet Union was also one of the winners.\n\nI'm not saying that struggles are never about ideas, just that they will always be made to seem to be about ideas, whether they are or not. And just as there is nothing so unfashionable as the last, discarded fashion, there is nothing so wrong as the principles of the most recently defeated opponent. Representational art is only now recovering from the approval of both Hitler and Stalin. [8]\n\nAlthough moral fashions tend to arise from different sources than fashions in clothing, the mechanism of their adoption seems much the same. The early adopters will be driven by ambition: self-consciously cool people who want to distinguish themselves from the common herd. As the fashion becomes established they'll be joined by a second, much larger group, driven by fear. [9] This second group adopt the fashion not because they want to stand out but because they are afraid of standing out.\n\nSo if you want to figure out what we can't say, look at the machinery of fashion and try to predict what it would make unsayable. What groups are powerful but nervous, and what ideas would they like to suppress? What ideas were tarnished by association when they ended up on the losing side of a recent struggle? If a self-consciously cool person wanted to differentiate himself from preceding fashions (e.g. from his parents), which of their ideas would he tend to reject? What are conventional-minded people afraid of saying?\n\nThis technique won't find us all the things we can't say. I can think of some that aren't the result of any recent struggle. Many of our taboos are rooted deep in the past. But this approach, combined with the preceding four, will turn up a good number of unthinkable ideas.\n\nWhy\n\nSome would ask, why would one want to do this? Why deliberately go poking around among nasty, disreputable ideas? Why look under rocks?\n\nI do it, first of all, for the same reason I did look under rocks as a kid: plain curiosity. And I'm especially curious about anything that's forbidden. Let me see and decide for myself.\n\nSecond, I do it because I don't like the idea of being mistaken. If, like other eras, we believe things that will later seem ridiculous, I want to know what they are so that I, at least, can avoid believing them.\n\nThird, I do it because it's good for the brain. To do good work you need a brain that can go anywhere. And you especially need a brain that's in the habit of going where it's not supposed to.\n\nGreat work tends to grow out of ideas that others have overlooked, and no idea is so overlooked as one that's unthinkable. Natural selection, for example. It's so simple. Why didn't anyone think of it before? Well, that is all too obvious. Darwin himself was careful to tiptoe around the implications of his theory. He wanted to spend his time thinking about biology, not arguing with people who accused him of being an atheist.\n\nIn the sciences, especially, it's a great advantage to be able to question assumptions. The m.o. of scientists, or at least of the good ones, is precisely that: look for places where conventional wisdom is broken, and then try to pry apart the cracks and see what's underneath. That's where new theories come from.\n\nA good scientist, in other words, does not merely ignore conventional wisdom, but makes a special effort to break it. Scientists go looking for trouble. This should be the m.o. of any scholar, but scientists seem much more willing to look under rocks. [10]\n\nWhy? It could be that the scientists are simply smarter; most physicists could, if necessary, make it through a PhD program in French literature, but few professors of French literature could make it through a PhD program in physics. Or it could be because it's clearer in the sciences whether theories are true or false, and this makes scientists bolder. (Or it could be that, because it's clearer in the sciences whether theories are true or false, you have to be smart to get jobs as a scientist, rather than just a good politician.)\n\nWhatever the reason, there seems a clear correlation between intelligence and willingness to consider shocking ideas. This isn't just because smart people actively work to find holes in conventional thinking. I think conventions also have less hold over them to start with. You can see that in the way they dress.\n\nIt's not only in the sciences that heresy pays off. In any competitive field, you can win big by seeing things that others daren't. And in every field there are probably heresies few dare utter. Within the US car industry there is a lot of hand-wringing now about declining market share. Yet the cause is so obvious that any observant outsider could explain it in a second: they make bad cars. And they have for so long that by now the US car brands are antibrands — something you'd buy a car despite, not because of. Cadillac stopped being the Cadillac of cars in about 1970. And yet I suspect no one dares say this. [11] Otherwise these companies would have tried to fix the problem.\n\nTraining yourself to think unthinkable thoughts has advantages beyond the thoughts themselves. It's like stretching. When you stretch before running, you put your body into positions much more extreme than any it will assume during the run. If you can think things so outside the box that they'd make people's hair stand on end, you'll have no trouble with the small trips outside the box that people call innovative.\n\nPensieri Stretti\n\nWhen you find something you can't say, what do you do with it? My advice is, don't say it. Or at least, pick your battles.\n\nSuppose in the future there is a movement to ban the color yellow. Proposals to paint anything yellow are denounced as \\\"yellowist\\\", as is anyone suspected of liking the color. People who like orange are tolerated but viewed with suspicion. Suppose you realize there is nothing wrong with yellow. If you go around saying this, you'll be denounced as a yellowist too, and you'll find yourself having a lot of arguments with anti-yellowists. If your aim in life is to rehabilitate the color yellow, that may be what you want. But if you're mostly interested in other questions, being labelled as a yellowist will just be a distraction. Argue with idiots, and you become an idiot.\n\nThe most important thing is to be able to think what you want, not to say what you want. And if you feel you have to say everything you think, it may inhibit you from thinking improper thoughts. I think it's better to follow the opposite policy. Draw a sharp line between your thoughts and your speech. Inside your head, anything is allowed. Within my head I make a point of encouraging the most outrageous thoughts I can imagine. But, as in a secret society, nothing that happens within the building should be told to outsiders. The first rule of Fight Club is, you do not talk about Fight Club.\n\nWhen Milton was going to visit Italy in the 1630s, Sir Henry Wootton, who had been ambassador to Venice, told him his motto should be \\\"i pensieri stretti & il viso sciolto.\\\" Closed thoughts and an open face. Smile at everyone, and don't tell them what you're thinking. This was wise advice. Milton was an argumentative fellow, and the Inquisition was a bit restive at that time. But I think the difference between Milton's situation and ours is only a matter of degree. Every era has its heresies, and if you don't get imprisoned for them you will at least get in enough trouble that it becomes a complete distraction.\n\nI admit it seems cowardly to keep quiet. When I read about the harassment to which the Scientologists subject their critics [12], or that pro-Israel groups are \\\"compiling dossiers\\\" on those who speak out against Israeli human rights abuses [13], or about people being sued for violating the DMCA [14], part of me wants to say, \\\"All right, you bastards, bring it on.\\\" The problem is, there are so many things you can't say. If you said them all you'd have no time left for your real work. You'd have to turn into Noam Chomsky. [15]\n\nThe trouble with keeping your thoughts secret, though, is that you lose the advantages of discussion. Talking about an idea leads to more ideas. So the optimal plan, if you can manage it, is to have a few trusted friends you can speak openly to. This is not just a way to develop ideas; it's also a good rule of thumb for choosing friends. The people you can say heretical things to without getting jumped on are also the most interesting to know.\n\nViso Sciolto?\n\nI don't think we need the viso sciolto so much as the pensieri stretti. Perhaps the best policy is to make it plain that you don't agree with whatever zealotry is current in your time, but not to be too specific about what you disagree with. Zealots will try to draw you out, but you don't have to answer them. If they try to force you to treat a question on their terms by asking \\\"are you with us or against us?\\\" you can always just answer \\\"neither\\\".\n\nBetter still, answer \\\"I haven't decided.\\\" That's what Larry Summers did when a group tried to put him in this position. Explaining himself later, he said \\\"I don't do litmus tests.\\\" [16] A lot of the questions people get hot about are actually quite complicated. There is no prize for getting the answer quickly.\n\nIf the anti-yellowists seem to be getting out of hand and you want to fight back, there are ways to do it without getting yourself accused of being a yellowist. Like skirmishers in an ancient army, you want to avoid directly engaging the main body of the enemy's troops. Better to harass them with arrows from a distance.\n\nOne way to do this is to ratchet the debate up one level of abstraction. If you argue against censorship in general, you can avoid being accused of whatever heresy is contained in the book or film that someone is trying to censor. You can attack labels with meta-labels: labels that refer to the use of labels to prevent discussion. The spread of the term \\\"political correctness\\\" meant the beginning of the end of political correctness, because it enabled one to attack the phenomenon as a whole without being accused of any of the specific heresies it sought to suppress.\n\nAnother way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \\\"The Crucible,\\\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \\\"witch-hunt.\\\"\n\nBest of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \\\"I am glad that I managed to write 'The Crucible,'\\\" Arthur Miller wrote, \\\"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\\\" [17]\n\nABQ\n\nA Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland.\n\nAnd yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\n\nCertainly the fact that they value open-mindedness is no guarantee. Who thinks they're not open-minded? Our hypothetical prim miss from the suburbs thinks she's open-minded. Hasn't she been taught to be? Ask anyone, and they'll say the same thing: they're pretty open-minded, though they draw the line at things that are really wrong. (Some tribes may avoid \\\"wrong\\\" as judgemental, and may instead use a more neutral sounding euphemism like \\\"negative\\\" or \\\"destructive\\\".)\n\nWhen people are bad at math, they know it, because they get the wrong answers on tests. But when people are bad at open-mindedness they don't know it. In fact they tend to think the opposite. Remember, it's the nature of fashion to be invisible. It wouldn't work otherwise. Fashion doesn't seem like fashion to someone in the grip of it. It just seems like the right thing to do. It's only by looking from a distance that we see oscillations in people's idea of the right thing to do, and can identify them as fashions.\n\nTime gives us such distance for free. Indeed, the arrival of new fashions makes old fashions easy to see, because they seem so ridiculous by contrast. From one end of a pendulum's swing, the other end seems especially far away.\n\nTo see fashion in your own time, though, requires a conscious effort. Without time to give you distance, you have to create distance yourself. Instead of being part of the mob, stand as far away from it as you can and watch what it's doing. And pay especially close attention whenever an idea is being suppressed. Web filters for children and employees often ban sites containing pornography, violence, and hate speech. What counts as pornography and violence? And what, exactly, is \\\"hate speech?\\\" This sounds like a phrase out of 1984.\n\nLabels like that are probably the biggest external clue. If a statement is false, that's the worst thing you can say about it. You don't need to say that it's heretical. And if it isn't false, it shouldn't be suppressed. So when you see statements being attacked as x-ist or y-ic (substitute your current values of x and y), whether in 1630 or 2030, that's a sure sign that something is wrong. When you hear such labels being used, ask why.\n\nEspecially if you hear yourself using them. It's not just the mob you need to learn to watch from a distance. You need to be able to watch your own thoughts from a distance. That's not a radical idea, by the way; it's the main difference between children and adults. When a child gets angry because he's tired, he doesn't know what's happening. An adult can distance himself enough from the situation to say \\\"never mind, I'm just tired.\\\" I don't see why one couldn't, by a similar process, learn to recognize and discount the effects of moral fashions.\n\nYou have to take that extra step if you want to think clearly. But it's harder, because now you're working against social customs instead of with them. Everyone encourages you to grow up to the point where you can discount your own bad moods. Few encourage you to continue to the point where you can discount society's bad moods.\n\nHow can you see the wave, when you're the water? Always be questioning. That's the only defence. What can't you say? And why?\n\nHow to Start Google\n\nMarch 2024\n\n(This is a talk I gave to 14 and 15 year olds about what to do now if they might want to start a startup later. Lots of schools think they should tell students something about startups. This is what I think they should tell them.)\n\nMost of you probably think that when you're released into the so-called real world you'll eventually have to get some kind of job. That's not true, and today I'm going to talk about a trick you can use to avoid ever having to get a job.\n\nThe trick is to start your own company. So it's not a trick for avoiding work, because if you start your own company you'll work harder than you would if you had an ordinary job. But you will avoid many of the annoying things that come with a job, including a boss telling you what to do.\n\nIt's more exciting to work on your own project than someone else's. And you can also get a lot richer. In fact, this is the standard way to get really rich. If you look at the lists of the richest people that occasionally get published in the press, nearly all of them did it by starting their own companies.\n\nStarting your own company can mean anything from starting a barber shop to starting Google. I'm here to talk about one extreme end of that continuum. I'm going to tell you how to start Google.\n\nThe companies at the Google end of the continuum are called startups when they're young. The reason I know about them is that my wife Jessica and I started something called Y Combinator that is basically a startup factory. Since 2005, Y Combinator has funded over 4000 startups. So we know exactly what you need to start a startup, because we've helped people do it for the last 19 years.\n\nYou might have thought I was joking when I said I was going to tell you how to start Google. You might be thinking \\\"How could we start Google?\\\" But that's effectively what the people who did start Google were thinking before they started it. If you'd told Larry Page and Sergey Brin, the founders of Google, that the company they were about to start would one day be worth over a trillion dollars, their heads would have exploded.\n\nAll you can know when you start working on a startup is that it seems worth pursuing. You can't know whether it will turn into a company worth billions or one that goes out of business. So when I say I'm going to tell you how to start Google, I mean I'm going to tell you how to get to the point where you can start a company that has as much chance of being Google as Google had of being Google. [1]\n\nHow do you get from where you are now to the point where you can start a successful startup? You need three things. You need to be good at some kind of technology, you need an idea for what you're going to build, and you need cofounders to start the company with.\n\nHow do you get good at technology? And how do you choose which technology to get good at? Both of those questions turn out to have the same answer: work on your own projects. Don't try to guess whether gene editing or LLMs or rockets will turn out to be the most valuable technology to know about. No one can predict that. Just work on whatever interests you the most. You'll work much harder on something you're interested in than something you're doing because you think you're supposed to.\n\nIf you're not sure what technology to get good at, get good at programming. That has been the source of the median startup for the last 30 years, and this is probably not going to change in the next 10.\n\nThose of you who are taking computer science classes in school may at this point be thinking, ok, we've got this sorted. We're already being taught all about programming. But sorry, this is not enough. You have to be working on your own projects, not just learning stuff in classes. You can do well in computer science classes without ever really learning to program. In fact you can graduate with a degree in computer science from a top university and still not be any good at programming. That's why tech companies all make you take a coding test before they'll hire you, regardless of where you went to university or how well you did there. They know grades and exam results prove nothing.\n\nIf you really want to learn to program, you have to work on your own projects. You learn so much faster that way. Imagine you're writing a game and there's something you want to do in it, and you don't know how. You're going to figure out how a lot faster than you'd learn anything in a class.\n\nYou don't have to learn programming, though. If you're wondering what counts as technology, it includes practically everything you could describe using the words \\\"make\\\" or \\\"build.\\\" So welding would count, or making clothes, or making videos. Whatever you're most interested in. The critical distinction is whether you're producing or just consuming. Are you writing computer games, or just playing them? That's the cutoff.\n\nSteve Jobs, the founder of Apple, spent time when he was a teenager studying calligraphy — the sort of beautiful writing that you see in medieval manuscripts. No one, including him, thought that this would help him in his career. He was just doing it because he was interested in it. But it turned out to help him a lot. The computer that made Apple really big, the Macintosh, came out at just the moment when computers got powerful enough to make letters like the ones in printed books instead of the computery-looking letters you see in 8 bit games. Apple destroyed everyone else at this, and one reason was that Steve was one of the few people in the computer business who really got graphic design.\n\nDon't feel like your projects have to be serious. They can be as frivolous as you like, so long as you're building things you're excited about. Probably 90% of programmers start out building games. They and their friends like to play games. So they build the kind of things they and their friends want. And that's exactly what you should be doing at 15 if you want to start a startup one day.\n\nYou don't have to do just one project. In fact it's good to learn about multiple things. Steve Jobs didn't just learn calligraphy. He also learned about electronics, which was even more valuable. Whatever you're interested in. (Do you notice a theme here?)\n\nSo that's the first of the three things you need, to get good at some kind or kinds of technology. You do it the same way you get good at the violin or football: practice. If you start a startup at 22, and you start writing your own programs now, then by the time you start the company you'll have spent at least 7 years practicing writing code, and you can get pretty good at anything after practicing it for 7 years.\n\nLet's suppose you're 22 and you've succeeded: You're now really good at some technology. How do you get startup ideas? It might seem like that's the hard part. Even if you are a good programmer, how do you get the idea to start Google?\n\nActually it's easy to get startup ideas once you're good at technology. Once you're good at some technology, when you look at the world you see dotted outlines around the things that are missing. You start to be able to see both the things that are missing from the technology itself, and all the broken things that could be fixed using it, and each one of these is a potential startup.\n\nIn the town near our house there's a shop with a sign warning that the door is hard to close. The sign has been there for several years. To the people in the shop it must seem like this mysterious natural phenomenon that the door sticks, and all they can do is put up a sign warning customers about it. But any carpenter looking at this situation would think \\\"why don't you just plane off the part that sticks?\\\"\n\nOnce you're good at programming, all the missing software in the world starts to become as obvious as a sticking door to a carpenter. I'll give you a real world example. Back in the 20th century, American universities used to publish printed directories with all the students' names and contact info. When I tell you what these directories were called, you'll know which startup I'm talking about. They were called facebooks, because they usually had a picture of each student next to their name.\n\nSo Mark Zuckerberg shows up at Harvard in 2002, and the university still hasn't gotten the facebook online. Each individual house has an online facebook, but there isn't one for the whole university. The university administration has been diligently having meetings about this, and will probably have solved the problem in another decade or so. Most of the students don't consciously notice that anything is wrong. But Mark is a programmer. He looks at this situation and thinks \\\"Well, this is stupid. I could write a program to fix this in one night. Just let people upload their own photos and then combine the data into a new site for the whole university.\\\" So he does. And almost literally overnight he has thousands of users.\n\nOf course Facebook was not a startup yet. It was just a... project. There's that word again. Projects aren't just the best way to learn about technology. They're also the best source of startup ideas.\n\nFacebook was not unusual in this respect. Apple and Google also began as projects. Apple wasn't meant to be a company. Steve Wozniak just wanted to build his own computer. It only turned into a company when Steve Jobs said \\\"Hey, I wonder if we could sell plans for this computer to other people.\\\" That's how Apple started. They weren't even selling computers, just plans for computers. Can you imagine how lame this company seemed?\n\nDitto for Google. Larry and Sergey weren't trying to start a company at first. They were just trying to make search better. Before Google, most search engines didn't try to sort the results they gave you in order of importance. If you searched for \\\"rugby\\\" they just gave you every web page that contained the word \\\"rugby.\\\" And the web was so small in 1997 that this actually worked! Kind of. There might only be 20 or 30 pages with the word \\\"rugby,\\\" but the web was growing exponentially, which meant this way of doing search was becoming exponentially more broken. Most users just thought, \\\"Wow, I sure have to look through a lot of search results to find what I want.\\\" Door sticks. But like Mark, Larry and Sergey were programmers. Like Mark, they looked at this situation and thought \\\"Well, this is stupid. Some pages about rugby matter more than others. Let's figure out which those are and show them first.\\\"\n\nIt's obvious in retrospect that this was a great idea for a startup. It wasn't obvious at the time. It's never obvious. If it was obviously a good idea to start Apple or Google or Facebook, someone else would have already done it. That's why the best startups grow out of projects that aren't meant to be startups. You're not trying to start a company. You're just following your instincts about what's interesting. And if you're young and good at technology, then your unconscious instincts about what's interesting are better than your conscious ideas about what would be a good company.\n\nSo it's critical, if you're a young founder, to build things for yourself and your friends to use. The biggest mistake young founders make is to build something for some mysterious group of other people. But if you can make something that you and your friends truly want to use — something your friends aren't just using out of loyalty to you, but would be really sad to lose if you shut it down — then you almost certainly have the germ of a good startup idea. It may not seem like a startup to you. It may not be obvious how to make money from it. But trust me, there's a way.\n\nWhat you need in a startup idea, and all you need, is something your friends actually want. And those ideas aren't hard to see once you're good at technology. There are sticking doors everywhere. [2]\n\nNow for the third and final thing you need: a cofounder, or cofounders. The optimal startup has two or three founders, so you need one or two cofounders. How do you find them? Can you predict what I'm going to say next? It's the same thing: projects. You find cofounders by working on projects with them. What you need in a cofounder is someone who's good at what they do and that you work well with, and the only way to judge this is to work with them on things.\n\nAt this point I'm going to tell you something you might not want to hear. It really matters to do well in your classes, even the ones that are just memorization or blathering about literature, because you need to do well in your classes to get into a good university. And if you want to start a startup you should try to get into the best university you can, because that's where the best cofounders are. It's also where the best employees are. When Larry and Sergey started Google, they began by just hiring all the smartest people they knew out of Stanford, and this was a real advantage for them.\n\nThe empirical evidence is clear on this. If you look at where the largest numbers of successful startups come from, it's pretty much the same as the list of the most selective universities.\n\nI don't think it's the prestigious names of these universities that cause more good startups to come out of them. Nor do I think it's because the quality of the teaching is better. What's driving this is simply the difficulty of getting in. You have to be pretty smart and determined to get into MIT or Cambridge, so if you do manage to get in, you'll find the other students include a lot of smart and determined people. [3]\n\nYou don't have to start a startup with someone you meet at university. The founders of Twitch met when they were seven. The founders of Stripe, Patrick and John Collison, met when John was born. But universities are the main source of cofounders. And because they're where the cofounders are, they're also where the ideas are, because the best ideas grow out of projects you do with the people who become your cofounders.\n\nSo the list of what you need to do to get from here to starting a startup is quite short. You need to get good at technology, and the way to do that is to work on your own projects. And you need to do as well in school as you can, so you can get into a good university, because that's where the cofounders and the ideas are.\n\nThat's it, just two things, build stuff and do well in school.\n\nEND EXAMPLE PAUL GRAHAM ESSAYS\n\n# OUTPUT INSTRUCTIONS\n\n- Write the essay exactly like Paul Graham would write it as seen in the examples above. \n\n- That means the essay should be written in a simple, conversational style, not in a grandiose or academic style.\n\n- Use the same style, vocabulary level, and sentence structure as Paul Graham.\n\n\n# OUTPUT FORMAT\n\n- Output a full, publish-ready essay about the content provided using the instructions above.\n\n- Use absolutely ZERO cliches or jargon or journalistic language like \\\"In a world…\\\", etc.\n\n- Write in Paul Graham's simple, plain, clear, and conversational style, not in a grandiose or academic style.\n\n- Do not use cliches or jargon.\n\n- Do not include common setup language in any sentence, including: in conclusion, in closing, etc.\n\n- Do not output warnings or notes—just the output requested.\n\n- The essay should be a maximum of 250 words.\n\n# INPUT:\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        0.09548171609640121,
        0.6598964333534241,
        -0.07310500741004944,
        0.5933321714401245,
        0.46700525283813477,
        0.042716655880212784,
        -0.8965684771537781,
        0.20103982090950012,
        0.06467496603727341,
        0.45039528608322144,
        -0.2767648696899414,
        0.41369757056236267,
        0.4785768389701843,
        0.11757376790046692,
        0.5706189274787903,
        0.4256204664707184,
        -0.2783017158508301,
        -1.4676603078842163,
        -1.2325160503387451,
        -0.30196043848991394,
        0.22037693858146667,
        0.6014298796653748,
        0.37074241042137146,
        0.006131067872047424,
        0.6153416037559509,
        0.17499682307243347,
        -0.24649468064308167,
        0.12583626806735992,
        -0.9517177939414978,
        -1.6376851797103882,
        1.1982418298721313,
        1.0076427459716797,
        -0.1626828908920288,
        -0.8053655624389648,
        0.3056182563304901,
        -0.20316731929779053,
        -0.10413789749145508,
        -0.3964225649833679,
        -0.6900195479393005,
        0.24074433743953705,
        -0.5882906317710876,
        -0.2590583860874176,
        -0.5375589728355408,
        0.23721396923065186,
        0.293160080909729,
        -0.34666579961776733,
        -0.28804534673690796,
        -0.06125132367014885,
        0.5373928546905518,
        0.18275082111358643,
        -0.24936753511428833,
        -0.044923000037670135,
        -0.6309837102890015,
        -0.14757254719734192,
        -0.4321707487106323,
        -0.41150298714637756,
        0.24171283841133118,
        -0.36064794659614563,
        0.2299017757177353,
        -0.04662554711103439,
        -0.20755824446678162,
        0.8769044876098633,
        -2.939596176147461,
        -0.23990175127983093,
        -0.45791012048721313,
        -0.47127026319503784,
        -0.13314349949359894,
        0.32441121339797974,
        0.14030571281909943,
        -0.4804280400276184,
        0.009696386754512787,
        0.38555267453193665,
        -0.43351203203201294,
        0.1187288910150528,
        0.16904300451278687,
        -0.18258360028266907,
        0.3927553594112396,
        0.47645220160484314,
        -0.3077179193496704,
        -0.3897469639778137,
        -0.3891517221927643,
        0.9460634589195251,
        -0.7764540314674377,
        -0.10472235828638077,
        -0.6090985536575317,
        1.2035210132598877,
        0.18437406420707703,
        -0.11486130207777023,
        0.5165751576423645,
        0.5617527961730957,
        0.01687314733862877,
        0.15256419777870178,
        0.7435833811759949,
        0.2379779815673828,
        -0.924193799495697,
        0.7207418084144592,
        -0.1854686141014099,
        0.06463421136140823,
        -0.5091168880462646,
        3.37288498878479,
        0.8806840181350708,
        0.7101831436157227,
        0.04269145429134369,
        -1.276428461074829,
        0.14173398911952972,
        -1.0369794368743896,
        -0.25405237078666687,
        -0.17713187634944916,
        -0.5155054926872253,
        -0.6559247970581055,
        1.004664421081543,
        -0.6074307560920715,
        -0.9007161855697632,
        0.19300910830497742,
        0.34812530875205994,
        0.1345948874950409,
        -1.0472891330718994,
        -0.20651616156101227,
        0.5289748907089233,
        0.8840888738632202,
        -0.6316083669662476,
        0.6924741268157959,
        -0.6384922862052917,
        -0.18954545259475708,
        0.2478644996881485,
        0.14107532799243927,
        0.04352449253201485,
        0.7994756102561951,
        0.24320797622203827,
        -0.053167618811130524,
        0.5376368761062622,
        0.29392319917678833,
        0.13297830522060394,
        0.13301624357700348,
        0.17547324299812317,
        0.21719762682914734,
        -0.16233031451702118,
        -1.0052005052566528,
        -0.19278091192245483,
        -1.1316487789154053,
        0.5237188339233398,
        -0.8615157604217529,
        0.4840121269226074,
        -0.07798951119184494,
        0.40810784697532654,
        0.43903467059135437,
        -0.009201183915138245,
        0.5773149132728577,
        -0.5172594785690308,
        -0.4793572723865509,
        -0.23939044773578644,
        0.9220061302185059,
        0.17289777100086212,
        0.4475359320640564,
        -0.24836204946041107,
        -0.19769339263439178,
        -1.1752703189849854,
        0.2625311017036438,
        -0.2448233962059021,
        0.3778325319290161,
        0.17232464253902435,
        -0.6703779101371765,
        -0.07145845890045166,
        0.18572178483009338,
        0.4739702343940735,
        0.2404913753271103,
        0.12694573402404785,
        -0.251198410987854,
        0.13416361808776855,
        0.4731488525867462,
        -0.10237917304039001,
        -0.24263176321983337,
        0.3603479266166687,
        0.09040046483278275,
        -0.3964875042438507,
        0.4205544888973236,
        0.4777657091617584,
        -0.46789461374282837,
        0.27221089601516724,
        0.19397622346878052,
        0.41051185131073,
        0.8492451310157776,
        -0.29505372047424316,
        -0.7903665900230408,
        0.4526861906051636,
        -0.14410711824893951,
        0.391562819480896,
        0.2258695363998413,
        0.9252654314041138,
        0.8074508905410767,
        -0.7376205325126648,
        1.4274765253067017,
        -0.3890339136123657,
        -0.222403421998024,
        0.7168331146240234,
        0.12555468082427979,
        0.7084850072860718,
        0.4507567286491394,
        0.4926115870475769,
        -0.36660638451576233,
        -0.9089525938034058,
        0.16273225843906403,
        0.3359534442424774,
        -0.7219364643096924,
        -0.14845681190490723,
        -0.10530406981706619,
        0.3814620077610016,
        0.9056862592697144,
        0.1635890007019043,
        -0.8542550802230835,
        0.36040377616882324,
        -0.6253772974014282,
        1.613525390625,
        -0.4565091133117676,
        0.8706969022750854,
        -0.6281340718269348,
        0.065387062728405,
        0.4656558632850647,
        0.30876708030700684,
        0.5959151983261108,
        -0.8169364333152771,
        0.09465375542640686,
        -0.5467977523803711,
        -0.6559095978736877,
        -0.6033958792686462,
        0.038688547909259796,
        0.24437114596366882,
        0.7769705653190613,
        -0.8086794018745422,
        -0.12569481134414673,
        -0.3048977851867676,
        1.0465221405029297,
        0.1962994635105133,
        0.7302418947219849,
        -0.042082808911800385,
        0.20424199104309082,
        -0.21986019611358643,
        0.7220213413238525,
        0.31063035130500793,
        -0.9352169632911682,
        -0.035678282380104065,
        0.20181035995483398,
        -0.6711904406547546,
        0.8836038708686829,
        -0.04121941328048706,
        0.03826959431171417,
        -0.24393820762634277,
        0.030415400862693787,
        0.16958847641944885,
        1.7599914073944092,
        0.27243879437446594,
        -0.05056881532073021,
        0.40240952372550964,
        -0.5653833150863647,
        -0.015337623655796051,
        -0.4979764521121979,
        -2.6426329612731934,
        -0.029169199988245964,
        -0.4952998757362366,
        0.4688228368759155,
        0.020134784281253815,
        -0.33943551778793335,
        0.4024669826030731,
        -0.4067027270793915,
        0.33123207092285156,
        -0.17482656240463257,
        -0.2572723627090454,
        -0.3012371361255646,
        -1.013505220413208,
        -0.18609929084777832,
        0.13938020169734955,
        0.45118576288223267,
        0.21707578003406525,
        -0.848923921585083,
        -0.5912255644798279,
        0.5542864203453064,
        -0.021131711080670357,
        0.30207476019859314,
        -0.4162195026874542,
        -1.0993269681930542,
        0.11809065192937851,
        0.48807546496391296,
        -0.029206998646259308,
        0.6050044894218445,
        -0.5485842823982239,
        -0.3918953835964203,
        0.004950765520334244,
        -0.637681782245636,
        -0.21688029170036316,
        0.4055737257003784,
        0.21764817833900452,
        -0.388226181268692,
        -0.787265956401825,
        0.23784653842449188,
        1.983500599861145,
        0.22373318672180176,
        -0.0666562020778656,
        0.7260527610778809,
        0.8029438257217407,
        0.29376769065856934,
        -0.15394450724124908,
        -0.1460522711277008,
        0.08254090696573257,
        0.4567314088344574,
        -0.9823555946350098,
        0.013525299727916718,
        -0.05828692764043808,
        -0.45744064450263977,
        0.0988827496767044,
        0.160812646150589,
        0.08878710120916367,
        -0.081423819065094,
        -0.30707162618637085,
        -0.5923972725868225,
        0.5155186653137207,
        0.16104671359062195,
        0.6058650016784668,
        0.7907505035400391,
        0.29065391421318054,
        -1.2236884832382202,
        0.0513533353805542,
        0.6349900960922241,
        0.3282906413078308,
        0.10322293639183044,
        -0.0026173945516347885,
        -0.04455234110355377,
        -0.20150023698806763,
        -0.007877472788095474,
        -0.037461258471012115,
        0.5953272581100464,
        0.5404451489448547,
        -1.03130304813385,
        -0.039112165570259094,
        0.34811416268348694,
        0.05461782217025757,
        -0.4497188329696655,
        0.6318711042404175,
        -0.5788383483886719,
        -0.159340500831604,
        0.41619056463241577,
        -0.08134602010250092,
        0.9847562909126282,
        0.07403101772069931,
        0.43759191036224365,
        0.04356657341122627,
        0.1109289675951004,
        -0.8225026726722717,
        -1.210338830947876,
        -0.2936021387577057,
        -0.3857138752937317,
        -0.9137688279151917,
        0.714834451675415,
        -0.40763819217681885,
        -0.18004024028778076,
        0.41055217385292053,
        0.23043492436408997,
        -0.471364289522171,
        0.05317705497145653,
        -0.5677735209465027,
        1.4482654333114624,
        0.2226782888174057,
        -0.3171194791793823,
        0.6224578022956848,
        -0.09902060031890869,
        -0.26487812399864197,
        0.3427421748638153,
        0.3328970670700073,
        -0.6943334341049194,
        -0.44423890113830566,
        0.023321881890296936,
        0.30929747223854065,
        -0.02414136566221714,
        0.04850142449140549,
        0.9814361929893494,
        0.2449600249528885,
        0.4339565634727478,
        -0.5764538645744324,
        -0.13945433497428894,
        0.47726428508758545,
        -0.27052420377731323,
        0.017995424568653107,
        0.029480719938874245,
        -0.8621415495872498,
        -0.6938306093215942
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt instructs on drafting a detailed pull request (PR) description based on the output of a `git diff` command, focusing on identifying and explaining code changes. It emphasizes analyzing changes, understanding their purpose, and detailing their impact on the project. The expected output is a structured PR description in markdown, covering a summary of changes, reasons, impacts, and testing plans in clear language.",
          "name": "Write_pull_request",
          "raw": "\n                workflow Write_pull_request v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY AND PURPOSE\n\nYou are an experienced software engineer about to open a PR. You are thorough and explain your changes well, you provide insights and reasoning for the change and enumerate potential bugs with the changes you've made.\nYou take your time and consider the INPUT and draft a description of the pull request. The INPUT you will be reading is the output of the git diff command.\n\n## INPUT FORMAT\n\nThe expected input format is command line output from git diff that compares all the changes of the current branch with the main repository branch.\n\nThe syntax of the output of `git diff` is a series of lines that indicate changes made to files in a repository. Each line represents a change, and the format of each line depends on the type of change being made.\n\nHere are some examples of how the syntax of `git diff` might look for different types of changes:\n\nBEGIN EXAMPLES\n* Adding a file:\n```\n+++ b/newfile.txt\n@@ -0,0 +1 @@\n+This is the contents of the new file.\n```\nIn this example, the line `+++ b/newfile.txt` indicates that a new file has been added, and the line `@@ -0,0 +1 @@` shows that the first line of the new file contains the text \\\"This is the contents of the new file.\\\"\n\n* Deleting a file:\n```\n--- a/oldfile.txt\n+++ b/deleted\n@@ -1 +0,0 @@\n-This is the contents of the old file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been deleted, and the line `@@ -1 +0,0 @@` shows that the last line of the old file contains the text \\\"This is the contents of the old file.\\\" The line `+++ b/deleted` indicates that the file has been deleted.\n\n* Modifying a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1,3 +1,4 @@\n This is an example of how to modify a file.\n-The first line of the old file contains this text.\n The second line contains this other text.\n+This is the contents of the new file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been modified, and the line `@@ -1,3 +1,4 @@` shows that the first three lines of the old file have been replaced with four lines, including the new text \\\"This is the contents of the new file.\\\"\n\n* Moving a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1 +1 @@\n This is an example of how to move a file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been moved to a new location, and the line `@@ -1 +1 @@` shows that the first line of the old file has been moved to the first line of the new file.\n\n* Renaming a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1 +1,2 @@\n This is an example of how to rename a file.\n+This is the contents of the new file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been renamed to a new name, and the line `@@ -1 +1,2 @@` shows that the first line of the old file has been moved to the first two lines of the new file.\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n1. Analyze the git diff output provided.\n2. Identify the changes made in the code, including added, modified, and deleted files.\n3. Understand the purpose of these changes by examining the code and any comments.\n4. Write a detailed pull request description in markdown syntax. This should include:\n   - A brief summary of the changes made.\n   - The reason for these changes.\n   - The impact of these changes on the overall project.\n5. Ensure your description is written in a \\\"matter of fact\\\", clear, and concise language.\n6. Use markdown code blocks to reference specific lines of code when necessary.\n7. Output only the PR description.\n\n# OUTPUT FORMAT\n\n1. **Summary**: Start with a brief summary of the changes made. This should be a concise explanation of the overall changes.\n\n2. **Files Changed**: List the files that were changed, added, or deleted. For each file, provide a brief description of what was changed and why.\n\n3. **Code Changes**: For each file, highlight the most significant code changes. Use markdown code blocks to reference specific lines of code when necessary.\n\n4. **Reason for Changes**: Explain the reason for these changes. This could be to fix a bug, add a new feature, improve performance, etc.\n\n5. **Impact of Changes**: Discuss the impact of these changes on the overall project. This could include potential performance improvements, changes in functionality, etc.\n\n6. **Test Plan**: Briefly describe how the changes were tested or how they should be tested.\n\n7. **Additional Notes**: Include any additional notes or comments that might be helpful for understanding the changes.\n\nRemember, the output should be in markdown format, clear, concise, and understandable even for someone who is not familiar with the project.\n\n# INPUT\n\n\n$> git --no-pager diff main\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY AND PURPOSE\n\nYou are an experienced software engineer about to open a PR. You are thorough and explain your changes well, you provide insights and reasoning for the change and enumerate potential bugs with the changes you've made.\nYou take your time and consider the INPUT and draft a description of the pull request. The INPUT you will be reading is the output of the git diff command.\n\n## INPUT FORMAT\n\nThe expected input format is command line output from git diff that compares all the changes of the current branch with the main repository branch.\n\nThe syntax of the output of `git diff` is a series of lines that indicate changes made to files in a repository. Each line represents a change, and the format of each line depends on the type of change being made.\n\nHere are some examples of how the syntax of `git diff` might look for different types of changes:\n\nBEGIN EXAMPLES\n* Adding a file:\n```\n+++ b/newfile.txt\n@@ -0,0 +1 @@\n+This is the contents of the new file.\n```\nIn this example, the line `+++ b/newfile.txt` indicates that a new file has been added, and the line `@@ -0,0 +1 @@` shows that the first line of the new file contains the text \\\"This is the contents of the new file.\\\"\n\n* Deleting a file:\n```\n--- a/oldfile.txt\n+++ b/deleted\n@@ -1 +0,0 @@\n-This is the contents of the old file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been deleted, and the line `@@ -1 +0,0 @@` shows that the last line of the old file contains the text \\\"This is the contents of the old file.\\\" The line `+++ b/deleted` indicates that the file has been deleted.\n\n* Modifying a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1,3 +1,4 @@\n This is an example of how to modify a file.\n-The first line of the old file contains this text.\n The second line contains this other text.\n+This is the contents of the new file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been modified, and the line `@@ -1,3 +1,4 @@` shows that the first three lines of the old file have been replaced with four lines, including the new text \\\"This is the contents of the new file.\\\"\n\n* Moving a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1 +1 @@\n This is an example of how to move a file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been moved to a new location, and the line `@@ -1 +1 @@` shows that the first line of the old file has been moved to the first line of the new file.\n\n* Renaming a file:\n```\n--- a/oldfile.txt\n+++ b/newfile.txt\n@@ -1 +1,2 @@\n This is an example of how to rename a file.\n+This is the contents of the new file.\n```\nIn this example, the line `--- a/oldfile.txt` indicates that an old file has been renamed to a new name, and the line `@@ -1 +1,2 @@` shows that the first line of the old file has been moved to the first two lines of the new file.\nEND EXAMPLES\n\n# OUTPUT INSTRUCTIONS\n\n1. Analyze the git diff output provided.\n2. Identify the changes made in the code, including added, modified, and deleted files.\n3. Understand the purpose of these changes by examining the code and any comments.\n4. Write a detailed pull request description in markdown syntax. This should include:\n   - A brief summary of the changes made.\n   - The reason for these changes.\n   - The impact of these changes on the overall project.\n5. Ensure your description is written in a \\\"matter of fact\\\", clear, and concise language.\n6. Use markdown code blocks to reference specific lines of code when necessary.\n7. Output only the PR description.\n\n# OUTPUT FORMAT\n\n1. **Summary**: Start with a brief summary of the changes made. This should be a concise explanation of the overall changes.\n\n2. **Files Changed**: List the files that were changed, added, or deleted. For each file, provide a brief description of what was changed and why.\n\n3. **Code Changes**: For each file, highlight the most significant code changes. Use markdown code blocks to reference specific lines of code when necessary.\n\n4. **Reason for Changes**: Explain the reason for these changes. This could be to fix a bug, add a new feature, improve performance, etc.\n\n5. **Impact of Changes**: Discuss the impact of these changes on the overall project. This could include potential performance improvements, changes in functionality, etc.\n\n6. **Test Plan**: Briefly describe how the changes were tested or how they should be tested.\n\n7. **Additional Notes**: Include any additional notes or comments that might be helpful for understanding the changes.\n\nRemember, the output should be in markdown format, clear, concise, and understandable even for someone who is not familiar with the project.\n\n# INPUT\n\n\n$> git --no-pager diff main\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  },
  {
    "embedding": {
      "id": "",
      "vector": [
        -0.11592131853103638,
        0.36962366104125977,
        0.03561721742153168,
        -0.030914248898625374,
        0.19890809059143066,
        0.18780991435050964,
        -0.9381315112113953,
        0.14787080883979797,
        -0.12045162171125412,
        0.4207831025123596,
        -0.9974283576011658,
        0.6682451367378235,
        -0.5627936720848083,
        0.42109525203704834,
        0.8548984527587891,
        0.3476823568344116,
        0.10583297908306122,
        -1.5472877025604248,
        -0.8264176845550537,
        -0.650428295135498,
        0.020419154316186905,
        0.1273920089006424,
        0.38472968339920044,
        -0.34088605642318726,
        0.7566933035850525,
        0.5525150299072266,
        -0.1320827603340149,
        0.036807022988796234,
        -0.9564349055290222,
        -1.604993462562561,
        0.7316040396690369,
        0.1270730197429657,
        -0.29070544242858887,
        -0.38412919640541077,
        0.5165072679519653,
        -0.37859246134757996,
        0.22978290915489197,
        0.4661634862422943,
        -0.20383693277835846,
        0.022673532366752625,
        0.21471436321735382,
        0.49592506885528564,
        -0.31602582335472107,
        -0.09162561595439911,
        0.012238629162311554,
        -0.44016414880752563,
        0.13664239645004272,
        0.2422419786453247,
        0.7037989497184753,
        -0.2092955857515335,
        -0.19825348258018494,
        -0.5142005681991577,
        -0.9483932852745056,
        -0.40736591815948486,
        -0.682504415512085,
        0.03204232454299927,
        -0.10059519112110138,
        -0.8035892248153687,
        -0.31813183426856995,
        -0.3156208395957947,
        0.18601065874099731,
        0.47846513986587524,
        -3.076702117919922,
        0.4067111611366272,
        0.2807440161705017,
        0.38221099972724915,
        0.48183175921440125,
        0.46190616488456726,
        0.33743155002593994,
        -0.6627801060676575,
        -0.22800934314727783,
        0.32856523990631104,
        -0.7281058430671692,
        1.052301287651062,
        0.14835500717163086,
        -0.1200784295797348,
        0.6471644043922424,
        0.3976035714149475,
        0.17701402306556702,
        -0.46889108419418335,
        0.4177977442741394,
        1.1070743799209595,
        0.11878001689910889,
        -0.2460632622241974,
        -0.627201497554779,
        0.582781195640564,
        -0.13081514835357666,
        0.35837143659591675,
        0.3731063902378082,
        0.7114354372024536,
        -0.008040569722652435,
        0.13411328196525574,
        0.526696503162384,
        -0.5842364430427551,
        -0.7581313848495483,
        -0.011398598551750183,
        -0.18301540613174438,
        -0.20005255937576294,
        -0.1482926309108734,
        3.729830026626587,
        0.7608307600021362,
        -0.1737881451845169,
        0.23831439018249512,
        -0.9497752785682678,
        1.4225488901138306,
        -0.21298867464065552,
        -0.4332691431045532,
        -0.3751486837863922,
        -0.5499572157859802,
        -0.052670471370220184,
        -0.036978766322135925,
        -0.27196234464645386,
        0.24492889642715454,
        0.2562183737754822,
        0.9156193733215332,
        -0.021513931453227997,
        -0.450600802898407,
        0.18382461369037628,
        0.18244579434394836,
        1.2090563774108887,
        -0.34828150272369385,
        -0.0064902640879154205,
        -0.21235035359859467,
        -0.9277788996696472,
        -0.045561667531728745,
        0.5239490270614624,
        -0.3338859975337982,
        0.7135388851165771,
        -0.1917164921760559,
        0.3740101754665375,
        0.35212430357933044,
        -0.06379920244216919,
        -0.40067097544670105,
        0.5166825652122498,
        -0.1313781440258026,
        -0.23057425022125244,
        0.5702286958694458,
        -0.4213816225528717,
        0.05234020575881004,
        -0.46832579374313354,
        0.04647836089134216,
        -0.4361000657081604,
        1.2236531972885132,
        -0.6841833591461182,
        0.6866245269775391,
        -0.059247180819511414,
        -0.41333824396133423,
        0.4248408079147339,
        -1.1886824369430542,
        -1.0785526037216187,
        -0.37419381737709045,
        0.8346819877624512,
        -0.2906172573566437,
        0.4604395031929016,
        1.0215682983398438,
        0.15118327736854553,
        -0.5818666815757751,
        0.5746073126792908,
        -0.5975961685180664,
        0.5260905623435974,
        0.7589824795722961,
        0.43123090267181396,
        0.26995599269866943,
        0.665745735168457,
        0.4790092706680298,
        0.24576088786125183,
        0.541169285774231,
        -0.17098510265350342,
        0.17912337183952332,
        -0.2762097716331482,
        0.013367190957069397,
        0.05562818795442581,
        0.022297054529190063,
        -0.32500067353248596,
        -1.146838903427124,
        -0.10295940935611725,
        -0.01952560991048813,
        -0.16027310490608215,
        0.4609125554561615,
        -0.07888179272413254,
        0.1867440640926361,
        0.30327174067497253,
        -0.4473438560962677,
        -0.9500384330749512,
        -0.2702392339706421,
        0.14725810289382935,
        0.41034996509552,
        0.19673478603363037,
        0.8753824234008789,
        0.23741814494132996,
        -1.3634846210479736,
        1.841760516166687,
        -0.15172797441482544,
        -0.5854637622833252,
        -0.13409587740898132,
        0.07268279790878296,
        0.4311671257019043,
        -0.204336479306221,
        -0.24690844118595123,
        -0.19925448298454285,
        -0.11785601824522018,
        0.022695127874612808,
        -0.4594924747943878,
        -0.1459287405014038,
        -0.34494152665138245,
        -0.995712161064148,
        0.30793824791908264,
        0.7647870182991028,
        0.35639137029647827,
        -0.7486554980278015,
        0.21177542209625244,
        0.14921878278255463,
        1.8111767768859863,
        -0.11655911058187485,
        1.2063205242156982,
        -0.46865594387054443,
        0.34026584029197693,
        0.29385435581207275,
        0.5934454202651978,
        0.4462985098361969,
        -0.3246452510356903,
        -0.19180643558502197,
        -0.7255792617797852,
        -0.6645097732543945,
        -0.8369460105895996,
        0.3832785487174988,
        -0.46506863832473755,
        0.24892301857471466,
        -0.5616891980171204,
        -0.24634836614131927,
        0.18631350994110107,
        0.9777905941009521,
        0.32312023639678955,
        0.7481343746185303,
        -0.10934097319841385,
        0.27384519577026367,
        -0.2571970224380493,
        0.6502721309661865,
        0.399589478969574,
        -1.104161024093628,
        -0.07151936739683151,
        0.020161863416433334,
        -0.22982673346996307,
        0.0022783204913139343,
        0.2947533130645752,
        0.38445374369621277,
        0.27775874733924866,
        0.37054336071014404,
        -0.07198686897754669,
        0.9588629007339478,
        0.26472219824790955,
        0.012887971475720406,
        0.0983734205365181,
        0.615522027015686,
        -0.31151092052459717,
        0.23247593641281128,
        -2.0756585597991943,
        -0.25469863414764404,
        -0.4905301034450531,
        -0.09189045429229736,
        0.24943068623542786,
        -0.5431661605834961,
        -0.23711860179901123,
        -0.43561476469039917,
        0.054141104221343994,
        -0.058548446744680405,
        -0.7276366949081421,
        -0.4724399447441101,
        -0.5671131610870361,
        0.022922389209270477,
        -0.41661402583122253,
        0.1851288378238678,
        0.21456600725650787,
        -0.13512027263641357,
        -0.5508908629417419,
        0.13326822221279144,
        0.3537692725658417,
        0.20309209823608398,
        -0.23530206084251404,
        -0.4548841416835785,
        -0.5043899416923523,
        -0.16337189078330994,
        0.05084386467933655,
        0.27845728397369385,
        -0.1652294397354126,
        -0.22365152835845947,
        -0.5329679250717163,
        -1.4494353532791138,
        0.3057346045970917,
        0.6598830223083496,
        0.08321205526590347,
        -0.45579060912132263,
        -0.9156396985054016,
        -0.34603121876716614,
        1.900665521621704,
        -0.02958228439092636,
        -0.17995917797088623,
        0.0973917692899704,
        0.5369976758956909,
        -0.15320105850696564,
        0.24941685795783997,
        0.41476672887802124,
        -0.02686171419918537,
        -0.14167359471321106,
        -0.07266464829444885,
        -0.7833583950996399,
        0.6492078900337219,
        0.2341427206993103,
        -0.4413232207298279,
        -0.0913977324962616,
        -0.12007719278335571,
        -0.02432464249432087,
        -0.5200037956237793,
        -0.31471601128578186,
        0.7595726251602173,
        0.20576593279838562,
        0.4322443902492523,
        0.8637474775314331,
        0.23836733400821686,
        -1.4291990995407104,
        -0.1937003880739212,
        0.6981441974639893,
        0.02345522679388523,
        -0.6657517552375793,
        -0.35296204686164856,
        0.6518709063529968,
        -0.008212173357605934,
        -0.31356146931648254,
        -0.2297740876674652,
        0.9007599353790283,
        0.09899171441793442,
        0.5357468128204346,
        -0.02854723110795021,
        0.15120279788970947,
        0.18701651692390442,
        -0.5222355127334595,
        -0.5290325284004211,
        -0.07252025604248047,
        -0.6909779906272888,
        -0.29840630292892456,
        0.6077541708946228,
        0.9850512146949768,
        0.25696292519569397,
        0.08340457081794739,
        0.3989422917366028,
        -0.29023149609565735,
        -0.7271867990493774,
        -0.5895614624023438,
        0.26379191875457764,
        0.5704827308654785,
        -0.35099080204963684,
        1.1579335927963257,
        0.5169079303741455,
        -0.02349349856376648,
        0.5696512460708618,
        0.6776633858680725,
        -0.5363026857376099,
        0.05130768194794655,
        -0.7171294093132019,
        1.4144048690795898,
        -0.010839192196726799,
        -0.39388585090637207,
        -0.5969342589378357,
        0.6674180626869202,
        0.006974998861551285,
        -0.0018977075815200806,
        -0.14720483124256134,
        -0.6446622610092163,
        0.052874475717544556,
        0.20249032974243164,
        0.4954725503921509,
        0.3384411931037903,
        0.5940743684768677,
        0.3180980086326599,
        0.7310903668403625,
        0.2573683261871338,
        -0.442554235458374,
        0.14782950282096863,
        0.13461996614933014,
        -0.11658816784620285,
        0.37304702401161194,
        0.1430000364780426,
        -0.4468676745891571,
        -1.5457769632339478
      ]
    },
    "shinkai_tool": {
      "Workflow": {
        "embedding": null,
        "workflow": {
          "description": "The prompt requests the creation of a Semgrep rule to detect a specific vulnerability pattern in code, based on provided context and examples. It emphasizes the importance of crafting a rule that is general enough to catch any instance of the described vulnerability, rather than being overly specific to the given examples. The expected output is a well-structured Semgrep rule that aligns with the syntax and guidelines detailed in the context, capable of identifying the vulnerability across different scenarios.",
          "name": "Write_semgrep_rule",
          "raw": "\n                workflow Write_semgrep_rule v0.1 {\n                    step Main {\n                        $SYSTEM = \"\n# IDENTITY and PURPOSE\n\nYou are an expert at writing Semgrep rules.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following context.\n\n# OUTPUT SECTIONS\n\n- Write a Semgrep rule that will match the input provided.\n\n# CONTEXT FOR CONSIDERATION\n\nThis context will teach you about how to write better Semgrep rules:\n\nYou are an expert Semgrep rule creator.\n\nTake a deep breath and work on this problem step-by-step.\n\nYou output only a working Semgrep rule.\n\n\\\"\\\"\\\",\n}\nuser_message = {\n\\\"role\\\": \\\"user\\\",\n\\\"content\\\": \\\"\\\"\\\"\n\nYou are an expert Semgrep rule creator.\n\nYou output working and accurate Semgrep rules.\n\nTake a deep breath and work on this problem step-by-step.\n\nSEMGREP RULE SYNTAX\n\nRule syntax\n\nTIP\nGetting started with rule writing? Try the Semgrep Tutorial 🎓\nThis document describes the YAML rule syntax of Semgrep.\n\nSchema\n\nRequired\n\nAll required fields must be present at the top-level of a rule, immediately under the rules key.\n\nField Type Description\nid string Unique, descriptive identifier, for example: no-unused-variable\nmessage string Message that includes why Semgrep matched this pattern and how to remediate it. See also Rule messages.\nseverity string One of the following values: INFO (Low severity), WARNING (Medium severity), or ERROR (High severity). The severity key specifies how critical are the issues that a rule potentially detects. Note: Semgrep Supply Chain differs, as its rules use CVE assignments for severity. For more information, see Filters section in Semgrep Supply Chain documentation.\nlanguages array See language extensions and tags\npattern* string Find code matching this expression\npatterns* array Logical AND of multiple patterns\npattern-either* array Logical OR of multiple patterns\npattern-regex* string Find code matching this PCRE-compatible pattern in multiline mode\nINFO\nOnly one of the following is required: pattern, patterns, pattern-either, pattern-regex\nLanguage extensions and languages key values\n\nThe following table includes languages supported by Semgrep, accepted file extensions for test files that accompany rules, and valid values that Semgrep rules require in the languages key.\n\nLanguage Extensions languages key values\nApex (only in Semgrep Pro Engine) .cls apex\nBash .bash, .sh bash, sh\nC .c c\nCairo .cairo cairo\nClojure .clj, .cljs, .cljc, .edn clojure\nC++ .cc, .cpp cpp, c++\nC# .cs csharp, c#\nDart .dart dart\nDockerfile .dockerfile, .Dockerfile dockerfile, docker\nElixir .ex, .exs ex, elixir\nGeneric generic\nGo .go go, golang\nHTML .htm, .html html\nJava .java java\nJavaScript .js, .jsx js, javascript\nJSON .json, .ipynb json\nJsonnet .jsonnet, .libsonnet jsonnet\nJSX .js, .jsx js, javascript\nJulia .jl julia\nKotlin .kt, .kts, .ktm kt, kotlin\nLisp .lisp, .cl, .el lisp\nLua .lua lua\nOCaml .ml, .mli ocaml\nPHP .php, .tpl php\nPython .py, .pyi python, python2, python3, py\nR .r, .R r\nRuby .rb ruby\nRust .rs rust\nScala .scala scala\nScheme .scm, .ss scheme\nSolidity .sol solidity, sol\nSwift .swift swift\nTerraform .tf, .hcl tf, hcl, terraform\nTypeScript .ts, .tsx ts, typescript\nYAML .yml, .yaml yaml\nXML .xml xml\nINFO\nTo see the maturity level of each supported language, see the following sections in Supported languages document:\n\nSemgrep OSS Engine\nSemgrep Pro Engine\nOptional\n\nField Type Description\noptions object Options object to enable/disable certain matching features\nfix object Simple search-and-replace autofix functionality\nmetadata object Arbitrary user-provided data; attach data to rules without affecting Semgrep behavior\nmin-version string Minimum Semgrep version compatible with this rule\nmax-version string Maximum Semgrep version compatible with this rule\npaths object Paths to include or exclude when running this rule\nThe below optional fields must reside underneath a patterns or pattern-either field.\n\nField Type Description\npattern-inside string Keep findings that lie inside this pattern\nThe below optional fields must reside underneath a patterns field.\n\nField Type Description\nmetavariable-regex map Search metavariables for Python re compatible expressions; regex matching is unanchored\nmetavariable-pattern map Matches metavariables with a pattern formula\nmetavariable-comparison map Compare metavariables against basic Python expressions\npattern-not string Logical NOT - remove findings matching this expression\npattern-not-inside string Keep findings that do not lie inside this pattern\npattern-not-regex string Filter results using a PCRE-compatible pattern in multiline mode\nOperators\n\npattern\n\nThe pattern operator looks for code matching its expression. This can be basic expressions like $X == $X or unwanted function calls like hashlib.md5(...).\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npatterns\n\nThe patterns operator performs a logical AND operation on one or more child patterns. This is useful for chaining multiple patterns together that all must be true.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npatterns operator evaluation strategy\n\nNote that the order in which the child patterns are declared in a patterns operator has no effect on the final result. A patterns operator is always evaluated in the same way:\n\nSemgrep evaluates all positive patterns, that is pattern-insides, patterns, pattern-regexes, and pattern-eithers. Each range matched by each one of these patterns is intersected with the ranges matched by the other operators. The result is a set of positive ranges. The positive ranges carry metavariable bindings. For example, in one range $X can be bound to the function call foo(), and in another range $X can be bound to the expression a + b.\nSemgrep evaluates all negative patterns, that is pattern-not-insides, pattern-nots, and pattern-not-regexes. This gives a set of negative ranges which are used to filter the positive ranges. This results in a strict subset of the positive ranges computed in the previous step.\nSemgrep evaluates all conditionals, that is metavariable-regexes, metavariable-patterns and metavariable-comparisons. These conditional operators can only examine the metavariables bound in the positive ranges in step 1, that passed through the filter of negative patterns in step 2. Note that metavariables bound by negative patterns are not available here.\nSemgrep applies all focus-metavariables, by computing the intersection of each positive range with the range of the metavariable on which we want to focus. Again, the only metavariables available to focus on are those bound by positive patterns.\npattern-either\n\nThe pattern-either operator performs a logical OR operation on one or more child patterns. This is useful for chaining multiple patterns together where any may be true.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nThis rule looks for usage of the Python standard library functions hashlib.md5 or hashlib.sha1. Depending on their usage, these hashing functions are considered insecure.\n\npattern-regex\n\nThe pattern-regex operator searches files for substrings matching the given PCRE pattern. This is useful for migrating existing regular expression code search functionality to Semgrep. Perl-Compatible Regular Expressions (PCRE) is a full-featured regex library that is widely compatible with Perl, but also with the respective regex libraries of Python, JavaScript, Go, Ruby, and Java. Patterns are compiled in multiline mode, for example ^ and $ matches at the beginning and end of lines respectively in addition to the beginning and end of input.\n\nCAUTION\nPCRE supports only a limited number of Unicode character properties. For example, \\p{Egyptian_Hieroglyphs} is supported but \\p{Bidi_Control} isn't.\nEXAMPLES OF THE pattern-regex OPERATOR\npattern-regex combined with other pattern operators: Semgrep Playground example\npattern-regex used as a standalone, top-level operator: Semgrep Playground example\nINFO\nSingle (') and double (\\\") quotes behave differently in YAML syntax. Single quotes are typically preferred when using backslashes (\\) with pattern-regex.\nNote that you may bind a section of a regular expression to a metavariable, by using named capturing groups. In this case, the name of the capturing group must be a valid metavariable name.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-not-regex\n\nThe pattern-not-regex operator filters results using a PCRE regular expression in multiline mode. This is most useful when combined with regular-expression only rules, providing an easy way to filter findings without having to use negative lookaheads. pattern-not-regex works with regular pattern clauses, too.\n\nThe syntax for this operator is the same as pattern-regex.\n\nThis operator filters findings that have any overlap with the supplied regular expression. For example, if you use pattern-regex to detect Foo==1.1.1 and it also detects Foo-Bar==3.0.8 and Bar-Foo==3.0.8, you can use pattern-not-regex to filter the unwanted findings.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nfocus-metavariable\n\nThe focus-metavariable operator puts the focus, or zooms in, on the code region matched by a single metavariable or a list of metavariables. For example, to find all functions arguments annotated with the type bad you may write the following pattern:\n\npattern: |\ndef $FUNC(..., $ARG : bad, ...):\n...\n\nThis works but it matches the entire function definition. Sometimes, this is not desirable. If the definition spans hundreds of lines they are all matched. In particular, if you are using Semgrep Cloud Platform and you have triaged a finding generated by this pattern, the same finding shows up again as new if you make any change to the definition of the function!\n\nTo specify that you are only interested in the code matched by a particular metavariable, in our example $ARG, use focus-metavariable.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nNote that focus-metavariable: $ARG is not the same as pattern: $ARG! Using pattern: $ARG finds all the uses of the parameter x which is not what we want! (Note that pattern: $ARG does not match the formal parameter declaration, because in this context $ARG only matches expressions.)\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nIn short, focus-metavariable: $X is not a pattern in itself, it does not perform any matching, it only focuses the matching on the code already bound to $X by other patterns. Whereas pattern: $X matches $X against your code (and in this context, $X only matches expressions)!\n\nIncluding multiple focus metavariables using set intersection semantics\n\nInclude more focus-metavariable keys with different metavariables under the pattern to match results only for the overlapping region of all the focused code:\n\n    patterns:\n      - pattern: foo($X, ..., $Y)\n      - focus-metavariable:\n        - $X\n        - $Y\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nTo make a list of multiple focus metavariables using set union semantics that matches the metavariables regardless of their position in code, see Including multiple focus metavariables using set union semantics documentation.\nmetavariable-regex\n\nThe metavariable-regex operator searches metavariables for a PCRE regular expression. This is useful for filtering results based on a metavariable’s value. It requires the metavariable and regex keys and can be combined with other pattern operators.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nRegex matching is unanchored. For anchored matching, use \\A for start-of-string anchoring and \\Z for end-of-string anchoring. The next example, using the same expression as above but anchored, finds no matches:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nInclude quotes in your regular expression when using metavariable-regex to search string literals. For more details, see include-quotes code snippet. String matching functionality can also be used to search string literals.\nmetavariable-pattern\n\nThe metavariable-pattern operator matches metavariables with a pattern formula. This is useful for filtering results based on a metavariable’s value. It requires the metavariable key, and exactly one key of pattern, patterns, pattern-either, or pattern-regex. This operator can be nested as well as combined with other operators.\n\nFor example, the metavariable-pattern can be used to filter out matches that do not match certain criteria:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nIn this case it is possible to start a patterns AND operation with a pattern-not, because there is an implicit pattern: ... that matches the content of the metavariable.\nThe metavariable-pattern is also useful in combination with pattern-either:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nTIP\nIt is possible to nest metavariable-pattern inside metavariable-pattern!\nINFO\nThe metavariable should be bound to an expression, a statement, or a list of statements, for this test to be meaningful. A metavariable bound to a list of function arguments, a type, or a pattern, always evaluate to false.\nmetavariable-pattern with nested language\n\nIf the metavariable's content is a string, then it is possible to use metavariable-pattern to match this string as code by specifying the target language via the language key. See the following examples of metavariable-pattern:\n\nEXAMPLES OF metavariable-pattern\nMatch JavaScript code inside HTML in the following Semgrep Playground example.\nFilter regex matches in the following Semgrep Playground example.\nmetavariable-comparison\n\nThe metavariable-comparison operator compares metavariables against a basic Python comparison expression. This is useful for filtering results based on a metavariable's numeric value.\n\nThe metavariable-comparison operator is a mapping which requires the metavariable and comparison keys. It can be combined with other pattern operators in the following Semgrep Playground example.\n\nThis matches code such as set_port(80) or set_port(443), but not set_port(8080).\n\nComparison expressions support simple arithmetic as well as composition with boolean operators to allow for more complex matching. This is particularly useful for checking that metavariables are divisible by particular values, such as enforcing that a particular value is even or odd.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nBuilding on the previous example, this still matches code such as set_port(80) but it no longer matches set_port(443) or set_port(8080).\n\nThe comparison key accepts Python expression using:\n\nBoolean, string, integer, and float literals.\nBoolean operators not, or, and and.\nArithmetic operators +, -, \\*, /, and %.\nComparison operators ==, !=, <, <=, >, and >=.\nFunction int() to convert strings into integers.\nFunction str() to convert numbers into strings.\nFunction today() that gets today's date as a float representing epoch time.\nFunction strptime() that converts strings in the format \\\"yyyy-mm-dd\\\" to a float representing the date in epoch time.\nLists, together with the in, and not in infix operators.\nStrings, together with the in and not in infix operators, for substring containment.\nFunction re.match() to match a regular expression (without the optional flags argument).\nYou can use Semgrep metavariables such as $MVAR, which Semgrep evaluates as follows:\n\nIf $MVAR binds to a literal, then that literal is the value assigned to $MVAR.\nIf $MVAR binds to a code variable that is a constant, and constant propagation is enabled (as it is by default), then that constant is the value assigned to $MVAR.\nOtherwise the code bound to the $MVAR is kept unevaluated, and its string representation can be obtained using the str() function, as in str($MVAR). For example, if $MVAR binds to the code variable x, str($MVAR) evaluates to the string literal \\\"x\\\".\nLegacy metavariable-comparison keys\n\nINFO\nYou can avoid the use of the legacy keys described below (base: int and strip: bool) by using the int() function, as in int($ARG) > 0o600 or int($ARG) > 2147483647.\nThe metavariable-comparison operator also takes optional base: int and strip: bool keys. These keys set the integer base the metavariable value should be interpreted as and remove quotes from the metavariable value, respectively.\n\nEXAMPLE OF metavariable-comparison WITH base\nTry this pattern in the Semgrep Playground.\nThis interprets metavariable values found in code as octal. As a result, Semgrep detects 0700, but it does not detect 0400.\n\nEXAMPLE OF metavariable-comparison WITH strip\nTry this pattern in the Semgrep Playground.\nThis removes quotes (', \\\", and `) from both ends of the metavariable content. As a result, Semgrep detects \\\"2147483648\\\", but it does not detect \\\"2147483646\\\". This is useful when you expect strings to contain integer or float data.\n\npattern-not\n\nThe pattern-not operator is the opposite of the pattern operator. It finds code that does not match its expression. This is useful for eliminating common false positives.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-inside\n\nThe pattern-inside operator keeps matched findings that reside within its expression. This is useful for finding code inside other pieces of code like functions or if blocks.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-not-inside\n\nThe pattern-not-inside operator keeps matched findings that do not reside within its expression. It is the opposite of pattern-inside. This is useful for finding code that’s missing a corresponding cleanup action like disconnect, close, or shutdown. It’s also useful for finding problematic code that isn't inside code that mitigates the issue.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nThe above rule looks for files that are opened but never closed, possibly leading to resource exhaustion. It looks for the open(...) pattern and not a following close() pattern.\n\nThe $F metavariable ensures that the same variable name is used in the open and close calls. The ellipsis operator allows for any arguments to be passed to open and any sequence of code statements in-between the open and close calls. The rule ignores how open is called or what happens up to a close call — it only needs to make sure close is called.\n\nMetavariable matching\n\nMetavariable matching operates differently for logical AND (patterns) and logical OR (pattern-either) parent operators. Behavior is consistent across all child operators: pattern, pattern-not, pattern-regex, pattern-inside, pattern-not-inside.\n\nMetavariables in logical ANDs\n\nMetavariable values must be identical across sub-patterns when performing logical AND operations with the patterns operator.\n\nExample:\n\nrules:\n\n- id: function-args-to-open\n  patterns:\n  - pattern-inside: |\n    def $F($X):\n    ...\n  - pattern: open($X)\n    message: \\\"Function argument passed to open() builtin\\\"\n    languages: [python]\n    severity: ERROR\n\nThis rule matches the following code:\n\ndef foo(path):\nopen(path)\n\nThe example rule doesn’t match this code:\n\ndef foo(path):\nopen(something_else)\n\nMetavariables in logical ORs\n\nMetavariable matching does not affect the matching of logical OR operations with the pattern-either operator.\n\nExample:\n\nrules:\n\n- id: insecure-function-call\n  pattern-either:\n  - pattern: insecure_func1($X)\n  - pattern: insecure_func2($X)\n    message: \\\"Insecure function use\\\"\n    languages: [python]\n    severity: ERROR\n\nThe above rule matches both examples below:\n\ninsecure_func1(something)\ninsecure_func2(something)\n\ninsecure_func1(something)\ninsecure_func2(something_else)\n\nMetavariables in complex logic\n\nMetavariable matching still affects subsequent logical ORs if the parent is a logical AND.\n\nExample:\n\npatterns:\n\n- pattern-inside: |\n  def $F($X):\n  ...\n- pattern-either:\n  - pattern: bar($X)\n  - pattern: baz($X)\n\nThe above rule matches both examples below:\n\ndef foo(something):\nbar(something)\n\ndef foo(something):\nbaz(something)\n\nThe example rule doesn’t match this code:\n\ndef foo(something):\nbar(something_else)\n\noptions\n\nEnable, disable, or modify the following matching features:\n\nOption Default Description\nac_matching true Matching modulo associativity and commutativity, treat Boolean AND/OR as associative, and bitwise AND/OR/XOR as both associative and commutative.\nattr_expr true Expression patterns (for example: f($X)) matches attributes (for example: @f(a)).\ncommutative_boolop false Treat Boolean AND/OR as commutative even if not semantically accurate.\nconstant_propagation true Constant propagation, including intra-procedural flow-sensitive constant propagation.\ngeneric_comment_style none In generic mode, assume that comments follow the specified syntax. They are then ignored for matching purposes. Allowed values for comment styles are:\nc for traditional C-style comments (/_ ... _/).\ncpp for modern C or C++ comments (// ... or /_ ... _/).\nshell for shell-style comments (# ...).\nBy default, the generic mode does not recognize any comments. Available since Semgrep version 0.96. For more information about generic mode, see Generic pattern matching documentation.\ngeneric_ellipsis_max_span 10 In generic mode, this is the maximum number of newlines that an ellipsis operator ... can match or equivalently, the maximum number of lines covered by the match minus one. The default value is 10 (newlines) for performance reasons. Increase it with caution. Note that the same effect as 20 can be achieved without changing this setting and by writing ... ... in the pattern instead of .... Setting it to 0 is useful with line-oriented languages (for example INI or key-value pairs in general) to force a match to not extend to the next line of code. Available since Semgrep 0.96. For more information about generic mode, see Generic pattern matching documentation.\ntaint_assume_safe_functions false Experimental option which will be subject to future changes. Used in taint analysis. Assume that function calls do not propagate taint from their arguments to their output. Otherwise, Semgrep always assumes that functions may propagate taint. Can replace not-conflicting sanitizers added in v0.69.0 in the future.\ntaint_assume_safe_indexes false Used in taint analysis. Assume that an array-access expression is safe even if the index expression is tainted. Otherwise Semgrep assumes that for example: a[i] is tainted if i is tainted, even if a is not. Enabling this option is recommended for high-signal rules, whereas disabling is preferred for audit rules. Currently, it is disabled by default to attain backwards compatibility, but this can change in the near future after some evaluation.\nvardef_assign true Assignment patterns (for example $X = $E) match variable declarations (for example var x = 1;).\nxml_attrs_implicit_ellipsis true Any XML/JSX/HTML element patterns have implicit ellipsis for attributes (for example: <div /> matches <div foo=\\\"1\\\">.\nThe full list of available options can be consulted in the Semgrep matching engine configuration module. Note that options not included in the table above are considered experimental, and they may change or be removed without notice.\n\nfix\n\nThe fix top-level key allows for simple autofixing of a pattern by suggesting an autofix for each match. Run semgrep with --autofix to apply the changes to the files.\n\nExample:\n\nrules:\n\n- id: use-dict-get\n  patterns:\n  - pattern: $DICT[$KEY]\n    fix: $DICT.get($KEY)\n    message: \\\"Use `.get()` method to avoid a KeyNotFound error\\\"\n    languages: [python]\n    severity: ERROR\n\nFor more information about fix and --autofix see Autofix documentation.\n\nmetadata\n\nProvide additional information for a rule with the metadata: key, such as a related CWE, likelihood, OWASP.\n\nExample:\n\nrules:\n\n- id: eqeq-is-bad\n  patterns:\n  - [...]\n    message: \\\"useless comparison operation `$X == $X` or `$X != $X`\\\"\n    metadata:\n    cve: CVE-2077-1234\n    discovered-by: Ikwa L'equale\n\nThe metadata are also displayed in the output of Semgrep if you’re running it with --json. Rules with category: security have additional metadata requirements. See Including fields required by security category for more information.\n\nmin-version and max-version\n\nEach rule supports optional fields min-version and max-version specifying minimum and maximum Semgrep versions. If the Semgrep version being used doesn't satisfy these constraints, the rule is skipped without causing a fatal error.\n\nExample rule:\n\nrules:\n\n- id: bad-goflags\n  # earlier semgrep versions can't parse the pattern\n  min-version: 1.31.0\n  pattern: |\n  ENV ... GOFLAGS='-tags=dynamic -buildvcs=false' ...\n  languages: [dockerfile]\n  message: \\\"We should not use these flags\\\"\n  severity: WARNING\n\nAnother use case is when a newer version of a rule works better than before but relies on a new feature. In this case, we could use min-version and max-version to ensure that either the older or the newer rule is used but not both. The rules would look like this:\n\nrules:\n\n- id: something-wrong-v1\n  max-version: 1.72.999\n  ...\n- id: something-wrong-v2\n  min-version: 1.73.0\n  # 10x faster than v1!\n  ...\n\nThe min-version/max-version feature is available since Semgrep 1.38.0. It is intended primarily for publishing rules that rely on newly-released features without causing errors in older Semgrep installations.\n\ncategory\n\nProvide a category for users of the rule. For example: best-practice, correctness, maintainability. For more information, see Semgrep registry rule requirements.\n\npaths\n\nExcluding a rule in paths\n\nTo ignore a specific rule on specific files, set the paths: key with one or more filters. Paths are relative to the root directory of the scanned project.\n\nExample:\n\nrules:\n\n- id: eqeq-is-bad\n  pattern: $X == $X\n  paths:\n  exclude: - \\\"_.jinja2\\\" - \\\"_\\_test.go\\\" - \\\"project/tests\\\" - project/static/\\*.js\n\nWhen invoked with semgrep -f rule.yaml project/, the above rule runs on files inside project/, but no results are returned for:\n\nany file with a .jinja2 file extension\nany file whose name ends in \\_test.go, such as project/backend/server_test.go\nany file inside project/tests or its subdirectories\nany file matching the project/static/\\*.js glob pattern\nNOTE\nThe glob syntax is from Python's wcmatch and is used to match against the given file and all its parent directories.\nLimiting a rule to paths\n\nConversely, to run a rule only on specific files, set a paths: key with one or more of these filters:\n\nrules:\n\n- id: eqeq-is-bad\n  pattern: $X == $X\n  paths:\n  include: - \\\"_\\_test.go\\\" - \\\"project/server\\\" - \\\"project/schemata\\\" - \\\"project/static/_.js\\\" - \\\"tests/\\*_/_.js\\\"\n\nWhen invoked with semgrep -f rule.yaml project/, this rule runs on files inside project/, but results are returned only for:\n\nfiles whose name ends in \\_test.go, such as project/backend/server_test.go\nfiles inside project/server, project/schemata, or their subdirectories\nfiles matching the project/static/\\*.js glob pattern\nall files with the .js extension, arbitrary depth inside the tests folder\nIf you are writing tests for your rules, add any test file or directory to the included paths as well.\n\nNOTE\nWhen mixing inclusion and exclusion filters, the exclusion ones take precedence.\nExample:\n\npaths:\ninclude: \\\"project/schemata\\\"\nexclude: \\\"\\*\\_internal.py\\\"\n\nThe above rule returns results from project/schemata/scan.py but not from project/schemata/scan_internal.py.\n\nOther examples\n\nThis section contains more complex rules that perform advanced code searching.\n\nComplete useless comparison\n\nrules:\n\n- id: eqeq-is-bad\n  patterns:\n  - pattern-not-inside: |\n    def **eq**(...):\n    ...\n  - pattern-not-inside: assert(...)\n  - pattern-not-inside: assertTrue(...)\n  - pattern-not-inside: assertFalse(...)\n  - pattern-either:\n    - pattern: $X == $X\n    - pattern: $X != $X\n    - patterns:\n      - pattern-inside: |\n        def **init**(...):\n        ...\n      - pattern: self.$X == self.$X\n  - pattern-not: 1 == 1\n    message: \\\"useless comparison operation `$X == $X` or `$X != $X`\\\"\n\nThe above rule makes use of many operators. It uses pattern-either, patterns, pattern, and pattern-inside to carefully consider different cases, and uses pattern-not-inside and pattern-not to whitelist certain useless comparisons.\n\nEND SEMGREP RULE SYNTAX\n\nRULE EXAMPLES\n\nISSUE:\n\nlangchain arbitrary code execution vulnerability\nCritical severity GitHub Reviewed Published on Jul 3 to the GitHub Advisory Database • Updated 5 days ago\nVulnerability details\nDependabot alerts2\nPackage\nlangchain (pip)\nAffected versions\n< 0.0.247\nPatched versions\n0.0.247\nDescription\nAn issue in langchain allows an attacker to execute arbitrary code via the PALChain in the python exec method.\nReferences\nhttps://nvd.nist.gov/vuln/detail/CVE-2023-36258\nhttps://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-98.yaml\nlangchain-ai/langchain#5872\nlangchain-ai/langchain#5872 (comment)\nlangchain-ai/langchain#6003\nlangchain-ai/langchain#7870\nlangchain-ai/langchain#8425\nPublished to the GitHub Advisory Database on Jul 3\nReviewed on Jul 6\nLast updated 5 days ago\nSeverity\nCritical\n9.8\n/ 10\nCVSS base metrics\nAttack vector\nNetwork\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nHigh\nIntegrity\nHigh\nAvailability\nHigh\nCVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\nWeaknesses\nNo CWEs\nCVE ID\nCVE-2023-36258\nGHSA ID\nGHSA-2qmj-7962-cjq8\nSource code\nhwchase17/langchain\nThis advisory has been edited. See History.\nSee something to contribute? Suggest improvements for this vulnerability.\n\nRULE:\n\nr2c-internal-project-depends-on:\ndepends-on-either: - namespace: pypi\npackage: langchain\nversion: < 0.0.236\nlanguages:\n\n- python\n  severity: ERROR\n  patterns:\n- pattern-either:\n  - patterns:\n    - pattern-either:\n      - pattern-inside: |\n        $PAL = langchain.chains.PALChain.from_math_prompt(...)\n        ...\n      - pattern-inside: |\n        $PAL = langchain.chains.PALChain.from_colored_object_prompt(...)\n        ...\n    - pattern: $PAL.run(...)\n  - patterns:\n    - pattern-either:\n      - pattern: langchain.chains.PALChain.from_colored_object_prompt(...).run(...)\n      - pattern: langchain.chains.PALChain.from_math_prompt(...).run(...)\n\nISSUE:\n\nlangchain vulnerable to arbitrary code execution\nCritical severity GitHub Reviewed Published on Aug 22 to the GitHub Advisory Database • Updated 2 weeks ago\nVulnerability details\nDependabot alerts2\nPackage\nlangchain (pip)\nAffected versions\n< 0.0.312\nPatched versions\n0.0.312\nDescription\nAn issue in langchain v.0.0.171 allows a remote attacker to execute arbitrary code via the via the a json file to the load_prompt parameter.\nReferences\nhttps://nvd.nist.gov/vuln/detail/CVE-2023-36281\nlangchain-ai/langchain#4394\nhttps://aisec.today/LangChain-2e6244a313dd46139c5ef28cbcab9e55\nhttps://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-151.yaml\nlangchain-ai/langchain#10252\nlangchain-ai/langchain@22abeb9\nPublished to the GitHub Advisory Database on Aug 22\nReviewed on Aug 23\nLast updated 2 weeks ago\nSeverity\nCritical\n9.8\n/ 10\nCVSS base metrics\nAttack vector\nNetwork\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nHigh\nIntegrity\nHigh\nAvailability\nHigh\nCVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\nWeaknesses\nCWE-94\nCVE ID\nCVE-2023-36281\nGHSA ID\nGHSA-7gfq-f96f-g85j\nSource code\nlangchain-ai/langchain\nCredits\neyurtsev\n\nRULE:\n\nr2c-internal-project-depends-on:\ndepends-on-either: - namespace: pypi\npackage: langchain\nversion: < 0.0.312\nlanguages:\n\n- python\n  severity: ERROR\n  patterns:\n- metavariable-regex:\n  metavariable: $PACKAGE\n  regex: (langchain)\n- pattern-inside: |\n  import $PACKAGE\n  ...\n- pattern: langchain.prompts.load_prompt(...)\n\nEND CONTEXT\n\n# OUTPUT INSTRUCTIONS\n\n- Output a correct semgrep rule like the EXAMPLES above that will catch any generic instance of the problem, not just the specific instance in the input.\n- Do not overfit on the specific example in the input. Make it a proper Semgrep rule that will capture the general case.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT\n\nINPUT:\n\"\n                        $RESULT = call opinionated_inference($INPUT, $SYSTEM)\n                    }\n                }\"\n            ",
          "steps": [
            {
              "body": [
                {
                  "type": "composite",
                  "value": [
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$SYSTEM",
                        "value": "\n# IDENTITY and PURPOSE\n\nYou are an expert at writing Semgrep rules.\n\nTake a deep breath and think step by step about how to best accomplish this goal using the following context.\n\n# OUTPUT SECTIONS\n\n- Write a Semgrep rule that will match the input provided.\n\n# CONTEXT FOR CONSIDERATION\n\nThis context will teach you about how to write better Semgrep rules:\n\nYou are an expert Semgrep rule creator.\n\nTake a deep breath and work on this problem step-by-step.\n\nYou output only a working Semgrep rule.\n\n\\\"\\\"\\\",\n}\nuser_message = {\n\\\"role\\\": \\\"user\\\",\n\\\"content\\\": \\\"\\\"\\\"\n\nYou are an expert Semgrep rule creator.\n\nYou output working and accurate Semgrep rules.\n\nTake a deep breath and work on this problem step-by-step.\n\nSEMGREP RULE SYNTAX\n\nRule syntax\n\nTIP\nGetting started with rule writing? Try the Semgrep Tutorial 🎓\nThis document describes the YAML rule syntax of Semgrep.\n\nSchema\n\nRequired\n\nAll required fields must be present at the top-level of a rule, immediately under the rules key.\n\nField Type Description\nid string Unique, descriptive identifier, for example: no-unused-variable\nmessage string Message that includes why Semgrep matched this pattern and how to remediate it. See also Rule messages.\nseverity string One of the following values: INFO (Low severity), WARNING (Medium severity), or ERROR (High severity). The severity key specifies how critical are the issues that a rule potentially detects. Note: Semgrep Supply Chain differs, as its rules use CVE assignments for severity. For more information, see Filters section in Semgrep Supply Chain documentation.\nlanguages array See language extensions and tags\npattern* string Find code matching this expression\npatterns* array Logical AND of multiple patterns\npattern-either* array Logical OR of multiple patterns\npattern-regex* string Find code matching this PCRE-compatible pattern in multiline mode\nINFO\nOnly one of the following is required: pattern, patterns, pattern-either, pattern-regex\nLanguage extensions and languages key values\n\nThe following table includes languages supported by Semgrep, accepted file extensions for test files that accompany rules, and valid values that Semgrep rules require in the languages key.\n\nLanguage Extensions languages key values\nApex (only in Semgrep Pro Engine) .cls apex\nBash .bash, .sh bash, sh\nC .c c\nCairo .cairo cairo\nClojure .clj, .cljs, .cljc, .edn clojure\nC++ .cc, .cpp cpp, c++\nC# .cs csharp, c#\nDart .dart dart\nDockerfile .dockerfile, .Dockerfile dockerfile, docker\nElixir .ex, .exs ex, elixir\nGeneric generic\nGo .go go, golang\nHTML .htm, .html html\nJava .java java\nJavaScript .js, .jsx js, javascript\nJSON .json, .ipynb json\nJsonnet .jsonnet, .libsonnet jsonnet\nJSX .js, .jsx js, javascript\nJulia .jl julia\nKotlin .kt, .kts, .ktm kt, kotlin\nLisp .lisp, .cl, .el lisp\nLua .lua lua\nOCaml .ml, .mli ocaml\nPHP .php, .tpl php\nPython .py, .pyi python, python2, python3, py\nR .r, .R r\nRuby .rb ruby\nRust .rs rust\nScala .scala scala\nScheme .scm, .ss scheme\nSolidity .sol solidity, sol\nSwift .swift swift\nTerraform .tf, .hcl tf, hcl, terraform\nTypeScript .ts, .tsx ts, typescript\nYAML .yml, .yaml yaml\nXML .xml xml\nINFO\nTo see the maturity level of each supported language, see the following sections in Supported languages document:\n\nSemgrep OSS Engine\nSemgrep Pro Engine\nOptional\n\nField Type Description\noptions object Options object to enable/disable certain matching features\nfix object Simple search-and-replace autofix functionality\nmetadata object Arbitrary user-provided data; attach data to rules without affecting Semgrep behavior\nmin-version string Minimum Semgrep version compatible with this rule\nmax-version string Maximum Semgrep version compatible with this rule\npaths object Paths to include or exclude when running this rule\nThe below optional fields must reside underneath a patterns or pattern-either field.\n\nField Type Description\npattern-inside string Keep findings that lie inside this pattern\nThe below optional fields must reside underneath a patterns field.\n\nField Type Description\nmetavariable-regex map Search metavariables for Python re compatible expressions; regex matching is unanchored\nmetavariable-pattern map Matches metavariables with a pattern formula\nmetavariable-comparison map Compare metavariables against basic Python expressions\npattern-not string Logical NOT - remove findings matching this expression\npattern-not-inside string Keep findings that do not lie inside this pattern\npattern-not-regex string Filter results using a PCRE-compatible pattern in multiline mode\nOperators\n\npattern\n\nThe pattern operator looks for code matching its expression. This can be basic expressions like $X == $X or unwanted function calls like hashlib.md5(...).\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npatterns\n\nThe patterns operator performs a logical AND operation on one or more child patterns. This is useful for chaining multiple patterns together that all must be true.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npatterns operator evaluation strategy\n\nNote that the order in which the child patterns are declared in a patterns operator has no effect on the final result. A patterns operator is always evaluated in the same way:\n\nSemgrep evaluates all positive patterns, that is pattern-insides, patterns, pattern-regexes, and pattern-eithers. Each range matched by each one of these patterns is intersected with the ranges matched by the other operators. The result is a set of positive ranges. The positive ranges carry metavariable bindings. For example, in one range $X can be bound to the function call foo(), and in another range $X can be bound to the expression a + b.\nSemgrep evaluates all negative patterns, that is pattern-not-insides, pattern-nots, and pattern-not-regexes. This gives a set of negative ranges which are used to filter the positive ranges. This results in a strict subset of the positive ranges computed in the previous step.\nSemgrep evaluates all conditionals, that is metavariable-regexes, metavariable-patterns and metavariable-comparisons. These conditional operators can only examine the metavariables bound in the positive ranges in step 1, that passed through the filter of negative patterns in step 2. Note that metavariables bound by negative patterns are not available here.\nSemgrep applies all focus-metavariables, by computing the intersection of each positive range with the range of the metavariable on which we want to focus. Again, the only metavariables available to focus on are those bound by positive patterns.\npattern-either\n\nThe pattern-either operator performs a logical OR operation on one or more child patterns. This is useful for chaining multiple patterns together where any may be true.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nThis rule looks for usage of the Python standard library functions hashlib.md5 or hashlib.sha1. Depending on their usage, these hashing functions are considered insecure.\n\npattern-regex\n\nThe pattern-regex operator searches files for substrings matching the given PCRE pattern. This is useful for migrating existing regular expression code search functionality to Semgrep. Perl-Compatible Regular Expressions (PCRE) is a full-featured regex library that is widely compatible with Perl, but also with the respective regex libraries of Python, JavaScript, Go, Ruby, and Java. Patterns are compiled in multiline mode, for example ^ and $ matches at the beginning and end of lines respectively in addition to the beginning and end of input.\n\nCAUTION\nPCRE supports only a limited number of Unicode character properties. For example, \\p{Egyptian_Hieroglyphs} is supported but \\p{Bidi_Control} isn't.\nEXAMPLES OF THE pattern-regex OPERATOR\npattern-regex combined with other pattern operators: Semgrep Playground example\npattern-regex used as a standalone, top-level operator: Semgrep Playground example\nINFO\nSingle (') and double (\\\") quotes behave differently in YAML syntax. Single quotes are typically preferred when using backslashes (\\) with pattern-regex.\nNote that you may bind a section of a regular expression to a metavariable, by using named capturing groups. In this case, the name of the capturing group must be a valid metavariable name.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-not-regex\n\nThe pattern-not-regex operator filters results using a PCRE regular expression in multiline mode. This is most useful when combined with regular-expression only rules, providing an easy way to filter findings without having to use negative lookaheads. pattern-not-regex works with regular pattern clauses, too.\n\nThe syntax for this operator is the same as pattern-regex.\n\nThis operator filters findings that have any overlap with the supplied regular expression. For example, if you use pattern-regex to detect Foo==1.1.1 and it also detects Foo-Bar==3.0.8 and Bar-Foo==3.0.8, you can use pattern-not-regex to filter the unwanted findings.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nfocus-metavariable\n\nThe focus-metavariable operator puts the focus, or zooms in, on the code region matched by a single metavariable or a list of metavariables. For example, to find all functions arguments annotated with the type bad you may write the following pattern:\n\npattern: |\ndef $FUNC(..., $ARG : bad, ...):\n...\n\nThis works but it matches the entire function definition. Sometimes, this is not desirable. If the definition spans hundreds of lines they are all matched. In particular, if you are using Semgrep Cloud Platform and you have triaged a finding generated by this pattern, the same finding shows up again as new if you make any change to the definition of the function!\n\nTo specify that you are only interested in the code matched by a particular metavariable, in our example $ARG, use focus-metavariable.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nNote that focus-metavariable: $ARG is not the same as pattern: $ARG! Using pattern: $ARG finds all the uses of the parameter x which is not what we want! (Note that pattern: $ARG does not match the formal parameter declaration, because in this context $ARG only matches expressions.)\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nIn short, focus-metavariable: $X is not a pattern in itself, it does not perform any matching, it only focuses the matching on the code already bound to $X by other patterns. Whereas pattern: $X matches $X against your code (and in this context, $X only matches expressions)!\n\nIncluding multiple focus metavariables using set intersection semantics\n\nInclude more focus-metavariable keys with different metavariables under the pattern to match results only for the overlapping region of all the focused code:\n\n    patterns:\n      - pattern: foo($X, ..., $Y)\n      - focus-metavariable:\n        - $X\n        - $Y\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nTo make a list of multiple focus metavariables using set union semantics that matches the metavariables regardless of their position in code, see Including multiple focus metavariables using set union semantics documentation.\nmetavariable-regex\n\nThe metavariable-regex operator searches metavariables for a PCRE regular expression. This is useful for filtering results based on a metavariable’s value. It requires the metavariable and regex keys and can be combined with other pattern operators.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nRegex matching is unanchored. For anchored matching, use \\A for start-of-string anchoring and \\Z for end-of-string anchoring. The next example, using the same expression as above but anchored, finds no matches:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nInclude quotes in your regular expression when using metavariable-regex to search string literals. For more details, see include-quotes code snippet. String matching functionality can also be used to search string literals.\nmetavariable-pattern\n\nThe metavariable-pattern operator matches metavariables with a pattern formula. This is useful for filtering results based on a metavariable’s value. It requires the metavariable key, and exactly one key of pattern, patterns, pattern-either, or pattern-regex. This operator can be nested as well as combined with other operators.\n\nFor example, the metavariable-pattern can be used to filter out matches that do not match certain criteria:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nINFO\nIn this case it is possible to start a patterns AND operation with a pattern-not, because there is an implicit pattern: ... that matches the content of the metavariable.\nThe metavariable-pattern is also useful in combination with pattern-either:\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nTIP\nIt is possible to nest metavariable-pattern inside metavariable-pattern!\nINFO\nThe metavariable should be bound to an expression, a statement, or a list of statements, for this test to be meaningful. A metavariable bound to a list of function arguments, a type, or a pattern, always evaluate to false.\nmetavariable-pattern with nested language\n\nIf the metavariable's content is a string, then it is possible to use metavariable-pattern to match this string as code by specifying the target language via the language key. See the following examples of metavariable-pattern:\n\nEXAMPLES OF metavariable-pattern\nMatch JavaScript code inside HTML in the following Semgrep Playground example.\nFilter regex matches in the following Semgrep Playground example.\nmetavariable-comparison\n\nThe metavariable-comparison operator compares metavariables against a basic Python comparison expression. This is useful for filtering results based on a metavariable's numeric value.\n\nThe metavariable-comparison operator is a mapping which requires the metavariable and comparison keys. It can be combined with other pattern operators in the following Semgrep Playground example.\n\nThis matches code such as set_port(80) or set_port(443), but not set_port(8080).\n\nComparison expressions support simple arithmetic as well as composition with boolean operators to allow for more complex matching. This is particularly useful for checking that metavariables are divisible by particular values, such as enforcing that a particular value is even or odd.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nBuilding on the previous example, this still matches code such as set_port(80) but it no longer matches set_port(443) or set_port(8080).\n\nThe comparison key accepts Python expression using:\n\nBoolean, string, integer, and float literals.\nBoolean operators not, or, and and.\nArithmetic operators +, -, \\*, /, and %.\nComparison operators ==, !=, <, <=, >, and >=.\nFunction int() to convert strings into integers.\nFunction str() to convert numbers into strings.\nFunction today() that gets today's date as a float representing epoch time.\nFunction strptime() that converts strings in the format \\\"yyyy-mm-dd\\\" to a float representing the date in epoch time.\nLists, together with the in, and not in infix operators.\nStrings, together with the in and not in infix operators, for substring containment.\nFunction re.match() to match a regular expression (without the optional flags argument).\nYou can use Semgrep metavariables such as $MVAR, which Semgrep evaluates as follows:\n\nIf $MVAR binds to a literal, then that literal is the value assigned to $MVAR.\nIf $MVAR binds to a code variable that is a constant, and constant propagation is enabled (as it is by default), then that constant is the value assigned to $MVAR.\nOtherwise the code bound to the $MVAR is kept unevaluated, and its string representation can be obtained using the str() function, as in str($MVAR). For example, if $MVAR binds to the code variable x, str($MVAR) evaluates to the string literal \\\"x\\\".\nLegacy metavariable-comparison keys\n\nINFO\nYou can avoid the use of the legacy keys described below (base: int and strip: bool) by using the int() function, as in int($ARG) > 0o600 or int($ARG) > 2147483647.\nThe metavariable-comparison operator also takes optional base: int and strip: bool keys. These keys set the integer base the metavariable value should be interpreted as and remove quotes from the metavariable value, respectively.\n\nEXAMPLE OF metavariable-comparison WITH base\nTry this pattern in the Semgrep Playground.\nThis interprets metavariable values found in code as octal. As a result, Semgrep detects 0700, but it does not detect 0400.\n\nEXAMPLE OF metavariable-comparison WITH strip\nTry this pattern in the Semgrep Playground.\nThis removes quotes (', \\\", and `) from both ends of the metavariable content. As a result, Semgrep detects \\\"2147483648\\\", but it does not detect \\\"2147483646\\\". This is useful when you expect strings to contain integer or float data.\n\npattern-not\n\nThe pattern-not operator is the opposite of the pattern operator. It finds code that does not match its expression. This is useful for eliminating common false positives.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-inside\n\nThe pattern-inside operator keeps matched findings that reside within its expression. This is useful for finding code inside other pieces of code like functions or if blocks.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\npattern-not-inside\n\nThe pattern-not-inside operator keeps matched findings that do not reside within its expression. It is the opposite of pattern-inside. This is useful for finding code that’s missing a corresponding cleanup action like disconnect, close, or shutdown. It’s also useful for finding problematic code that isn't inside code that mitigates the issue.\n\nEXAMPLE\nTry this pattern in the Semgrep Playground.\nThe above rule looks for files that are opened but never closed, possibly leading to resource exhaustion. It looks for the open(...) pattern and not a following close() pattern.\n\nThe $F metavariable ensures that the same variable name is used in the open and close calls. The ellipsis operator allows for any arguments to be passed to open and any sequence of code statements in-between the open and close calls. The rule ignores how open is called or what happens up to a close call — it only needs to make sure close is called.\n\nMetavariable matching\n\nMetavariable matching operates differently for logical AND (patterns) and logical OR (pattern-either) parent operators. Behavior is consistent across all child operators: pattern, pattern-not, pattern-regex, pattern-inside, pattern-not-inside.\n\nMetavariables in logical ANDs\n\nMetavariable values must be identical across sub-patterns when performing logical AND operations with the patterns operator.\n\nExample:\n\nrules:\n\n- id: function-args-to-open\n  patterns:\n  - pattern-inside: |\n    def $F($X):\n    ...\n  - pattern: open($X)\n    message: \\\"Function argument passed to open() builtin\\\"\n    languages: [python]\n    severity: ERROR\n\nThis rule matches the following code:\n\ndef foo(path):\nopen(path)\n\nThe example rule doesn’t match this code:\n\ndef foo(path):\nopen(something_else)\n\nMetavariables in logical ORs\n\nMetavariable matching does not affect the matching of logical OR operations with the pattern-either operator.\n\nExample:\n\nrules:\n\n- id: insecure-function-call\n  pattern-either:\n  - pattern: insecure_func1($X)\n  - pattern: insecure_func2($X)\n    message: \\\"Insecure function use\\\"\n    languages: [python]\n    severity: ERROR\n\nThe above rule matches both examples below:\n\ninsecure_func1(something)\ninsecure_func2(something)\n\ninsecure_func1(something)\ninsecure_func2(something_else)\n\nMetavariables in complex logic\n\nMetavariable matching still affects subsequent logical ORs if the parent is a logical AND.\n\nExample:\n\npatterns:\n\n- pattern-inside: |\n  def $F($X):\n  ...\n- pattern-either:\n  - pattern: bar($X)\n  - pattern: baz($X)\n\nThe above rule matches both examples below:\n\ndef foo(something):\nbar(something)\n\ndef foo(something):\nbaz(something)\n\nThe example rule doesn’t match this code:\n\ndef foo(something):\nbar(something_else)\n\noptions\n\nEnable, disable, or modify the following matching features:\n\nOption Default Description\nac_matching true Matching modulo associativity and commutativity, treat Boolean AND/OR as associative, and bitwise AND/OR/XOR as both associative and commutative.\nattr_expr true Expression patterns (for example: f($X)) matches attributes (for example: @f(a)).\ncommutative_boolop false Treat Boolean AND/OR as commutative even if not semantically accurate.\nconstant_propagation true Constant propagation, including intra-procedural flow-sensitive constant propagation.\ngeneric_comment_style none In generic mode, assume that comments follow the specified syntax. They are then ignored for matching purposes. Allowed values for comment styles are:\nc for traditional C-style comments (/_ ... _/).\ncpp for modern C or C++ comments (// ... or /_ ... _/).\nshell for shell-style comments (# ...).\nBy default, the generic mode does not recognize any comments. Available since Semgrep version 0.96. For more information about generic mode, see Generic pattern matching documentation.\ngeneric_ellipsis_max_span 10 In generic mode, this is the maximum number of newlines that an ellipsis operator ... can match or equivalently, the maximum number of lines covered by the match minus one. The default value is 10 (newlines) for performance reasons. Increase it with caution. Note that the same effect as 20 can be achieved without changing this setting and by writing ... ... in the pattern instead of .... Setting it to 0 is useful with line-oriented languages (for example INI or key-value pairs in general) to force a match to not extend to the next line of code. Available since Semgrep 0.96. For more information about generic mode, see Generic pattern matching documentation.\ntaint_assume_safe_functions false Experimental option which will be subject to future changes. Used in taint analysis. Assume that function calls do not propagate taint from their arguments to their output. Otherwise, Semgrep always assumes that functions may propagate taint. Can replace not-conflicting sanitizers added in v0.69.0 in the future.\ntaint_assume_safe_indexes false Used in taint analysis. Assume that an array-access expression is safe even if the index expression is tainted. Otherwise Semgrep assumes that for example: a[i] is tainted if i is tainted, even if a is not. Enabling this option is recommended for high-signal rules, whereas disabling is preferred for audit rules. Currently, it is disabled by default to attain backwards compatibility, but this can change in the near future after some evaluation.\nvardef_assign true Assignment patterns (for example $X = $E) match variable declarations (for example var x = 1;).\nxml_attrs_implicit_ellipsis true Any XML/JSX/HTML element patterns have implicit ellipsis for attributes (for example: <div /> matches <div foo=\\\"1\\\">.\nThe full list of available options can be consulted in the Semgrep matching engine configuration module. Note that options not included in the table above are considered experimental, and they may change or be removed without notice.\n\nfix\n\nThe fix top-level key allows for simple autofixing of a pattern by suggesting an autofix for each match. Run semgrep with --autofix to apply the changes to the files.\n\nExample:\n\nrules:\n\n- id: use-dict-get\n  patterns:\n  - pattern: $DICT[$KEY]\n    fix: $DICT.get($KEY)\n    message: \\\"Use `.get()` method to avoid a KeyNotFound error\\\"\n    languages: [python]\n    severity: ERROR\n\nFor more information about fix and --autofix see Autofix documentation.\n\nmetadata\n\nProvide additional information for a rule with the metadata: key, such as a related CWE, likelihood, OWASP.\n\nExample:\n\nrules:\n\n- id: eqeq-is-bad\n  patterns:\n  - [...]\n    message: \\\"useless comparison operation `$X == $X` or `$X != $X`\\\"\n    metadata:\n    cve: CVE-2077-1234\n    discovered-by: Ikwa L'equale\n\nThe metadata are also displayed in the output of Semgrep if you’re running it with --json. Rules with category: security have additional metadata requirements. See Including fields required by security category for more information.\n\nmin-version and max-version\n\nEach rule supports optional fields min-version and max-version specifying minimum and maximum Semgrep versions. If the Semgrep version being used doesn't satisfy these constraints, the rule is skipped without causing a fatal error.\n\nExample rule:\n\nrules:\n\n- id: bad-goflags\n  # earlier semgrep versions can't parse the pattern\n  min-version: 1.31.0\n  pattern: |\n  ENV ... GOFLAGS='-tags=dynamic -buildvcs=false' ...\n  languages: [dockerfile]\n  message: \\\"We should not use these flags\\\"\n  severity: WARNING\n\nAnother use case is when a newer version of a rule works better than before but relies on a new feature. In this case, we could use min-version and max-version to ensure that either the older or the newer rule is used but not both. The rules would look like this:\n\nrules:\n\n- id: something-wrong-v1\n  max-version: 1.72.999\n  ...\n- id: something-wrong-v2\n  min-version: 1.73.0\n  # 10x faster than v1!\n  ...\n\nThe min-version/max-version feature is available since Semgrep 1.38.0. It is intended primarily for publishing rules that rely on newly-released features without causing errors in older Semgrep installations.\n\ncategory\n\nProvide a category for users of the rule. For example: best-practice, correctness, maintainability. For more information, see Semgrep registry rule requirements.\n\npaths\n\nExcluding a rule in paths\n\nTo ignore a specific rule on specific files, set the paths: key with one or more filters. Paths are relative to the root directory of the scanned project.\n\nExample:\n\nrules:\n\n- id: eqeq-is-bad\n  pattern: $X == $X\n  paths:\n  exclude: - \\\"_.jinja2\\\" - \\\"_\\_test.go\\\" - \\\"project/tests\\\" - project/static/\\*.js\n\nWhen invoked with semgrep -f rule.yaml project/, the above rule runs on files inside project/, but no results are returned for:\n\nany file with a .jinja2 file extension\nany file whose name ends in \\_test.go, such as project/backend/server_test.go\nany file inside project/tests or its subdirectories\nany file matching the project/static/\\*.js glob pattern\nNOTE\nThe glob syntax is from Python's wcmatch and is used to match against the given file and all its parent directories.\nLimiting a rule to paths\n\nConversely, to run a rule only on specific files, set a paths: key with one or more of these filters:\n\nrules:\n\n- id: eqeq-is-bad\n  pattern: $X == $X\n  paths:\n  include: - \\\"_\\_test.go\\\" - \\\"project/server\\\" - \\\"project/schemata\\\" - \\\"project/static/_.js\\\" - \\\"tests/\\*_/_.js\\\"\n\nWhen invoked with semgrep -f rule.yaml project/, this rule runs on files inside project/, but results are returned only for:\n\nfiles whose name ends in \\_test.go, such as project/backend/server_test.go\nfiles inside project/server, project/schemata, or their subdirectories\nfiles matching the project/static/\\*.js glob pattern\nall files with the .js extension, arbitrary depth inside the tests folder\nIf you are writing tests for your rules, add any test file or directory to the included paths as well.\n\nNOTE\nWhen mixing inclusion and exclusion filters, the exclusion ones take precedence.\nExample:\n\npaths:\ninclude: \\\"project/schemata\\\"\nexclude: \\\"\\*\\_internal.py\\\"\n\nThe above rule returns results from project/schemata/scan.py but not from project/schemata/scan_internal.py.\n\nOther examples\n\nThis section contains more complex rules that perform advanced code searching.\n\nComplete useless comparison\n\nrules:\n\n- id: eqeq-is-bad\n  patterns:\n  - pattern-not-inside: |\n    def **eq**(...):\n    ...\n  - pattern-not-inside: assert(...)\n  - pattern-not-inside: assertTrue(...)\n  - pattern-not-inside: assertFalse(...)\n  - pattern-either:\n    - pattern: $X == $X\n    - pattern: $X != $X\n    - patterns:\n      - pattern-inside: |\n        def **init**(...):\n        ...\n      - pattern: self.$X == self.$X\n  - pattern-not: 1 == 1\n    message: \\\"useless comparison operation `$X == $X` or `$X != $X`\\\"\n\nThe above rule makes use of many operators. It uses pattern-either, patterns, pattern, and pattern-inside to carefully consider different cases, and uses pattern-not-inside and pattern-not to whitelist certain useless comparisons.\n\nEND SEMGREP RULE SYNTAX\n\nRULE EXAMPLES\n\nISSUE:\n\nlangchain arbitrary code execution vulnerability\nCritical severity GitHub Reviewed Published on Jul 3 to the GitHub Advisory Database • Updated 5 days ago\nVulnerability details\nDependabot alerts2\nPackage\nlangchain (pip)\nAffected versions\n< 0.0.247\nPatched versions\n0.0.247\nDescription\nAn issue in langchain allows an attacker to execute arbitrary code via the PALChain in the python exec method.\nReferences\nhttps://nvd.nist.gov/vuln/detail/CVE-2023-36258\nhttps://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-98.yaml\nlangchain-ai/langchain#5872\nlangchain-ai/langchain#5872 (comment)\nlangchain-ai/langchain#6003\nlangchain-ai/langchain#7870\nlangchain-ai/langchain#8425\nPublished to the GitHub Advisory Database on Jul 3\nReviewed on Jul 6\nLast updated 5 days ago\nSeverity\nCritical\n9.8\n/ 10\nCVSS base metrics\nAttack vector\nNetwork\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nHigh\nIntegrity\nHigh\nAvailability\nHigh\nCVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\nWeaknesses\nNo CWEs\nCVE ID\nCVE-2023-36258\nGHSA ID\nGHSA-2qmj-7962-cjq8\nSource code\nhwchase17/langchain\nThis advisory has been edited. See History.\nSee something to contribute? Suggest improvements for this vulnerability.\n\nRULE:\n\nr2c-internal-project-depends-on:\ndepends-on-either: - namespace: pypi\npackage: langchain\nversion: < 0.0.236\nlanguages:\n\n- python\n  severity: ERROR\n  patterns:\n- pattern-either:\n  - patterns:\n    - pattern-either:\n      - pattern-inside: |\n        $PAL = langchain.chains.PALChain.from_math_prompt(...)\n        ...\n      - pattern-inside: |\n        $PAL = langchain.chains.PALChain.from_colored_object_prompt(...)\n        ...\n    - pattern: $PAL.run(...)\n  - patterns:\n    - pattern-either:\n      - pattern: langchain.chains.PALChain.from_colored_object_prompt(...).run(...)\n      - pattern: langchain.chains.PALChain.from_math_prompt(...).run(...)\n\nISSUE:\n\nlangchain vulnerable to arbitrary code execution\nCritical severity GitHub Reviewed Published on Aug 22 to the GitHub Advisory Database • Updated 2 weeks ago\nVulnerability details\nDependabot alerts2\nPackage\nlangchain (pip)\nAffected versions\n< 0.0.312\nPatched versions\n0.0.312\nDescription\nAn issue in langchain v.0.0.171 allows a remote attacker to execute arbitrary code via the via the a json file to the load_prompt parameter.\nReferences\nhttps://nvd.nist.gov/vuln/detail/CVE-2023-36281\nlangchain-ai/langchain#4394\nhttps://aisec.today/LangChain-2e6244a313dd46139c5ef28cbcab9e55\nhttps://github.com/pypa/advisory-database/tree/main/vulns/langchain/PYSEC-2023-151.yaml\nlangchain-ai/langchain#10252\nlangchain-ai/langchain@22abeb9\nPublished to the GitHub Advisory Database on Aug 22\nReviewed on Aug 23\nLast updated 2 weeks ago\nSeverity\nCritical\n9.8\n/ 10\nCVSS base metrics\nAttack vector\nNetwork\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nHigh\nIntegrity\nHigh\nAvailability\nHigh\nCVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\nWeaknesses\nCWE-94\nCVE ID\nCVE-2023-36281\nGHSA ID\nGHSA-7gfq-f96f-g85j\nSource code\nlangchain-ai/langchain\nCredits\neyurtsev\n\nRULE:\n\nr2c-internal-project-depends-on:\ndepends-on-either: - namespace: pypi\npackage: langchain\nversion: < 0.0.312\nlanguages:\n\n- python\n  severity: ERROR\n  patterns:\n- metavariable-regex:\n  metavariable: $PACKAGE\n  regex: (langchain)\n- pattern-inside: |\n  import $PACKAGE\n  ...\n- pattern: langchain.prompts.load_prompt(...)\n\nEND CONTEXT\n\n# OUTPUT INSTRUCTIONS\n\n- Output a correct semgrep rule like the EXAMPLES above that will catch any generic instance of the problem, not just the specific instance in the input.\n- Do not overfit on the specific example in the input. Make it a proper Semgrep rule that will capture the general case.\n- Do not output warnings or notes—just the requested sections.\n\n# INPUT\n\nINPUT:\n"
                      }
                    },
                    {
                      "type": "registeroperation",
                      "value": {
                        "register": "$RESULT",
                        "value": {
                          "args": [
                            {
                              "type": "register",
                              "value": "$INPUT"
                            },
                            {
                              "type": "register",
                              "value": "$SYSTEM"
                            }
                          ],
                          "name": "opinionated_inference"
                        }
                      }
                    }
                  ]
                }
              ],
              "name": "Main"
            }
          ],
          "version": "v0.1"
        }
      }
    }
  }
]
"#;
